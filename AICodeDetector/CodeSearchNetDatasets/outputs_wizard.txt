{"prompt": "def print_help(self, classes=False):\n        \"\"\"Print the help for each Configurable class in self.classes.\n\n        If classes=False (the default), only flags and aliases are printed.\n        \"\"\"", "output": "\n        = self.classes\n        =.values()\n        = [c for c in if not c.is_hidden]\n      .sort(key=lambda x: x.name)\n        = if classes else [c for c in if not c.is_hidden and not c.is_flag]\n      .sort(key=lambda x: x.name)\n        = [c for c in if not c.is_hidden and not c.is_alias]\n      .sort(key=lambda x: x.name)\n        = [", "solution": "\n        self.print_subcommands()\n        self.print_options()\n\n        if classes:\n            if self.classes:\n                print \"Class parameters\"\n                print \"----------------\"\n                print\n                for p in wrap_paragraphs(self.keyvalue_description):\n                    print p\n                    print\n\n            for cls in self.classes:\n                cls.class_print_help()\n                print\n        else:\n            print \"To see all available configurables, use `--help-all`\"\n            print"}
{"prompt": "def derive_fallback_fernet_keys(passwords, user_id):\n    \"\"\"\n    Derive a list of per-user Fernet keys from a list of master keys and a\n    username.\n\n    If a None is encountered in ``passwords``, it is forwarded.\n\n    Parameters\n    ----------\n    passwords : list[unicode]\n        List of ascii-encodable keys to derive.\n    user_id : unicode or None\n        ascii-encodable user_id to use as salt\n    \"\"\"", "output": "\n    if user_id is None:\n        return [None] * len(passwords)\n\n   _key =_key_from_password(passwords[0], user_id)\n   _key_bytes = bytes(base64.b64decode(str(passwords[0])), 'utf-8')\n   _key_bytes =_key.encrypt(\n       _key_bytes,\n        padding.OAEP(\n            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n            algorithm=hashes.SHA256(),\n            label=None\n       ", "solution": "\n    # Normally I wouldn't advocate for these kinds of assertions, but we really\n    # really really don't want to mess up deriving encryption keys.\n    assert isinstance(passwords, (list, tuple)), \\\n        \"Expected list or tuple of keys, got %s.\" % type(passwords)\n\n    def derive_single_allow_none(k):\n        if k is None:\n            return None\n        return derive_single_fernet_key(k, user_id).decode('ascii')\n\n    return list(map(derive_single_allow_none, passwords))"}
{"prompt": "def format_help(self, checkerref=False):\n        \"\"\"return the help string for the given message id\"\"\"", "output": "\n        if checkerref:\n            return self.help_checkerref\n        else:\n            return self.help_general\n\n    def format_usage(self, checkerref=False):\n        \"\"\"return the usage string for the given message id\"\"\"\n        if checkerref:\n            return self.usage_checkerref\n        else:\n            return self.usage_general\n\n    def format_example(self, checkerref=False):\n        \"\"\"return the example string for the given message id\"\"\"\n        if checkerref:\n            return self.example_checkerref\n        else:\n            return self.example_general\n\n    def format_description(self, checker", "solution": "\n        desc = self.descr\n        if checkerref:\n            desc += \" This message belongs to the %s checker.\" % self.checker.name\n        title = self.msg\n        if self.symbol:\n            msgid = \"%s (%s)\" % (self.symbol, self.msgid)\n        else:\n            msgid = self.msgid\n        if self.minversion or self.maxversion:\n            restr = []\n            if self.minversion:\n                restr.append(\"< %s\" % \".\".join([str(n) for n in self.minversion]))\n            if self.maxversion:\n                restr.append(\">= %s\" % \".\".join([str(n) for n in self.maxversion]))\n            restr = \" or \".join(restr)\n            if checkerref:\n                desc += \" It can't be emitted when using Python %s.\" % restr\n            else:\n                desc += \" This message can't be emitted when using Python %s.\" % restr\n        desc = normalize_text(\" \".join(desc.split()), indent=\"  \")\n        if title != \"%s\":\n            title = title.splitlines()[0]\n\n            return \":%s: *%s*\\n%s\" % (msgid, title.rstrip(\" \"), desc)\n        return \":%s:\\n%s\" % (msgid, desc)"}
{"prompt": "def f_get(self, *args):\n        \"\"\"Returns annotations\n\n        If len(args)>1, then returns a list of annotations.\n\n        `f_get(X)` with *X* integer will return the annotation with name `annotation_X`.\n\n        If the annotation contains only a single entry you can call `f_get()` without arguments.\n        If you call `f_get()` and the annotation contains more than one element a ValueError is\n        thrown.\n\n        \"\"\"", "output": "\n        if len(args) == 1:\n            if isinstance(args[0], int):\n                return self.annotations[args[0]]\n            else:\n                raise ValueError(\"f_get() expects an integer as argument\")\n        else:\n            return [self.annotations[i] for i in args]\n\n    def f_set(self, *args):\n        \"\"\"Sets annotations\n\n        If len(args)>1, then sets a list of annotations.\n\n        `f_set(X, Y)` with *X* integer and *Y* list will set the annotation with name `annotation_X`\n        to the", "solution": "\n\n        if len(args) == 0:\n            if len(self._dict) == 1:\n                return self._dict[list(self._dict.keys())[0]]\n            elif len(self._dict) > 1:\n                raise ValueError('Your annotation contains more than one entry: '\n                                 '`%s` Please use >>f_get<< with one of these.' %\n                                 (str(list(self._dict.keys()))))\n            else:\n                raise AttributeError('Your annotation is empty, cannot access data.')\n\n        result_list = []\n        for name in args:\n            name = self._translate_key(name)\n            try:\n                result_list.append(self._dict[name])\n            except KeyError:\n                raise AttributeError('Your annotation does not contain %s.' % name)\n\n        if len(args) == 1:\n            return result_list[0]\n        else:\n            return tuple(result_list)"}
{"prompt": "def size(full_path):\r\n    \"\"\"Shows file size.\r\n    :param full_path: full path to the file.\r\n    \"\"\"", "output": "\r\n    try:\r\n        size_bytes = os.path.getsize(full_path)\r\n        size_mb = size_bytes / (1024 * 1024)\r\n        print(f\"{size_mb:.2f} MB\")\r\n    except FileNotFoundError:\r\n        print(\"File not found.\")\r\n\r\n\r\ndef main():\r\n    \"\"\"Main function.\"\"\"\r\n    full_path = input(\"Enter the full path to the file: \")\r\n    size(full_path)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n", "solution": "\r\n\r\n    file_size = os.path.getsize(full_path)\r\n    str_file_size = str(file_size)\r\n    print(str_file_size, 'b')\r\n\r\n    # Show size in b, kb, mb or gb depending on the dimension\r\n    if len(str_file_size) >= 10:\r\n        print('{0:.2f}'.format(file_size / 1073741824), 'gb')\r\n    elif len(str_file_size) >= 7:\r\n        print('{0:.2f}'.format(file_size / 1048576), 'mb')\r\n    elif len(str_file_size) >= 4:\r\n        print('{0:.2f}'.format(file_size / 1024), 'kb')"}
{"prompt": "def init_cinder_consumer(self, mq):\n        \"\"\"\n        Init openstack cinder mq\n\n        1. Check if enable listening cinder notification\n        2. Create consumer\n\n        :param mq: class ternya.mq.MQ\n        \"\"\"", "output": "\n        if self.config.get('enable_cinder_notification', False):\n            self.cinder_consumer = mq.create_consumer(\n                'cinder',\n                self.config['cinder_exchange'],\n                self.config['cinder_queue'],\n                self.config['cinder_routing_key'],\n                self.config['cinder_queue_durable'],\n                self.config['cinder_queue_exclusive'],\n                self.config['cinder_queue_auto_delete'],\n                self.config['cinder_queue_arguments']\n            )\n            self.cinder_", "solution": "\n        if not self.enable_component_notification(Openstack.Cinder):\n            log.debug(\"disable listening cinder notification\")\n            return\n\n        for i in range(self.config.cinder_mq_consumer_count):\n            mq.create_consumer(self.config.cinder_mq_exchange,\n                               self.config.cinder_mq_queue,\n                               ProcessFactory.process(Openstack.Cinder))\n\n        log.debug(\"enable listening openstack cinder notification.\")"}
{"prompt": "def max_global_iteration(self):\n        \"\"\"Return global iterator with last iteration number\"\"\"", "output": "\n        return self.global_iteration\n\n    def update_global_iteration(self, global_iteration):\n        \"\"\"Update global iterator with last iteration number\"\"\"\n        self.global_iteration = global_iteration\n\n    def update___(self,__):\n        \"\"\"Update and test set\"\"\"\n        self.train_set = train_set\n        self.test_set = test_set\n\n    def update____size(self,_set_size, test_set_size):\n        \"\"\"Update size of training and testing sets\"\"\"\n        self.train_set", "solution": "\n        return self.indices_to_global_iterator({\n            symbol_pos_int(var_name): end-1 for var_name, start, end, incr in self._loop_stack\n        })"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        return self.arguments\n\n    def get_description(self):\n        \"\"\"\n        Extracts the description of this CLI\n        \"\"\"\n        return self.description\n\n    def get_usage(self):\n        \"\"\"\n        Extracts the usage of this CLI\n        \"\"\"\n        return self.usage\n\n    def get_example(self):\n        \"\"\"\n        Extracts the example of this CLI\n        \"\"\"\n        return self.example\n\n    def get_arguments_description(self):\n        \"\"\"\n        Extracts the description of the arguments of this CLI\n        \"\"\"\n        return self.arguments_description\n\n    def get_arguments_type(self):\n        \"\"\"\n        Extracts the", "solution": "\n        ApiCli.get_arguments(self)\n\n        # Get the host group name\n        if self.args.host_group_name is not None:\n            self.host_group_name = self.args.host_group_name\n\n        # Get the list of sources separated by commas\n        if self.args.sources is not None:\n            self.sources = self.args.sources\n\n        payload = {}\n        if self.host_group_name is not None:\n            payload['name'] = self.host_group_name\n\n        if self.sources is not None:\n            source_list = str.split(self.sources, ',')\n            if 'hostnames' not in payload:\n                payload['hostnames'] = []\n\n            for s in source_list:\n                payload['hostnames'].append(s)\n        self.data = json.dumps(payload, sort_keys=True)\n        self.headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}"}
{"prompt": "def next_retry_datetime(self):\n        \"\"\"\n        Get datetime of the next retry if the task instance fails. For exponential\n        backoff, retry_delay is used as base and will be converted to seconds.\n        \"\"\"", "output": "\n        retry_delay = self.retry_delay\n        if isinstance(retry_delay, timedelta):\n            retry_delay = retry_delay.total_seconds()\n        return datetime.utcnow() + timedelta(seconds=retry_delay)\n\n    def get_retry_delay(self, attempt):\n        \"\"\"\n        Get the delay for the given attempt number.\n        \"\"\"\n        if attempt == 1:\n            return self.retry_delay\n        else:\n            return self.retry_delay * (2 ** (attempt - 1))\n\n    def __repr__(self):\n        return f\"<{self.__class__.__name__} {", "solution": "\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            # deterministic per task instance\n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)\n            modded_hash = min_backoff + hash % min_backoff\n            # timedelta has a maximum representable value. The exponentiation\n            # here means this value can be exceeded after a certain number\n            # of tries (around 50 if the initial delay is 1s, even fewer if\n            # the delay is larger). Cap the value here before creating a\n            # timedelta object so the operation doesn't fail.\n            delay_backoff_in_seconds = min(\n                modded_hash,\n                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay"}
{"prompt": "def showCode(self, width=80):\n        \"\"\"Show all words of the code in a nice format.\n        \"\"\"", "output": "\n        = self.code.split()\n        = []\n        for i in range(len(self.code)):\n            if i % width == 0:\n                if i > 0:\n                    print()\n                print(' '.join(self.code[i-width:i]))\n            else:\n                print(self.code[i], end=' ')\n        print()\n        print(' '.join(self.code[-width:]))\n\n    def showCodeWithSpaces(self, width=80):\n        \"\"\"Show all words of the code in a nice format, with spaces between words.\n        \"\"\"\n        =", "solution": "\n        #make table of all symbols with binary strings\n        symbolStrings = [\n            (self.bitPattern(s.index), self.mnemonic(s.index))\n            for s in self\n            ]\n        #determine column widths the way Lisp programmers do it\n        leftColWidth, rightColWidth = map(max, map(\n            map,\n            repeat(len),\n            zip(*symbolStrings)\n            ))\n        colwidth = leftColWidth+rightColWidth\n        columns = 81//(colwidth+2)\n        rows = -(-len(symbolStrings)//columns)\n        def justify(bs):\n            b,s = bs\n            return b.rjust(leftColWidth)+':'+s.ljust(rightColWidth)\n        for i in range(rows):\n            print(' '.join(map(justify, symbolStrings[i::rows])).rstrip())"}
{"prompt": "def normalize(ast: Node) -> Node:\n    \"\"\"\n    Normalize an AST nodes.\n\n    all builtins containers are replace by referencable subclasses\n    \"\"\"", "output": "\n    if isinstance(ast, list):\n        return [normalize(node) for node in ast]\n    elif isinstance(ast, dict):\n        return {key: normalize(value) for key, value in ast.items()}\n    elif isinstance(ast, Node):\n        if ast.type == \"builtin_container\":\n            return ast.type_class(ast.value)\n        else:\n            return ast\n    else:\n        return ast\n\n\ndef parse_ast(code: str) -> Node:\n    \"\"\"\n    Parse a string of Python code into an AST.\n    \"\"\"\n    return normalize(ast.parse(code,", "solution": "\n    res = ast\n    typemap = {DictNode, ListNode, TupleNode}\n    if type(ast) is dict:\n        res = DictNode(ast)\n    elif type(ast) is list:\n        res = ListNode(ast)\n    elif type(ast) is tuple:\n        res = TupleNode(ast)\n    # in-depth change\n    if hasattr(res, 'items'):\n        for k, v in res.items():\n            res[k] = normalize(v)\n    elif hasattr(res, '__getitem__'):\n        for idx, v in zip(range(len(res)), res):\n            res[idx] = normalize(v)\n    if type(res) not in typemap and hasattr(res, '__dict__'):\n        subattr = vars(res)\n        for k, v in subattr.items():\n            setattr(res, k, normalize(v))\n    return res"}
{"prompt": "def get_memory_maps(self):\n        \"\"\"Return process's mapped memory regions as a list of nameduples.\n        Fields are explained in 'man proc'; here is an updated (Apr 2012)\n        version: http://goo.gl/fmebo\n        \"\"\"", "output": "\n        = self._get_memory_maps_info()\n        return [MemoryMap(start=start, end=end, perms=perms, path=path)\n                for start, end, perms, path in zip(*[iter(info)] * 4)]\n\n    def _get_memory_maps_info(self):\n        \"\"\"Return process's mapped memory regions as a list of tuples.\n        Fields are explained in'man proc'; here is an updated (Apr 2012)\n        version: http://goo.gl/fmebo\n        \"\"\"\n        # Get information\n        with open('/proc", "solution": "\n        f = None\n        try:\n            f = open(\"/proc/%s/smaps\" % self.pid)\n            first_line = f.readline()\n            current_block = [first_line]\n\n            def get_blocks():\n                data = {}\n                for line in f:\n                    fields = line.split(None, 5)\n                    if len(fields) >= 5:\n                        yield (current_block.pop(), data)\n                        current_block.append(line)\n                    else:\n                        data[fields[0]] = int(fields[1]) * 1024\n                yield (current_block.pop(), data)\n\n            if first_line:  # smaps file can be empty\n                for header, data in get_blocks():\n                    hfields = header.split(None, 5)\n                    try:\n                        addr, perms, offset, dev, inode, path = hfields\n                    except ValueError:\n                        addr, perms, offset, dev, inode, path = hfields + ['']\n                    if not path:\n                        path = '[anon]'\n                    else:\n                        path = path.strip()\n                    yield (addr, perms, path,\n                           data['Rss:'],\n                           data['Size:'],\n                           data.get('Pss:', 0),\n                           data['Shared_Clean:'], data['Shared_Clean:'],\n                           data['Private_Clean:'], data['Private_Dirty:'],\n                           data['Referenced:'],\n                           data['Anonymous:'],\n                           data['Swap:'])\n            f.close()\n        except EnvironmentError:\n            # XXX - Can't use wrap_exceptions decorator as we're\n            # returning a generator;  this probably needs some\n            # refactoring in order to avoid this code duplication.\n            if f is not None:\n                f.close()\n            err = sys.exc_info()[1]\n            if err.errno in (errno.ENOENT, errno.ESRCH):\n                raise NoSuchProcess(self.pid, self._process_name)\n            if err.errno in (errno.EPERM, errno.EACCES):\n                raise AccessDenied(self.pid, self._process_name)\n            raise\n        except:\n            if f is not None:\n                f.close()\n            raise"}
{"prompt": "def add_path(path, config=None):\n    \"\"\"Ensure that the path, or the root of the current package (if\n    path is in a package), is in sys.path.\n    \"\"\"", "output": "\n    if path in sys.path:\n        return\n    if.is_package(path):\n        path =.get_package_root(path)\n    sys.path.insert(0, path)\n    if config is not None:\n        config.add_path(path)\n\n\ndef remove_path(path, config=None):\n    \"\"\"Remove the path, or the root of the current package (if\n    path is in a package), from sys.path.\n    \"\"\"\n    if path not in sys.path:\n        return\n    if.is_package(path):\n        path =.", "solution": "\n\n    # FIXME add any src-looking dirs seen too... need to get config for that\n    \n    log.debug('Add path %s' % path)    \n    if not path:\n        return []\n    added = []\n    parent = os.path.dirname(path)\n    if (parent\n        and os.path.exists(os.path.join(path, '__init__.py'))):\n        added.extend(add_path(parent, config))\n    elif not path in sys.path:\n        log.debug(\"insert %s into sys.path\", path)\n        sys.path.insert(0, path)\n        added.append(path)\n    if config and config.srcDirs:\n        for dirname in config.srcDirs:\n            dirpath = os.path.join(path, dirname)\n            if os.path.isdir(dirpath):\n                sys.path.insert(0, dirpath)\n                added.append(dirpath)\n    return added"}
{"prompt": "def _kl_pareto_pareto(a, b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(a || b) with a and b Pareto.\n\n  Args:\n    a: instance of a Pareto distribution object.\n    b: instance of a Pareto distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_pareto_pareto\".\n\n  Returns:\n    Batchwise KL(a || b)\n  \"\"\"", "output": "\n  with tf.name_scope(name, \"kl_pareto_pareto\", [a, b]):\n   _a = a.mean\n   _b = b.mean\n   _a_var = a.variance\n   _b_var = b.variance\n   _a_shape = a.shape\n   _b_shape = b.shape\n\n   _a_flat = tf.reshape(tf.transpose(tf.reshape(tf.transpose(tf.reshape(tf.transpose(tf.reshape(tf.transpose(tf.reshape(tf.transpose(tf", "solution": "\n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 55\n    # Terminology is different from source to source for Pareto distributions.\n    # The 'concentration' parameter corresponds to 'a' in that source, and the\n    # 'scale' parameter corresponds to 'm'.\n    final_batch_shape = distribution_util.get_broadcast_shape(\n        a.concentration, b.concentration, a.scale, b.scale)\n    common_type = dtype_util.common_dtype(\n        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)\n    return tf.where(\n        a.scale >= b.scale,\n        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +\n        tf.math.log(a.concentration) - tf.math.log(b.concentration) +\n        b.concentration / a.concentration - 1.0,\n        tf.broadcast_to(tf.cast(np.inf, common_type), final_batch_shape))"}
{"prompt": "def flush(self):\n        \"\"\"Flush incomming socket messages.\"\"\"", "output": "\n        while self.in_queue.qsize() > 0:\n            try:\n                msg = self.in_queue.get(timeout=0.1)\n                self.out_queue.put(msg)\n            except queue.Empty:\n                pass\n\n    def run(self):\n        \"\"\"Run the_server.\"\"\"\n        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.server_socket.bind((self", "solution": "\n        debug('flushing incomming socket messages')\n        try:\n            while True:\n                msg = self.socket.recv(self.buffer_size)\n                debug(b'< ' + msg)\n        except socket.error:\n            pass"}
{"prompt": "def vcf_records(self, format_tags=None, qualified=False):\n        \"\"\"Generates parsed VcfRecord objects.\n\n        Typically called in a for loop to process each vcf record in a\n        VcfReader. VcfReader must be opened in advanced and closed when\n        complete. Skips all headers.\n\n        Args:\n            qualified: When True, sample names are prefixed with file name\n\n        Returns:\n            Parsed VcfRecord\n\n        Raises:\n            StopIteration: when reader is exhausted.\n            TypeError: if reader is closed.\n        \"\"\"", "output": "\n        if self.closed:\n            raise TypeError(\"VcfReader is closed\")\n\n        while True:\n            record = self.reader.next()\n            if record.is_header:\n                continue\n            if format_tags:\n                for tag in format_tags:\n                    if tag not in record.FORMAT:\n                        raise ValueError(f\"Format tag {tag} not found in record\")\n            if qualified:\n                record.samples = [f\"{self.file_name}:{sample}\" for sample in record.samples]\n            yield record\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next", "solution": "\n        if qualified:\n            sample_names = self.qualified_sample_names\n        else:\n            sample_names = self.sample_names\n\n        for line in self._file_reader.read_lines():\n            if line.startswith(\"#\"):\n                continue\n            vcf_record = vcf.VcfRecord.parse_record(line, sample_names)\n            if format_tags:\n                vcf_record = self.modify_format_tag(vcf_record, format_tags)\n            yield vcf_record"}
{"prompt": "def main(args,parser,subparser):\n    \"\"\"the list command corresponds with listing images for an external\n       resource. This is different from listing images that are local to the\n       database, which should be done with \"images\"\n    \"\"\"", "output": "\n    parser.add_argument(\"resource_id\",type=int,help=\"the ID of the external resource\")\n    parser.add_argument(\"-l\",\"--limit\",type=int,default=10,help=\"the maximum number of images to return (default: 10)\")\n    parser.add_argument(\"-s\",\"--sort\",type=str,default=\"created_at\",help=\"the field to sort by (default: created_at)\")\n    parser.add_argument(\"-d\",\"--direction\",type=str,default=\"desc\",help=\"the direction to sort (asc or desc)", "solution": "\n    from sregistry.main import get_client\n    cli = get_client(quiet=args.quiet)\n    \n    for query in args.query:\n        if query in ['','*']:\n            query = None\n\n        cli.ls(query=query)"}
{"prompt": "def parse_hpo_genes(hpo_lines):\n    \"\"\"Parse HPO gene information\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with hgnc symbols as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        if line.startswith('HPO:'):\n            hpo_id = line.split()[1]\n            hgnc_symbol = line.split()[2].split(' ')[0]\n            diseases[hgnc_symbol] = hpo_id\n    return diseases\n\n\ndef parse_hpo_diseases(hpo_lines):\n    \"\"\"Parse HPO disease information\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with hpo ids as keys", "solution": "\n    LOG.info(\"Parsing HPO genes ...\")\n    genes = {}\n    for index, line in enumerate(hpo_lines):\n        # First line is header\n        if index == 0:\n            continue\n        if len(line) < 5:\n            continue\n        gene_info = parse_hpo_gene(line)\n        hgnc_symbol = gene_info['hgnc_symbol']\n        description = gene_info['description']\n        \n        if hgnc_symbol not in genes:\n            genes[hgnc_symbol] = {\n                'hgnc_symbol': hgnc_symbol\n            }\n        \n        gene = genes[hgnc_symbol]\n        if description == 'Incomplete penetrance':\n            gene['incomplete_penetrance'] = True\n        if description == 'Autosomal dominant inheritance':\n            gene['ad'] = True\n        if description == 'Autosomal recessive inheritance':\n            gene['ar'] = True\n        if description == 'Mithochondrial inheritance':\n            gene['mt'] = True\n        if description == 'X-linked dominant inheritance':\n            gene['xd'] = True\n        if description == 'X-linked recessive inheritance':\n            gene['xr'] = True\n        if description == 'Y-linked inheritance':\n            gene['x'] = True\n        if description == 'X-linked inheritance':\n            gene['y'] = True\n    LOG.info(\"Parsing done.\")\n    return genes"}
{"prompt": "def start_tag(el):\n    \"\"\"\n    The text representation of the start tag for a tag.\n    \"\"\"", "output": "\n    return '<' + el.tag + '>'\n\ndef end_tag(el):\n    \"\"\"\n    The text representation of the end tag for a tag.\n    \"\"\"\n    return '</' + el.tag + '>'\n\ndef text(el):\n    \"\"\"\n    The text representation of a text node.\n    \"\"\"\n    return el.text\n\ndef comment(el):\n    \"\"\"\n    The text representation of a comment node.\n    \"\"\"\n    return '<!--' + el.text + '-->'\n\ndef processing_instruction(el):\n    \"\"\"\n    The text representation of a processing instruction node.\n    \"\"\"", "solution": "\n    return '<%s%s>' % (\n        el.tag, ''.join([' %s=\"%s\"' % (name, html_escape(value, True))\n                         for name, value in el.attrib.items()]))"}
{"prompt": "def get_file_content(url, comes_from=None, session=None):\n    \"\"\"Gets the content of a file; it may be a filename, file: URL, or\n    http: URL.  Returns (location, content).  Content is unicode.\"\"\"", "output": "\n    if session is None:\n        session = requests.Session()\n\n    if comes_from is None:\n        comes_from = url\n\n    # Check if the URL is a filename\n    if os.path.exists(url):\n        with open(url, 'r') as f:\n            content = f.read()\n    # Check if the URL is a file: URL\n    elif url.startswith('file:'):\n        with open(url[5:], 'r') as f:\n            content = f.read()\n    # Check if the URL is an http: URL\n    elif url.startswith('http'):\n        response =", "solution": "\n    if session is None:\n        raise TypeError(\n            \"get_file_content() missing 1 required keyword argument: 'session'\"\n        )\n\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if (scheme == 'file' and comes_from and\n                comes_from.startswith('http')):\n            raise InstallationError(\n                'Requirements file %s references URL %s, which is local'\n                % (comes_from, url))\n        if scheme == 'file':\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = match.group(1) + ':' + path.split('|', 1)[1]\n            path = urllib_parse.unquote(path)\n            if path.startswith('/'):\n                path = '/' + path.lstrip('/')\n            url = path\n        else:\n            # FIXME: catch some errors\n            resp = session.get(url)\n            resp.raise_for_status()\n\n            if six.PY3:\n                return resp.url, resp.text\n            else:\n                return resp.url, resp.content\n    try:\n        with open(url) as f:\n            content = f.read()\n    except IOError as exc:\n        raise InstallationError(\n            'Could not open requirements file: %s' % str(exc)\n        )\n    return url, content"}
{"prompt": "def _get_repetition(extractor, text, *, bounds=(0, None), ignore_whitespace=False):\n  \"\"\"Tries to pull text with extractor repeatedly.\n\n  Bounds is a 2-tuple of (lbound, ubound) where lbound is a number and ubound is a number or None.\n  If the ubound is None, this method will execute extractor on text until extrator raises DeadEnd.\n  Otherwise, extractor will be called until it raises DeadEnd, or it has extracted ubound times.\n\n  If the number of children extracted is >= lbound, then a ParseNode with type repetition is\n  returned. Otherwise, DeadEnd is raised.\n\n  Bounds are interpreted as (lbound, ubound]\n\n  This method is used to implement:\n  - option (0, 1)\n  - zero_or_more (0, None)\n  - one_or_more (1, None)\n  - exact_repeat (n, n)\n  \"\"\"", "output": "ition is returned.\n  Otherwise, None is returned.\n\n  Args:\n    extractor: A function that takes a string and returns a list of ParseNodes.\n    text: The text to extract from.\n    bounds: A 2-tuple of (lbound, ubound) where lbound is a number and ubound is a number or None.\n    ignore_whitespace: If True, whitespace is ignored when extracting.\n\n  Returns:\n    A ParseNode with type repetition if the number of children extracted is >= lbound, otherwise None.\n  \"\"\"\n  children = []\n  lbound, ubound = bounds\n  while True:", "solution": "\n  minr, maxr = bounds\n  children = []\n\n  while maxr is None or len(children) <= maxr:\n    ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n    try:\n      child = _call_extractor(extractor, use_text)\n      child.add_ignored(ignored_ws)\n    except DeadEnd:\n      break\n\n    if child.is_empty:\n      break\n\n    children.append(child)\n    text = text[child.consumed:]\n\n\n  if len(children) >= minr:\n    return ParseNode(ParseNodeType.repetition,\n                      children=children)\n  else:\n    raise DeadEnd()"}
{"prompt": "def get_query(self, query):\n        \"\"\" Grab all issues matching a github query \"\"\"", "output": "\n        issues = []\n        for page in range(1, 10000):\n            url = f\"https://api.github.com/search/issues?q={query}&page={page}\"\n            response = requests.get(url)\n            response.raise_for_status()\n            data = response.json()\n            issues.extend(data['items'])\n            if len(data['items']) < 100:\n                break\n        return issues\n\n    def get_issue_data(self, issue):\n        \"\"\" Get the data for a single issue \"\"\"\n        url = f\"https://api.", "solution": "\n        issues = {}\n        for issue in self.client.get_query(query):\n            url = issue['html_url']\n            try:\n                repo = self.get_repository_from_issue(issue)\n            except ValueError as e:\n                log.critical(e)\n            else:\n                issues[url] = (repo, issue)\n        return issues"}
{"prompt": "def build(self, secret_key):\n        \"\"\"Builds a final copy of the token using the given secret key.\n\n        :param secret_key(string): The secret key that corresponds to this builder's access key.\n        \"\"\"", "output": "\n        self.token = self.build_token(secret_key)\n\n    def build_token(self, secret_key):\n        \"\"\"Builds the final token using the given secret key.\n\n        :param secret_key(string): The secret key that corresponds to this builder's access key.\n        \"\"\"\n        # Build the header\n        header = {\n            'alg': 'HS256',\n            'typ': 'JWT'\n        }\n\n        # Build the payload\n        payload = {\n            'iss': self.issuer,\n           'sub': self.subject,\n            'aud': self.audience,\n            'exp':", "solution": "\n        key = jwk.JWK(\n            kty='oct',\n            k=base64url_encode(uuid.UUID(secret_key).bytes),\n        )\n\n        header = {\n            'alg': 'dir',\n            'enc': 'A128GCM',\n            'zip': 'DEF',\n            'cty': 'JWT',\n            'kid': self._access_key,\n        }\n\n        now = int(time.time())\n\n        payload = {\n            'iat': now,\n            'nbf': now,\n        }\n\n        if self._expiration is not None:\n            payload['exp'] = int(calendar.timegm(self._expiration.utctimetuple()))\n\n        if len(self._view_identifiers) > 0:\n            payload[VIEW_IDENTIFIERS_CLAIM_NAME] = self._view_identifiers\n\n        if len(self._parameters) > 0:\n            parameters = []\n            for parameter in self._parameters:\n                serialized = {\n                    'field': parameter.field,\n                    'op': parameter.op,\n                }\n\n                if hasattr(parameter, '__iter__'):\n                    serialized['any'] = list(parameter.value)\n                else:\n                    serialized['value'] = parameter.value\n\n                parameters.append(serialized)\n\n            payload[PARAMETERS_CLAIM_NAME] = parameters\n\n        if len(self._attributes) > 0:\n            payload[ATTRIBUTES_CLAIM_NAME] = self._attributes\n\n        tok = jwe.JWE(json_encode(payload), protected=header)\n        tok.add_recipient(key)\n\n        return tok.serialize(compact=True)"}
{"prompt": "def cpu_percent(interval=0.1, percpu=False):\n    \"\"\"Return a float representing the current system-wide CPU\n    utilization as a percentage.\n\n    When interval is > 0.0 compares system CPU times elapsed before\n    and after the interval (blocking).\n\n    When interval is 0.0 or None compares system CPU times elapsed\n    since last call or module import, returning immediately.\n    In this case is recommended for accuracy that this function be\n    called with at least 0.1 seconds between calls.\n\n    When percpu is True returns a list of floats representing the\n    utilization as a percentage for each CPU.\n    First element of the list refers to first CPU, second element\n    to second CPU and so on.\n    The order of the list is consistent across calls.\n    \"\"\"", "output": " a percentage for each CPU.\n\n    \"\"\"\n    if interval is None:\n        interval = 0.0\n    if interval <= 0.0:\n        interval = 0.1\n\n    if_cpu_times is None:\n        import resource\n        import os\n        import time\n\n        def query_cpu_times():\n            cpu_times = []\n            for i in range(os.sysconf(\"SC_NPROCESSORS_ONLN\")):\n                cpu_times.append(resource.getrusage(resource.RUSAGE_SELF).ru_utime)\n            return cpu_times\n\n        query_cpu_times_", "solution": "\n    global _last_cpu_times\n    global _last_per_cpu_times\n    blocking = interval is not None and interval > 0.0\n\n    def calculate(t1, t2):\n        t1_all = sum(t1)\n        t1_busy = t1_all - t1.idle\n\n        t2_all = sum(t2)\n        t2_busy = t2_all - t2.idle\n\n        # this usually indicates a float precision issue\n        if t2_busy <= t1_busy:\n            return 0.0\n\n        busy_delta = t2_busy - t1_busy\n        all_delta = t2_all - t1_all\n        busy_perc = (busy_delta / all_delta) * 100\n        return round(busy_perc, 1)\n\n    # system-wide usage\n    if not percpu:\n        if blocking:\n            t1 = cpu_times()\n            time.sleep(interval)\n        else:\n            t1 = _last_cpu_times\n        _last_cpu_times = cpu_times()\n        return calculate(t1, _last_cpu_times)\n    # per-cpu usage\n    else:\n        ret = []\n        if blocking:\n            tot1 = cpu_times(percpu=True)\n            time.sleep(interval)\n        else:\n            tot1 = _last_per_cpu_times\n        _last_per_cpu_times = cpu_times(percpu=True)\n        for t1, t2 in zip(tot1, _last_per_cpu_times):\n            ret.append(calculate(t1, t2))\n        return ret"}
{"prompt": "def load_filters():\n    \"\"\"\n    Loads and returns all filters.\n    \"\"\"", "output": "\n    filters = []\n    for filename in os.listdir(FILTERS_DIR):\n        if filename.endswith('.py') and filename!= '__init__.py':\n            module_name = '_filters.filters.' + filename[:-3]\n            filters.append(import_module(module_name))\n    return filters\n\n\ndef get_filter_by_name(name):\n    \"\"\"\n    Returns the filter with the given name.\n    \"\"\"\n    for filter_module in load_filters():\n        for filter_class in filter_module.__dict__.values():\n            if issubclass(filter_class, Filter) and", "solution": "\n    all_filters = {}\n    for m in JINJA_FILTERS:\n        if hasattr(m, \"filters\"):\n            all_filters.update(m.filters())\n    return all_filters"}
{"prompt": "def weighted(loads):\n    \"\"\"Pick two at random using inverse load as weight.\n\n    Return the less loaded of the two.\n    \"\"\"", "output": "\n    total_loads = sum(loads)\n    if total_loads == 0:\n        return None\n    else:\n        load1 = loads[0] / total_loads\n        load2 = loads[1] / total_loads\n        if load1 > load2:\n            return 0\n        else:\n            return 1\n\n\ndef_choice(choices):\n    \"\"\"Choose one at random from a list of choices.\n\n    Return the chosen choice.\n    \"\"\"\n    total_choices = len(choices)\n    if total_choices == 0:\n        return None\n    else:\n        choice1 = choices[0]", "solution": "\n    # weight 0 a million times more than 1:\n    weights = 1./(1e-6+numpy.array(loads))\n    sums = weights.cumsum()\n    t = sums[-1]\n    x = random()*t\n    y = random()*t\n    idx = 0\n    idy = 0\n    while sums[idx] < x:\n        idx += 1\n    while sums[idy] < y:\n        idy += 1\n    if weights[idy] > weights[idx]:\n        return idy\n    else:\n        return idx"}
{"prompt": "def print_figure(fig, fmt='png'):\n    \"\"\"Convert a figure to svg or png for inline display.\"\"\"", "output": "\n    if fmt =='svg':\n        fig.savefig(BytesIO(), format='svg')\n        return fig.canvas.tostring_rgb()\n    elif fmt == 'png':\n        fig.savefig(BytesIO(), format='png')\n        return fig.canvas.tostring_rgb()\n    else:\n        raise ValueError(\"Unsupported format: {}\".format(fmt))\n\n\ndef plot___(x,,,_,_est,_,_,_est_est,_est_est_est,_est_est_est_", "solution": "\n    # When there's an empty figure, we shouldn't return anything, otherwise we\n    # get big blank areas in the qt console.\n    if not fig.axes and not fig.lines:\n        return\n\n    fc = fig.get_facecolor()\n    ec = fig.get_edgecolor()\n    fig.set_facecolor('white')\n    fig.set_edgecolor('white')\n    try:\n        bytes_io = BytesIO()\n        fig.canvas.print_figure(bytes_io, format=fmt, bbox_inches='tight')\n        data = bytes_io.getvalue()\n    finally:\n        fig.set_facecolor(fc)\n        fig.set_edgecolor(ec)\n    return data"}
{"prompt": "def json_to_bounds(segments_json):\n    \"\"\"Extracts the boundaries from a json file and puts them into\n        an np array.\"\"\"", "output": "\n    with open(segments_json, 'r') as f:\n        segments = json.load(f)\n\n   _bounds = np.zeros((len(segments), 4), dtype=np.float32)\n   _bounds[:, 0] = segments[:, 'x1']\n    me_bounds[:, 1] =_bounds[:, 0] + segments[:, 'width']\n   _bounds[:, 2] = me_bounds[:, 1] +_bounds[:, 0]\n    me_bounds[:, 3] =_bounds[:, 2] +_bounds", "solution": "\n    f = open(segments_json)\n    segments = json.load(f)[\"segments\"]\n    bounds = []\n    for segment in segments:\n        bounds.append(segment[\"start\"])\n    bounds.append(bounds[-1] + segments[-1][\"duration\"]) # Add last boundary\n    f.close()\n    return np.asarray(bounds)"}
{"prompt": "def _bio_to_string(bio):\n    \"\"\"\n    Copy the contents of an OpenSSL BIO object into a Python byte string.\n    \"\"\"", "output": "\n    return bio.read()\n\ndef _string_to_bio(string):\n    \"\"\"\n    Create an OpenSSL BIO object from a Python byte string.\n    \"\"\"\n    bio = BIO.MemoryBuffer(string)\n    bio.read(len(string))\n    return bio\n\ndef _load_certificate(filename):\n    \"\"\"\n    Load an X.509 certificate from a file.\n    \"\"\"\n    with open(filename, 'rb') as f:\n        cert_data = f.read()\n    bio = _string_to_bio(cert_data)\n    cert = X509.load_", "solution": "\n    result_buffer = _ffi.new('char**')\n    buffer_length = _lib.BIO_get_mem_data(bio, result_buffer)\n    return _ffi.buffer(result_buffer[0], buffer_length)[:]"}
{"prompt": "def create_from(cls, backend):\n        \"\"\"\n        Create device specification with values in backend configuration.\n        Args:\n            backend(Backend): backend configuration\n        Returns:\n            DeviceSpecification: created device specification\n        Raises:\n            PulseError: when an invalid backend is specified\n        \"\"\"", "output": "\n        if backend.name == \"qiskit\":\n            return QiskitDeviceSpecification(backend)\n        elif backend.name == \"pyquil\":\n            return PyquilDeviceSpecification(backend)\n        else:\n            raise PulseError(f\"Invalid backend: {backend.name}\")\n\n\nclass QiskitDeviceSpecification(DeviceSpecification):\n    \"\"\"\n    Qiskit device specification.\n    \"\"\"\n\n    def __init__(self, backend):\n        super().__init__(backend)\n        self.backend = backend\n\n    def get_backend(self):\n        return self.backend\n\n    def get_backend_name(self):", "solution": "\n        backend_config = backend.configuration()\n\n        # TODO : Remove usage of config.defaults when backend.defaults() is updated.\n        try:\n            backend_default = backend.defaults()\n        except ModelValidationError:\n            from collections import namedtuple\n            BackendDefault = namedtuple('BackendDefault', ('qubit_freq_est', 'meas_freq_est'))\n\n            backend_default = BackendDefault(\n                qubit_freq_est=backend_config.defaults['qubit_freq_est'],\n                meas_freq_est=backend_config.defaults['meas_freq_est']\n            )\n\n        # system size\n        n_qubits = backend_config.n_qubits\n        n_registers = backend_config.n_registers\n        n_uchannels = backend_config.n_uchannels\n\n        if n_uchannels > 0 and n_uchannels != n_qubits:\n            raise PulseError(\"This version assumes no U-channels or #U-cannels==#qubits.\")\n\n        # frequency information\n        qubit_lo_freqs = backend_default.qubit_freq_est\n        qubit_lo_ranges = backend_config.qubit_lo_range\n        meas_lo_freqs = backend_default.meas_freq_est\n        meas_lo_ranges = backend_config.meas_lo_range\n\n        # generate channels with assuming their numberings are aligned with qubits\n        drives = [\n            DriveChannel(i, qubit_lo_freqs[i], tuple(qubit_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        measures = [\n            MeasureChannel(i, meas_lo_freqs[i], tuple(meas_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        acquires = [AcquireChannel(i) for i in range(n_qubits)]\n        controls = [ControlChannel(i) for i in range(n_uchannels)]\n\n        qubits = []\n        for i in range(n_qubits):\n            # TODO: get qubits <-> channels relationship from backend\n            qubit = Qubit(i,\n                          drive_channels=[drives[i]],\n                          control_channels=None if n_uchannels == 0 else controls[i],\n                          measure_channels=[measures[i]],\n                          acquire_channels=[acquires[i]])\n            qubits.append(qubit)\n\n        registers = [RegisterSlot(i) for i in range(n_registers)]\n        # TODO: get #mem_slots from backend\n        mem_slots = [MemorySlot(i) for i in range(len(qubits))]\n\n        return DeviceSpecification(qubits, registers, mem_slots)"}
{"prompt": "def remove_memberships(self, team, users):\n        \"\"\"\n        **Description**\n            Remove user memberships from specified team.\n\n        **Arguments**\n            - **team**: the name of the team from which user memberships are removed\n            - **users**: list of usernames which should be removed from team\n\n        **Example**\n            `examples/user_team_mgmt_extended.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt_extended.py>`_\n        \"\"\"", "output": "\n        self.sdc_client.remove_team_memberships(team, users)\n\n    def add_users_to_team(self, team, users):\n        \"\"\"\n        **Description**\n            Add users to specified team.\n\n        **Arguments**\n            - **team**: the name of the team to which users are added\n            - **users**: list of usernames which should be added to team\n\n        **Example**\n            `examples/user_team_mgmt_extended.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt", "solution": "\n\n        res = self.list_memberships(team)\n\n        if res[0] is False:\n            return res\n\n        old_memberships = res[1]\n        new_memberships = {k: v for k, v in old_memberships.items() if k not in users}\n\n        res = self.edit_team(team, new_memberships)\n\n        if res[0] is False:\n            return res\n        else:\n            return [True, None]"}
{"prompt": "def price_converter(obj):\n    \"\"\"Ensures that string prices are converted into Price objects.\"\"\"", "output": "\n    if isinstance(obj, str):\n        return Price(obj)\n    return obj\n\n\nclass Product(models.Model):\n    \"\"\"A product that can be sold.\"\"\"\n    name = models.CharField(max_length=255)\n    description = models.TextField(blank=True)\n    price = models.DecimalField(max_digits=10, decimal_places=2, validators=[price_converter])\n    stock = models.PositiveIntegerField(default=0)\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_", "solution": "\n    if isinstance(obj, str):\n        obj = PriceClass.parse(obj)\n    return obj"}
{"prompt": "def finalize(self, remove_all_handlers=True):\n        \"\"\"Finalizes the manager, closes and removes all handlers if desired.\"\"\"", "output": "\n        if remove_all_handlers:\n            self.remove_all_handlers()\n        self.close()\n\n    def remove_all_handlers(self):\n        \"\"\"Removes all handlers from the manager.\"\"\"\n        for handler in self.handlers:\n            self.remove_handler(handler)\n\n    def remove_handler(self, handler):\n        \"\"\"Removes a specific handler from the manager.\"\"\"\n        self.handlers.remove(handler)\n        handler.close()\n\n    def add_handler(self, handler):\n        \"\"\"Adds a new handler to the manager.\"\"\"\n        self.handlers.append(handler)\n        handler.start()", "solution": "\n        for tool in self._tools:\n            tool.finalize()\n        self._tools = []\n        self._stdout_to_logger = None\n        for config in (self._sp_config, self._mp_config):\n            if hasattr(config, 'close'):\n                config.close()\n        self._sp_config = None\n        self._mp_config = None\n        if remove_all_handlers:\n            self.tabula_rasa()"}
{"prompt": "def _experiments_to_circuits(qobj):\n    \"\"\"Return a list of QuantumCircuit object(s) from a qobj\n\n    Args:\n        qobj (Qobj): The Qobj object to convert to QuantumCircuits\n    Returns:\n        list: A list of QuantumCircuit objects from the qobj\n\n    \"\"\"", "output": "\n    circuits = []\n    for exp in qobj.experiments:\n        circuit = QuantumCircuit.from_qasm_str(exp.header.compiled_circuit)\n        for param in exp.parameters:\n            circuit.parameters.append(param)\n        for sample in exp.samples:\n            circuit.append(sample.circuit, sample.params)\n        circuits.append(circuit)\n    return circuits\n\n\ndef _get_qobj_from_file(file_path):\n    \"\"\"Return a Qobj object from a file path\n\n    Args:\n        file_path (str): The path to the", "solution": "\n    if qobj.experiments:\n        circuits = []\n        for x in qobj.experiments:\n            quantum_registers = [QuantumRegister(i[1], name=i[0])\n                                 for i in x.header.qreg_sizes]\n            classical_registers = [ClassicalRegister(i[1], name=i[0])\n                                   for i in x.header.creg_sizes]\n            circuit = QuantumCircuit(*quantum_registers,\n                                     *classical_registers,\n                                     name=x.header.name)\n            qreg_dict = {}\n            creg_dict = {}\n            for reg in quantum_registers:\n                qreg_dict[reg.name] = reg\n            for reg in classical_registers:\n                creg_dict[reg.name] = reg\n            for i in x.instructions:\n                instr_method = getattr(circuit, i.name)\n                qubits = []\n                try:\n                    for qubit in i.qubits:\n                        qubit_label = x.header.qubit_labels[qubit]\n                        qubits.append(\n                            qreg_dict[qubit_label[0]][qubit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                clbits = []\n                try:\n                    for clbit in i.memory:\n                        clbit_label = x.header.clbit_labels[clbit]\n                        clbits.append(\n                            creg_dict[clbit_label[0]][clbit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                params = []\n                try:\n                    params = i.params\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                if i.name in ['snapshot']:\n                    instr_method(\n                        i.label,\n                        snapshot_type=i.snapshot_type,\n                        qubits=qubits,\n                        params=params)\n                elif i.name == 'initialize':\n                    instr_method(params, qubits)\n                else:\n                    instr_method(*params, *qubits, *clbits)\n            circuits.append(circuit)\n        return circuits\n    return None"}
{"prompt": "def visit_assign(self, node):\n        \"\"\"check that if assigning to a function call, the function is\n        possibly returning something valuable\n        \"\"\"", "output": "\n        if isinstance(node.value, ast.Call):\n            func_name = node.value.func.id\n            if func_name in self.functions:\n                func_def = self.functions[func_name]\n                if func_def.returns_something:\n                    self.add_issue(\n                        \"Assigning to a function call that may return something valuable\",\n                        node.value.func.lineno,\n                        node.value.func.col_offset,\n                    )\n\n    def visit_return(self, node):\n        \"\"\"check that if a function returns something, it is used\"\"\"\n        if isinstance(", "solution": "\n        if not isinstance(node.value, astroid.Call):\n            return\n        function_node = safe_infer(node.value.func)\n        # skip class, generator and incomplete function definition\n        funcs = (astroid.FunctionDef, astroid.UnboundMethod, astroid.BoundMethod)\n        if not (\n            isinstance(function_node, funcs)\n            and function_node.root().fully_defined()\n            and not function_node.decorators\n        ):\n            return\n        if (\n            function_node.is_generator()\n            or function_node.is_abstract(pass_is_abstract=False)\n            or isinstance(function_node, astroid.AsyncFunctionDef)\n        ):\n            return\n        returns = list(\n            function_node.nodes_of_class(astroid.Return, skip_klass=astroid.FunctionDef)\n        )\n        if not returns:\n            self.add_message(\"assignment-from-no-return\", node=node)\n        else:\n            for rnode in returns:\n                if not (\n                    isinstance(rnode.value, astroid.Const)\n                    and rnode.value.value is None\n                    or rnode.value is None\n                ):\n                    break\n            else:\n                self.add_message(\"assignment-from-none\", node=node)"}
{"prompt": "def read_array(self, start, end):\n        \"\"\"Return floats from `start` to `end` inclusive, indexed from 1.\n\n        The entire range of floats is immediately read into memory from\n        the file, making this efficient for small sequences of floats\n        whose values are all needed immediately.\n\n        \"\"\"", "output": "\n        if end < start:\n            raise ValueError(\"end must be greater than or equal to start\")\n        if end > self.size:\n            raise ValueError(\"end must be less than or equal to the size of the file\")\n        if start < 1:\n            raise ValueError(\"start must be greater than or equal to 1\")\n        if end < 1:\n            raise ValueError(\"end must be greater than or equal to 1\")\n\n        if end - start + 1 > self.size - self.tell():\n            raise ValueError(\"end must be less than or equal to the size of the file\")\n\n        self.", "solution": "\n        f = self.file\n        f.seek(8 * (start - 1))\n        length = 1 + end - start\n        data = f.read(8 * length)\n        return ndarray(length, self.endian + 'd', data)"}
{"prompt": "def batches(dataset):\n    \"\"\"Returns a callable that chooses sequences from netcdf data.\"\"\"", "output": "\n    def batch_generator():\n        for i in range(0, len(dataset), batch_size):\n            yield dataset[i:i+batch_size]\n    return batch_generator\n\ndef load_data(batch_size, data_dir,_var,_time,_lat,_lon,_level,_time_units,_lat_units,_lon_units,_level_units,_time_bounds,_lat_bounds,_lon_bounds,_level_bounds,_time_resolution,", "solution": "\n    seq_lengths = dataset.variables['seqLengths'].data\n    seq_begins = np.concatenate(([0], np.cumsum(seq_lengths)[:-1]))\n\n    def sample():\n        chosen = np.random.choice(\n            list(range(len(seq_lengths))), BATCH_SIZE, replace=False)\n        return batch_at(dataset.variables['inputs'].data,\n                        dataset.variables['targetClasses'].data,\n                        seq_begins[chosen],\n                        seq_lengths[chosen])\n\n    return sample"}
{"prompt": "def _raise_if_wrong_file_signature(stream):\n    \"\"\" Reads the 4 first bytes of the stream to check that is LASF\"\"\"", "output": "\n    signature = stream.read(4)\n    if signature!= b'LASF':\n        raise ValueError('Invalid file signature')\n\n\ndef read_las_file(file_path):\n    \"\"\" Reads a LAS file and returns a LASFile object. \"\"\"\n    with open(file_path, 'rb') as file:\n        _raise_if_wrong_file_signature(file)\n        las_file = LASFile()\n        las_file.read_from_file(file)\n        return las_file\n\n\ndef write_las_file(las_file, file_path):\n   ", "solution": "\n    file_sig = stream.read(len(headers.LAS_FILE_SIGNATURE))\n    if file_sig != headers.LAS_FILE_SIGNATURE:\n        raise errors.PylasError(\n            \"File Signature ({}) is not {}\".format(file_sig, headers.LAS_FILE_SIGNATURE)\n        )"}
{"prompt": "def find_data_files(self, package, src_dir):\n        \"\"\"Return filenames for package's data files in 'src_dir'\"\"\"", "output": "\n        data_files = []\n        for root, dirs, files in os.walk(src_dir):\n            for f in files:\n                if f.endswith('.json') or f.endswith('.yaml'):\n                    data_files.append(os.path.join(root, f))\n        return data_files\n\n    def find_package_data(self, package, src_dir):\n        \"\"\"Return filenames for package's data files in'src_dir'\"\"\"\n        data_files = self.find_data_files(package, src_dir)\n        package_data = {}\n        for f in data_", "solution": "\n        globs = (self.package_data.get('', [])\n                 + self.package_data.get(package, []))\n        files = self.manifest_files.get(package, [])[:]\n        for pattern in globs:\n            # Each pattern has to be converted to a platform-specific path\n            files.extend(glob(os.path.join(src_dir, convert_path(pattern))))\n        return self.exclude_data_files(package, src_dir, files)"}
{"prompt": "def to_id(string: str) -> str:\n    \"\"\"Get a spotify ID from a URI or open.spotify URL.\n\n    Paramters\n    ---------\n    string : str\n        The string to operate on.\n\n    Returns\n    -------\n    id : str\n        The Spotify ID from the string.\n    \"\"\"", "output": "\n    if \"open.spotify.com\" in string:\n        return string.split(\"/\")[-1]\n    else:\n        return string.split(\":\")[2]\n\n\ndef get_tracks(uri: str) -> List[dict]:\n    \"\"\"Get a list of tracks from a URI or open.spotify URL.\n\n    Parameters\n    ----------\n    uri : str\n        The URI or open.spotify URL to get the tracks from.\n\n    Returns\n    -------\n    tracks : List[dict]\n        A list of track dictionaries.\n    \"\"\"\n    id = to_id(uri)\n    tracks = spotipy.util.get_tracks(", "solution": "\n    string = string.strip()\n\n    match = _URI_RE.match(string)\n\n    if match is None:\n        match = _OPEN_RE.match(string)\n\n        if match is None:\n            return string\n        else:\n            return match.group(2)\n    else:\n        return match.group(1)"}
{"prompt": "def marquee(txt='',width=78,mark='*'):\n    \"\"\"Return the input string centered in a 'marquee'.\n\n    :Examples:\n\n        In [16]: marquee('A test',40)\n        Out[16]: '**************** A test ****************'\n\n        In [17]: marquee('A test',40,'-')\n        Out[17]: '---------------- A test ----------------'\n\n        In [18]: marquee('A test',40,' ')\n        Out[18]: '                 A test                 '\n\n    \"\"\"", "output": "\n    # Calculate the number of spaces to add to the left and right of the text\n    spaces = (width - len(txt)) // 2\n    # Add the spaces to the left and right of the text\n    txt ='' * spaces + txt +'' * spaces\n    # Add the mark to the left and right of the text\n    txt = mark * (width - len(txt)) + txt + mark * (width - len(txt))\n    return txt\n\n\n# Test the function\nprint(marquee('A test', 40))\nprint(marquee('A test', 40", "solution": "\n    if not txt:\n        return (mark*width)[:width]\n    nmark = (width-len(txt)-2)//len(mark)//2\n    if nmark < 0: nmark =0\n    marks = mark*nmark\n    return '%s %s %s' % (marks,txt,marks)"}
{"prompt": "def add(self, gate, qargs, cargs):\n        \"\"\"Add an instruction and its context (where it's attached).\"\"\"", "output": "\n        self.instructions.append((gate, qargs, cargs))\n\n    def add_barrier(self, qargs):\n        \"\"\"Add a barrier instruction.\"\"\"\n        self.instructions.append(('barrier', qargs, []))\n\n    def add_reset(self, qargs):\n        \"\"\"Add a reset instruction.\"\"\"\n        self.instructions.append(('reset', qargs, []))\n\n    def add_measure(self, qargs, cargs):\n        \"\"\"Add a measure instruction.\"\"\"\n        self.instructions.append(('measure', qargs, cargs))\n\n    def add_conditional(self,,_q", "solution": "\n        if not isinstance(gate, Instruction):\n            raise QiskitError(\"attempt to add non-Instruction\" +\n                              \" to InstructionSet\")\n        self.instructions.append(gate)\n        self.qargs.append(qargs)\n        self.cargs.append(cargs)"}
{"prompt": "def read_char(self, c: str) -> bool:\n        \"\"\"\n        Consume the c head byte, increment current index and return True\n        else return False. It use peekchar and it's the same as '' in BNF.\n        \"\"\"", "output": "\n        if self.current_index < len(self.input_stream):\n            if self.input_stream[self.current_index] == c:\n                self.current_index += 1\n                return True\n            else:\n                return False\n        else:\n            return False\n\n    def read_string(self, delimiter: str) -> str:\n        \"\"\"\n        Consume the string until the delimiter is found, increment current index and return the string.\n        \"\"\"\n        result = ''\n        while True:\n            if self.current_index < len(self.input_stream):\n                if self.input_stream[self", "solution": "\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()"}
{"prompt": "def disassemble(msg, msg_nocr, section, co, lasti=-1, start_line=-1,\n                end_line=None, relative_pos=False, highlight='light',\n                start_offset=0, end_offset=None):\n    \"\"\"Disassemble a code object.\"\"\"", "output": "\n    if end_line is None:\n        end_line = len(msg_nocr.split('\\n'))\n\n    if relative_pos:\n        start_line += 1\n        end_line += 1\n\n    if start_line < 0:\n        start_line = 0\n    if end_line > len(msg_nocr.split('\\n')):\n        end_line = len(msg_nocr.split('\\n'))\n\n    if start_line > end_line:\n        start_line, end_line = end_line, start_line\n\n    if start_offset < 0:", "solution": "\n    return disassemble_bytes(msg, msg_nocr, co.co_code, lasti, co.co_firstlineno,\n                             start_line, end_line, relative_pos,\n                        co.co_varnames, co.co_names, co.co_consts,\n                        co.co_cellvars, co.co_freevars,\n                        dict(findlinestarts(co)), highlight,\n                        start_offset=start_offset, end_offset=end_offset)"}
{"prompt": "def _unpack_storm_date(date):\n        \"\"\"\n        given a packed storm date field, unpack and return 'YYYY-MM-DD' string.\n        \"\"\"", "output": "\n        year = (date >> 24) & 0xff\n        month = (date >> 16) & 0xff\n        day = date & 0xff\n        return f\"{year:04d}-{month:02d}-{day:02d}\"\n\n\ndef _unpack___date(date):\n    \"\"\"\n    given a packed date field, unpack and return 'YYYY-MM-DD' string.\n    \"\"\"\n    year = (date >> 24) & 0xff\n    month = (date >> 16) & 0xff\n    day = date", "solution": "\n        year = (date & 0x7f) + 2000  # 7 bits\n        day = (date >> 7) & 0x01f  # 5 bits\n        month = (date >> 12) & 0x0f  # 4 bits\n        return \"%s-%s-%s\" % (year, month, day)"}
{"prompt": "def poll_operation_until_done(self, name, polling_interval_in_seconds):\n        \"\"\"\n        Poll backup operation state until it's completed.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n        :type polling_interval_in_seconds: int\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"", "output": "\n        operation = self.get_operation(name)\n        while operation.status!= \"SUCCEEDED\" and operation.status!= \"FAILED\":\n            time.sleep(polling_interval_in_seconds)\n            operation = self.get_operation(name)\n        return operation\n\n    def get_operation(self, name):\n        \"\"\"\n        Get the details of a backup operation.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"\n        return self.client.get_operation(name)\n\n    def get_backup_operation", "solution": "\n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result"}
{"prompt": "def cost(self, t_node, branch_length, multiplicity=2.0):\n        \"\"\"\n        returns the cost associated with a branch starting at t_node\n        t_node is time before present, the branch goes back in time\n\n        Args:\n            - t_node:           time of the node\n            - branch_length:    branch length, determines when this branch merges with sister\n            - multiplicity:     2 if merger is binary, higher if this is a polytomy\n        \"\"\"", "output": "\n       _length =_length_ branch_length\n       _cost =_length *_length\n       _cost *=_length *_length\n       _cost *=_length *_length\n       _cost *=_length *_length\n       _cost *=_length *_length\n       _cost *=_length *_length\n       _cost *=_length *_length\n       _cost *=_length *_length\n       _cost *=_length *_length\n       _cost *=_", "solution": "\n        merger_time = t_node+branch_length\n        return self.integral_merger_rate(merger_time) - self.integral_merger_rate(t_node)\\\n                 - np.log(self.total_merger_rate(merger_time))*(multiplicity-1.0)/multiplicity"}
{"prompt": "def run_cell_magic(self, magic_name, line, cell):\n        \"\"\"Execute the given cell magic.\n        \n        Parameters\n        ----------\n        magic_name : str\n          Name of the desired magic function, without '%' prefix.\n\n        line : str\n          The rest of the first input line as a single string.\n\n        cell : str\n          The body of the cell as a (possibly multiline) string.\n        \"\"\"", "output": "\n        if magic_name == 'run':\n            self.run_cell(cell)\n        else:\n            raise ValueError(f\"Unknown magic function '{magic_name}'.\")\n\n    def run_cell(self, cell):\n        \"\"\"Execute the given cell as a Python code.\n        \n        Parameters\n        ----------\n        cell : str\n          The body of the cell as a (possibly multiline) string.\n        \"\"\"\n        try:\n            exec(cell, self.user_ns)\n        except Exception as e:\n            self.log_error(f\"Error executing cell: {e}\")\n            raise e\n\n    def log_error(self,", "solution": "\n        fn = self.find_cell_magic(magic_name)\n        if fn is None:\n            lm = self.find_line_magic(magic_name)\n            etpl = \"Cell magic function `%%%%%s` not found%s.\"\n            extra = '' if lm is None else (' (But line magic `%%%s` exists, '\n                                    'did you mean that instead?)' % magic_name )\n            error(etpl % (magic_name, extra))\n        else:\n            # Note: this is the distance in the stack to the user's frame.\n            # This will need to be updated if the internal calling logic gets\n            # refactored, or else we'll be expanding the wrong variables.\n            stack_depth = 2\n            magic_arg_s = self.var_expand(line, stack_depth)\n            with self.builtin_trap:\n                result = fn(line, cell)\n            return result"}
{"prompt": "def update_trace_watch(self):\n        \"\"\"Parses the nextflow trace file and retrieves the path of report JSON\n        files that have not been sent to the service yet.\n        \"\"\"", "output": "\n        trace_file = self.get_nextflow_trace_file()\n        if not trace_file:\n            return\n\n        with open(trace_file, 'r') as f:\n            for line in f:\n                if'report.json' in line:\n                    report_json_path = line.strip()\n                    if not os.path.exists(report_json_path):\n                        self.report_json_paths.append(report_json_path)\n\n    def get_nextflow_trace_file(self):\n        \"\"\"Returns the path of the nextflow trace file.\"\"\"\n        trace_files =", "solution": "\n\n        # Check the size stamp of the tracefile. Only proceed with the parsing\n        # if it changed from the previous size.\n        size_stamp = os.path.getsize(self.trace_file)\n        self.trace_retry = 0\n        if size_stamp and size_stamp == self.trace_sizestamp:\n            return\n        else:\n            logger.debug(\"Updating trace size stamp to: {}\".format(size_stamp))\n            self.trace_sizestamp = size_stamp\n\n        with open(self.trace_file) as fh:\n\n            # Skip potential empty lines at the start of file\n            header = next(fh).strip()\n            while not header:\n                header = next(fh).strip()\n\n            # Get header mappings before parsing the file\n            hm = self._header_mapping(header)\n\n            for line in fh:\n                # Skip empty lines\n                if line.strip() == \"\":\n                    continue\n\n                fields = line.strip().split(\"\\t\")\n\n                # Skip if task ID was already processes\n                if fields[hm[\"task_id\"]] in self.stored_ids:\n                    continue\n\n                if fields[hm[\"process\"]] == \"report\":\n                    self.report_queue.append(\n                        self._expand_path(fields[hm[\"hash\"]])\n                    )\n                    self.send = True\n\n                # Add the processed trace line to the stored ids. It will be\n                # skipped in future parsers\n                self.stored_ids.append(fields[hm[\"task_id\"]])"}
{"prompt": "def permission_check(apikey, endpoint):\n        \"\"\"\n        return (user, seckey) if url end point is in allowed entry point list\n        \"\"\"", "output": "\n        if endpoint in ALLOWED_ENDPOINTS:\n            user = apikey\n            seckey = None\n            return (user, seckey)\n        else:\n            return None\n\n    def check_apikey(apikey):\n        \"\"\"\n        check if apikey is valid\n        \"\"\"\n        if apikey in ALLOWED_APIKEYS:\n            return True\n        else:\n            return False\n\n    def check_seckey(apikey, seckey):\n        \"\"\"\n        check if seckey is valid for the given apikey\n        \"\"\"\n        if apikey in ALLOWED_APIKEYS:\n            if seckey in ALLOWED_SECK", "solution": "\n        try:\n            ak = APIKeys.objects.get(apikey=apikey)\n            apitree = cPickle.loads(ak.apitree.encode(\"ascii\"))\n            if apitree.match(endpoint):\n                return ak.user if ak.user else AnonymousUser(), ak.seckey\n        except APIKeys.DoesNotExist:\n            pass\n        return None, None"}
{"prompt": "def __create_criterion(self, criterion_str):\n        \"\"\" instantiate criterion from config string \"\"\"", "output": "\n        if criterion_str == 'cross_entropy':\n            criterion = nn.CrossEntropyLoss()\n        elif criterion_str =='mse':\n            criterion = nn.MSELoss()\n        else:\n            raise ValueError('Invalid criterion: {}'.format(criterion_str))\n        return criterion\n\n    def __create_optimizer(self, optimizer_str, params):\n        \"\"\" instantiate optimizer from config string \"\"\"\n        if optimizer_str == 'adam':\n            optimizer = optim.Adam(params, lr=self.config['lr'])\n        elif optimizer_str =='sgd':\n            optimizer = optim.SGD(params, lr", "solution": "\n        parsed = criterion_str.split(\"(\")\n        type_str = parsed[0].strip().lower()\n        parsed[1] = parsed[1].split(\")\")[0].strip()\n\n        for criterion_class in self.custom_criterions:\n            if criterion_class.get_type_string() == type_str:\n                return criterion_class(self, parsed[1])\n        raise ValueError(\n            \"Unsupported autostop criterion type: %s\" % criterion_str)"}
{"prompt": "def verboseRead(self, alphabet, context='', skipExtra=False):\n        \"\"\"Read symbol and extra from stream and explain what happens.\n        Returns the value of the symbol\n        >>> olleke.pos = 0\n        >>> l = Layout(olleke)\n        >>> l.verboseRead(WindowSizeAlphabet())\n        0000  1b                   1011 WSIZE   windowsize=(1<<22)-16=4194288\n        4194288\n        \"\"\"", "output": "\n        = self.stream\n        = self.verbose\n        = self.explain\n        = self.explainRead\n        = self.explainReadSymbol\n        = self.explainReadExtra\n        = self.explainReadValue\n        = self.explainReadWindowSize\n        = self.explainReadWindowSizeValue\n        = self.explainReadWindowSizeBits\n        = self.explainReadWindowSizeBitsValue\n        = self.explainReadWindowSizeBitsBits\n        = self.explainReadWindowSizeBitsBitsValue\n        = self.explainReadWindowSizeBitsBitsBits\n        = self", "solution": "\n        #TODO 2: verbosity level, e.g. show only codes and maps in header\n        stream = self.stream\n        pos = stream.pos\n        if skipExtra:\n            length, symbol = alphabet.readTuple(stream)\n            extraBits, extra = 0, None\n        else:\n            length, symbol, extraBits, extra = alphabet.readTupleAndExtra(\n                stream)\n        #fields: address, hex data, binary data, name of alphabet, explanation\n        hexdata = self.makeHexData(pos)\n        addressField = '{:04x}'.format(pos+7>>3) if hexdata else ''\n        bitdata = self.formatBitData(pos, length, extraBits)\n        #bitPtr moves bitdata so that the bytes are easier to read\n        #jump back to right if a new byte starts\n        if '|' in bitdata[1:]:\n            #start over on the right side\n            self.bitPtr = self.width\n        fillWidth = self.bitPtr-(len(hexdata)+len(bitdata))\n        if fillWidth<0: fillWidth = 0\n        print('{:<5s} {:<{}s} {:7s} {}'.format(\n            addressField,\n            hexdata+' '*fillWidth+bitdata, self.width,\n            context+alphabet.name,\n            symbol if skipExtra else symbol.explanation(extra),\n            ))\n        #jump to the right if we started with a '|'\n        #because we didn't jump before printing\n        if bitdata.startswith('|'): self.bitPtr = self.width\n        else: self.bitPtr -= len(bitdata)\n        return symbol if skipExtra else symbol.value(extra)"}
{"prompt": "def builtin(cls, name):\n        \"\"\"\n        Generate a default legend.\n\n        Args:\n            name (str): The name of the legend you want. Not case sensitive.\n                 'nsdoe': Nova Scotia Dept. of Energy\n                 'canstrat': Canstrat\n                 'nagmdm__6_2': USGS N. Am. Geol. Map Data Model 6.2\n                 'nagmdm__6_1': USGS N. Am. Geol. Map Data Model 6.1\n                 'nagmdm__4_3': USGS N. Am. Geol. Map Data Model 4.3\n                 'sgmc': USGS State Geologic Map Compilation\n\n            Default 'nagmdm__6_2'.\n\n        Returns:\n            Legend: The legend stored in `defaults.py`.\n        \"\"\"", "output": " Geol. Map Data Model 4.3\n                 'nagmdm__4_2': USGS N. Am. Geol. Map Data Model 4.2\n                 'nagmdm__4_1': USGS N. Am. Geol. Map Data Model 4.1\n                 'nagmdm__3_1': USGS N. Am. Geol. Map Data Model 3.1\n                 'nagmdm__3_0': USGS N. Am. Geol. Map Data Model 3.0\n                 'nagmdm__2_0':", "solution": "\n        names = {\n                 'nsdoe': LEGEND__NSDOE,\n                 'canstrat': LEGEND__Canstrat,\n                 'nagmdm__6_2': LEGEND__NAGMDM__6_2,\n                 'nagmdm__6_1': LEGEND__NAGMDM__6_1,\n                 'nagmdm__4_3': LEGEND__NAGMDM__4_3,\n                 'sgmc': LEGEND__SGMC,\n                 }\n        return cls.from_csv(text=names[name.lower()])"}
{"prompt": "def parse_email(data, strip_attachment_payloads=False):\n    \"\"\"\n    A simplified email parser\n\n    Args:\n        data: The RFC 822 message string, or MSG binary\n        strip_attachment_payloads (bool): Remove attachment payloads\n\n    Returns (dict): Parsed email data\n    \"\"\"", "output": "2021-04-28: Added filtering to remove-relatedilicious content\n    \"\"\"\n    import re\n    import email\n    import email.policy\n    import email.utils\n\n    def decode_mime_header(header):\n        \"\"\"\n        Decode MIME header string to unicode\n\n        Args:\n            header (str): MIME header string\n\n        Returns:\n            str: Decoded header string\n        \"\"\"\n        try:\n            return header.decode('utf-8')\n        except UnicodeDecodeError:\n            return header\n\n    def extract_email_address(email_address):\n        \"\"\"\n        Extract email address from email header", "solution": "\n\n    if type(data) == bytes:\n        if is_outlook_msg(data):\n            data = convert_outlook_msg(data)\n        data = data.decode(\"utf-8\", errors=\"replace\")\n    parsed_email = mailparser.parse_from_string(data)\n    headers = json.loads(parsed_email.headers_json).copy()\n    parsed_email = json.loads(parsed_email.mail_json).copy()\n    parsed_email[\"headers\"] = headers\n\n    if \"received\" in parsed_email:\n        for received in parsed_email[\"received\"]:\n            if \"date_utc\" in received:\n                if received[\"date_utc\"] is None:\n                    del received[\"date_utc\"]\n                else:\n                    received[\"date_utc\"] = received[\"date_utc\"].replace(\"T\",\n                                                                        \" \")\n\n    if \"from\" not in parsed_email:\n        if \"From\" in parsed_email[\"headers\"]:\n            parsed_email[\"from\"] = parsed_email[\"Headers\"][\"From\"]\n        else:\n            parsed_email[\"from\"] = None\n\n    if parsed_email[\"from\"] is not None:\n        parsed_email[\"from\"] = parse_email_address(parsed_email[\"from\"][0])\n\n    if \"date\" in parsed_email:\n        parsed_email[\"date\"] = parsed_email[\"date\"].replace(\"T\", \" \")\n    else:\n        parsed_email[\"date\"] = None\n    if \"reply_to\" in parsed_email:\n        parsed_email[\"reply_to\"] = list(map(lambda x: parse_email_address(x),\n                                            parsed_email[\"reply_to\"]))\n    else:\n        parsed_email[\"reply_to\"] = []\n\n    if \"to\" in parsed_email:\n        parsed_email[\"to\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"to\"]))\n    else:\n        parsed_email[\"to\"] = []\n\n    if \"cc\" in parsed_email:\n        parsed_email[\"cc\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"cc\"]))\n    else:\n        parsed_email[\"cc\"] = []\n\n    if \"bcc\" in parsed_email:\n        parsed_email[\"bcc\"] = list(map(lambda x: parse_email_address(x),\n                                       parsed_email[\"bcc\"]))\n    else:\n        parsed_email[\"bcc\"] = []\n\n    if \"delivered_to\" in parsed_email:\n        parsed_email[\"delivered_to\"] = list(\n            map(lambda x: parse_email_address(x),\n                parsed_email[\"delivered_to\"])\n        )\n\n    if \"attachments\" not in parsed_email:\n        parsed_email[\"attachments\"] = []\n    else:\n        for attachment in parsed_email[\"attachments\"]:\n            if \"payload\" in attachment:\n                payload = attachment[\"payload\"]\n                try:\n                    if \"content_transfer_encoding\" in attachment:\n                        if attachment[\"content_transfer_encoding\"] == \"base64\":\n                            payload = decode_base64(payload)\n                        else:\n                            payload = str.encode(payload)\n                    attachment[\"sha256\"] = hashlib.sha256(payload).hexdigest()\n                except Exception as e:\n                    logger.debug(\"Unable to decode attachment: {0}\".format(\n                        e.__str__()\n                    ))\n        if strip_attachment_payloads:\n            for attachment in parsed_email[\"attachments\"]:\n                if \"payload\" in attachment:\n                    del attachment[\"payload\"]\n\n    if \"subject\" not in parsed_email:\n        parsed_email[\"subject\"] = None\n\n    parsed_email[\"filename_safe_subject\"] = get_filename_safe_string(\n        parsed_email[\"subject\"])\n\n    if \"body\" not in parsed_email:\n        parsed_email[\"body\"] = None\n\n    return parsed_email"}
{"prompt": "def get_news(self, tickers=[], tags=[], sources=[], startDate=None,\n                 endDate=None, limit=100, offset=0, sortBy=\"publishedDate\",\n                 fmt='json'):\n        \"\"\"Return list of news articles matching given search terms\n            https://api.tiingo.com/docs/tiingo/news\n\n            # Dates are in YYYY-MM-DD Format.\n\n            Args:\n                tickers [string] : List of unique Stock Tickers to search\n                tags [string] : List of topics tagged by Tiingo Algorithms\n                sources [string]: List of base urls to include as news sources\n                startDate, endDate [date]: Boundaries of news search window\n                limit (int): Max results returned. Default 100, max 1000\n                offset (int): Search results offset, used for paginating\n                sortBy (string): \"publishedDate\" OR (#TODO: UPDATE THIS)\n        \"\"\"", "output": " sources\n                startDate [string] : Start date of news articles to search\n                endDate [string] : End date of news articles to search\n                limit [int] : Maximum number of news articles to return\n                offset [int] : Number of news articles to skip before returning\n                sortBy [string] : Sort news articles by publishedDate or title\n                fmt [string] : Output format of news articles (json or csv)\n\n            Returns:\n                List of news articles matching search terms\n        \"\"\"\n        params = {\n            \"token\": self.token,\n            \"tickers\": tickers,\n            \"tags\": tags,\n            \"sources\": sources", "solution": "\n        url = \"tiingo/news\"\n        params = {\n            'limit': limit,\n            'offset': offset,\n            'sortBy': sortBy,\n            'tickers': tickers,\n            'sources': sources,\n            'tags': tags,\n            'startDate': startDate,\n            'endDate': endDate\n        }\n        response = self._request('GET', url, params=params)\n        data = response.json()\n        if fmt == 'json':\n            return data\n        elif fmt == 'object':\n            return [dict_to_object(item, \"NewsArticle\") for item in data]"}
{"prompt": "def _iter_nodes(self, node, recursive=False, max_depth=float('inf'),\n                    with_links=True, in_search=False, predicate=None):\n        \"\"\"Returns an iterator over nodes hanging below a given start node.\n\n        :param node:\n\n            Start node\n\n        :param recursive:\n\n            Whether recursively also iterate over the children of the start node's children\n\n        :param max_depth:\n\n            Maximum depth to search for\n\n        :param in_search:\n\n            if it is used during get search and if detailed info should be returned\n\n        :param with_links:\n\n            If links should be considered\n\n        :param predicate:\n\n            A predicate to filter nodes\n\n        :return: Iterator\n\n        \"\"\"", "output": " included in the search\n\n        :param predicate:\n\n            A function that takes a node and returns True if it should be included in the search.\n            The default is to include all nodes.\n        \"\"\"\n        if predicate is None or predicate(node):\n            yield node\n\n        if not recursive:\n            return\n\n        for child in node.children:\n            yield from self._iter_nodes(child, recursive, max_depth, with_links, in_search, predicate)\n\n    def get_search(self, start_node, recursive=False, max_depth=float('inf'), with_links=True, predicate=None):", "solution": "\n        def _run_predicate(x, run_name_set):\n            branch = x.v_run_branch\n            return branch == 'trajectory' or branch in run_name_set\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if predicate is None:\n            predicate = lambda x: True\n        elif isinstance(predicate, (tuple, list)):\n            # Create a predicate from a list of run names or run indices\n            run_list = predicate\n            run_name_set = set()\n            for item in run_list:\n                if item == -1:\n                    run_name_set.add(self._root_instance.f_wildcard('$', -1))\n                elif isinstance(item, int):\n                    run_name_set.add(self._root_instance.f_idx_to_run(item))\n                else:\n                    run_name_set.add(item)\n            predicate = lambda x: _run_predicate(x, run_name_set)\n\n        if recursive:\n            return NaturalNamingInterface._recursive_traversal_bfs(node,\n                                            self._root_instance._linked_by,\n                                            max_depth, with_links,\n                                            in_search, predicate)\n        else:\n            iterator = (x for x in self._make_child_iterator(node, with_links) if\n                        predicate(x[2]))\n            if in_search:\n                return iterator # Here we return tuples: (depth, name, object)\n            else:\n                return (x[2] for x in iterator)"}
{"prompt": "def du_handler(self, args):\n    \"\"\"Handler for size command\"\"\"", "output": "\n   _size = args.size\n   _size =_size.split('x')\n   _size = [int(x) for x in t_size]\n   _size = tuple(t_size)\n   _size =_size[::-1]\n   _size = tuple(t_size)\n   _size = (max(t_size[0], t_size[1]), min(t_size[0], t_size[1]))\n   _size = tuple(t_size)\n   _size = (min(t_size", "solution": "\n    for src, size in self.s3handler().size(args[1:]):\n      message('%s\\t%s' % (size, src))"}
{"prompt": "def gauge(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Gauge metric with the specified value\"\"\"", "output": "\n        self._send_metric(name, value, rate, 'gauge')\n\n    def histogram(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Histogram metric with the specified value\"\"\"\n        self._send_metric(name, value, rate, 'histogram')\n\n    def set(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Set metric with the specified value\"\"\"\n        self._send_metric(name, value, rate,'set')\n\n    def _send", "solution": "\n\n        if self._should_send_metric(name, rate):\n            if not is_numeric(value):\n                value = float(value)\n            self._request(\n                Gauge(\n                    self._create_metric_name_for_request(name),\n                    value,\n                    rate\n                ).to_request()\n            )"}
{"prompt": "def watch_port_events(port, chip, pin_function_maps, event_queue,\n                      return_after_kbdint=False):\n    \"\"\"Waits for a port event. When a port event occurs it is placed onto the\n    event queue.\n\n    :param port: The port we are waiting for interrupts on (GPIOA/GPIOB).\n    :type port: int\n    :param chip: The chip we are waiting for interrupts on.\n    :type chip: :class:`pifacecommon.mcp23s17.MCP23S17`\n    :param pin_function_maps: A list of classes that have inheritted from\n        :class:`FunctionMap`\\ s describing what to do with events.\n    :type pin_function_maps: list\n    :param event_queue: A queue to put events on.\n    :type event_queue: :py:class:`multiprocessing.Queue`\n    \"\"\"", "output": "_maps: A dictionary of_pin ->_function mappings.\n    :type pin_function_maps: dict\n    :param event_queue: The queue to place the port event onto.\n    :type event_queue: Queue\n    :param return_after_kbdint: If True, the function will return after a keyboard\n        interrupt is detected.\n    :type return_after_kbdint: bool\n    :return: The_pin that caused the port event.\n    :rtype: int\n    \"\"\"\n   _pin =_pin_from_port_event(port, chip, pin_function", "solution": "\n    # set up epoll\n    gpio25 = open(GPIO_INTERRUPT_DEVICE_VALUE, 'r')  # change to use 'with'?\n    epoll = select.epoll()\n    epoll.register(gpio25, select.EPOLLIN | select.EPOLLET)\n\n    while True:\n        # wait here until input\n        try:\n            events = epoll.poll()\n        except KeyboardInterrupt as e:\n            if return_after_kbdint:\n                return\n            else:\n                raise e\n        except IOError as e:\n            # ignore \"Interrupted system call\" error.\n            # I don't really like this solution. Ignoring problems is bad!\n            if e.errno != errno.EINTR:\n                raise\n\n        # find out where the interrupt came from and put it on the event queue\n        if port == pifacecommon.mcp23s17.GPIOA:\n            interrupt_flag = chip.intfa.value\n        else:\n            interrupt_flag = chip.intfb.value\n\n        if interrupt_flag == 0:\n            continue  # The interrupt has not been flagged on this board\n        else:\n            if port == pifacecommon.mcp23s17.GPIOA:\n                interrupt_capture = chip.intcapa.value\n            else:\n                interrupt_capture = chip.intcapb.value\n            event_queue.add_event(InterruptEvent(\n                interrupt_flag, interrupt_capture, chip, time.time()))\n\n    epoll.close()"}
{"prompt": "def _parse_retry_after(self, response):\n        \"\"\"Parse Retry-After header from response if it is set.\"\"\"", "output": "\n        try:\n            retry_after = int(response.headers.get('Retry-After', 0))\n            if retry_after > 0:\n                return retry_after\n        except ValueError:\n            pass\n        return None\n\n    def _handle_retry_after(self, response, retry_after):\n        \"\"\"Handle Retry-After header if it is set.\"\"\"\n        if retry_after is not None:\n            response.headers['Retry-After'] = str(retry_after)\n            response.status_code = 503\n            response.reason = 'Service Unavailable'\n            response.text = 'Retry", "solution": "\n        value = response.headers.get('Retry-After')\n\n        if not value:\n            seconds = 0\n        elif re.match(r'^\\s*[0-9]+\\s*$', value):\n            seconds = int(value)\n        else:\n            date_tuple = email.utils.parsedate(value)\n            if date_tuple is None:\n                seconds = 0\n            else:\n                seconds = time.mktime(date_tuple) - time.time()\n        return max(0, seconds)"}
{"prompt": "def _utf8_params(params):\n    \"\"\"encode a dictionary of URL parameters (including iterables) as utf-8\"\"\"", "output": "\n    return {k: _utf8(v) for k, v in params.items()}\n\n\ndef _utf8(s):\n    \"\"\"convert a string to utf-8 if necessary\"\"\"\n    if isinstance(s, str):\n        return s.encode('utf-8')\n    elif isinstance(s, (list, tuple)):\n        return [_utf8(x) for x in s]\n    else:\n        return s\n\n\ndef _url_encode(params):\n    \"\"\"encode a dictionary of URL parameters as a string\"\"\"\n    return urllib.parse.urlencode(_utf8_params(params))\n\n", "solution": "\n    assert isinstance(params, dict)\n    encoded_params = []\n    for k, v in params.items():\n        if v is None:\n            continue\n        if isinstance(v, integer_types + (float,)):\n            v = str(v)\n        if isinstance(v, (list, tuple)):\n            v = [to_bytes(x) for x in v]\n        else:\n            v = to_bytes(v)\n        encoded_params.append((k, v))\n    return dict(encoded_params)"}
{"prompt": "def parse_variant_id(chrom, pos, ref, alt, variant_type):\n    \"\"\"Parse the variant id for a variant\n\n    variant_id is used to identify variants within a certain type of\n    analysis. It is not human readable since it is a md5 key.\n\n    Args:\n        chrom(str)\n        pos(str)\n        ref(str)\n        alt(str)\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        variant_id(str): The variant id converted to md5 string\n    \"\"\"", "output": "\n    variant_id = hashlib.md5(\n        (chrom + pos + ref + alt + variant_type).encode('utf-8')\n    ).hexdigest()\n    return variant_id\n\n\ndef parse_variant_id_from_variant_dict(variant_dict, variant_type):\n    \"\"\"Parse the variant id for a variant from a variant dictionary\n\n    variant_id is used to identify variants within a certain type of\n    analysis. It is not human readable since it is a md5 key.\n\n    Args:\n        variant_dict(dict): A dictionary containing the variant information\n        variant_type(str", "solution": "\n    return generate_md5_key([chrom, pos, ref, alt, variant_type])"}
{"prompt": "def _construct_schema(elements, nsmap):\n    \"\"\"Consruct fiona schema based on given elements\n\n    :param list Element: list of elements\n    :param dict nsmap: namespace map\n\n    :return dict: schema\n    \"\"\"", "output": "\n    schema = {\n        'geometry': 'Point',\n        'properties': {}\n    }\n\n    for element in elements:\n        if element.tag in nsmap:\n            tag = nsmap[element.tag]\n        else:\n            tag = element.tag\n\n        if tag == 'gml:Point':\n            schema['geometry'] = 'Point'\n        elif tag == 'gml:LineString':\n            schema['geometry'] = 'LineString'\n        elif tag == 'gml:Polygon':\n            schema['geometry'] = 'Polygon'\n        elif tag == 'gml:MultiPoint':\n            schema['geometry'] = 'MultiPoint'", "solution": "\n\n    schema = {\n        'properties': {},\n        'geometry': None\n    }\n\n    schema_key = None\n    gml_key = None\n\n    # if nsmap is defined, use it\n    if nsmap:\n        for key in nsmap:\n            if nsmap[key] == XS_NAMESPACE:\n                schema_key = key\n            if nsmap[key] in GML_NAMESPACES:\n                gml_key = key\n    # if no nsmap is defined, we have to guess\n    else:\n        gml_key = 'gml'\n        schema_key = 'xsd'\n\n    mappings = {\n        'PointPropertyType': 'Point',\n        'PolygonPropertyType': 'Polygon',\n        'LineStringPropertyType': 'LineString',\n        'MultiPointPropertyType': 'MultiPoint',\n        'MultiLineStringPropertyType': 'MultiLineString',\n        'MultiPolygonPropertyType': 'MultiPolygon',\n        'MultiGeometryPropertyType': 'MultiGeometry',\n        'GeometryPropertyType': 'GeometryCollection',\n        'SurfacePropertyType': '3D Polygon',\n        'MultiSurfacePropertyType': '3D MultiPolygon'\n    }\n\n    for element in elements:\n        data_type = element.attrib['type'].replace(gml_key + ':', '')\n        name = element.attrib['name']\n\n        if data_type in mappings:\n            schema['geometry'] = mappings[data_type]\n            schema['geometry_column'] = name\n        else:\n            schema['properties'][name] = data_type.replace(schema_key+':', '')\n\n    if schema['properties'] or schema['geometry']:\n        return schema\n    else:\n        return None"}
{"prompt": "def _flatten(child, parent):\n    \"\"\"Custom flattening method for the parse tree.\"\"\"", "output": "\n    if isinstance(child, list):\n        for item in child:\n            _flatten(item, parent)\n    else:\n        parent.append(child)\n\n\ndef flatten_tree(tree):\n    \"\"\"Flatten a parse tree into a list of tokens.\"\"\"\n    flattened = []\n    _flatten(tree, flattened)\n    return flattened\n", "solution": "\n    return parent.is_type(TokenType.expression) and child.node_type == parent.node_type"}
{"prompt": "def read(self, resource_id):\n        \"\"\"Read object in elasticsearch using the resource_id.\n        :param resource_id: id of the object to be read.\n        :return: object value from elasticsearch.\n        \"\"\"", "output": "\n        try:\n            response = self.es.get(index=self.index_name, id=resource_id)\n            return response['_source']\n        except NotFoundError:\n            raise ResourceNotFoundError(f\"Resource with id {resource_id} not found in elasticsearch.\")\n\n    def update(self, resource_id, data):\n        \"\"\"Update object in elasticsearch using the resource_id and data.\n        :param resource_id: id of the object to be updated.\n        :param data: data to be updated in the object.\n        :return: updated object value from elasticsearch.\n        \"\"\"\n        try:\n            response", "solution": "\n        self.logger.debug('elasticsearch::read::{}'.format(resource_id))\n        return self.driver._es.get(\n            index=self.driver._index,\n            id=resource_id,\n            doc_type='_doc'\n        )['_source']"}
{"prompt": "def date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):\n        \"\"\"use previously calculated variation of the rate to estimate\n        the uncertainty in a particular numdate due to rate variation.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n            node for which the confidence interval is to be calculated\n        interval : tuple, optional\n            Array of length two, or tuple, defining the bounds of the confidence interval\n\n        \"\"\"", "output": "\n        if not hasattr(self, 'rate_variation'):\n            raise ValueError('rate_variation must be calculated before calculating uncertainty due to rate')\n        rate_variation = self.rate_variation\n        numdate = node.numdate\n        rate = node.rate\n        rate_uncertainty = rate_variation[numdate]\n        rate_lower_bound = rate - interval[0] * rate_uncertainty\n        rate_upper_bound = rate + interval[1] * rate_uncertainty\n        return rate_lower_bound, rate_upper_bound\n\n    def calculate_rate_variation(self,", "solution": "\n        if hasattr(node, \"numdate_rate_variation\"):\n            from scipy.special import erfinv\n            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0\n                    for x in interval]\n            l,c,u = [x[1] for x in node.numdate_rate_variation]\n            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])\n\n        else:\n            return None"}
{"prompt": "def score(self, x, w=None, **kwargs):\n        \"\"\"Compute R^2 coefficient of determination for a given input.\n\n        Parameters\n        ----------\n        x : ndarray (num-examples, num-inputs)\n            An array containing data to be fed into the network. Multiple\n            examples are arranged as rows in this array, with columns containing\n            the variables for each example.\n\n        Returns\n        -------\n        r2 : float\n            The R^2 correlation between the prediction of this netork and its\n            input. This can serve as one measure of the information loss of the\n            autoencoder.\n        \"\"\"", "output": "\n        if w is None:\n            w = self.get_weights()\n       _x = x.dot(w)\n       _x -= np.mean(test_x, axis=0)\n       _x /= np.std(test_x, axis=0)\n       _x =_x.reshape(-1, 1)\n       _x =_x.dot(w.T)\n       _x -= np.mean(test_x, axis=0)\n       _x /= np.std(test_x, axis=0)\n       _", "solution": "\n        return super(Autoencoder, self).score(x, x, w=w, **kwargs)"}
{"prompt": "def set_parent(self, parent):\n        \"\"\"Set parent ``Expression`` for this object.\n\n        Args:\n            parent (Expression): The ``Expression`` which contains this object.\n\n        Raises:\n            FiqlObjectException: Parent must be of type ``Expression``.\n        \"\"\"", "output": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of type Expression\")\n        self._parent = parent\n\n    def get_parent(self):\n        \"\"\"Get the parent ``Expression`` for this object.\n\n        Returns:\n            Expression: The parent ``Expression`` for this object.\n        \"\"\"\n        return self._parent\n\n    def set_children(self, children):\n        \"\"\"Set the children of this object.\n\n        Args:\n            children (list): A list of child ``Expression`` objects.\n\n        Raises:\n            FiqlObjectException: Children must be of type ``Expression``.\n        \"\"\"", "solution": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of %s not %s\" % (\n                Expression, type(parent)))\n        self.parent = parent"}
{"prompt": "def _build_kernel_function_declaration(self, name='kernel'):\n        \"\"\"Build and return kernel function declaration\"\"\"", "output": "\n       _type = self.input_type\n       _type =_type.replace('float', 'double')\n       _type = f'({self.input_type} *input, {self.output_type} *output, int n)'\n       _type = f'{self.output_type} {name}({self.input_type} *input, {self.output_type} *output, int n)'\n        return f'{self.input_type} *input, {self.output_type} *output, int n){self.output_type} {", "solution": "\n        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)\n        scalar_declarations = self._build_scalar_declarations(with_init=False)\n        const_declarations = self._build_const_declartions(with_init=False)\n        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +\n                                                          const_declarations),\n                              type=c_ast.TypeDecl(declname=name,\n                                                  quals=[],\n                                                  type=c_ast.IdentifierType(names=['void'])))"}
{"prompt": "def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,\n                                   length=8):\n  \"\"\"Visualizes a qualitative analysis of a given model.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    model: A DisentangledSequentialVAE model.\n    samples: Number of samples to draw from the latent distributions.\n    batch_size: Number of sequences to generate.\n    length: Number of timesteps to generate for each sequence.\n  \"\"\"", "output": "\n _inputs =__inputs =_outputs =__outputs =_z_mean = None\n _z_log_var =_z_log_var__z_mean = None\n _z_log_var_ = None\n\n _inputs =_outputs =_z_mean = None\n _z_log_var = None\n\n _inputs =_outputs = None\n _z_mean = None\n _z_log_var = None\n\n _inputs =_outputs = None\n ", "solution": "\n  average = lambda dist: tf.reduce_mean(\n      input_tensor=dist.mean(), axis=0)  # avg over samples\n  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n                                    samples=samples)\n    visualize_reconstruction(inputs, average(reconstruct()))\n    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n                             name=\"static_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n                             name=\"dynamic_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),\n                             name=\"swap_static\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),\n                             name=\"swap_dynamic\")\n\n  with tf.compat.v1.name_scope(\"generation\"):\n    generate = functools.partial(model.generate, batch_size=batch_size,\n                                 length=length, samples=samples)\n    image_summary(average(generate(fix_static=True)), \"fix_static\")\n    image_summary(average(generate(fix_dynamic=True)), \"fix_dynamic\")"}
{"prompt": "def __single_arity_fn_to_py_ast(\n    ctx: GeneratorContext,\n    node: Fn,\n    method: FnMethod,\n    def_name: Optional[str] = None,\n    meta_node: Optional[MetaNode] = None,\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a function with a single arity.\"\"\"", "output": "\n    if def_name is None:\n        def_name = node.name\n\n    if meta_node is None:\n        meta_node = node.meta_node\n\n    if method == FnMethod.CLASS_METHOD:\n        decorator_name = \"classmethod\"\n    elif method == FnMethod.STATIC_METHOD:\n        decorator_name = \"staticmethod\"\n    else:\n        decorator_name = None\n\n    if decorator_name:\n        decorator_node = ast.Name(decorator_name, ast.Load())\n        decorator_node.ctx = ast.Load()\n        decorator_node.lineno = meta_node.lineno\n       ", "solution": "\n    assert node.op == NodeOp.FN\n    assert method.op == NodeOp.FN_METHOD\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n    with ctx.new_symbol_table(py_fn_name), ctx.new_recur_point(\n        method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n    ):\n        # Allow named anonymous functions to recursively call themselves\n        if lisp_fn_name is not None:\n            ctx.symbol_table.new_symbol(\n                sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n            )\n\n        fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n            ctx, method.params, method.body\n        )\n        meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n        return GeneratedPyAST(\n            node=ast.Name(id=py_fn_name, ctx=ast.Load()),\n            dependencies=list(\n                chain(\n                    meta_deps,\n                    [\n                        py_fn_node(\n                            name=py_fn_name,\n                            args=ast.arguments(\n                                args=fn_args,\n                                kwarg=None,\n                                vararg=varg,\n                                kwonlyargs=[],\n                                defaults=[],\n                                kw_defaults=[],\n                            ),\n                            body=fn_body_ast,\n                            decorator_list=list(\n                                chain(\n                                    meta_decorators,\n                                    [_BASILISP_FN_FN_NAME],\n                                    [_TRAMPOLINE_FN_NAME]\n                                    if ctx.recur_point.has_recur\n                                    else [],\n                                )\n                            ),\n                            returns=None,\n                        )\n                    ],\n                )\n            ),\n        )"}
{"prompt": "def activate(fn=None):\n    \"\"\"\n    Enables the HTTP traffic interceptors.\n\n    This function can be used as decorator.\n\n    Arguments:\n        fn (function|coroutinefunction): Optional function argument\n            if used as decorator.\n\n    Returns:\n        function: decorator wrapper function, only if called as decorator,\n            otherwise ``None``.\n\n    Example::\n\n        # Standard use case\n        pook.activate()\n        pook.mock('server.com/foo').reply(404)\n\n        res = requests.get('server.com/foo')\n        assert res.status_code == 404\n        pook.disable()\n\n        # Decorator use case\n        @pook.activate\n        def test_request():\n            pook.mock('server.com/foo').reply(404)\n\n            res = requests.get('server.com/foo')\n            assert res.status_code == 404\n    \"\"\"", "output": "disable()\n\n        # Decorator use case\n        @pook.activate()\n        def test_foo():\n            res = requests.get('server.com/foo')\n            assert res.status_code == 404\n            pook.disable()\n\n        test_foo()\n    \"\"\"\n    if fn is None:\n        return _activate_decorator\n    else:\n        _activate()\n        return fn\n\n\ndef _activate_decorator(fn):\n    _activate()\n    return fn\n\n\ndef _activate():\n    global _pook_active\n    if _pook_active:\n        return\n    _pook_active =", "solution": "\n    # If not used as decorator, activate the engine and exit\n    if not isfunction(fn):\n        _engine.activate()\n        return None\n\n    # If used as decorator for an async coroutine, wrap it\n    if iscoroutinefunction is not None and iscoroutinefunction(fn):\n        return activate_async(fn, _engine)\n\n    @functools.wraps(fn)\n    def wrapper(*args, **kw):\n        _engine.activate()\n        try:\n            fn(*args, **kw)\n        finally:\n            _engine.disable()\n\n    return wrapper"}
{"prompt": "def parse(self, string):\n        \"\"\"\n        Parse lipid definition from string:\n\n            alhead=C P, allink=A A, altail=TCC CCCC, alname=DPSM, charge=0.0\n        \"\"\"", "output": "\n        # Split string by comma\n        parts = string.split(',')\n\n        # Check ifineable\n        if parts[0].strip() == 'alhead':\n            self.head_group = parts[1].strip()\n            self.head_name = parts[2].strip()\n        elif parts[0].strip() == 'allink':\n            self.link_group = parts[1].strip()\n            self.link_name = parts[2].strip()\n        elif parts[0].strip() == 'altail':\n            self.tail_group = parts[1].strip()\n            self.", "solution": "\n        fields = [i.split(\"=\") for i in string.split(', ')]\n        for what, val in fields:\n            what = what.strip()\n            val  = val.split()\n            if what.endswith(\"head\"):\n                self.head = val\n            elif what.endswith(\"link\"):\n                self.link = val\n            elif what.endswith(\"tail\"):\n                self.tail = val\n            elif what == \"charge\":\n                self.charge = float(val[0])\n            elif what.endswith(\"name\") and not self.name:\n                self.name = val[0]\n        if self.charge is None:\n            # Infer charge from head groups\n            self.charge = sum([headgroup_charges[bead] for bead in self.head])"}
{"prompt": "def combine_columns_aggregation(\n        df,\n        id_cols: List[str],\n        cols_for_combination: Dict[str, str],\n        agg_func: Union[str, List[str], Dict[str, str]] = 'sum'\n):\n    \"\"\"\n    Aggregates data to reproduce \"All\" category for requester\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list*): the columns id to group\n    - `cols_for_combination` (*dict*): colums corresponding to\n       the filters as key and their default value as value\n\n    *optional :*\n    - `agg_func` (*str*, *list* or *dict*): the function(s) to use for aggregating the data.\n       Accepted combinations are:\n       - string function name\n       - list of functions and/or function names, e.g. [np.sum, 'mean']\n       - dict of axis labels -> functions, function names or list of such.\n    \"\"\"", "output": " value\n    - `agg_func` (*str* or *list* or *dict*): the aggregation function\n       to apply to the columns\n\n    ### Returns\n\n    *dataframe*: the aggregated dataframe with the \"All\" category for the requester\n\n    ---\n\n    ### Example\n\n    ```python\n    import pandas as pd\n    from_data_analysis.data_processing import combine_columns_aggregation\n\n    # example data\n    df = pd.DataFrame({\n        'id': [1, 2, 3, 4, 5],\n       'requester': ['A', 'A', 'B', 'B', 'B'],", "solution": "\n    requesters_cols = list(cols_for_combination.keys())\n    requester_combination = [\n        list(item) for i in range(0, len(requesters_cols) + 1)\n        for item in itertools.combinations(requesters_cols, i)]\n    dfs_result = []\n    for comb in requester_combination:\n        df_tmp = df.groupby(id_cols + comb).agg(agg_func).reset_index()\n        for key in (set(cols_for_combination.keys()) - set(comb)):\n            df_tmp[key] = cols_for_combination[key]\n        dfs_result.append(df_tmp)\n\n    return pd.concat(dfs_result, sort=False, ignore_index=True)"}
{"prompt": "def _backup_file(self, file, patch):\n        \"\"\" Creates a backup of file \"\"\"", "output": "\n        if os.path.isfile(file):\n            backup_file = file + '.bak'\n            shutil.copyfile(file, backup_file)\n            print('Backup of {} created at {}'.format(file, backup_file))\n        else:\n            print('{} does not exist, skipping backup'.format(file))\n\n    def _restore_file(self, file, backup_file):\n        \"\"\" Restores a backup of file \"\"\"\n        if os.path.isfile(backup_file):\n            shutil.copyfile(backup_file, file)\n            print('Restored backup of {} to {}'.format(file,", "solution": "\n        dest_dir = self.quilt_pc + patch.get_name()\n        file_dir = file.get_directory()\n        if file_dir:\n            #TODO get relative path\n            dest_dir = dest_dir + file_dir\n        backup = Backup()\n        backup.backup_file(file, dest_dir, copy_empty=True)"}
{"prompt": "def delete(self, url, **kwargs):\n        \"\"\"Sends a DELETE request.\n\n        Args:\n            url(basestring): The URL of the API endpoint.\n            **kwargs:\n                erc(int): The expected (success) response code for the request.\n                others: Passed on to the requests package.\n\n        Raises:\n            ApiError: If anything other than the expected response code is\n                returned by the Webex Teams API endpoint.\n\n        \"\"\"", "output": "\n        response = self._request('DELETE', url, **kwargs)\n        if response.status_code!= kwargs.get('erc', 204):\n            raise ApiError(response)\n\n    def _request(self, method, url, **kwargs):\n        \"\"\"Sends a request to the Webex Teams API endpoint.\n\n        Args:\n            method(str): The HTTP method to use for the request.\n            url(basestring): The URL of the API endpoint.\n            **kwargs:\n                headers(dict): The headers to include in the request.\n                data(dict): The data to include in the request.", "solution": "\n        check_type(url, basestring, may_be_none=False)\n\n        # Expected response code\n        erc = kwargs.pop('erc', EXPECTED_RESPONSE_CODE['DELETE'])\n\n        self.request('DELETE', url, erc, **kwargs)"}
{"prompt": "def meteor_random_id(name=None, length=17):\n    \"\"\"Generate a new ID, optionally using namespace of given `name`.\"\"\"", "output": "\n    if name is None:\n        return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n    else:\n        return f'{name}-{meteor_random_id(length=length)}'\n\n\ndef meteor_random_string(length=17):\n    \"\"\"Generate a new random string of given `length`.\"\"\"\n    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n\n\ndef meteor_random_password(length=17):\n    \"\"\"Generate a new random password of given `length`.\"\"\"", "solution": "\n    if name is None:\n        stream = THREAD_LOCAL.alea_random\n    else:\n        stream = THREAD_LOCAL.random_streams[name]\n    return stream.random_string(length, METEOR_ID_CHARS)"}
{"prompt": "def INIT(self):\n        \"\"\"INIT state.\n\n        [:rfc:`2131#section-4.4.1`]::\n\n            The client SHOULD wait a random time between one and ten\n            seconds to desynchronize the use of DHCP at startup\n\n        .. todo::\n           - The initial delay is implemented, but probably is not in other\n             implementations. Check what other implementations do.\n        \"\"\"", "output": "\n        self.delay = random.randint(1, 10)\n        self.logger.info(\"Waiting for %d seconds before starting DHCP\", self.delay)\n        time.sleep(self.delay)\n        self.state = self.DHCP_INIT\n\n    def DHCP_INIT(self):\n        \"\"\"DHCP_INIT state.\n\n        [:rfc:`2131#section-4.4.2`]::\n\n            The client SHOULD wait a random time between one and ten\n            seconds to desynchronize the use of DHCP at startup\n\n       .. todo::\n           - The initial delay is implemented, but probably", "solution": "\n        # NOTE: in case INIT is reached from other state, initialize attributes\n        # reset all variables.\n        logger.debug('In state: INIT')\n        if self.current_state is not STATE_PREINIT:\n            self.reset()\n        self.current_state = STATE_INIT\n        # NOTE: see previous TODO, maybe this is not needed.\n        if self.delay_selecting:\n            if self.delay_before_selecting is None:\n                delay_before_selecting = gen_delay_selecting()\n            else:\n                delay_before_selecting = self.delay_before_selecting\n        else:\n            delay_before_selecting = 0\n        self.set_timeout(self.current_state,\n                         self.timeout_delay_before_selecting,\n                         delay_before_selecting)\n        if self.timeout_select is not None:\n            self.set_timeout(STATE_SELECTING,\n                             self.timeout_selecting,\n                             self.timeout_select)"}
{"prompt": "def bisect(seq, func=bool):\n\t\"\"\"\n\tSplit a sequence into two sequences:  the first is elements that\n\treturn False for func(element) and the second for True for\n\tfunc(element).\n\tBy default, func is ``bool``, so uses the truth value of the object.\n\n\t>>> is_odd = lambda n: n%2\n\t>>> even, odd = bisect(range(5), is_odd)\n\t>>> list(odd)\n\t[1, 3]\n\t>>> list(even)\n\t[0, 2, 4]\n\n\t>>> other, zeros = bisect(reversed(range(5)))\n\t>>> list(zeros)\n\t[0]\n\t>>> list(other)\n\t[4, 3, 2, 1]\n\n\t\"\"\"", "output": ", 4]\n\t\"\"\"\n\ttrue_elements = [element for element in seq if not func(element)]\n\tfalse_elements = [element for element in seq if func(element)]\n\treturn true_elements, false_elements\n\n# Example usage:\nis_odd = lambda n: n%2\neven, odd = bisect(range(5), is_odd)\nprint(list(odd)) # Output: [1, 3]\nprint(list(even)) # Output: [0, 2, 4]", "solution": "\n\tqueues = GroupbySaved(seq, func)\n\treturn queues.get_first_n_queues(2)"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        if default:\n            prompt += \" (default: %s) \" % default\n        else:\n            prompt += \": \"\n        response = input(prompt)\n        if not response:\n            return default\n        return response\n\n    def ask_for_password(self, prompt):\n        \"\"\" Ask for a password and return it as a string. \"\"\"\n        password = getpass.getpass(prompt)\n        while not password:\n            password = getpass.getpass(\"Password cannot be empty. Please try again: \")\n        return password\n\n    def ask_for_confirmation(self, prompt):\n        \"\"\" Ask for confirmation of an action and", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def check_type(self, value, attr, data):\n        \"\"\"Validates a value against the correct type of the field.\n\n        It calls ``_expected_types`` to get a list of valid types.\n\n        Subclasses can do one of the following:\n\n            1. They can override the ``valid_types`` property with a tuple with\n            the expected types for this field.\n\n            2. They can override the ``_expected_types`` method to return a\n            tuple of expected types for the field.\n\n            3. They can change ``check_type`` completely to customize\n            validation.\n\n        This method or the overrides must return the ``value`` parameter\n        untouched.\n        \"\"\"", "output": " should return a boolean indicating whether\n        the value is valid or not.\n        \"\"\"\n        expected_types = self._expected_types(attr, data)\n        return isinstance(value, expected_types)\n\n    def _expected_types(self, attr, data):\n        \"\"\"Returns a tuple of expected types for the field.\n\n        This method is called by ``check_type`` to get a list of valid types.\n\n        Subclasses can override this method to return a tuple of expected types\n        for the field.\n        \"\"\"\n        return self.valid_types\n\n    def _validate_field(self, attr, data):\n        \"\"\"", "solution": "\n        expected_types = self._expected_types()\n        if not isinstance(value, expected_types):\n            raise self._not_expected_type(\n                value, expected_types, fields=[self], field_names=attr, data=data)\n        return value"}
{"prompt": "def _get_selection_cursor(self, start, end):\n        \"\"\" Convenience method that returns a cursor with text selected between\n            the positions 'start' and 'end'.\n        \"\"\"", "output": "\n        return self.textCursor(start, end)\n\n    def _get_selection_text(self):\n        \"\"\" Convenience method that returns the selected text. \"\"\"\n        return self.textCursor().selectedText()\n\n    def _set_selection_text(self, text):\n        \"\"\" Convenience method that sets the selected text to 'text'. \"\"\"\n        self.textCursor().insertText(text)\n\n    def _get_selection_start(self):\n        \"\"\" Convenience method that returns the start position of the selected text. \"\"\"\n        return self.textCursor().selectionStart()\n\n    def _get_selection_end(self):", "solution": "\n        cursor = self._control.textCursor()\n        cursor.setPosition(start)\n        cursor.setPosition(end, QtGui.QTextCursor.KeepAnchor)\n        return cursor"}
{"prompt": "def delay(self, sender, receiver, packet, add_delay=0):\n        \"\"\"\n        bandwidths are inaccurate, as we don't account for parallel transfers here\n        \"\"\"", "output": "\n        delay_ =_.delay(sender, receiver, packet,_.bandwidth)\n        if add_delay:\n            delay_ += add_delay\n        return delay_\n\n    def_delay(self,_delay):\n        \"\"\"\n        adds delay to delay delay delay delay delay\n        \"\"\"\n        return self.delay + self._delay + self.delay + self.delay + self.delay + self.delay + self.delay + self.delay + self.delay + self.delay + self.delay + self.delay + self.delay + self", "solution": "\n        bw = min(sender.ul_bandwidth, receiver.dl_bandwidth)\n        delay = sender.base_latency + receiver.base_latency\n        delay += len(packet) / bw\n        delay += add_delay\n        return delay"}
{"prompt": "def look_ahead_match(rating, tokens):\n    \"\"\"iterate through all tokens looking for matches of cleansed tokens\n    or token regexes, skipping tokens left empty by cleansing and\n    coping with Token objects that produce multiple space-separated\n    strings when cleansed.  Yields tokens that match.\n\n    \"\"\"", "output": "\n    for token in tokens:\n        if isinstance(token, Token):\n            for match in look_ahead_match(rating, token.tokens):\n                yield match\n        else:\n            if token.strip() == \"\":\n                continue\n            if any(re.match(regex, token) for regex in rating.token_regexes):\n                yield token\n\n\ndef clean_tokens(tokens, rating):\n    \"\"\"clean and tokenize tokens, skipping tokens left empty by cleansing\n    and coping with Token objects that produce multiple space-separated\n    strings when cleansed.\n\n    \"\"\"\n    for token in tokens:\n        if isinstance(", "solution": "\n    ## this ensures that all cleansed tokens are non-zero length\n    all_mregexes = []\n    for m in rating.mentions:\n        mregexes = []\n        mpatterns = m.decode('utf8').split(' ')\n        for mpat in mpatterns:\n            if mpat.startswith('ur\"^') and mpat.endswith('$\"'): # is not regex\n                ## chop out the meat of the regex so we can reconstitute it below\n                mpat = mpat[4:-2]\n            else:\n                mpat = cleanse(mpat)\n            if mpat:\n                ## make a unicode raw string\n                ## https://docs.python.org/2/reference/lexical_analysis.html#string-literals\n                mpat = ur'^%s$' % mpat\n                logger.debug('look_ahead_match compiling regex: %s', mpat)\n                mregexes.append(re.compile(mpat, re.UNICODE | re.IGNORECASE))\n\n        if not mregexes:\n            logger.warn('got empty cleansed mention: %r\\nrating=%r' % (m, rating))\n\n        all_mregexes.append(mregexes)\n\n    ## now that we have all_mregexes, go through all the tokens\n    for i in range(len(tokens)):\n        for mregexes in all_mregexes:\n            if mregexes[0].match(tokens[i][0][0]):\n                ## found the start of a possible match, so iterate\n                ## through the tuples of cleansed strings for each\n                ## Token while stepping through the cleansed strings\n                ## for this mention.\n                m_j = 1\n                i_j = 0\n                last_token_matched = 0\n                matched = True\n                while m_j < len(mregexes):\n                    i_j += 1\n                    if i_j == len(tokens[i + last_token_matched][0]):\n                        i_j = 0\n                        last_token_matched += 1\n                        if i + last_token_matched == len(tokens):\n                            matched = False\n                            break\n                    target_token = tokens[i + last_token_matched][0][i_j]\n                    ## this next line is the actual string comparison\n                    if mregexes[m_j].match(target_token):\n                        m_j += 1\n                    elif target_token == '':\n                        continue\n                    else:\n                        matched = False\n                        break\n                if matched:\n                    ## yield each matched token only once\n                    toks = set()\n                    for j in xrange(last_token_matched + 1):\n                        toks.add(tokens[i + j][1])\n                    for tok in toks:\n                        yield tok"}
{"prompt": "def stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports):\n    \"\"\"return lines with leading/trailing whitespace and any ignored code\n    features removed\n    \"\"\"", "output": "\n    lines = [line.strip() for line in lines]\n    if ignore_comments:\n        lines = [line for line in lines if not line.startswith(\"#\")]\n    if ignore_docstrings:\n        lines = [line for line in lines if not line.startswith(\"'''\") and not line.startswith('\"\"\"')]\n    if ignore_imports:\n        lines = [line for line in lines if not line.startswith(\"import\")]\n    return lines\n\n\ndef remove_comments(lines, ignore_comments):\n    \"\"\"remove comments from lines\"\"\"\n    if ignore_comments:\n        lines = [line for line in lines", "solution": "\n    if ignore_imports:\n        tree = astroid.parse(\"\".join(lines))\n        node_is_import_by_lineno = (\n            (node.lineno, isinstance(node, (astroid.Import, astroid.ImportFrom)))\n            for node in tree.body\n        )\n        line_begins_import = {\n            lineno: all(is_import for _, is_import in node_is_import_group)\n            for lineno, node_is_import_group in groupby(\n                node_is_import_by_lineno, key=lambda x: x[0]\n            )\n        }\n        current_line_is_import = False\n\n    strippedlines = []\n    docstring = None\n    for lineno, line in enumerate(lines, start=1):\n        line = line.strip()\n        if ignore_docstrings:\n            if not docstring and any(\n                line.startswith(i) for i in ['"}
{"prompt": "def add_field(self, model, field):\n        \"\"\"Ran when a field is added to a model.\"\"\"", "output": "\n        if field.name == 'created_at':\n            model.created_at = models.DateTimeField(auto_now_add=True)\n        elif field.name == 'updated_at':\n            model.updated_at = models.DateTimeField(auto_now=True)\n        else:\n            pass\n\n    def delete_field(self, model, field):\n        \"\"\"Ran when a field is deleted from a model.\"\"\"\n        if field.name == 'created_at':\n            model.created_at = models.DateTimeField(auto_now_add=True)\n        elif field.name == 'updated_at", "solution": "\n\n        for key in self._iterate_required_keys(field):\n            self._create_hstore_required(\n                model._meta.db_table,\n                field,\n                key\n            )"}
{"prompt": "def get_organisation_information(self, query_params=None):\n        \"\"\"\n        Get information fot this organisation. Returns a dictionary of values.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n\n        url = self.api_url + \"/organisation\"\n        response = self.session.get(url, params=query_params)\n        response.raise_for_status()\n        return response.json()\n\n    def get_organisation_users(self, query_params=None):\n        \"\"\"\n        Get a list of users for this organisation. Returns a list of dictionaries, each containing information about a user.\n        \"\"\"\n        if query_params is None:\n            query_params = {}\n\n        url = self.api_url + \"/organisation/users", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )"}
{"prompt": "def verified(context, collaborator, test, outpath=None):\n    \"\"\"Export variants which have been verified for an institute\n        and write them to an excel file.\n\n    Args:\n        collaborator(str): institute id\n        test(bool): True if the function is called for testing purposes\n        outpath(str): path to output file\n\n    Returns:\n        written_files(int): number of written or simulated files\n    \"\"\"", "output": "\n   _verified =_variants.loc[\n        (\n            (\n               _variants[\"institute_id\"] == collaborator\n            )\n            & (\n                (\n                    (\n                       _variants[\"variant_status\"] == \"verified\"\n                    )\n                    | (\n                        (\n                           _variants[\"variant_status\"] == \"simulated\"\n                        )\n                        & (\n                           _variants[\"test_status\"] == \"pass\"\n                        )\n                    )\n                )\n            )\n        )\n    ]\n\n   _verified.to_excel(\n        outpath,\n        index=False,\n        sheet_name=\"Verified\",\n        columns=[", "solution": "\n    written_files = 0\n    collaborator = collaborator or 'cust000'\n    LOG.info('Exporting verified variants for cust {}'.format(collaborator))\n\n    adapter = context.obj['adapter']\n    verified_vars = adapter.verified(institute_id=collaborator)\n    LOG.info('FOUND {} verified variants for institute {}'.format(len(verified_vars), collaborator))\n\n\n    if not verified_vars:\n        LOG.warning('There are no verified variants for institute {} in database!'.format(collaborator))\n        return None\n\n    document_lines = export_verified_variants(verified_vars)\n\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n    document_name = '.'.join(['verified_variants', collaborator, today]) + '.xlsx'\n\n    # If this was a test and lines are created return success\n    if test and document_lines:\n        written_files +=1\n        LOG.info('Success. Verified variants file contains {} lines'.format(len(document_lines)))\n        return written_files\n\n    # create workbook and new sheet\n    # set up outfolder\n    if not outpath:\n        outpath = str(os.getcwd())\n    workbook = Workbook(os.path.join(outpath,document_name))\n    Report_Sheet = workbook.add_worksheet()\n\n    # Write the column header\n    row = 0\n    for col,field in enumerate(VERIFIED_VARIANTS_HEADER):\n        Report_Sheet.write(row,col,field)\n\n    # Write variant lines, after header (start at line 1)\n    for row, line in enumerate(document_lines,1): # each line becomes a row in the document\n        for col, field in enumerate(line): # each field in line becomes a cell\n            Report_Sheet.write(row,col,field)\n    workbook.close()\n\n    if os.path.exists(os.path.join(outpath,document_name)):\n        LOG.info('Success. Verified variants file of {} lines was written to disk'. format(len(document_lines)))\n        written_files += 1\n\n    return written_files"}
{"prompt": "def _plot_result(self, traj, result_name):\n        \"\"\"Plots a state variable graph for several neurons into one figure\"\"\"", "output": "\n       a_ =a.get_units(self.model,a.units.voltage)\n       a_spikes =a.get_units(self.model,a.units.spike_count)\n       a_ =a.get_units(self.model,a.units.spike_times)\n\n       a.plot_state_variables(\n           a_ut,a_spikes,a_ut,a_spikes,a_ut,a_spikes,a_ut,a", "solution": "\n        result = traj.f_get(result_name)\n        varname = result.record_variables[0]\n        values = result[varname]\n        times = result.t\n\n        record = result.record\n\n        for idx, celia_neuron in enumerate(record):\n            plt.subplot(len(record), 1, idx+1)\n            plt.plot(times, values[idx,:])\n            if idx==0:\n                plt.title('%s' % varname)\n            if idx==1:\n                plt.ylabel('%s' % ( varname))\n            if idx == len(record)-1:\n                plt.xlabel('t')"}
{"prompt": "def unregister_sizer(self, attr_name):\n        \"\"\"\n        Unregister the SizedImage subclass currently assigned to `attr_name`.\n\n        If a SizedImage subclass isn't already registered to `attr_name`\n        NotRegistered will raise.\n        \"\"\"", "output": "\n        if attr_name not in self._sizer_classes:\n            raise NotRegistered(f\"SizedImage subclass not registered for attribute {attr_name}\")\n        del self._sizer_classes[attr_name]\n\n    def _get_sizer_class(self, attr_name):\n        \"\"\"\n        Get the SizedImage subclass currently assigned to `attr_name`.\n\n        If a SizedImage subclass isn't registered to `attr_name`\n        NotRegistered will raise.\n        \"\"\"\n        if attr_name not in self._sizer_classes:\n            raise NotRegistered(f\"SizedImage subclass not registered for", "solution": "\n        if attr_name not in self._sizedimage_registry:\n            raise NotRegistered(\n                'No SizedImage subclass is registered to %s' % attr_name\n            )\n        else:\n            del self._sizedimage_registry[attr_name]"}
{"prompt": "def assert_no_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"", "output": "\n        query = TitleQuery(title, **kwargs)\n        return self.assert_no_element_by_query(query)\n\n    def assert_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page has the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n        query = TitleQuery(title, **kwargs)\n        return self", "solution": "\n\n        query = TitleQuery(title, **kwargs)\n\n        @self.synchronize(wait=query.wait)\n        def assert_no_title():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n            return True\n\n        return assert_no_title()"}
{"prompt": "def __fetch(self, url, payload):\n        \"\"\"Fetch requests from groupsio API\"\"\"", "output": "\n        headers = {'Authorization': f'Bearer {self.token}'}\n        response = requests.post(url, headers=headers, json=payload)\n        response.raise_for_status()\n        return response.json()\n\n    def get_groups(self):\n        \"\"\"Fetch all groups from groupsio API\"\"\"\n        url = f'{self.base_url}/groups'\n        payload = {'limit': 100}\n        groups = []\n        while True:\n            response = self.__fetch(url, payload)\n            groups.extend(response['groups'])\n            if response['next'] is None:\n                break", "solution": "\n\n        r = requests.get(url, params=payload, auth=self.auth, verify=self.verify)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise e\n\n        return r"}
{"prompt": "def f_shrink(self, force=False):\n        \"\"\" Shrinks the trajectory and removes all exploration ranges from the parameters.\n        Only possible if the trajectory has not been stored to disk before or was loaded as new.\n\n        :param force:\n\n            Usually you cannot shrink the trajectory if it has been stored to disk,\n            because there's no guarantee that it is actually shrunk if there\n            still exist explored parameters on disk. In case you are certain that\n            you did not store explored parameters to disk set or you deleted all\n            of them from disk set `force=True`.\n\n        :raises: TypeError if the trajectory was stored before.\n\n        \"\"\"", "output": " the trajectory has already been shrunk or if it has been stored to disk.\n        \"\"\"\n        if self._shrunk:\n            raise TypeError(\"The trajectory has already been shrunk.\")\n        if self._stored_to_disk:\n            raise TypeError(\"The trajectory has been stored to disk and cannot be shrunk.\")\n\n        self._shrunk = True\n        self._exploration_ranges = []\n        self._____exploration_ranges = []\n        self.__exploration_ranges = []\n        self._exploration_ranges__exploration_ranges =", "solution": "\n        if self._stored and not force:\n            raise TypeError('Your trajectory is already stored to disk or database, shrinking is '\n                            'not allowed.')\n\n        for param in self._explored_parameters.values():\n            param.f_unlock()\n            try:\n                param._shrink()\n            except Exception as exc:\n                self._logger.error('Could not shrink `%s` because of:`%s`' %\n                                   (param.v_full_name, repr(exc)))\n\n        # If we shrink, we do not have any explored parameters left and we can erase all\n        # run information, and the length of the trajectory is 1 again.\n        self._explored_parameters = {}\n        self._run_information = {}\n        self._single_run_ids = {}\n        self._add_run_info(0)\n        self._test_run_addition(1)"}
{"prompt": "def get_data_disk(self, service_name, deployment_name, role_name, lun):\n        \"\"\"\n        Retrieves the specified data disk from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            The Logical Unit Number (LUN) for the disk.\n        \"\"\"", "output": "\n        return self._get_disk(service_name, deployment_name, role_name, lun, 'data')\n\n    def get_os_disk(self, service_name, deployment_name, role_name):\n        \"\"\"\n        Retrieves the OS disk for a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        \"\"\"\n        return self._get_disk(service_name, deployment_name, role_name, 0, 'os')\n\n    def _get", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_get(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            DataVirtualHardDisk)"}
{"prompt": "def for_type(typ, func):\n    \"\"\"\n    Add a pretty printer for a given type.\n    \"\"\"", "output": "\n    def pretty_print(obj, ctx):\n        if isinstance(obj, typ):\n            return func(obj, ctx)\n        else:\n            return None\n    return pretty_print\n\n\ndef pretty_print_list(lst, ctx):\n    \"\"\"\n    Pretty print a list of objects.\n    \"\"\"\n    if not lst:\n        return \"[]\"\n    else:\n        return \"[\" + \", \".join(pretty_print(x, ctx) for x in lst) + \"]\"\n\n\ndef pretty_print_dict(dct, ctx):\n    \"\"\"\n    Pretty print a dictionary of objects.\n    \"\"\"\n    if not dct", "solution": "\n    oldfunc = _type_pprinters.get(typ, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _type_pprinters[typ] = func\n    return oldfunc"}
{"prompt": "def structured_mesh(shape = (2,2,2), dim = (1.,1.,1.)):\n    \"\"\"\n    Returns a structured mesh. \n    \n    :arg shape: 2 or 3 integers (eg: shape = (10, 10, 10)).\n    :type shape: tuple\n    :arg dim: 2 or 3 floats (eg: dim = (4., 2., 1.))\n    :type dim: tuple\n    \n    .. note::\n\n       This function does not use GMSH for mesh generation.\n\n\n    \n    >>> import argiope as ag\n    >>> mesh = ag.mesh.structured_mesh(shape =(10,10,10), dim=(1.,1.,1.)))\n    \"\"\"", "output": ".structured_mesh((10, 10, 10), (4., 2., 1.))\n    >>> print(mesh.nodes)\n    [[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n     [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n     [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n    ", "solution": "\n    # PREPROCESSING\n    shape = np.array(shape)\n    dim   = np.array(dim) \n    Ne = shape.prod()\n    Nn = (shape + 1).prod()\n    # LABELS\n    nindex = np.arange(Nn) + 1\n    eindex = np.arange(Ne) + 1\n    # COORDINATES\n    coords = [ np.linspace(0., dim[i], shape[i] + 1) for i in range(len(shape))]\n    coords = np.array(np.meshgrid(*coords))\n    coords = np.array([c.swapaxes(0,1).flatten(\"F\") for c in coords]).T\n    if len(shape) == 2:\n        c = coords\n        coords = np.zeros((Nn, 3))\n        coords[:, :2] = c  \n    # CONNECTIVITY    \n    conn = _make_conn(shape)\n    # MESH INSTANCE\n    mesh = Mesh(nlabels = nindex,\n                coords  = coords,\n                elabels = eindex,\n                conn = conn,)\n    if len(shape) == 2: mesh.elements[(\"type\", \"argiope\")] = \"quad4\"\n    if len(shape) == 3: mesh.elements[(\"type\", \"argiope\")] = \"hexa8\"    \n    return mesh"}
{"prompt": "def inshape(shape, axes):\n    \"\"\"\n    Checks to see if a list of axes are contained within an array shape.\n\n    Parameters\n    ----------\n    shape : tuple[int]\n        the shape of a BoltArray\n\n    axes : tuple[int]\n        the axes to check against shape\n    \"\"\"", "output": "\n    for axis in axes:\n        if axis >= len(shape):\n            return False\n    return True\n\n\ndef_shape(shape,_shape):\n    \"\"\"\n    Checks to see if a list of axes are contained within an array shape.\n\n    Parameters\n    ----------\n    shape : tuple[int]\n        the shape of a BoltArray\n\n    axes : tuple[int]\n        the axes to check against shape\n    \"\"\"\n    for axis in range(len(shape)):\n        if shape[axis]!=_shape[axis]:\n            return False\n    return True\n\n\ndef_shape(shape,_shape):", "solution": "\n    valid = all([(axis < len(shape)) and (axis >= 0) for axis in axes])\n    if not valid:\n        raise ValueError(\"axes not valid for an ndarray of shape: %s\" % str(shape))"}
{"prompt": "def desc(value):\n    \"\"\" str: A normalized representation for a user-provided value. \"\"\"", "output": "\n    return str(value).lower()\n\n\ndef normalize_value(value):\n    \"\"\"\n    str: A normalized representation for a user-provided value.\n\n    Normalization is done by converting the value to lowercase and removing\n    any non-alphanumeric characters.\n    \"\"\"\n    return re.sub(r'[^a-zA-Z0-9]', '', str(value).lower())\n\n\ndef normalize_values(values):\n    \"\"\"\n    List[str]: A list of normalized representations for user-provided values.\n\n    Normalization is done by converting each value to lowercase and removing\n    any non-alphanumeric characters.", "solution": "\n\n    def normalize_strings(value):\n        if isinstance(value, list):\n            value = [normalize_strings(e) for e in value]\n\n        if isinstance(value, dict):\n            value = {normalize_strings(k): normalize_strings(v) for k, v in iter(value.items())}\n\n        if isregex(value):\n            value = value.pattern\n\n        if isbytes(value):\n            value = decode_bytes(value)\n\n        if PY2:\n            if isstring(value):\n                # In Python 2, strings (``unicode`` objects) represent as ``u'...'``, so ensure\n                # the string is encoded (as a ``str`` object) for cleaner representation.\n                value = encode_string(value)\n\n        return value\n\n    value = normalize_strings(value)\n\n    return repr(value)"}
{"prompt": "def recommend_k_items_slow(self, test, top_k=10, remove_seen=True):\n        \"\"\"Recommend top K items for all users which are in the test set.\n\n        Args:\n            test: test Spark dataframe\n            top_k: top n items to return\n            remove_seen: remove items test users have already seen in the past from the recommended set.\n        \"\"\"", "output": "\n       _seen = test.select(\"user_id\").distinct().rdd.flatMap(lambda x: x).collect()\n       _seen_set = set(map(int, o.item_id for o in test.select(\"item_id\").rdd.flatMap(lambda x: x).collect()))\n\n       _users = test.select(\"user_id\").distinct().rdd.flatMap(lambda x: x).collect()\n       _users_set = set(map(int, o.user_id for o in test.select(\"user_id\").rdd.flatMap(lambda x:", "solution": "\n\n        # TODO: remove seen\n        if remove_seen:\n            raise ValueError(\"Not implemented\")\n\n        self.get_user_affinity(test)\\\n            .write.mode(\"overwrite\")\\\n            .saveAsTable(self.f(\"{prefix}user_affinity\"))\n\n        # user_affinity * item_similarity\n        # filter top-k\n        query = self.f(\n            "}
{"prompt": "def basis_state(str_state, num):\n    \"\"\"\n    Return a basis state ndarray.\n\n    Args:\n        str_state (string): a string representing the state.\n        num (int): the number of qubits\n    Returns:\n        ndarray:  state(2**num) a quantum state with basis basis state.\n     Raises:\n        QiskitError: if the dimensions is wrong\n    \"\"\"", "output": "\n    if len(str_state)!= 2**num:\n        raise QiskitError(\"The dimensions is wrong\")\n    state = np.zeros(2**num)\n    for i in range(2**num):\n        if str_state[i] == '1':\n            state[i] = 1\n    return state\n\n\ndef state_vector(str_state, num):\n    \"\"\"\n    Return a state vector ndarray.\n\n    Args:\n        str_state (string): a string representing the state.\n        num (int): the number of qubits\n    Returns:\n        ndarray:  state(2", "solution": "\n    n = int(str_state, 2)\n    if num >= len(str_state):\n        state = np.zeros(1 << num, dtype=complex)\n        state[n] = 1\n        return state\n    else:\n        raise QiskitError('size of bitstring is greater than num.')"}
{"prompt": "def _sparse_tensor_dense_matmul(sp_a, b, **kwargs):\n  \"\"\"Returns (batched) matmul of a SparseTensor with a Tensor.\n\n  Args:\n    sp_a: `SparseTensor` representing a (batch of) matrices.\n    b: `Tensor` representing a (batch of) matrices, with the same batch shape of\n      `sp_a`. The shape must be compatible with the shape of `sp_a` and kwargs.\n    **kwargs: Keyword arguments to `tf.sparse_tensor_dense_matmul`.\n\n  Returns:\n    product: A dense (batch of) matrix-shaped Tensor of the same batch shape and\n    dtype as `sp_a` and `b`. If `sp_a` or `b` is adjointed through `kwargs` then\n    the shape is adjusted accordingly.\n  \"\"\"", "output": ") matrix, with the same batch shape as `sp_a`.\n  \"\"\"\n _shape =_shape_from_sparse_tensor(sp_a)\n _shape =_shape.concatenate(b.shape)\n _shape =_shape.concatenate(kwargs.get(\"transpose_a\", False))\n _shape =_shape.concatenate(kwargs.get(\"transpose_b\", False))\n _shape =_shape.concatenate(kwargs.get(\"adjoint_a\", False))\n _shape =_shape.concatenate(kwargs.get", "solution": "\n  batch_shape = _get_shape(sp_a)[:-2]\n\n  # Reshape the SparseTensor into a rank 3 SparseTensors, with the\n  # batch shape flattened to a single dimension. If the batch rank is 0, then\n  # we add a batch dimension of rank 1.\n  sp_a = tf.sparse.reshape(sp_a, tf.concat([[-1], _get_shape(sp_a)[-2:]],\n                                           axis=0))\n  # Reshape b to stack the batch dimension along the rows.\n  b = tf.reshape(b, tf.concat([[-1], _get_shape(b)[-1:]], axis=0))\n\n  # Convert the SparseTensor to a matrix in block diagonal form with blocks of\n  # matrices [M, N]. This allow us to use tf.sparse_tensor_dense_matmul which\n  # only accepts rank 2 (Sparse)Tensors.\n  out = tf.sparse.sparse_dense_matmul(_sparse_block_diag(sp_a), b, **kwargs)\n\n  # Finally retrieve the original batch shape from the resulting rank 2 Tensor.\n  # Note that we avoid inferring the final shape from `sp_a` or `b` because we\n  # might have transposed one or both of them.\n  return tf.reshape(\n      out,\n      tf.concat([batch_shape, [-1], _get_shape(out)[-1:]], axis=0))"}
{"prompt": "def require_parents(packages):\n        \"\"\"\n        Exclude any apparent package that apparently doesn't include its\n        parent.\n\n        For example, exclude 'foo.bar' if 'foo' is not present.\n        \"\"\"", "output": "\n        for package in packages:\n            if '.' in package:\n                parent = package.rsplit('.', 1)[0]\n                if parent not in packages:\n                    packages.remove(package)\n        return packages\n\n    def find_packages(path, exclude=None):\n        \"\"\"\n        Find all Python packages in a given directory.\n\n        Exclude any packages that appear to be missing their parent.\n        \"\"\"\n        packages = []\n        for root, dirs, files in os.walk(path):\n            if exclude and any(root.startswith(exclude) for exclude in exclude):\n                continue\n            for filename in files:\n                if filename.endswith", "solution": "\n        found = []\n        for pkg in packages:\n            base, sep, child = pkg.rpartition('.')\n            if base and base not in found:\n                continue\n            found.append(pkg)\n            yield pkg"}
{"prompt": "def make_report(self,traceback):\n        \"\"\"Return a string containing a crash report.\"\"\"", "output": "\n        report = traceback.format_exc()\n        report = report.replace(\"\\n\", \"\\n\\t\")\n        return report\n\n    def handle_exception(self, request, exception):\n        \"\"\"Handle an exception by generating a crash report and sending it to the user.\"\"\"\n        report = self.make_report(exception)\n        self.send_crash_report(request, report)\n\n    def send_crash_report(self, request, report):\n        \"\"\"Send a crash report to the user.\"\"\"\n        subject = \"Crash Report from %s\" % request.META.get(\"HTTP_HOST\")\n        message = \"", "solution": "\n\n        sec_sep = self.section_sep\n\n        report = ['*'*75+'\\n\\n'+'IPython post-mortem report\\n\\n']\n        rpt_add = report.append\n        rpt_add(sys_info())\n\n        try:\n            config = pformat(self.app.config)\n            rpt_add(sec_sep)\n            rpt_add('Application name: %s\\n\\n' % self.app_name)\n            rpt_add('Current user configuration structure:\\n\\n')\n            rpt_add(config)\n        except:\n            pass\n        rpt_add(sec_sep+'Crash traceback:\\n\\n' + traceback)\n\n        return ''.join(report)"}
{"prompt": "def _fmt(self, string):\n        \"\"\"Helper method to format our name into a string.\"\"\"", "output": "\n        return f\"{self.name} - {self.age} years old\"\n\n    def __str__(self):\n        \"\"\"Return a string representation of our object.\"\"\"\n        return self._fmt()\n\n\n# Example usage:\nperson1 = Person(\"Alice\", 25)\nprint(person1)  # Output: Alice - 25 years old\n", "solution": "\n        if '{' not in string:\n            string = '{}.' + string\n        return string.format(self.name)"}
{"prompt": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake MNIST-style data for unit testing.\"\"\"", "output": "\n  def input_fn():\n    return tf.data.Dataset.from_tensor_slices(\n        (tf.random.uniform((batch_size, 28, 28, 1)),\n         tf.random.uniform((batch_size, 10))))\n\n  return input_fn\n\n\ndef test______ net():\n  \"\"\"Tests_ut_net.\"\"\"\n _ut_net =_ut_net__ ut_net()\n _ut_net.build(input_shape=(None, 28, ", "solution": "\n  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype(\"float32\")\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn"}
{"prompt": "def marker(self, *args):\n        \"\"\"\n        Defines markers one at a time for your graph\n        args are of the form::\n            <marker type>,\n            <color>,\n            <data set index>,\n            <data point>,\n            <size>,\n            <priority>\n        see the official developers doc for the complete spec\n        APIPARAM: chm\n        \"\"\"", "output": "\n        =_marker(*args)\n      .set_marker(self.marker_type)\n        self.markers.append(Marker(self.marker_type, *args))\n        return self\n\n    def(self,_type,_args=None):\n        \"\"\"\n        Adds a new to the\n        APIPARAM: test_type, test_args\n        \"\"\"\n        if test_args is None:\n            test_args = {}\n        test =_test(test_type, test_args)\n        self.tests.append(test)\n        return self\n\n    def(self", "solution": "\n        if len(args[0]) == 1:\n            assert args[0] in MARKERS, 'Invalid marker type: %s'%args[0]\n        assert len(args) <= 6, 'Incorrect arguments %s'%str(args)\n        args = color_args(args, 1)\n        self.markers.append(','.join(map(str,args)) )\n        return self"}
{"prompt": "def camera_event_motion_enum(self, camera_id, **kwargs):\n        \"\"\"Return motion settings matching camera_id.\"\"\"", "output": "\n        return self.motion_settings[camera_id]\n\n    def camera_event_motion_set(self, camera_id,_enable,_timeout,_interval):\n        \"\"\"Set motion settings for camera_id.\"\"\"\n        self.motion_settings[camera_id] = (in_enable, in_timeout, in_interval)\n\n    def camera_event_motion_get(self, camera_id):\n        \"\"\"Return motion settings for camera_id.\"\"\"\n        return self.motion_settings[camera_id]\n\n    def camera_event_motion_delete(self, camera_id", "solution": "\n        api = self._api_info['camera_event']\n        payload = dict({\n            '_sid': self._sid,\n            'api': api['name'],\n            'method': 'MotionEnum',\n            'version': api['version'],\n            'camId': camera_id,\n        }, **kwargs)\n        response = self._get_json_with_retry(api['url'], payload)\n\n        return MotionSetting(camera_id, response['data']['MDParam'])"}
{"prompt": "def register(self, managed_object):\n        \"\"\"\n        Register a managed object with a KMIP appliance.\n\n        Args:\n            managed_object (ManagedObject): A managed object to register. An\n                instantiatable subclass of ManagedObject from the Pie API.\n\n        Returns:\n            string: The uid of the newly registered managed object.\n\n        Raises:\n            ClientConnectionNotOpen: if the client connection is unusable\n            KmipOperationFailure: if the operation result is a failure\n            TypeError: if the input argument is invalid\n        \"\"\"", "output": "\n        if not isinstance(managed_object, ManagedObject):\n            raise TypeError(\"Input argument must be a subclass of ManagedObject\")\n\n        if not self._client.is_open():\n            raise ClientConnectionNotOpen(\"Client connection is not open\")\n\n        try:\n            result = self._client.register(managed_object.to_dict())\n        except KmipOperationFailure as e:\n            raise KmipOperationFailure(e)\n\n        if result.result_status!= ResultStatus.SUCCESS:\n            raise KmipOperationFailure(result.result_status)\n\n        return result.unique_identifier\n\n    def unregister(self, unique", "solution": "\n        # Check input\n        if not isinstance(managed_object, pobjects.ManagedObject):\n            raise TypeError(\"managed object must be a Pie ManagedObject\")\n\n        # Extract and create attributes\n        object_attributes = list()\n\n        if hasattr(managed_object, 'cryptographic_usage_masks'):\n            if managed_object.cryptographic_usage_masks is not None:\n                mask_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_USAGE_MASK,\n                    managed_object.cryptographic_usage_masks\n                )\n                object_attributes.append(mask_attribute)\n        if hasattr(managed_object, 'operation_policy_name'):\n            if managed_object.operation_policy_name is not None:\n                opn_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.OPERATION_POLICY_NAME,\n                    managed_object.operation_policy_name\n                )\n                object_attributes.append(opn_attribute)\n        if hasattr(managed_object, 'names'):\n            if managed_object.names:\n                for name in managed_object.names:\n                    name_attribute = self.attribute_factory.create_attribute(\n                        enums.AttributeType.NAME,\n                        name\n                    )\n                    object_attributes.append(name_attribute)\n\n        template = cobjects.TemplateAttribute(attributes=object_attributes)\n        object_type = managed_object.object_type\n\n        # Register the managed object and handle the results\n        secret = self.object_factory.convert(managed_object)\n        result = self.proxy.register(object_type, template, secret)\n\n        status = result.result_status.value\n        if status == enums.ResultStatus.SUCCESS:\n            return result.uuid\n        else:\n            reason = result.result_reason.value\n            message = result.result_message.value\n            raise exceptions.KmipOperationFailure(status, reason, message)"}
{"prompt": "def visit_return(self, node):\n        \"\"\"1 - check is the node has a right sibling (if so, that's some\n        unreachable code)\n        2 - check is the node is inside the finally clause of a try...finally\n        block\n        \"\"\"", "output": "\n        if node.next_sibling is not None:\n            self.add_message(\"unreachable\", node=node)\n        if self.in_finally:\n            self.add_message(\"unreachable\", node=node)\n\n    def visit_raise(self, node):\n        \"\"\"1 - check if the node is inside the finally clause of a try...finally\n        block\n        \"\"\"\n        if self.in_finally:\n            self.add_message(\"unreachable\", node=node)\n\n    def visit_try(self, node):\n        \"\"\"1 - check if the node is inside a try...finally block\n        \"\"\"\n        self.", "solution": "\n        self._check_unreachable(node)\n        # Is it inside final body of a try...finally bloc ?\n        self._check_not_in_finally(node, \"return\", (astroid.FunctionDef,))"}
{"prompt": "def read(self, size=-1):\n        \"\"\"\n        read([size]) -> read at most size bytes, returned as a string.\n\n        If the size argument is negative or omitted, read until EOF is reached.\n        Notice that when in non-blocking mode, less data than what was\n        requested may be returned, even if no size parameter was given.\n        \"\"\"", "output": "\n        if self.closed:\n            raise IOError(\"I/O operation on closed file\")\n        if size < 0:\n            size = None\n        return self._sock.recv(size)\n\n    def readline(self, size=-1):\n        \"\"\"\n        readline([size]) -> read a line from the file, returned as a string.\n\n        If the size argument is negative or omitted, read until EOF is reached.\n        The line separator is not included in the returned string.\n        \"\"\"\n        if self.closed:\n            raise IOError(\"I/O operation on closed file\")\n        if size < 0:\n            size", "solution": "\n        if self.left is not None:\n            size = min(size, self.left)\n        if self.closed:\n            raise ValueError('I/O operation on closed file')\n        if size < 0:\n            return ''.join(self)\n        elif not size:\n            chunk = ''\n        elif self.buf:\n            chunk = self.buf\n            self.buf = None\n        else:\n            try:\n                chunk = next(self.iterator)\n            except StopIteration:\n                return ''\n        if len(chunk) > size:\n            self.buf = chunk[size:]\n            chunk = chunk[:size]\n        if self.left is not None:\n            self.left -= len(chunk)\n        return chunk"}
{"prompt": "def balanced_rows(n, iterable, fillvalue=None):\n\t\"\"\"\n\tLike grouper, but balance the rows to minimize fill per row.\n\tbalanced_rows(3, 'ABCDEFG', 'x') --> ABC DEx FGx\"\n\t\"\"\"", "output": "\n\trows = []\n\tfor i in range(n):\n\t\trows.append([])\n\tfor item in iterable:\n\t\trows[i % n].append(item)\n\treturnlify(rows, fillvalue)\n\ndeflify(rows, fillvalue):\n\t\"\"\"\n\tReplaces with inpleballspleballsplepleballspleballs\n\t\"\"\"\n\tfor row in rows:\n\t\tfor i, item in enumerate(row):\n\t\t\tif item == fillvalue:\n\t\t\t\trow[i]", "solution": "\n\titerable, iterable_copy = itertools.tee(iterable)\n\tcount = len(tuple(iterable_copy))\n\tfor allocation in partition_items(count, n):\n\t\trow = itertools.islice(iterable, allocation)\n\t\tif allocation < n:\n\t\t\trow = itertools.chain(row, [fillvalue])\n\t\tyield tuple(row)"}
{"prompt": "def timetree_likelihood(self):\n        \"\"\"\n        Return the likelihood of the data given the current branch length in the tree\n        \"\"\"", "output": "\n        if self.is_leaf():\n            return self.likelihood\n        else:\n            = self.children[0]\n           _likelihood = self.children[0].timetree_likelihood()\n           _likelihood = self.children[1].timetree_likelihood()\n           _likelihood += self.children[0].timetree_likelihood()\n            return_likelihood - (self.children[0].timetree_likelihood() + self.children[1].timetree_likelihood()) / 2\n\n    def timetree_prior(self):\n        \"\"\"\n        Return the prior probability of the current branch length in", "solution": "\n        LH = 0\n        for node in self.tree.find_clades(order='preorder'):  # sum the likelihood contributions of all branches\n            if node.up is None: # root node\n                continue\n            LH -= node.branch_length_interpolator(node.branch_length)\n\n        # add the root sequence LH and return\n        if self.aln:\n            LH += self.gtr.sequence_logLH(self.tree.root.cseq, pattern_multiplicity=self.multiplicity)\n        return LH"}
{"prompt": "def read(*paths):\n    \"\"\"Build a file path from *paths* and return the contents.\"\"\"", "output": "\n    with open(os.path.join(*paths), 'r') as f:\n        return f.read()\n\nsetup(\n    name='im',\n    version='0.1.0',\n    description='Alog viewer for thelog.com platform',\n    long_description=read('README.rst'),\n    author='im',\n    author_email='<EMAIL>',\n    url='https://github.com/im/mutim',\n    packages=find_packages(),\n    install_requires=[\n        'beautifulsoup4',\n       'requests',\n        'pytz',\n        '", "solution": "\n    filename = os.path.join(*paths)\n    with codecs.open(filename, mode='r', encoding='utf-8') as handle:\n        return handle.read()"}
{"prompt": "def bootstrap(self, path_or_uri):\n        \"\"\" Initialize a database.\n\n        :param database_path: The absolute path to the database to initialize.\n        \"\"\"", "output": "\n        if not os.path.exists(path_or_uri):\n            raise ValueError(\"Database path does not exist: {}\".format(path_or_uri))\n\n        self.path = path_or_uri\n        self.conn = sqlite3.connect(self.path)\n        self.conn.row_factory = sqlite3.Row\n        self.cursor = self.conn.cursor()\n\n        # Create table if it doesn't exist\n        self.cursor.execute('''CREATE TABLE IF NOT EXISTS (\n            id INTEGER PRIMARY KEY,\n           _id INTEGER,\n           _name TEXT,\n           _description", "solution": "\n        _logger.debug(\"Bootstrapping new database: %s\", path_or_uri)\n        self.database_uri = _urify_db(path_or_uri)\n        db = sa.create_engine(self.database_uri)\n        Base.metadata.create_all(db)"}
{"prompt": "def apply_mirroring(storage, debug):\n    \"\"\"Applies L4: mirroring\n\n    See: http://unicode.org/reports/tr9/#L4\n\n    \"\"\"", "output": "\n    = = storage.read()\n   \n\n   \n    =.replace(u'\\u202A', u'\\u202B')\n    =.replace(u'\\u202C', u'\\u202D')\n    =.replace(u'\\u202E', u'\\u202F')\n\n    =.replace(u'\\u2066', u'\\u2067')\n    =.replace(u'\\u2068', u'\\u2069')", "solution": "\n    # L4. A character is depicted by a mirrored glyph if and only if (a) the\n    # resolved directionality of that character is R, and (b) the\n    # Bidi_Mirrored property value of that character is true.\n    for _ch in storage['chars']:\n        unichar = _ch['ch']\n        if mirrored(unichar) and \\\n                _embedding_direction(_ch['level']) == 'R':\n            _ch['ch'] = MIRRORED.get(unichar, unichar)\n\n    if debug:\n        debug_storage(storage)"}
{"prompt": "def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool:\n    \"\"\"return True if <name> is a method overridden from an ancestor\"\"\"", "output": "\n    if not class_node.is_class():\n        return False\n    for base in class_node.bases:\n        if isinstance(base, astroid.node_classes.ClassDef):\n            if_method_name(base, name):\n                return True\n    return False\n\n\ndef is_method_defined_in_ancestor(class_node: astroid.node_classes.NodeNG, name: str) -> bool:\n    \"\"\"return True if <name> is a method defined in an ancestor\"\"\"\n    if not class_node.is_class():\n        return False\n    for base in class_", "solution": "\n    for ancestor in class_node.ancestors():\n        if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef):\n            return True\n    return False"}
{"prompt": "def _add_empty_child(self, parent, code, final=False):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 \u0441 \u043a\u043e\u0434\u043e\u043c code\n        \"\"\"", "output": "\n        child =.Node(code)\n        if final:\n            child.final = True\n        parent.children.append(child)\n        return child\n\n    def _add_child(self, parent, code,_index,_count,_final=False):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 \u0441 \u043a\u043e\u0434\u043e\u043c code\n        \"\"\"\n        child = self._add_empty_child(parent, code, final=True)\n        child._index = me_index\n        child.me_count = me_count\n        if", "solution": "\n        self.graph[parent][code] = self.nodes_number\n        self.graph.append(self._make_default_node())\n        self.data.append(None)\n        self.final.append(final)\n        self.nodes_number += 1\n        return (self.nodes_number - 1)"}
{"prompt": "def _check_year(year, month, error, error_msg):\n    \"\"\"Checks that the year is within 50 years from now.\"\"\"", "output": "\n    if year < 2020:\n        raise ValueError(f\"{error_msg} year must be within 50 years from now.\")\n\n\ndef _check_month(month, error, error_msg):\n    \"\"\"Checks that the month is valid.\"\"\"\n    if month < 1 or month > 12:\n        raise ValueError(f\"{error_msg} month must be between 1 and 12.\")\n\n\ndef _check_day(day, month, year, error, error_msg):\n    \"\"\"Checks that the day is valid for the given month and year.\"\"\"\n    if month", "solution": "\n    if year not in xrange((now.year - 50), (now.year + 51)):\n        year = now.year\n        month = now.month\n        error = error_msg\n    return year, month, error"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n      , n = self.shape\n       _prime = n\n       _prime_prime = n\n       _prime_prime_prime = n\n        =_prime_prime_prime = 1\n       _prime = n\n       _prime_prime = n\n       _prime_prime_prime = n\n        =_prime_prime_prime = 1\n       _prime = n\n       _prime_prime = n\n       _prime_prime_prime = n\n        = 1\n       _prime = n\n       _prime_prime = n\n       _", "solution": "\n        din, dout = self.dim\n        dtr = self._data[0].shape[0] // dout\n        stine = [None, None]\n        for i, mat in enumerate(self._data):\n            if mat is not None:\n                stine[i] = np.reshape(\n                    np.transpose(np.reshape(mat, (dout, dtr, din)), (2, 1, 0)),\n                    (din * dtr, dout))\n        return Stinespring(\n            tuple(stine),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def newick(self):\n        \"\"\"The representation of the Node in Newick format.\"\"\"", "output": "\n        if self.is_root():\n            return self.name\n        else:\n            return f\"{self.name}({self.children[0].newick()}, {self.children[1].newick()})\"\n\n    def is_root(self):\n        \"\"\"Check if the Node is the root of the tree.\"\"\"\n        return self.parent is None\n\n    def is_leaf(self):\n        \"\"\"Check if the Node is a leaf node.\"\"\"\n        return len(self.children) == 0\n\n    def is_internal(self):\n        \"\"\"Check if the Node is an internal node.\"\"\"\n        return not self.is", "solution": "\n        label = self.name or ''\n        if self._length:\n            label += ':' + self._length\n        descendants = ','.join([n.newick for n in self.descendants])\n        if descendants:\n            descendants = '(' + descendants + ')'\n        return descendants + label"}
{"prompt": "async def get_postcode_random() -> Postcode:\n    \"\"\"\n    Gets a random postcode object..\n    Acts as a middleware between us and the API, caching results.\n    :return: The PostCode object else None if the postcode does not exist.\n    \"\"\"", "output": "\n    postcode_cache = await get_postcode_cache()\n    if not postcode_cache:\n        return None\n\n    postcode = random.choice(postcode_cache)\n    return postcode\n\n\nasync def get_postcode_cache() -> List[Postcode]:\n    \"\"\"\n    Gets the cached postcode list from the database.\n    :return: The list of postcode objects.\n    \"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(POSTCODE_CACHE_URL) as response:\n            if response.status!= 200:\n                return []\n\n            postcode", "solution": "\n    try:\n        postcode = await fetch_postcode_random()\n    except (ApiError, CircuitBreakerError):\n        raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n\n    if postcode is not None:\n        postcode.save()\n    return postcode"}
{"prompt": "def tasks(self, from_date=DEFAULT_DATETIME):\n        \"\"\"Retrieve tasks.\n\n        :param from_date: retrieve tasks that where updated from that date;\n            dates are converted epoch time.\n        \"\"\"", "output": "\n        tasks = []\n        for task in self._client.get_tasks(from_date=from_date):\n            tasks.append(Task(self._client, task))\n        return tasks\n\n    def create_task(self, name, description=None, due_date=None, priority=None,\n                    tags=None):\n        \"\"\"Create a new task.\n\n        :param name: the name of the task.\n        :param description: an optional description of the task.\n        :param due_date: an optional due date for the task; dates are converted\n            epoch time.\n        :param priority: an optional", "solution": "\n        # Convert 'from_date' to epoch timestamp.\n        # Zero value (1970-01-01 00:00:00) is not allowed for\n        # 'modifiedStart' so it will be set to 1, by default.\n        ts = int(datetime_to_utc(from_date).timestamp()) or 1\n\n        consts = {\n            self.PMODIFIED_START: ts\n        }\n\n        attachments = {\n            self. PPROJECTS: True\n        }\n\n        params = {\n            self.PCONSTRAINTS: consts,\n            self.PATTACHMENTS: attachments,\n            self.PORDER: self.VOUTDATED,\n        }\n\n        while True:\n            r = self._call(self.MANIPHEST_TASKS, params)\n            yield r\n            j = json.loads(r)\n            after = j['result']['cursor']['after']\n            if not after:\n                break\n            params[self.PAFTER] = after"}
{"prompt": "def ensure_dir(path):\n    \"\"\"Ensure that a needed directory exists, creating it if it doesn't\"\"\"", "output": "\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef read_json(path):\n    \"\"\"Read a JSON file and return its contents\"\"\"\n    with open(path, 'r') as f:\n        return json.load(f)\n\n\ndef write_json(data, path):\n    \"\"\"Write a JSON file with the given contents\"\"\"\n    ensure_dir(os.path.dirname(path))\n    with open(path, 'w') as f:\n        json.dump(data, f, indent=2)\n\n\ndef read_csv(path):\n   ", "solution": "\n    try:\n        log.info('Ensuring directory exists: %s' % path)\n        os.makedirs(path)\n    except OSError:\n        if not os.path.isdir(path):\n            raise"}
{"prompt": "def parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"", "output": "otype)\n    \n    Args:\n        lines (list): A list of lines from the mim2gene file\n    \n    Returns:\n        dict: A dictionary where the keys are the mim numbers and the values are the entry types\n    \"\"\"\n    mim2gene = {}\n    for line in lines:\n        if line.startswith('MIM Number'):\n            continue\n        mim, entry_type = line.strip().split('\\t')\n        mim2gene[mim] = entry_type\n    return mim2gene\n\n\ndef parse_mim2phenotype(lines):\n    \"\"\"Parse the file called mim2", "solution": "\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry"}
{"prompt": "def directive(directname=None):\n    \"\"\"Attach a class to a parsing class and register it as a parser directive.\n\n        The class is registered with its name unless directname is provided.\n    \"\"\"", "output": "\n    def decorator(cls):\n        if directname is None:\n            directname = cls.__name__\n       _parser.register_directive(directname, cls)\n        return cls\n    return decorator\n\ndef directive_parser(cls):\n    \"\"\"Decorator to register a class as a parser directive.\"\"\"\n    return directive(cls.__name__)(cls)\n\nclass Parser(object):\n    \"\"\"Base class for allml parsers.\"\"\"\n\n    def __init__(self, stream):\n        self.stream = stream\n        self.directives = {}\n        self.parse_state = None\n\n    def register_directive(self, name", "solution": "\n    global _directives\n    class_dir_list = _directives\n\n    def wrapper(f):\n        nonlocal directname\n        if directname is None:\n            directname = f.__name__\n        f.ns_name = directname\n        set_one(class_dir_list, directname, f)\n        return f\n    return wrapper"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a Redis connection.\n        \"\"\"", "output": "\n        return self.redis_conn\n\n    def get_key(self, key):\n        \"\"\"\n        Returns the value of a key.\n        \"\"\"\n        return self.redis_conn.get(key)\n\n    def set_key(self, key, value):\n        \"\"\"\n        Sets a key to a value.\n        \"\"\"\n        self.redis_conn.set(key, value)\n\n    def delete_key(self, key):\n        \"\"\"\n        Deletes a key.\n        \"\"\"\n        self.redis_conn.delete(key)\n\n    def get_keys(self, pattern):\n        \"\"\"\n        Returns a list of keys matching a", "solution": "\n        conn = self.get_connection(self.redis_conn_id)\n        self.host = conn.host\n        self.port = conn.port\n        self.password = None if str(conn.password).lower() in ['none', 'false', ''] else conn.password\n        self.db = conn.extra_dejson.get('db', None)\n\n        if not self.redis:\n            self.log.debug(\n                'Initializing redis object for conn_id \"%s\" on %s:%s:%s',\n                self.redis_conn_id, self.host, self.port, self.db\n            )\n            self.redis = Redis(\n                host=self.host,\n                port=self.port,\n                password=self.password,\n                db=self.db)\n\n        return self.redis"}
{"prompt": "def decode(self, packet):\n        \"\"\"\n        Decode a PUBREL control packet. \n        \"\"\"", "output": "\n        # Extract the packet identifier\n        packet_id = packet[1]\n\n        # Check if the packet identifier is valid\n        if packet_id not in self.publishes:\n            raise ProtocolError(\"Invalid PUBREL packet identifier\")\n\n        # Remove the packet from the list of publishes\n        self.publishes.remove(packet_id)\n\n        # Send a PUBCOMP packet to acknowledge the PUBREL packet\n        self.send_pubcomp(packet_id)\n\n    def send_pubcomp(self, packet_id):\n        \"\"\"\n        Send a PUBCOMP packet to acknowledge the PUBREL packet.", "solution": "\n        self.encoded = packet\n        lenLen = 1\n        while packet[lenLen] & 0x80:\n            lenLen += 1\n        packet_remaining = packet[lenLen+1:]\n        self.msgId  = decode16Int(packet_remaining)\n        self.dup = (packet[0] & 0x08) == 0x08"}
{"prompt": "def init_log( quiet, debug ):\n   \"\"\"\n   setup system logging to desired verbosity.\n   \"\"\"", "output": "\n   if quiet:\n      logging.basicConfig( level=logging.ERROR )\n   elif debug:\n      logging.basicConfig( level=logging.DEBUG )\n   else:\n      logging.basicConfig( level=logging.INFO )\n\ndef get_logger( name, quiet, debug ):\n   \"\"\"\n   get a logger for the given name.\n   \"\"\"\n   logger = logging.getLogger( name )\n   init_log( quiet, debug )\n   return logger\n\ndef get_log_level( level ):\n   \"\"\"\n   get the corresponding logging level for the given string.\n   \"\"\"\n   if level == 'error':\n     ", "solution": "\n   from logging.handlers import SysLogHandler\n   fmt = logging.Formatter( os.path.basename(sys.argv[0]) +\n         \".%(name)s %(levelname)s - %(message)s\")\n   facility = SysLogHandler.LOG_DAEMON\n   syslog = SysLogHandler(address='/dev/log',facility=facility)\n   syslog.setFormatter( fmt )\n   log.addHandler(syslog)\n   if not quiet:\n      console = logging.StreamHandler()\n      console.setFormatter( fmt )\n      log.addHandler(console)\n      log.setLevel(logging.INFO)\n      if debug:\n         log.setLevel(logging.DEBUG)"}
{"prompt": "def loadPlugins(self):\n        \"\"\"Load plugins in nose.plugins.builtin\n        \"\"\"", "output": "\n        for plugin in.builtin.plugins:\n            if plugin.name not in self.plugins:\n                self.plugins[plugin.name] = plugin\n\n    def loadTestsFromNames(self, names):\n        \"\"\"Load tests from names.\n        \"\"\"\n        tests = []\n        for name in names:\n            if name in self.plugins:\n                tests.extend(self.plugins[name].loadTestsFromName(name))\n        return tests\n\n    def loadTestsFromName(self, name):\n        \"\"\"Load tests from name.\n        \"\"\"\n        if name in self.plugins:\n            return self.plugins[name].loadTestsFromName", "solution": "\n        from nose.plugins import builtin\n        for plug in builtin.plugins:\n            self.addPlugin(plug())\n        super(BuiltinPluginManager, self).loadPlugins()"}
{"prompt": "def verify(self, pkey):\n        \"\"\"\n        Verifies the signature on this certificate signing request.\n\n        :param PKey key: A public key.\n\n        :return: ``True`` if the signature is correct.\n        :rtype: bool\n\n        :raises OpenSSL.crypto.Error: If the signature is invalid or there is a\n            problem verifying the signature.\n        \"\"\"", "output": "\n        try:\n            self._verify_signature(pkey)\n            return True\n        except OpenSSL.crypto.Error:\n            return False\n\n    def _verify_signature(self, pkey):\n        \"\"\"\n        Verifies the signature on this certificate signing request.\n\n        :param PKey key: A public key.\n\n        :raises OpenSSL.crypto.Error: If the signature is invalid or there is a\n            problem verifying the signature.\n        \"\"\"\n        # Load the public key\n        pubkey = pkey.publickey()\n\n        # Load the certificate signing request\n        csr = OpenSSL.crypto.load_certificate_request(OpenSSL.crypto", "solution": "\n        if not isinstance(pkey, PKey):\n            raise TypeError(\"pkey must be a PKey instance\")\n\n        result = _lib.X509_REQ_verify(self._req, pkey._pkey)\n        if result <= 0:\n            _raise_current_error()\n\n        return result"}
{"prompt": "def add_card(self, query_params=None):\n        \"\"\"\n        Create a card for this list. Returns a Card object.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        response = self.client.post(self.url, data=query_params)\n        return Card(self.client, response.json())\n\n    def update_card(self, card_id, query_params=None):\n        \"\"\"\n        Update a card for this list. Returns a Card object.\n        \"\"\"\n        if query_params is None:\n            query_params = {}\n        response = self.client.put(self.url + '/' + str(card_id), data=query_params)\n        return Card(self.client", "solution": "\n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)"}
{"prompt": "def get_default_value(self):\n        \"\"\"Instantiate a default value instance.\n\n        This is called when the containing HasTraits classes'\n        :meth:`__new__` method is called to ensure that a unique instance\n        is created for each HasTraits instance.\n        \"\"\"", "output": "\n        return self.default_value_class()\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Create a new instance of the class.\n\n        This method is called when a new instance of the class is needed.\n        It ensures that a unique instance is created for each HasTraits\n        instance by checking if an instance with the same class and\n        arguments already exists. If so, it returns that instance. Otherwise,\n        it creates a new instance using the :meth:`__new__` method of the\n        default value class.\n        \"\"\"\n        key = (cls, args, frozenset(kwargs.items()))", "solution": "\n        dv  = self.default_value\n        if isinstance(dv, DefaultValueGenerator):\n            return dv.generate(self.klass)\n        else:\n            return dv"}
{"prompt": "def forward(self, input, target):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n        Takes an input object, and computes the corresponding loss of the criterion,\n        compared with `target`\n\n        :param input: ndarray or list of ndarray\n        :param target: ndarray or list of ndarray\n        :return: value of loss\n        \"\"\"", "output": "\n       _input = input\n       _target = target\n\n        if isinstance(input, list):\n            input =_input[0]\n            target = [t[0] for t in target]\n        else:\n            input = [input]\n            target = [t[0] for t in target]\n\n       _input = input\n       _target = target\n\n       _input = input\n       _target = target\n\n       _input = input\n       _target = target\n\n       _input = input\n       _target = target\n\n       _input = input\n       _target = target\n\n       _input", "solution": "\n        jinput, input_is_table = Layer.check_input(input)\n        jtarget, target_is_table = Layer.check_input(target)\n        output = callBigDlFunc(self.bigdl_type,\n                               \"criterionForward\",\n                               self.value,\n                               jinput,\n                               input_is_table,\n                               jtarget,\n                               target_is_table)\n        return output"}
{"prompt": "def split_line(self, line, cursor_pos=None):\n        \"\"\"Split a line of text with a cursor at the given position.\n        \"\"\"", "output": "\n        if cursor_pos is None:\n            cursor_pos = len(line)\n        line = line[:cursor_pos]\n        if not line:\n            return []\n        if line[-1] =='':\n            line = line[:-1]\n        if line[-1] == '\\t':\n            line = line[:-1]\n        if line[-1] == '\\n':\n            line = line[:-1]\n        return line.split()\n\n    def get_line_and_cursor(self, text, cursor_pos):\n        \"\"\"Get the line and cursor position for the given text and cursor position.\n        \"\"\"\n        lines", "solution": "\n        l = line if cursor_pos is None else line[:cursor_pos]\n        return self._delim_re.split(l)[-1]"}
{"prompt": "def signing_keys_as_jwks(self):\n        \"\"\"\n        Build a JWKS from the signing keys belonging to the self signer\n\n        :return: Dictionary\n        \"\"\"", "output": "\n        jwks = {\n            \"keys\": []\n        }\n        for key in self.signing_keys:\n            jwks[\"keys\"].append(key.to_dict())\n        return jwks\n\n    def verify_signature(self, signature, data, key_id=None):\n        \"\"\"\n        Verify a signature using the self signer's private key\n\n        :param signature: Signature to verify\n        :param data: Data to verify the signature for\n        :param key_id: Optional key ID to verify the signature for\n        :return: True if the signature is valid, False otherwise\n        \"\"\"\n        for key in self", "solution": "\n        _l = [x.serialize() for x in self.self_signer.keyjar.get_signing_key()]\n        if not _l:\n            _l = [x.serialize() for x in\n                  self.self_signer.keyjar.get_signing_key(owner=self.iss)]\n        return {'keys': _l}"}
{"prompt": "def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a fallback import block.\"\"\"", "output": "\n    return isinstance(node.parent, astroid.nodes.TryExcept)\n\n\ndef is_from_except_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from an except block.\"\"\"\n    return isinstance(node.parent, astroid.nodes.TryExcept)\n\n\ndef is_from_finally_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a finally block.\"\"\"\n    return isinstance(node.parent, astroid.nodes.TryExcept)\n\n\n", "solution": "\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n\n    if isinstance(context, astroid.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable(\n            handler.body for handler in context.handlers\n        )\n        handlers = context.handlers\n\n    has_fallback_imports = any(\n        isinstance(import_node, (astroid.ImportFrom, astroid.Import))\n        for import_node in other_body\n    )\n    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)\n    return ignores_import_error or has_fallback_imports"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report generated model in human readable form.\"\"\"", "output": "\n       _dict = self.to_dict()\n       _dict['model_type'] = 'text_classification'\n       _dict['labels'] = self.labels\n       _dict['label_map'] = self.label_map\n       _dict['tokenizer'] = self.tokenizer.to_dict()\n       _dict['max_length'] = self.max_length\n       _dict['embedding_dim'] = self.embedding_dim\n       _dict['num_hidden_layers'] = self.num_hidden_layers\n        test_data = self.test_data\n       ", "solution": "\n        if self._args and self._args.verbose > 2:\n            pprint(self.results)\n\n        for dimension, lc_info in self.results['dimensions'].items():\n            print(\"{}D layer condition:\".format(dimension), file=output_file)\n            for cache, lc_solution in sorted(lc_info['caches'].items()):\n                print(cache+\": \", end='', file=output_file)\n                if lc_solution['lt'] is sympy.true:\n                    print(\"unconditionally fulfilled\", file=output_file)\n                else:\n                    if lc_solution['eq'] is None:\n                        print(\"{}\".format(lc_solution['lt']), file=output_file)\n                    elif type(lc_solution['eq']) is not list:\n                        print(\"{}\".format(lc_solution['eq']), file=output_file)\n                    else:\n                        for solu in lc_solution['eq']:\n                            for s, v in solu.items():\n                                print(\"{} <= {}\".format(s, v), file=output_file)"}
{"prompt": "def _draw(self, mode, vertex_list=None):\n        \"\"\"Draw vertices in the domain.\n\n        If `vertex_list` is not specified, all vertices in the domain are\n        drawn.  This is the most efficient way to render primitives.\n\n        If `vertex_list` specifies a `VertexList`, only primitives in that\n        list will be drawn.\n\n        :Parameters:\n            `mode` : int\n                OpenGL drawing mode, e.g. ``GL_POINTS``, ``GL_LINES``, etc.\n            `vertex_list` : `VertexList`\n                Vertex list to draw, or ``None`` for all lists in this domain.\n\n        \"\"\"", "output": " to draw all vertices in the\n                domain.\n        \"\"\"\n       _mode = self._mode\n       _vertex_list = self._vertex_list\n       _vertex_array = self._vertex_array\n       _vertex_array_size = self._vertex_array_size\n       _vertex_array_type = self._vertex_array_type\n       _vertex_array_stride = self._vertex_array_stride\n       _vertex_array_offset = self._vertex_array_offset\n       _vertex_array_format = self._vertex_array_format\n       _vertex_", "solution": "\n        glPushClientAttrib(GL_CLIENT_VERTEX_ARRAY_BIT)\n        for buffer, attributes in self.buffer_attributes:\n            buffer.bind()\n            for attribute in attributes:\n                attribute.enable()\n                attribute.set_pointer(attribute.buffer.ptr)\n        if vertexbuffer._workaround_vbo_finish:\n            glFinish()\n\n        if vertex_list is not None:\n            glDrawArrays(mode, vertex_list.start, vertex_list.count)\n        else:\n            starts, sizes = self.allocator.get_allocated_regions()\n            primcount = len(starts)\n            if primcount == 0:\n                pass\n            elif primcount == 1:\n                # Common case\n                glDrawArrays(mode, starts[0], int(sizes[0]))\n            elif gl_info.have_version(1, 4):\n                starts = (GLint * primcount)(*starts)\n                sizes = (GLsizei * primcount)(*sizes)\n                glMultiDrawArrays(mode, starts, sizes, primcount)\n            else:\n                for start, size in zip(starts, sizes):\n                    glDrawArrays(mode, start, size)\n\n        for buffer, _ in self.buffer_attributes:\n            buffer.unbind()\n        glPopClientAttrib()"}
{"prompt": "def parse_rrset(e_rrset, connection, zone_id):\n    \"\"\"\n    This a parser that allows the passing of any valid ResourceRecordSet\n    tag. It will spit out the appropriate ResourceRecordSet object for the tag.\n\n    :param lxml.etree._Element e_rrset: The root node of the etree parsed\n        response from the API.\n    :param Route53Connection connection: The connection instance used to\n        query the API.\n    :param str zone_id: The zone ID of the HostedZone these rrsets belong to.\n    :rtype: ResourceRecordSet\n    :returns: An instantiated ResourceRecordSet object.\n    \"\"\"", "output": " :returns: The appropriate ResourceRecordSet object for the tag.\n    \"\"\"\n    if e_rrset.tag == 'ResourceRecordSet':\n        return ResourceRecordSet(e_rrset, connection, zone_id)\n    elif e_rrset.tag == 'ChangeResourceRecordSetsRequest':\n        return ChangeResourceRecordSetsRequest(e_rrset, connection, zone_id)\n    elif e_rrset.tag == 'Change':\n        return Change(e_rrset, connection, zone_id)\n    elif e_rrset.tag == 'ChangeBatch':\n        return ChangeBatch(e_", "solution": "\n\n    # This dict will be used to instantiate a ResourceRecordSet instance to yield.\n    kwargs = {\n        'connection': connection,\n        'zone_id': zone_id,\n    }\n    rrset_type = None\n\n    for e_field in e_rrset:\n        # Cheesy way to strip off the namespace.\n        tag_name = e_field.tag.split('}')[1]\n        field_text = e_field.text\n\n        if tag_name == 'Type':\n            # Need to store this to determine which ResourceRecordSet\n            # subclass to instantiate.\n            rrset_type = field_text\n            continue\n        elif tag_name == 'AliasTarget':\n            # A records have some special field values we need.\n            alias_hosted_zone_id, alias_dns_name = parse_rrset_alias(e_field)\n            kwargs['alias_hosted_zone_id'] = alias_hosted_zone_id\n            kwargs['alias_dns_name'] = alias_dns_name\n            # Alias A entries have no TTL.\n            kwargs['ttl'] = None\n            continue\n        elif tag_name == 'ResourceRecords':\n            kwargs['records'] = parse_rrset_record_values(e_field)\n            continue\n\n        # Map the XML tag name to a kwarg name.\n        kw_name = RRSET_TAG_TO_KWARG_MAP[tag_name]\n        # This will be the key/val pair used to instantiate the\n        # ResourceRecordSet instance.\n        kwargs[kw_name] = field_text\n\n    if not rrset_type:\n        raise Route53Error(\"No Type tag found in ListResourceRecordSetsResponse.\")\n\n    if 'records' not in kwargs:\n        # Not all rrsets have records.\n        kwargs['records'] = []\n\n    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP[rrset_type]\n    return RRSetSubclass(**kwargs)"}
{"prompt": "def get_params(degrees, translate, scale_ranges, shears, img_size):\n        \"\"\"Get parameters for affine transformation\n\n        Returns:\n            sequence: params to be passed to the affine transformation\n        \"\"\"", "output": "\n        # Compute the_size and_center based on the_size and_center_size\n       _size =_size * img_size\n       _center =_center * img_size\n\n        # Compute the_size and_center based on_size and_center_size\n       _size =_size * img_size\n       _center =_center * img_size\n\n        # Compute_size and_center based on_size and_center_size\n       _size =_size * img", "solution": "\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = random.uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            shear = random.uniform(shears[0], shears[1])\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear"}
{"prompt": "def topic(self, topic_id):\n        \"\"\"Retrive the topic with `topic_id` identifier.\n\n        :param topic_id: identifier of the topic to retrieve\n        \"\"\"", "output": "\n        return self._get(f\"/topics/{topic_id}\")\n\n    def create_topic(self, topic_name, description=None):\n        \"\"\"Create a new topic with the given `topic_name` and optional `description`.\n\n        :param topic_name: name of the topic to create\n        :param description: optional description of the topic\n        \"\"\"\n        data = {\"name\": topic_name, \"description\": description}\n        return self._post(\"/topics\", data)\n\n    def update_topic(self, topic_id, topic_name=None, description=None):\n        \"\"\"Update the topic with the", "solution": "\n        params = {\n            self.PKEY: self.api_key\n        }\n\n        # http://example.com/t/8.json\n        response = self._call(self.TOPIC, topic_id,\n                              params=params)\n\n        return response"}
{"prompt": "def decode(data):\n    \"\"\"\n    Decodes a PackBit encoded data.\n    \"\"\"", "output": "\n    =_init(data)\n    =_init(data)\n   _len =_len_init(data)\n   _len_len =_len_len_init(data)\n   _len_len_len =_len_len_len_init(data)\n   _len_len_len_len =_len_len_len_len_init(data)\n   _len_len_len_len_len =_len_len_len_len_len_init(data)\n   _len_", "solution": "\n    data = bytearray(data) # <- python 2/3 compatibility fix\n    result = bytearray()\n    pos = 0\n    while pos < len(data):\n        header_byte = data[pos]\n        if header_byte > 127:\n            header_byte -= 256\n        pos += 1\n\n        if 0 <= header_byte <= 127:\n            result.extend(data[pos:pos+header_byte+1])\n            pos += header_byte+1\n        elif header_byte == -128:\n            pass\n        else:\n            result.extend([data[pos]] * (1 - header_byte))\n            pos += 1\n\n    return bytes(result)"}
{"prompt": "def get_conn(self):\n        \"\"\"Return the FileService object.\"\"\"", "output": "\n        return self.file_service\n\n    def get_file_path(self, file_id):\n        \"\"\"Return the path to the file with the given ID.\"\"\"\n        return self.file_service.get_file_path(file_id)\n\n    def get_file_name(self, file_id):\n        \"\"\"Return the name of the file with the given ID.\"\"\"\n        return self.file_service.get_file_name(file_id)\n\n    def get_file_size(self, file_id):\n        \"\"\"Return the size of the file with the given ID.\"\"\"\n        return self.", "solution": "\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)"}
{"prompt": "def createClient(self):\n        \"\"\"Create a UDP connection to Riemann\"\"\"", "output": "\n        self.client = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self.client.settimeout(self.timeout)\n        self.client.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.client.bind((self.host, self.port))\n\n    def send(self, event):\n        \"\"\"Send an event to Riemann\"\"\"\n        try:\n            self.client.sendto(event.encode('utf-8'), (self.host, self.port))\n        except socket.timeout:\n            raise", "solution": "\n        server = self.config.get('server', '127.0.0.1')\n        port = self.config.get('port', 5555)\n\n        def connect(ip):\n            self.protocol = riemann.RiemannUDP(ip, port)\n            self.endpoint = reactor.listenUDP(0, self.protocol)\n\n        d = reactor.resolve(server)\n        d.addCallback(connect)\n        return d"}
{"prompt": "def _multi_gamma_sequence(self, a, p, name=\"multi_gamma_sequence\"):\n    \"\"\"Creates sequence used in multivariate (di)gamma; shape = shape(a)+[p].\"\"\"", "output": "\n    = tf.constant(a, dtype=self.dtype)\n    = tf.reshape(tf.tile(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(", "solution": "\n    with self._name_scope(name):\n      # Linspace only takes scalars, so we'll add in the offset afterwards.\n      seq = tf.linspace(\n          tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(\n              p, tf.int32))\n      return seq + tf.expand_dims(a, [-1])"}
{"prompt": "def skus(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-06-01: :class:`SkusOperations<azure.mgmt.storage.v2017_06_01.operations.SkusOperations>`\n           * 2017-10-01: :class:`SkusOperations<azure.mgmt.storage.v2017_10_01.operations.SkusOperations>`\n           * 2018-02-01: :class:`SkusOperations<azure.mgmt.storage.v2018_02_01.operations.SkusOperations>`\n           * 2018-03-01-preview: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01_preview.operations.SkusOperations>`\n           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage.v2018_07_01.operations.SkusOperations>`\n        \"\"\"", "output": "mgmt.storage.v2018_02_01.operations.SkusOperations>`\n           * 2018-03-01: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01.operations.SkusOperations>`\n           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage.v2018_07_01.operations.SkusOperations>`\n           * 2018-11-01: :class", "solution": "\n        api_version = self._get_api_version('skus')\n        if api_version == '2017-06-01':\n            from .v2017_06_01.operations import SkusOperations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-03-01-preview':\n            from .v2018_03_01_preview.operations import SkusOperations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import SkusOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def delete_local_file(file_name):\n    \"\"\"\n    Deletes the file associated with the file_name passed from local storage.\n    \n    :param str file_name: Filename of the file to be deleted\n    :return str: Filename of the file that was just deleted\n    \"\"\"", "output": "\n    if os.path.exists(file_name):\n        os.remove(file_name)\n        return file_name\n    else:\n        return \"File not found\"\n\n\ndef get_local_file_size(file_name):\n    \"\"\"\n    Returns the size of the file associated with the file_name passed from local storage.\n    \n    :param str file_name: Filename of the file to be checked\n    :return int: Size of the file in bytes\n    \"\"\"\n    if os.path.exists(file_name):\n        return os.path.getsize(file_name)\n    else:\n       ", "solution": "\n\n    try:\n        os.remove(file_name)\n        log.info(f\"Deletion for {file_name} has finished\")\n        return file_name\n    except OSError:\n        pass"}
{"prompt": "def output_json(self, file_name='/tmp/ND.json'):\n        \"\"\"\n        Arguments:\n            file_name(str : '/tmp/ND.json'): The file name to store the json to\n\n        Returns:\n            None\n        \"\"\"", "output": "\n        with open(file_name, 'w') as f:\n            json.dump(self.data, f, indent=2)\n\n    def __str__(self):\n        return json.dumps(self.data, indent=2)\n", "solution": "\n        complete_example = (\n            self.dataset, self.project, self.channels, self.metadata)\n        data = json.loads(self.nd_json(*complete_example))\n\n        # self.verify_json(data)\n        self.verify_path(data, VERIFY_BY_SLICE)\n\n        f = open(file_name, 'w')\n        f.write(str(data))\n        f.close()"}
{"prompt": "def open_graph(self, **args):\n        \"\"\"open a vcg graph\n        \"\"\"", "output": "\n        = self.vcg.open_graph(**args)\n        return self.vcg.graph_to_graph_object(v=v,=self.vcg.graph_to_networkx(v))\n\n    def_to_graph_object(self, v,):\n        \"\"\"convert a graph object to graph object\n        \"\"\"\n        = self.vcg.graph_to_networkx(v)\n        = self.vcg.graph_to_networkx(v,)\n      .graph['name'] ='modified'\n      .", "solution": "\n        self._stream.write(\"%sgraph:{\\n\" % self._indent)\n        self._inc_indent()\n        self._write_attributes(GRAPH_ATTRS, **args)"}
{"prompt": "def remove(cls, name: sym.Symbol) -> Optional[\"Namespace\"]:\n        \"\"\"Remove the namespace bound to the symbol `name` in the global\n        namespace cache and return that namespace.\n        Return None if the namespace did not exist in the cache.\"\"\"", "output": "\n        if name in cls._cache:\n            namespace = cls._cache[name]\n            del cls._cache[name]\n            return namespace\n        else:\n            return None\n\n    @classmethod\n    def clear(cls) -> None:\n        \"\"\"Clear the global namespace cache.\"\"\"\n        cls._cache.clear()\n\n    @classmethod\n    def get_global_namespace(cls) -> \"Namespace\":\n        \"\"\"Return the global namespace cache.\"\"\"\n        return cls._global_namespace\n\n    @classmethod\n    def set_global_namespace(cls, namespace: \"Namespace\") -> None:\n        \"\"\"Set the global namespace cache.\"\"\"\n        cls._global_namespace", "solution": "\n        while True:\n            oldval: lmap.Map = cls._NAMESPACES.deref()\n            ns: Optional[Namespace] = oldval.entry(name, None)\n            newval = oldval\n            if ns is not None:\n                newval = oldval.dissoc(name)\n            if cls._NAMESPACES.compare_and_set(oldval, newval):\n                return ns"}
{"prompt": "def einsum_vecmul_index(gate_indices, number_of_qubits):\n    \"\"\"Return the index string for Numpy.eignsum matrix-vector multiplication.\n\n    The returned indices are to perform a matrix multiplication A.v where\n    the matrix A is an M-qubit matrix, vector v is an N-qubit vector, and\n    M <= N, and identity matrices are implied on the subsystems where A has no\n    support on v.\n\n    Args:\n        gate_indices (list[int]): the indices of the right matrix subsystems\n                                  to contract with the left matrix.\n        number_of_qubits (int): the total number of qubits for the right matrix.\n\n    Returns:\n        str: An indices string for the Numpy.einsum function.\n    \"\"\"", "output": "of_qubits (int): the total number of qubits in the system.\n\n    Returns:\n        str: the index string for Numpy.eignsum matrix-vector multiplication.\n    \"\"\"\n   _indices = [f\"i{i}\" for i in range(number_of_qubits)]\n   _indices = [f\"j{i}\" for i in range(number_of_qubits)]\n   _indices.extend([f\"k{i}\" for i in range(number_of_qubits)])\n   _indices = [f\"l{i}\" for i in range", "solution": "\n\n    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,\n                                                                    number_of_qubits)\n\n    # Combine indices into matrix multiplication string format\n    # for numpy.einsum function\n    return \"{mat_l}{mat_r}, \".format(mat_l=mat_l, mat_r=mat_r) + \\\n           \"{tens_lin}->{tens_lout}\".format(tens_lin=tens_lin,\n                                            tens_lout=tens_lout)"}
{"prompt": "def get_client(self):\n        \"\"\"\n        Obtains a client for use, whether an existing unused client\n        or a brand new one if none are available.\n        \"\"\"", "output": "\n        if self.client is None:\n            self.client = self.create_client()\n        return self.client\n\n    def create_client(self):\n        \"\"\"\n        Creates a new client for use.\n        \"\"\"\n        client = Client(\n            client_id=self.client_id,\n            client_secret=self.client_secret,\n            redirect_uri=self.redirect_uri,\n            grant_types=self.grant_types,\n            scope=self.scope,\n            token_endpoint_auth_method=self.token_endpoint_auth_method,\n            token_endpoint_auth_signing_", "solution": "\n        client = None\n        try:\n            client = self.clients.get(block=False)\n        except queue.Empty:\n            pass\n        if not client:\n            self.client_id += 1\n            kwargs = dict(self.kwargs)\n            kwargs['verbose_id'] = kwargs.get(\n                'verbose_id', '') + str(self.client_id)\n            client = self.client_class(*self.args, **kwargs)\n        return client"}
{"prompt": "def _sem_open(name, value=None):\n    \"\"\" Construct or retrieve a semaphore with the given name\n\n    If value is None, try to retrieve an existing named semaphore.\n    Else create a new semaphore with the given value\n    \"\"\"", "output": "\n    if value is None:\n        try:\n            return sem_open(name, O_CREAT | O_EXCL, 0o600, 1)\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                return sem_open(name, O_CREAT, 0o600, 1)\n            raise\n    else:\n        return sem_open(name, O_CREAT | O_EXCL, 0o600, value)\n\n\ndef _sem_wait(sem):\n    \"\"\" Wait for a semaphore to be available\n\n   ", "solution": "\n    if value is None:\n        handle = pthread.sem_open(ctypes.c_char_p(name), 0)\n    else:\n        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,\n                                  ctypes.c_int(value))\n\n    if handle == SEM_FAILURE:\n        e = ctypes.get_errno()\n        if e == errno.EEXIST:\n            raise FileExistsError(\"a semaphore named %s already exists\" % name)\n        elif e == errno.ENOENT:\n            raise FileNotFoundError('cannot find semaphore named %s' % name)\n        elif e == errno.ENOSYS:\n            raise NotImplementedError('No semaphore implementation on this '\n                                      'system')\n        else:\n            raiseFromErrno()\n\n    return handle"}
{"prompt": "def delete(self, request, *args, **kwargs):\n        \"\"\"Delete auth token when `delete` request was issued.\"\"\"", "output": "\n        token = self.get_object()\n        token.delete()\n        return Response(status=status.HTTP_204_NO_CONTENT)\n", "solution": "\n        # Logic repeated from DRF because one cannot easily reuse it\n        auth = get_authorization_header(request).split()\n\n        if not auth or auth[0].lower() != b'token':\n            return response.Response(status=status.HTTP_400_BAD_REQUEST)\n\n        if len(auth) == 1:\n            msg = 'Invalid token header. No credentials provided.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n        elif len(auth) > 2:\n            msg = 'Invalid token header. Token string should not contain spaces.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n\n        try:\n            token = self.model.objects.get(key=auth[1])\n        except self.model.DoesNotExist:\n            pass\n        else:\n            token.delete()\n            signals.user_logged_out.send(\n                type(self),\n                user=token.user,\n                request=request,\n            )\n        return response.Response(status=status.HTTP_204_NO_CONTENT)"}
{"prompt": "def load_pkcs7_data(type, buffer):\n    \"\"\"\n    Load pkcs7 data from the string *buffer* encoded with the type\n    *type*.\n\n    :param type: The file type (one of FILETYPE_PEM or FILETYPE_ASN1)\n    :param buffer: The buffer with the pkcs7 data.\n    :return: The PKCS7 object\n    \"\"\"", "output": "\n    if type == FILETYPE_PEM:\n        return load_pem_pkcs7_data(buffer)\n    elif type == FILETYPE_ASN1:\n        return load_asn1_pkcs7_data(buffer)\n    else:\n        raise ValueError(\"Invalid file type: %s\" % type)\n\n\ndef load_pem_pkcs7_data(buffer):\n    \"\"\"\n    Load pkcs7 data from the string *buffer* encoded with PEM format.\n\n    :param buffer: The buffer with the pkcs7 data.\n    :return: The PKCS7 object\n    \"\"\"\n    try:\n        return pkcs7", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        pkcs7 = _lib.PEM_read_bio_PKCS7(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        pkcs7 = _lib.d2i_PKCS7_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if pkcs7 == _ffi.NULL:\n        _raise_current_error()\n\n    pypkcs7 = PKCS7.__new__(PKCS7)\n    pypkcs7._pkcs7 = _ffi.gc(pkcs7, _lib.PKCS7_free)\n    return pypkcs7"}
{"prompt": "def create_sprites_dataset(characters, actions, directions, channels=3,\n                           length=8, shuffle=False, fake_data=False):\n  \"\"\"Creates a tf.data pipeline for the sprites dataset.\n\n  Args:\n    characters: A list of (skin, hair, top, pants) tuples containing\n      relative paths to the sprite png image for each attribute.\n    actions: A list of Actions.\n    directions: A list of Directions.\n    channels: Number of image channels to yield.\n    length: Desired length of the sequences.\n    shuffle: Whether or not to shuffle the characters and sequences\n      start frame.\n    fake_data: Boolean for whether or not to yield synthetic data.\n\n  Returns:\n    A tf.data.Dataset yielding (seq, skin label index, hair label index,\n    top label index, pants label index, action label index, skin label\n    name, hair label_name, top label name, pants label name, action\n    label name) tuples.\n  \"\"\"", "output": " before creating the dataset.\n    fake_data: Whether or not to create a fake dataset for testing.\n\n  Returns:\n    A tf.data.Dataset of_sprites.\n  \"\"\"\n  if fake_data:\n   _sprites = [\n       _sprite.fake_data(length,_sprite.get_attributes(characters))\n        for _ in_sprite.Actions\n    ]\n  else:\n   _sprites = [\n       _sprite.load_sprites(characters,_sprite.get_attributes(characters))\n        for _ in_sprite.Actions\n    ]\n\n  def _parse_", "solution": "\n  if fake_data:\n    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])\n  else:\n    basedir = download_sprites()\n\n  action_names = [action.name for action in actions]\n  action_metadata = [(action.start_row, action.frames) for action in actions]\n\n  direction_rows = [direction.row_offset for direction in directions]\n\n  chars = tf.data.Dataset.from_tensor_slices(characters)\n  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()\n  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()\n  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()\n\n  if shuffle:\n    chars = chars.shuffle(len(characters))\n\n  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))\n\n  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))\n  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))\n  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))\n  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))\n  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))\n\n  def process_example(attrs, act_name, act_metadata, dir_row_offset):\n    "}
{"prompt": "def _prepare_subsection(data, sections, wrapper):\n    \"\"\"\n    Creates a subsection ready to be filled.\n    :param data: Semi-parsed dictionary.\n    :param sections: A list of sections.\n    :param wrapper: A wrapping object for key-value pairs.\n    :return: A newly created subsection.\n    \"\"\"", "output": "\n    subsection = wrapper()\n    for key, value in data.items():\n        if isinstance(value, dict):\n            subsection[key] = _prepare_subsection(value, sections, wrapper)\n        else:\n            subsection[key] = value\n    return subsection\n\n\ndef _parse_data(data, sections, wrapper):\n    \"\"\"\n    Parses the given data and creates subsections for each section.\n    :param data: A dictionary to be parsed.\n    :param sections: A list of sections.\n    :param wrapper: A wrapping object for key-value pairs.\n    :return: A list of", "solution": "\n    current = data\n    for i in sections[:-1]:\n        current = current[i]\n\n    current[sections[-1]] = wrapper()\n    return current[sections[-1]]"}
{"prompt": "def render_tag(self, context, *tag_args, **tag_kwargs):\n        \"\"\"\n        Rendering of the tag. It either assigns the value as variable, or renders it.\n        \"\"\"", "output": "\n        value = tag_args[0]\n        if value is None:\n            return ''\n        if isinstance(value, dict):\n            value = json.dumps(value)\n        if isinstance(value, list):\n            value = json.dumps(value)\n        if isinstance(value, (int, float, bool)):\n            return value\n        if isinstance(value, str):\n            return value\n        if hasattr(value, '__iter__'):\n            return value\n        return str(value)\n\n    def render_assign(self, context, *tag_args, **tag_kwargs):\n        \"\"\"\n        Assigning a value to a variable", "solution": "\n        # Be very explicit about which base functionality is used:\n        # Using super() for mixin support will not work nicely anyway here.\n        if self.as_var:\n            # Assign the value in the parent context\n            return BaseAssignmentNode.render_tag(self, context, *tag_args, **tag_kwargs)\n        else:\n            # Render the output using the BaseInclusionNode features\n            return BaseInclusionNode.render_tag(self, context, *tag_args, **tag_kwargs)"}
{"prompt": "def verify_token(self, request, *args, **kwargs):\n        \"\"\"\n        Use `token` to allow one-time access to a view.\n\n        Set the user as a class attribute or raise an `InvalidExpiredToken`.\n\n        Token expiry can be set in `settings` with `VERIFY_ACCOUNT_EXPIRY` and is\n        set in seconds.\n        \"\"\"", "output": "\n        token = request.GET.get('token', None)\n        if not token:\n            raise InvalidExpiredToken('Token is missing.')\n\n        try:\n            user = self.get_user_by_token(token)\n        except Token.DoesNotExist:\n            raise InvalidExpiredToken('Invalid token.')\n\n        if user.is_active:\n            raise InvalidExpiredToken('Account is already active.')\n\n        if user.is_staff:\n            raise InvalidExpiredToken('Account is a staff member.')\n\n        if user.date_joined < timezone.now() - timedelta(seconds=self.expiry):\n            raise InvalidExpiredToken('Token", "solution": "\n        User = get_user_model()\n\n        try:\n            max_age = settings.VERIFY_ACCOUNT_EXPIRY\n        except AttributeError:\n            max_age = self.DEFAULT_VERIFY_ACCOUNT_EXPIRY\n\n        try:\n            email_data = signing.loads(kwargs['token'], max_age=max_age)\n        except signing.BadSignature:\n            raise self.invalid_exception_class\n\n        email = email_data['email']\n\n        try:\n            self.user = User.objects.get_by_natural_key(email)\n        except User.DoesNotExist:\n            raise self.invalid_exception_class\n\n        if self.user.email_verified:\n            raise self.permission_denied_class"}
{"prompt": "def partitions(l, partition_size):\n    \"\"\"\n    >>> list(partitions([], 10))\n    []\n    >>> list(partitions([1,2,3,4,5], 1))\n    [[1], [2], [3], [4], [5]]\n    >>> list(partitions([1,2,3,4,5], 2))\n    [[1, 2], [3, 4], [5]]\n    >>> list(partitions([1,2,3,4,5], 5))\n    [[1, 2, 3, 4, 5]]\n\n    :param list l: List to be partitioned\n    :param int partition_size: Size of partitions\n    \"\"\"", "output": "\n    >>> list(partitions([1,2,3,4,5], 6))\n    [[1, 2, 3, 4, 5]]\n    >>> list(partitions([1,2,3,4,5], 7))\n    [[1, 2, 3, 4, 5]]\n    >>> list(partitions([1,2,3,4,5], 8))\n    [[1, 2, 3, 4, 5]]\n    >>> list(partitions([1,2,3,4,5], 9))\n    [[1,", "solution": "\n    for i in xrange(0, len(l), partition_size):\n        yield l[i:i + partition_size]"}
{"prompt": "def download_layers(self, repo_name, digest=None, destination=None):\n    \"\"\" download layers is a wrapper to do the following for a client loaded\n        with a manifest for an image:\n      \n        1. use the manifests to retrieve list of digests (get_digests)\n        2. atomically download the list to destination (get_layers)\n\n        This function uses the MultiProcess client to download layers\n        at the same time.\n    \"\"\"", "output": "\n    # get list of digests\n    digests = self.get_digests(repo_name, digest)\n\n    # download layers in parallel\n    with self.client.MultiProcessPool() as pool:\n        results = pool.map(self.get_layers, digests)\n\n    # merge results and save to destination\n    for digest, result in zip(digests, results):\n        if result is not None:\n            with open(os.path.join(destination, digest), 'wb') as f:\n                f.write(result)\n\n    return digests\n\n\ndef get_digests(self,", "solution": "\n    from sregistry.main.workers import ( Workers, download_task )\n\n    # 1. Get manifests if not retrieved\n    if not hasattr(self, 'manifests'):\n        self._get_manifests(repo_name, digest)\n\n    # Obtain list of digets, and destination for download\n    digests = self._get_digests()\n    destination = self._get_download_cache(destination)\n\n    # Create multiprocess download client\n    workers = Workers()\n\n    # Download each layer atomically\n    tasks = []\n    layers = []\n    for digest in digests:\n\n        targz = \"%s/%s.tar.gz\" % (destination, digest)\n\n        # Only download if not in cache already\n        if not os.path.exists(targz):\n            url = \"%s/%s/blobs/%s\" % (self.base, repo_name, digest)\n            tasks.append((url, self.headers, targz))\n        layers.append(targz)\n\n    # Download layers with multiprocess workers\n    if len(tasks) > 0:\n        download_layers = workers.run(func=download_task,\n                                      tasks=tasks)\n    # Create the metadata tar\n    metadata = self._create_metadata_tar(destination)\n    if metadata is not None:\n        layers.append(metadata)\n\n\n    return layers"}
{"prompt": "def process_item(self, item, spider):\n        \"\"\"\n        Process single item. Add item to items and then upload to S3 if size of items\n        >= max_chunk_size.\n        \"\"\"", "output": "\n        self.items.append(item)\n        if len(self.items) >= self.max_chunk_size:\n            self.upload_items()\n\n    def upload_items(self):\n        \"\"\"\n        Upload items to S3.\n        \"\"\"\n        s3 = boto3.resource('s3')\n        bucket = s3.Bucket(self.bucket_name)\n        for item in self.items:\n            key = self.key_prefix + item['id'] + '.json'\n            body = json.dumps(item).encode('utf-8')\n            bucket.put_object(Key=key", "solution": "\n        self.items.append(item)\n        if len(self.items) >= self.max_chunk_size:\n            self._upload_chunk(spider)\n\n        return item"}
{"prompt": "def handle_print(self, signame, set_print):\n        \"\"\"Set whether we print or not when this signal is caught.\"\"\"", "output": "\n        self.set_print = set_print\n\n    def handle_quit(self, signame,):\n        \"\"\"Quit the program.\"\"\"\n      .quit()\n\n    def handle_sigint(self, signame, frame):\n        \"\"\"Handle the SIGINT signal.\"\"\"\n        print(\"SIGINT received, exiting...\")\n        sys.exit(0)\n\n    def handle_sigterm(self, signame, frame):\n        \"\"\"Handle the SIGTERM signal.\"\"\"\n        print(\"SIGTERM received, exiting...\")\n        sys.exit(0)\n\n    def handle_sigusr1(self, signame, frame):\n        \"\"\"Handle", "solution": "\n        if set_print:\n            self.sigs[signame].print_method = self.dbgr.intf[-1].msg\n        else:\n            self.sigs[signame].print_method = None\n            pass\n        return set_print"}
{"prompt": "def _trj_check_version(self, version, python, force):\n        \"\"\"Checks for version mismatch\n\n        Raises a VersionMismatchError if version of loaded trajectory and current pypet version\n        do not match. In case of `force=True` error is not raised only a warning is emitted.\n\n        \"\"\"", "output": "2021-05-12: Added `python` parameter to check for Python version mismatch.\n        \"\"\"\n        if.version!= version:\n            if force:\n                warnings.warn(f\"Version mismatch: loaded trajectory is from pypet v{self.version}, but current pypet is v{version}.\")\n            else:\n                raise VersionMismatchError(f\"Version mismatch: loaded trajectory is from pypet v{self.version}, but current pypet is v{version}.\")\n\n    def _trj_check_python(self, python, force):\n        \"\"\"Checks", "solution": "\n        curr_python = pypetconstants.python_version_string\n\n        if (version != VERSION or curr_python != python) and not force:\n            raise pex.VersionMismatchError('Current pypet version is %s used under python %s '\n                                           '  but your trajectory'\n                                           ' was created with version %s and python %s.'\n                                           ' Use >>force=True<< to perform your load regardless'\n                                           ' of version mismatch.' %\n                                           (VERSION, curr_python, version, python))\n        elif version != VERSION or curr_python != python:\n            self._logger.warning('Current pypet version is %s with python %s but your trajectory'\n                                 ' was created with version %s under python %s.'\n                                 ' Yet, you enforced the load, so I will'\n                                 ' handle the trajectory despite the'\n                                 ' version mismatch.' %\n                                 (VERSION, curr_python, version, python))"}
{"prompt": "def get_branch_mutation_matrix(self, node, full_sequence=False):\n        \"\"\"uses results from marginal ancestral inference to return a joint\n        distribution of the sequence states at both ends of the branch.\n\n        Parameters\n        ----------\n        node : Phylo.clade\n            node of the tree\n        full_sequence : bool, optional\n            expand the sequence to the full sequence, if false (default)\n            the there will be one mutation matrix for each column in the\n            reduced alignment\n\n        Returns\n        -------\n        numpy.array\n            an Lxqxq stack of matrices (q=alphabet size, L (reduced)sequence length)\n        \"\"\"", "output": "L+1)/2 number ofable branches)\n            where L is the number ofable branches and eachable branch is represented\n            by aable branchable branchable branchable branchable branchable branchable branchable branchable branchable branchable branchable branch\n        \"\"\"\n       able_branches = self.get_valid_branches(node)\n       able_branch_mutations = self.get_valid_branch_mutations(node)\n       able_branch_mutations_matrix = self.get_branch_mutations_matrix(node,", "solution": "\n        pp,pc = self.marginal_branch_profile(node)\n\n        # calculate pc_i [e^Qt]_ij pp_j for each site\n        expQt = self.gtr.expQt(self._branch_length_to_gtr(node))\n        if len(expQt.shape)==3: # site specific model\n            mut_matrix_stack = np.einsum('ai,aj,ija->aij', pc, pp, expQt)\n        else:\n            mut_matrix_stack = np.einsum('ai,aj,ij->aij', pc, pp, expQt)\n\n        # normalize this distribution\n        normalizer = mut_matrix_stack.sum(axis=2).sum(axis=1)\n        mut_matrix_stack = np.einsum('aij,a->aij', mut_matrix_stack, 1.0/normalizer)\n\n        # expand to full sequence if requested\n        if full_sequence:\n            return mut_matrix_stack[self.full_to_reduced_sequence_map]\n        else:\n            return mut_matrix_stack"}
{"prompt": "def meas_gate(self, circuit, qreg, op):\n        \"\"\"\n        Add measurement gates to a circuit.\n\n        Args:\n            circuit (QuantumCircuit): circuit to add measurement to.\n            qreg (tuple(QuantumRegister,int)): quantum register being measured.\n            op (str): the basis label for the measurement.\n        \"\"\"", "output": "\n        circuit.measure(qreg, op)\n\n    def_circuit(self,_circuit,_qreg,_op,_circuit__qreg,_op_2):\n        \"\"\"\n        Add measurement gates to a circuit.\n\n        Args:\n           _circuit (QuantumCircuit): circuit to add measurement to.\n            qreg (tuple(QuantumRegister,int)): quantum register being measured.\n            op (str): the basis label for the measurement.\n        \"\"\"\n        self.meas_gate(new_circuit, new_qreg, new_op)\n        self", "solution": "\n        if self.meas_fun is None:\n            pass\n        else:\n            self.meas_fun(circuit, qreg, op)"}
{"prompt": "def log(wave):\n    r\"\"\"\n    Return the natural logarithm of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.log\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": " error)\n\n     * TypeError (Input waveform is not a Waveform object)\n\n     * TypeError (Input waveform is not a vector-valued Waveform object)\n\n     * TypeError (Input waveform is not a scalar-valued Waveform object)\n\n     * TypeError (Input waveform is not a-valued Waveform object)\n\n     * TypeError (Input waveform is not a-valued Waveform object)\n\n     * TypeError (Input waveform is not a-valued Waveform object)\n\n     * TypeError (Input waveform is not a-valued Waveform object)\n\n     * TypeError (", "solution": "\n    pexdoc.exh.addex(\n        ValueError, \"Math domain error\", bool((min(wave._dep_vector) <= 0))\n    )\n    return _operation(wave, \"log\", \"\", np.log)"}
{"prompt": "async def save_tracks(self, *tracks):\n        \"\"\"Save one or more tracks to the current user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        tracks : Sequence[Union[Track, str]]\n            A sequence of track objects or spotify IDs\n        \"\"\"", "output": "\n        if not tracks:\n            raise ValueError(\"No tracks provided\")\n\n        if notify.is_logged_in():\n            raise ValueError(\"You must be logged in to save tracks\")\n\n        if not isinstance(tracks[0], Track):\n            tracks = [Track(track_id=track) for track in tracks]\n\n        for track in tracks:\n            if not isinstance(track, Track):\n                raise ValueError(\"All tracks must be of type Track\")\n\n        await self._client.user_saved_tracks_add(tracks)\n\n    async def get_saved_tracks(self):\n        \"\"\"Get the current user's saved", "solution": "\n        _tracks = [(obj if isinstance(obj, str) else obj.id) for obj in tracks]\n        await self.user.http.save_tracks(','.join(_tracks))"}
{"prompt": "def get_logging_level():\n    \"\"\"get_logging_level will configure a logging to standard out based on the user's\n    selected level, which should be in an environment variable called\n    MESSAGELEVEL. if MESSAGELEVEL is not set, the maximum level\n    (5) is assumed (all messages).\n    \"\"\"", "output": "\n    level = os.getenv(\"MESSAGELEVEL\", 5)\n    if level == \"0\":\n        level = logging.ERROR\n    elif level == \"1\":\n        level = logging.WARNING\n    elif level == \"2\":\n        level = logging.INFO\n    elif level == \"3\":\n        level = logging.DEBUG\n    else:\n        level = logging.INFO\n    return level\n\n\ndef configure_logging(level=None):\n    \"\"\"configure_logging will configure a logging to standard out based on the user's\n    selected level, which should be in an environment variable called\n    MESSAGELEVEL. if MESSAGELEVEL is not set,", "solution": "\n    level = os.environ.get(\"MESSAGELEVEL\", INFO)\n\n    # User knows logging levels and set one\n    if isinstance(level, int):\n        return level\n\n    # Otherwise it's a string\n    if level == \"CRITICAL\":\n        return CRITICAL\n    elif level == \"ABORT\":\n        return ABORT\n    elif level == \"ERROR\":\n        return ERROR\n    elif level == \"WARNING\":\n        return WARNING\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"INFO\":\n        return INFO\n    elif level == \"QUIET\":\n        return QUIET\n    elif level.startswith(\"VERBOSE\"):\n        return VERBOSE3\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"DEBUG\":\n        return DEBUG\n\n    return level"}
{"prompt": "def format_task_numbers_with_links(tasks):\n    \"\"\"Returns formatting for the tasks section of asana.\"\"\"", "output": "\n    formatted_tasks = []\n    for task in tasks:\n        formatted_task = {\n            \"id\": task[\"gid\"],\n            \"name\": task[\"name\"],\n            \"notes\": task[\"notes\"],\n            \"completed\": task[\"completed\"],\n            \"due_on\": task[\"due_on\"],\n            \"assignee\": task[\"assignee\"],\n            \"completed_at\": task[\"completed_at\"],\n            \"created_at\": task[\"created_at\"],\n            \"completed_by\": task[\"completed_by\"],\n            \"assignee_status\": task[\"assignee_status\"],\n            \"assignee_status", "solution": "\n\n    project_id = data.get('asana-project', None)\n\n    def _task_format(task_id):\n        if project_id:\n            asana_url = tool.ToolApp.make_asana_url(project_id, task_id)\n            return \"[#%d](%s)\" % (task_id, asana_url)\n        else:\n            return \"#%d\" % task_id\n\n    return \"\\n\".join([_task_format(tid) for tid in tasks])"}
{"prompt": "def _mode(self):\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"", "output": "\n    return self.mean_direction\n\ndef _mean_direction(self):\n    \"\"\"The mean direction of the distribution is the.\"\"\"\n    = self.mean_intensity\n    =.reshape(-1, 1)\n    = /.sum(axis=0)\n    =.T\n    =.reshape(-1)\n    =.reshape(1, -1)\n    =.T\n    =.reshape(1, -1)\n    =.T\n    =", "solution": "\n    return (self.mean_direction +\n            tf.zeros_like(self.concentration)[..., tf.newaxis])"}
{"prompt": "def is_detached(self):\n        \"\"\"Check if the repo is in a detached state.\n\n        The repository is in a detached state when HEAD is not a symbolic\n        reference.\n\n        :returns: whether the repository is detached or not\n\n        :raises RepositoryError: when an error occurs checking the state\n            of the repository\n        \"\"\"", "output": "\n        try:\n            return not self.head.is_symbolic\n        except (AttributeError, TypeError):\n            raise RepositoryError(\"Unable to determine if the repository is detached\")\n\n    def is_dirty(self):\n        \"\"\"Check if the repository is dirty.\n\n        The repository is dirty if there are changes to the working tree,\n        the index, or the HEAD.\n\n        :returns: whether the repository is dirty or not\n\n        :raises RepositoryError: when an error occurs checking the state\n            of the repository\n        \"\"\"\n        try:\n            return self.is_dirty_working_tree() or self.is_dirty_", "solution": "\n        cmd_sym = ['git', 'symbolic-ref', 'HEAD']\n\n        try:\n            self._exec(cmd_sym, cwd=self.dirpath, env=self.gitenv)\n        except RepositoryError as e:\n            if e.msg.find(\"ref HEAD is not a symbolic ref\") == -1:\n                raise e\n            return True\n        else:\n            return False"}
{"prompt": "def publish_events(\n            self, topic_hostname, events, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Publishes a batch of events to an Azure Event Grid topic.\n\n        :param topic_hostname: The host name of the topic, e.g.\n         topic1.westus2-1.eventgrid.azure.net\n        :type topic_hostname: str\n        :param events: An array of events to be published to Event Grid.\n        :type events: list[~azure.eventgrid.models.EventGridEvent]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`\n        \"\"\"", "output": ": headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: EventGridEventBatch or ClientRawResponse if raw=true\n        :rtype: ~azure.eventgrid.models.EventGridEventBatch or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\n        \"\"\"\n        topic_hostname = self._validate_topic_hostname(topic", "solution": "\n        # Construct URL\n        url = self.publish_events.metadata['url']\n        path_format_arguments = {\n            'topicHostname': self._serialize.url(\"topic_hostname\", topic_hostname, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(events, '[EventGridEvent]')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise HttpOperationError(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response"}
{"prompt": "def get_imap_capabilities(server):\n    \"\"\"\n    Returns a list of an IMAP server's capabilities\n\n    Args:\n        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient\n\n    Returns (list): A list of capabilities\n    \"\"\"", "output": "\n    capabilities = []\n    try:\n        capabilities = server.capability()\n    except imapclient.exceptions.IMAPClientError:\n        pass\n    return capabilities\n\n\ndef get_imap_folders(server, folder):\n    \"\"\"\n    Returns a list of folders in the specified folder\n\n    Args:\n        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient\n        folder (str): The name of the folder to search in\n\n    Returns (list): A list of folder names\n    \"\"\"\n    folders = []\n    try:\n        folders = server.list_folders(folder)\n    except imapclient", "solution": "\n\n    capabilities = list(map(str, list(server.capabilities())))\n    for i in range(len(capabilities)):\n        capabilities[i] = str(capabilities[i]).replace(\"b'\",\n                                                       \"\").replace(\"'\",\n                                                                   \"\")\n    logger.debug(\"IMAP server supports: {0}\".format(capabilities))\n\n    return capabilities"}
{"prompt": "def update_path(self, board, color, path):\n        \"\"\" update win/loss count along path \"\"\"", "output": "\n        for i in range(len(path) - 1):\n            if board[path[i]][path[i + 1]] == color:\n                self.win_count += 1\n            else:\n                self.loss_count += 1\n\n    def update_board(self, board, color, path):\n        \"\"\" update board with color along path \"\"\"\n        for i in range(len(path) - 1):\n            board[path[i]][path[i + 1]] = color\n\n    def is_win(self):\n        \"\"\" check if player has won \"\"\"\n        return self.win_", "solution": "\n        wins = board.score(BLACK) >= board.score(WHITE)\n        for node in path:\n            if color == BLACK:\n                color = WHITE\n            else:\n                color = BLACK\n            if wins == (color == BLACK):\n                node.wins += 1\n            else:\n                node.losses += 1\n            if node.parent:\n                node.parent.bestchild = node.parent.best_child()"}
{"prompt": "def read(self):\n        \"\"\"\n        Load the metrics file from the given path\n        \"\"\"", "output": "\n        with open(self.path, 'r') as f:\n            self.data = json.load(f)\n\n    def write(self):\n        \"\"\"\n        Write the metrics to the file\n        \"\"\"\n        with open(self.path, 'w') as f:\n            json.dump(self.data, f, indent=2)\n\n    def add(self, key, value):\n        \"\"\"\n        Add a metric to the metrics file\n        \"\"\"\n        if key not in self.data:\n            self.data[key] = []\n        self.data[key].append(value)\n\n    def get(self,", "solution": "\n        f = open(self.path, \"r\")\n        self.manifest_json = f.read()"}
{"prompt": "def update(self, teamId, name=None, **request_parameters):\n        \"\"\"Update details for a team, by ID.\n\n        Args:\n            teamId(basestring): The team ID.\n            name(basestring): A user-friendly name for the team.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Team: A Team object with the updated Webex Teams team details.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        if not isinstance(teamId, basestring):\n            raise TypeError('Argument \"teamId\" must be a string.')\n\n        if name is not None and not isinstance(name, basestring):\n            raise TypeError('Argument \"name\" must be a string or None.')\n\n        headers = self._create_request_headers()\n        url = self._build_url('teams', teamId)\n        data = {}\n\n        if name is not None:\n            data['displayName'] = name\n\n        response = self._request('PUT', url, headers=headers, data=data, **request_parameters)\n        return Team(response.", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(name, basestring)\n\n        put_data = dict_from_items_with_values(\n            request_parameters,\n            name=name,\n        )\n\n        # API request\n        json_data = self._session.put(API_ENDPOINT + '/' + teamId,\n                                      json=put_data)\n\n        # Return a team object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def _odds_val(self):\n        \"\"\"Determine a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.odds` value.\n\n        :returns: The derived value\n        \"\"\"", "output": "\n        return_or_random(self.odds)\n\n    def __val(self):\n        \"\"\"Determine a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.uniform_range` value.\n\n        :returns: The derived value\n        \"\"\"\n        return random.uniform(self.uniform_range[0], self.uniform_range[1])\n\n    def _ rand_val(self):\n        \"\"\"Determine a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.random_range` value.\n\n        :returns: The derived value\n       ", "solution": "\n        if len(self.odds) == 0:\n            self.odds = [(1.00, [self.min, self.max])]\n\n        rand_val = rand.random()\n        total = 0\n        for percent,v in self.odds:\n            if total <= rand_val < total+percent:\n                found_v = v\n                break\n            total += percent\n\n        res = None\n        if isinstance(v, (tuple,list)):\n            rand_func = rand.randfloat if type(v[0]) is float else rand.randint\n\n            if len(v) == 2:\n                res = rand_func(v[0], v[1])\n            elif len(v) == 1:\n                res = v[0]\n        else:\n            res = v\n\n        return res"}
{"prompt": "def _check_key_value_types(obj, key_type, value_type, key_check=isinstance, value_check=isinstance):\n    \"\"\"Ensures argument obj is a dictionary, and enforces that the keys/values conform to the types\n    specified by key_type, value_type.\n    \"\"\"", "output": "\n    if not isinstance(obj, dict):\n        raise TypeError(\"obj must be a dictionary\")\n    for key, value in obj.items():\n        if not key_check(key, key_type):\n            raise TypeError(f\"key '{key}' is not of type {key_type}\")\n        if not value_check(value, value_type):\n            raise TypeError(f\"value '{value}' is not of type {value_type}\")\n\n\ndef _check_key_value_types_optional(obj, key_type=None, value_type=None, key_check=isinstance, value", "solution": "\n    if not isinstance(obj, dict):\n        raise_with_traceback(_type_mismatch_error(obj, dict))\n\n    if key_type is str:\n        key_type = string_types\n\n    if value_type is str:\n        value_type = string_types\n\n    for key, value in obj.items():\n        if key_type and not key_check(key, key_type):\n            raise_with_traceback(\n                CheckError(\n                    'Key in dictionary mismatches type. Expected {key_type}. Got {obj_repr}'.format(\n                        key_type=repr(key_type), obj_repr=repr(key)\n                    )\n                )\n            )\n        if value_type and not value_check(value, value_type):\n            raise_with_traceback(\n                CheckError(\n                    'Value in dictionary mismatches expected type for key {key}. Expected value '\n                    'of type {vtype}. Got value {value} of type {obj_type}.'.format(\n                        vtype=repr(value_type), obj_type=type(value), key=key, value=value\n                    )\n                )\n            )\n    return obj"}
{"prompt": "def _clear_temp(self):\n        \"\"\"\n        Clear temp directory from created csv and ods files during\n        communicator operations.\n        \"\"\"", "output": "\n        if os.path.exists(self.temp_dir):\n            shutil.rmtree(self.temp_dir)\n\n    def _create_temp_dir(self):\n        \"\"\"\n        Create a temporary directory to store csv and ods files during\n        communicator operations.\n        \"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n\n    def _write_csv(self,_data,_headers,_index,_index_label,_index_type):\n        \"\"\"\n        Write the data to a csv file.\n\n        Args:\n            out_data (list): The", "solution": "\n        temp_files = [LOCAL_ODS, GDOCS_TRANS_CSV, GDOCS_META_CSV,\n                      LOCAL_TRANS_CSV, LOCAL_META_CSV]\n        for temp_file in temp_files:\n            file_path = os.path.join(self.temp_path, temp_file)\n            if os.path.exists(file_path):\n                os.remove(file_path)"}
{"prompt": "def to_zyz(self):\n        \"\"\"Converts a unit-length quaternion to a sequence\n        of ZYZ Euler angles.\n\n        Returns:\n            ndarray: Array of Euler angles.\n        \"\"\"", "output": "\n        q = self.normalize()\n        z = np.arccos(q[2])\n        y = np.arcsin(q[0] / np.sin(z))\n        x = np.arctan2(q[1] / np.sin(z), q[3] / np.sin(z))\n        return np.array([x, y, z])\n\n    def to_euler(self, order='xyz'):\n        \"\"\"Converts a unit-length quaternion to a sequence\n        of Euler angles.\n\n        Args:\n            order (str): Order of Euler angles. Options are '", "solution": "\n        mat = self.to_matrix()\n        euler = np.zeros(3, dtype=float)\n        if mat[2, 2] < 1:\n            if mat[2, 2] > -1:\n                euler[0] = math.atan2(mat[1, 2], mat[0, 2])\n                euler[1] = math.acos(mat[2, 2])\n                euler[2] = math.atan2(mat[2, 1], -mat[2, 0])\n            else:\n                euler[0] = -math.atan2(mat[1, 0], mat[1, 1])\n                euler[1] = np.pi\n        else:\n            euler[0] = math.atan2(mat[1, 0], mat[1, 1])\n        return euler"}
{"prompt": "def export_variants(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"Export causative variants for a collaborator\n\n    Args:\n        adapter(MongoAdapter)\n        collaborator(str)\n        document_id(str): Search for a specific variant\n        case_id(str): Search causative variants for a case\n\n    Yields:\n        variant_obj(scout.Models.Variant): Variants marked as causative ordered by position.\n    \"\"\"", "output": "\n    if document_id:\n        variants = adapter.find_variants(collaborator, document_id=document_id)\n    elif case_id:\n        variants = adapter.find_causative_variants(collaborator, case_id=case_id)\n    else:\n        variants = adapter.find_causative_variants(collaborator)\n\n    for variant_obj in variants:\n        yield variant_obj\n\n\ndef export_variant_summary(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"Export variant summary for a collaborator\n\n    Args:\n        adapter", "solution": "\n\n    # Store the variants in a list for sorting\n    variants = []\n    if document_id:\n        yield adapter.variant(document_id)\n        return\n\n    variant_ids = adapter.get_causatives(\n        institute_id=collaborator,\n        case_id=case_id\n        )\n    ##TODO add check so that same variant is not included more than once\n    for document_id in variant_ids:\n\n        variant_obj = adapter.variant(document_id)\n        chrom = variant_obj['chromosome']\n        # Convert chromosome to integer for sorting\n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.info(\"Unknown chromosome %s\", chrom)\n            continue\n\n        # Add chromosome and position to prepare for sorting\n        variants.append((chrom_int, variant_obj['position'], variant_obj))\n\n    # Sort varants based on position\n    variants.sort(key=lambda x: (x[0], x[1]))\n\n    for variant in variants:\n        variant_obj = variant[2]\n        yield variant_obj"}
{"prompt": "def delete_uneeded(self):\n        \"\"\"\n        Delete the directory which are not registered into our structure.\n        \"\"\"", "output": "\n        for dir_name in os.listdir(self.root_dir):\n            if not os.path.isdir(os.path.join(self.root_dir, dir_name)):\n                continue\n            if not os.path.exists(os.path.join(self.root_dir, dir_name, 'info.json')):\n                shutil.rmtree(os.path.join(self.root_dir, dir_name))\n\n    def register_directory(self, directory):\n        \"\"\"\n        Register a directory into our structure.\n        \"\"\"\n        if not os.path.isdir(directory):", "solution": "\n\n        # We get the structure we have to apply.\n        structure = self._get_structure()\n\n        # We get the list of key which is implicitly the list of directory we do not bave to delete.\n        list_of_key = list(structure.keys())\n\n        # We move to the content of the parent as we know that we are creating only one directory.\n        # Note: if one day we will have to create multiple directory, we will have to change\n        # the following.\n        structure = structure[list_of_key[0]]\n\n        # We also set the parent directory as we are going to construct its childen.\n        parent_path = list_of_key[0]\n\n        if not parent_path.endswith(PyFunceble.directory_separator):\n            parent_path += PyFunceble.directory_separator\n\n        for root, _, _ in PyFunceble.walk(parent_path):\n            # We loop through each directories of the parent path.\n\n            # We fix the path in order to avoid issues.\n            root = Directory(root).fix_path()\n\n            if root.replace(parent_path, \"\") not in structure:\n                # The currently read directory is not in our structure.\n\n                # We delete it.\n                PyFunceble.rmtree(root)"}
{"prompt": "def is_effective_member(self, group_id, netid):\n        \"\"\"\n        Returns True if the netid is in the group, False otherwise.\n        \"\"\"", "output": "\n        try:\n            group = self.get_group(group_id)\n            return netid in group.members\n        except KeyError:\n            return False\n\n    def get_group(self, group_id):\n        \"\"\"\n        Returns the group with the given group_id.\n        \"\"\"\n        return self.groups[group_id]\n\n    def get_groups(self):\n        \"\"\"\n        Returns a list of all groups.\n        \"\"\"\n        return self.groups.values()\n\n    def get_group_members(self, group_id):\n        \"\"\"\n        Returns a list of all members in the group with the given group_id", "solution": "\n        self._valid_group_id(group_id)\n\n        # GWS doesn't accept EPPNs on effective member checks, for UW users\n        netid = re.sub('@washington.edu', '', netid)\n\n        url = \"{}/group/{}/effective_member/{}\".format(self.API,\n                                                       group_id,\n                                                       netid)\n\n        try:\n            data = self._get_resource(url)\n            return True  # 200\n        except DataFailureException as ex:\n            if ex.status == 404:\n                return False\n            else:\n                raise"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for root, dirs, files in os.walk(rootpath):\r\n        for filename in files:\r\n            if filename.endswith('.py') and filename not in skip:\r\n                modname = os.path.splitext(filename)[0]\r\n                modpath = os.path.join(root, filename)\r\n                mod = imp.load_source(modname, modpath)\r\n                modules[modname] = mod\r\n    return modules\r\n\r\n\r\ndef find_submodules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every submodule in the directory tree and return a dict\r\n    Hacked from sphinx.", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def Rock(*args, **kwargs):\n    \"\"\"\n    Graceful deprecation for old class name.\n    \"\"\"", "output": "\n    warnings.warn(\n        \"The.Rock class is deprecated and will be removed in a future release. Please usedl.models.Rock instead.\",\n        DeprecationWarning,\n    )\n    returndl.models.Rock(*args, **kwargs)\n\n\nclass Rock(nn.Module):\n    \"\"\"\n    Rock model from thedl library.\n\n    Args:\n       _channels (int): Number of channels inception.\n       _kernel_size (int): Size of kernel inception.\n       _stride (int): Stride of kernel in", "solution": "\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        w = \"The 'Rock' class was renamed 'Component'. \"\n        w += \"Please update your code.\"\n        warnings.warn(w, DeprecationWarning, stacklevel=2)\n\n    return Component(*args, **kwargs)"}
{"prompt": "def metadata(self):\n        \"\"\"Get metadata information in XML format.\"\"\"", "output": "\n        return self._metadata\n\n    def_xml(self):\n        \"\"\"Get XML representation of the.\"\"\"\n        return self._xml\n\n    def(self):\n        \"\"\"Get the in XML format.\"\"\"\n        return self._xml\n\n    def_xml(self):\n        \"\"\"Get XML representation of the.\"\"\"\n        return self._xml\n\n    def(self):\n        \"\"\"Get the in XML format.\"\"\"\n        return self._xml\n\n    def_xml(self):\n        \"\"\"Get XML representation of the.\"\"\"\n        return self._xml\n\n    def_xml(self):\n        \"\"\"Get XML representation of the test.\"\"\"", "solution": "\n\n        params = {\n            self.PCTYPE: self.CTYPE_XML\n        }\n\n        response = self.call(self.CGI_BUG, params)\n\n        return response"}
{"prompt": "def put(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a PUT request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"", "output": "\n        pass\n\n    def delete(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a DELETE request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"\n        pass\n\n    def handle_request(self, request):\n        \"\"\"\n        Handle a request.\n\n        This method is called by the web framework to handle the request. It\n        dispatches the request to the appropriate method", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        self.set_status(200)"}
{"prompt": "def safe_infer(\n    node: astroid.node_classes.NodeNG, context=None\n) -> Optional[astroid.node_classes.NodeNG]:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"", "output": "\n    if context is None:\n        context = astroid.infer.InferenceContext()\n\n    try:\n        inferred = astroid.infer.infer_node(node, context)\n        if inferred is None:\n            return None\n        if isinstance(inferred, list):\n            if len(inferred) > 1:\n                return None\n            inferred = inferred[0]\n        return inferred\n    except (astroid.InferenceError, astroid.AmbiguousInferenceError):\n        return None\n\n\ndef get_inferred_value(node: astroid.node_classes.NodeNG) -> Optional[Any]:\n    \"\"\"Return", "solution": "\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except astroid.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value"}
{"prompt": "def IsNotNone(*fields, default=None):\n    \"\"\"Selects whichever field is not None, in the specified order.\n\n    Arguments:\n        fields:\n            The fields to attempt to get a value from,\n            in order.\n\n        default:\n            The value to return in case all values are None.\n\n    Returns:\n        A Case-When expression that tries each field and\n        returns the specified default value when all of\n        them are None.\n    \"\"\"", "output": "\n    return Case(*[When(field is not None, field) for field in fields], default=default)\n\n\ndef IsNotNoneOrZero(*fields, default=None):\n    \"\"\"Selects whichever field is not None or zero, in the specified order.\n\n    Arguments:\n        fields:\n            The fields to attempt to get a value from,\n            in order.\n\n        default:\n            The value to return in case all values are None or zero.\n\n    Returns:\n        A Case-When expression that tries each field and\n        returns the specified default value when all of\n        them are None or zero.\n    \"\"\"\n   ", "solution": "\n\n    when_clauses = [\n        expressions.When(\n            ~expressions.Q(**{field: None}),\n            then=expressions.F(field)\n        )\n        for field in reversed(fields)\n    ]\n\n    return expressions.Case(\n        *when_clauses,\n        default=expressions.Value(default),\n        output_field=CharField()\n    )"}
{"prompt": "def authorize_redirect(self, oauth_scope, callback_uri=None,\n                           ax_attrs=[\"name\",\"email\",\"language\",\"username\"]):\n        \"\"\"Authenticates and authorizes for the given Google resource.\n\n        Some of the available resources are:\n\n        * Gmail Contacts - http://www.google.com/m8/feeds/\n        * Calendar - http://www.google.com/calendar/feeds/\n        * Finance - http://finance.google.com/finance/feeds/\n\n        You can authorize multiple resources by separating the resource\n        URLs with a space.\n        \"\"\"", "output": "\n        if not callback_uri:\n            callback_uri = self.get_default_callback_uri()\n\n        # Build the authorization URL\n        authorize_url = self.build_authorize_url(oauth_scope, callback_uri, ax_attrs)\n\n        # Redirect the user to the authorization URL\n        self.redirect(authorize_url)\n\n    def get_default_callback_uri(self):\n        \"\"\"Returns the default callback URI for the application.\"\"\"\n        return self.get_config(\"callback_uri\")\n\n    def build_authorize_url(self, oauth_scope, callback_uri, ax_attrs):", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs,\n                                 oauth_scope=oauth_scope)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def main(self,argv=None):\n        \"\"\"Run as a command-line script.\"\"\"", "output": "\n        if argv is None:\n            argv = sys.argv\n\n        # Parse command-line options\n        =_parser()\n        args = vars(yargparse.parse_args(argv[1:], parser=yargparse, namespace=yargparse.Namespace()))\n\n        # Set_parser options\n        for k, v in args.items():\n            setattr(yargparse, k, v)\n\n        # Run_parser\n       _parser.run()\n\nif __name__ == '__main__':\n    main()\n", "solution": "\n\n        parser = optparse.OptionParser(usage=USAGE % self.__class__.__name__)\n        newopt = parser.add_option\n        newopt('-i','--interact',action='store_true',default=False,\n               help='Interact with the program after the script is run.')\n\n        opts,args = parser.parse_args(argv)\n\n        if len(args) != 1:\n            print >> sys.stderr,\"You must supply exactly one file to run.\"\n            sys.exit(1)\n\n        self.run_file(args[0],opts.interact)"}
{"prompt": "def init_s3_keys(opt):\n    \"\"\"Initialize s3 access keys from environment variable or s3cfg config file.\"\"\"", "output": "\n    if opt.s3_access_key_id is None:\n        opt.s3_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n    if opt.s3_secret_access_key is None:\n        opt.s3_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n    if opt.s3_config_file is None:\n        opt.s3_config_file = os.path.expanduser('~/.s3cfg')\n    return opt\n\n\ndef init_s3_", "solution": "\n    S3Handler.S3_KEYS = S3Handler.s3_keys_from_cmdline(opt) or S3Handler.s3_keys_from_env() \\\n                        or S3Handler.s3_keys_from_s3cfg(opt)"}
{"prompt": "def _reshuffle(mat, shape):\n    \"\"\"Reshuffle the indicies of a bipartite matrix A[ij,kl] -> A[lj,ki].\"\"\"", "output": "\n  , ve = shape\n   _new = ve\n    ve_new = ve\n   _map = np.zeros(shape, dtype=int)\n    ve_map = np.zeros(shape, dtype=int)\n   _map[0, :] = np.arange(ve)\n    ve_map[:, 0] = np.arange(ve)\n   _map[1, :] = np.arange(ve)\n    ve_map[:, 1] = np.arange(ve)\n   _map[2, :] = np.arange(ve)\n    ve_map[:,", "solution": "\n    return np.reshape(\n        np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)),\n        (shape[3] * shape[1], shape[0] * shape[2]))"}
{"prompt": "def get_by_symbol_name(self, name: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"", "output": "\n        return self.get_by_symbol_type(name, \"signature\")\n\n    def get_by_symbol_type(self, name: str, symbol_type: str) -> Scope:\n        \"\"\" Retrieve a Set of all symbols by symbol type \"\"\"\n        return self.get_by_name(name, symbol_type)\n\n    def get_by_name(self, name: str, symbol_type: str) -> Scope:\n        \"\"\" Retrieve a Set of all symbols by name and symbol type \"\"\"\n        symbols = set()\n        for scope in self.scopes:\n            for symbol in scope.symbols:", "solution": "\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope"}
{"prompt": "def to_ndarray(self):\n        \"\"\"\n        Transfer JTensor to ndarray.\n        As SparseTensor may generate an very big ndarray, so we don't support this function for SparseTensor.\n        :return: a ndarray\n        \"\"\"", "output": "\n        if isinstance(self, SparseTensor):\n            raise NotImplementedError(\"SparseTensor doesn't support to_ndarray function.\")\n        return self.to_numpy()\n\n    def to_numpy(self):\n        \"\"\"\n        Transfer JTensor to numpy.\n        :return: a numpy array\n        \"\"\"\n        if self.is_sparse():\n            raise NotImplementedError(\"SparseTensor doesn't support to_numpy function.\")\n        return self.to_dense().to_numpy()\n\n    def to_dense(self):\n        \"\"\"\n        Transfer JTensor to dense tensor.\n        :return: a dense tensor\n        \"\"\"\n        if self.is_sparse():\n            raise", "solution": "\n        assert self.indices is None, \"sparseTensor to ndarray is not supported\"\n        return np.array(self.storage, dtype=get_dtype(self.bigdl_type)).reshape(self.shape)"}
{"prompt": "def deprecated(msg=''):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\n\n    :param msg:\n\n        Additional message added to the warning.\n\n    \"\"\"", "output": "\n    def decorator(func):\n        def new_func(*args, **kwargs):\n            import warnings\n            warnings.warn(f\"{func.__name__} is deprecated. {msg}\",\n                          category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        new_func.__name__ = func.__name__\n        new_func.__doc__ = func.__doc__\n        new_func.__dict__.update(func.__dict__)\n        return new_func\n    return decorator\n", "solution": "\n\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warning_string = \"Call to deprecated function or property `%s`.\" % func.__name__\n            warning_string = warning_string + ' ' + msg\n            warnings.warn(\n                warning_string,\n                category=DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return wrapper"}
{"prompt": "def uncache_zipdir(path):\n    \"\"\"Ensure that the importer caches dont have stale info for `path`\"\"\"", "output": "\n    import zipfile\n    with zipfile.ZipFile(path, 'r') as zf:\n        zf.extractall(path)\n\ndef uncache_zipfile(path):\n    \"\"\"Ensure that the importer caches dont have stale info for `path`\"\"\"\n    import zipfile\n    with zipfile.ZipFile(path, 'r') as zf:\n        zf.extractall(path)\n\ndef uncache_dir(path):\n    \"\"\"Ensure that the importer caches dont have stale info for `path`\"\"\"\n    import os\n    for root, dirs, files in os.walk(", "solution": "\n    from zipimport import _zip_directory_cache as zdc\n    _uncache(path, zdc)\n    _uncache(path, sys.path_importer_cache)"}
{"prompt": "def adjust_frame(self, pos, absolute_pos):\n        \"\"\"Adjust stack frame by pos positions. If absolute_pos then\n        pos is an absolute number. Otherwise it is a relative number.\n\n        A negative number indexes from the other end.\"\"\"", "output": "\n        if absolute_pos:\n            self.frame_pos += pos\n        else:\n            self.frame_pos += self.frame_size * pos\n\n    def get_frame_pos(self):\n        return self.frame_pos\n\n    def get_frame_size(self):\n        return self.frame_size\n\n    def get_frame_name(self):\n        return self.frame_name\n\n    def get_frame_locals(self):\n        return self.frame_locals\n\n    def get_frame_globals(self):\n        return self.frame_globals\n\n    def get_frame_code(self):\n        return", "solution": "\n        if not self.curframe:\n            Mmsg.errmsg(self, \"No stack.\")\n            return\n\n        # Below we remove any negativity. At the end, pos will be\n        # the new value of self.curindex.\n        if absolute_pos:\n            if pos >= 0:\n                pos = len(self.stack)-pos-1\n            else:\n                pos = -pos-1\n        else:\n            pos += self.curindex\n\n        if pos < 0:\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the oldest frame.\")\n            return\n        elif pos >= len(self.stack):\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the newest frame.\")\n            return\n\n        self.curindex = pos\n        self.curframe = self.stack[self.curindex][0]\n        self.print_location()\n        self.list_lineno = None\n        return"}
{"prompt": "def filter_unused_variable(line, previous_line=''):\n    \"\"\"Return line if used, otherwise return None.\"\"\"", "output": "\n    if previous_line.startswith('import ') or previous_line.startswith('from '):\n        return line\n    if line.startswith('def '):\n        return line\n    if line.startswith('class '):\n        return line\n    if line.startswith('global '):\n        return line\n    if line.startswith('nonlocal '):\n        return line\n    if line.startswith('assert '):\n        return line\n    if line.startswith('if '):\n        return line\n    if line.startswith('elif '):\n        return line\n    if line.startswith('else:'):\n        return line\n    if line.startswith('while '", "solution": "\n    if re.match(EXCEPT_REGEX, line):\n        return re.sub(r' as \\w+:$', ':', line, count=1)\n    elif multiline_statement(line, previous_line):\n        return line\n    elif line.count('=') == 1:\n        split_line = line.split('=')\n        assert len(split_line) == 2\n        value = split_line[1].lstrip()\n        if ',' in split_line[0]:\n            return line\n\n        if is_literal_or_name(value):\n            # Rather than removing the line, replace with it \"pass\" to avoid\n            # a possible hanging block with no body.\n            value = 'pass' + get_line_ending(line)\n\n        return get_indentation(line) + value\n    else:\n        return line"}
{"prompt": "def initialize(cls):\n        \"\"\"\n        Creates an **\"Example\"** entity of kind **\"NDBConfig\"** in the\n        datastore if the model is empty and raises and error to inform you that\n        you should populate the model with data.\n\n        .. note::\n\n            The *Datastore Viewer* in the ``_ah/admin/`` won't let you add\n            properties to a model if there is not an entity with that\n            property already. Therefore it is a good idea to keep the\n            **\"Example\"** entity (which has all possible properties set) in\n            the datastore.\n\n        \"\"\"", "output": "\n        if not cls.query().get():\n            raise ValueError(\n                \"Please populate the model with data before running this \"\n                \"script.\"\n            )\n\n\nclass NDBConfig(ndb.Model):\n    \"\"\"\n    A simple entity to store configuration settings for the application.\n\n    The entity has the following properties:\n\n    - **name**: The name of the setting.\n    - **value**: The value of the setting.\n    - **description**: A description of the setting.\n    - **type**: The type of the setting.\n    - **required**: Whether the setting is required or not.\n    - **choices**: A", "solution": "\n\n        if not len(cls.query().fetch()):\n\n            example = cls.get_or_insert('Example')\n\n            example.class_ = 'Provider class e.g. ' + \\\n                             '\"authomatic.providers.oauth2.Facebook\".'\n            example.provider_name = 'Your custom provider name e.g. \"fb\".'\n\n            # AuthorizationProvider\n            example.consumer_key = 'Consumer key.'\n            example.consumer_secret = 'Consumer secret'\n            example.provider_id = 1\n\n            # OAuth2\n            example.scope = 'coma, separated, list, of, scopes'\n\n            # AuthenticationProvider\n            example.identifier_param = 'Querystring parameter for claimed ' + \\\n                                       'id. default is \"id\"'\n\n            # Save the example\n            example.put()\n\n            # Raise an information error.\n            raise GAEError(\n                'A NDBConfig data model was created! Go to Datastore Viewer '\n                'in your dashboard and populate it with data!')"}
{"prompt": "def _config_session():\n        \"\"\"\n        Configure session for particular device\n\n        Returns:\n            tensorflow.Session\n        \"\"\"", "output": "\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        return tf.Session(config=config)\n\n\ndef_model(model_name,_data,_labels,__data,_labels_pred,_labels_true,__data_pred,_labels_pred__data_true,_labels_pred_true,_labels_pred_true,_labels_pred_true,_labels_pred_true,_labels_pred_true,", "solution": "\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.visible_device_list = '0'\n        return tf.Session(config=config)"}
{"prompt": "def _ensure_executor_running(self):\n        \"\"\"ensures all workers and management thread are running\n        \"\"\"", "output": "\n        if not self.executor.is_running():\n            self.executor.start()\n            self.executor_thread = threading.Thread(target=self.executor.run)\n            self.executor_thread.start()\n\n    def _stop_executor(self):\n        \"\"\"stops the executor and management thread\n        \"\"\"\n        if self.executor.is_running():\n            self.executor.stop()\n            self.executor_thread.join()\n\n    def _ensure_management_thread_running(self):\n        \"\"\"ensures the management thread is running\n        \"\"\"\n        if not self.management_thread.is_running():", "solution": "\n        with self._processes_management_lock:\n            if len(self._processes) != self._max_workers:\n                self._adjust_process_count()\n            self._start_queue_management_thread()"}
{"prompt": "def getPos(self):\n        \"\"\"\n        Returns the absolute position and size of the layer.\n        \n        This method is intended for use in vertex position calculation, as the border and offset have already been applied.\n        \n        The returned value is a 4-tuple of ``(sx,sy,ex,ey)``\\ .\n        The two values starting with an s are the \"start\" position, or the lower-left corner.\n        The second pair of values signify the \"end\" position, or upper-right corner.\n        \"\"\"", "output": "\n        return (self.x, self.y, self.x + self.width, self.y + self.height)\n    \n    def getBounds(self):\n        \"\"\"\n        Returns the absolute position and size of the layer, including the border and offset.\n        \n        This method is intended for use in bounding box calculation, as the border and offset have already been applied.\n        \n        The returned value is a 4-tuple of ``(sx,sy,ex,ey)``\\.\n        The two values starting with an s are the \"start\" position, or the lower-left corner.\n        The second pair of values sign", "solution": "\n        # Returns sx,sy,ex,ey\n        # sx,sy are bottom-left/lowest\n        # ex,ey are top-right/highest\n        sx,sy = self.widget.pos[0]+self.border[0]+self.offset[0],                       self.widget.pos[1]+self.border[1]+self.offset[1]\n        ex,ey = self.widget.pos[0]+self.widget.size[0]-self.border[0]+self.offset[0],   self.widget.pos[1]+self.widget.size[1]-self.border[1]+self.offset[1]\n        return sx,sy,ex,ey"}
{"prompt": "def users(self):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"", "output": "\n        return self._users\n\n    @property\n    def channels(self):\n        \"\"\"\n        List of channels of this slack team\n        \"\"\"\n        return self._channels\n\n    @property\n    def groups(self):\n        \"\"\"\n        List of groups of this slack team\n        \"\"\"\n        return self._groups\n\n    @property\n    def ims(self):\n        \"\"\"\n        List of of this slack team\n        \"\"\"\n        return self._ims\n\n    @property\n    def bot_users(self):\n        \"\"\"\n        List of bot users of this slack team\n        \"\"\"\n        return self._bot_users\n\n    @property\n    def bot_groups(self):\n        \"\"\"", "solution": "\n        if not self._users:\n            self._users = self._call_api('users.list')['members']\n        return self._users"}
{"prompt": "def decode_instruction(instruction):\n        \"\"\"\n        Decode whole instruction and return list of args.\n        Usually, returned arg[0] is the instruction opcode.\n\n        example:\n        >> args = decode_instruction('4.size,4.1024;')\n        >> args == ['size', '1024']\n        >> True\n\n        :param instruction: Instruction string.\n\n        :return: list\n        \"\"\"", "output": "\n        # Split instruction by comma\n        args = instruction.split(',')\n        # Remove spaces\n        args = [arg.strip() for arg in args]\n        # Remove symbol\n        args = [arg.replace('.', '') for arg in args]\n        # Return list of args\n        return args\n\n\ndef encode_instruction(opcode, *args):\n    \"\"\"\n    Encode instruction with opcode and args.\n    Args are converted to string and joined by comma.\n\n    example:\n    >> encode_instruction('size', 1024)\n    >> '4.size,4.1024;'\n\n    :param opcode: Instruction", "solution": "\n        if not instruction.endswith(INST_TERM):\n            raise InvalidInstruction('Instruction termination not found.')\n\n        # Use proper encoding\n        instruction = utf8(instruction)\n\n        # Get arg size\n        elems = instruction.split(ELEM_SEP, 1)\n\n        try:\n            arg_size = int(elems[0])\n        except Exception:\n            # Expected ValueError\n            raise InvalidInstruction(\n                'Invalid arg length.' +\n                ' Possibly due to missing element separator!')\n\n        arg_str = elems[1][:arg_size]\n\n        remaining = elems[1][arg_size:]\n\n        args = [arg_str]\n\n        if remaining.startswith(ARG_SEP):\n            # Ignore the ARG_SEP to parse next arg.\n            remaining = remaining[1:]\n        elif remaining == INST_TERM:\n            # This was the last arg!\n            return args\n        else:\n            # The remaining is neither starting with ARG_SEP nor INST_TERM.\n            raise InvalidInstruction(\n                'Instruction arg (%s) has invalid length.' % arg_str)\n\n        next_args = GuacamoleInstruction.decode_instruction(remaining)\n\n        if next_args:\n            args = args + next_args\n\n        return args"}
{"prompt": "def handle_combo(self,combo,symbol,modifiers,release=False,mod=True):\n        \"\"\"\n        Handles a key combination and dispatches associated events.\n        \n        First, all keybind handlers registered via :py:meth:`add` will be handled,\n        then the pyglet event :peng3d:pgevent:`on_key_combo` with params ``(combo,symbol,modifiers,release,mod)`` is sent to the :py:class:`Peng()` instance.\n        \n        Also sends the events :peng3d:event:`peng3d:keybind.combo`\\, :peng3d:event:`peng3d:keybind.combo.press` and :peng3d:event`peng3d:keybind.combo.release`\\ .\n        \n        :params str combo: Key combination pressed\n        :params int symbol: Key pressed, passed from the same argument within pyglet\n        :params int modifiers: Modifiers held while the key was pressed\n        :params bool release: If the combo was released\n        :params bool mod: If the combo was sent without mods\n        \"\"\"", "output": "3d:event:`peng3d:keybind.symbol`\\, :peng3d:event:`peng3d:keybind.modifiers`\\, :peng3d:event:`peng3d:keybind.release`\\, and :peng3d:event:`peng3d:keybind.mod`\\, with the same params.\n        \n        Args:\n            combo (str): The key combination.\n            symbol (int): The key symbol.\n            modifiers (int): The key modifiers.\n            release (bool): Whether the key is released or pressed.\n            mod (bool): Whether the key is a modifier key.", "solution": "\n        if self.peng.cfg[\"controls.keybinds.debug\"]:\n            print(\"combo: nm=%s %s\"%(mod,combo))\n        if mod:\n            for kbname in self.keybinds.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        else:\n            for kbname in self.keybinds_nm.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        self.peng.sendPygletEvent(\"on_key_combo\",(combo,symbol,modifiers,release,mod))\n        self.peng.sendEvent(\"peng3d:keybind.combo\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        if release:\n            self.peng.sendEvent(\"peng3d:keybind.combo.release\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        else:\n            self.peng.sendEvent(\"peng3d:keybind.combo.press\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})"}
{"prompt": "def expand_tokens(tokens, equal=False):\n    \"\"\"Given a list of tokens, return a generator of the chunks of\n    text for the data in the tokens.\n    \"\"\"", "output": "\n    for token in tokens:\n        if token.startswith(\"{{\") and token.endswith(\"}}\"):\n            # This is a token that needs to be expanded\n            yield token[2:-2]\n        elif equal and token.startswith(\"=\"):\n            # This is an equal sign, which is a special case\n            yield token\n        else:\n            # This is a normal token, so yield it as is\n            yield token\n\n\ndef expand_tokens_in_string(string, equal=False):\n    \"\"\"Given a string, return a new string with the tokens expanded.\n    \"\"\"\n    tokens = string.split()\n    expanded", "solution": "\n    for token in tokens:\n        for pre in token.pre_tags:\n            yield pre\n        if not equal or not token.hide_when_equal:\n            if token.trailing_whitespace:\n                yield token.html() + token.trailing_whitespace\n            else:\n                yield token.html()\n        for post in token.post_tags:\n            yield post"}
{"prompt": "def replay_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use replay detection for messages protected by\n        :meth:`get_mic` and :meth:`wrap`. False if replay detection cannot be used.\n        \"\"\"", "output": "\n        return self._replay_detection_negotiated\n\n    def get_mic(self, message):\n        \"\"\"\n        Get the message integrity code (MIC) for a message.\n\n        :param message: The message to get the MIC for.\n        :type message: bytes\n        :return: The MIC for the message.\n        :rtype: bytes\n        \"\"\"\n        raise NotImplementedError\n\n    def wrap(self, message, mic):\n        \"\"\"\n        Wrap a message with the message integrity code (MIC).\n\n        :param message: The message to wrap.\n        :type message: bytes\n        :param mic: The MIC for the message.\n        :", "solution": "\n        return (\n            self.flags & C.GSS_C_REPLAY_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"", "output": "\n        self.write(layout.table_header)\n        for row in layout.table_rows:\n            self.write(row)\n        self.write(layout.table_footer)\n\n    def visit_table_row(self, row):\n        \"\"\"display a single row of a table\"\"\"\n        self.write(row)\n\n    def visit_table_cell(self, cell):\n        \"\"\"display a single cell of a table\"\"\"\n        self.write(cell)\n\n    def visit_table_header(self, header):\n        \"\"\"display the header of a table\"\"\"\n        self.write(header)\n\n    def visit", "solution": "\n        table_content = self.get_table_content(layout)\n        # get columns width\n        cols_width = [0] * len(table_content[0])\n        for row in table_content:\n            for index, col in enumerate(row):\n                cols_width[index] = max(cols_width[index], len(col))\n        self.default_table(layout, table_content, cols_width)\n        self.writeln()"}
{"prompt": "def variants(context, collaborator, document_id, case_id, json):\n    \"\"\"Export causatives for a collaborator in .vcf format\"\"\"", "output": "\n   _json = json.loads(json)\n   _json[\"variant_id\"] = f\"{collaborator}_{document_id}_{case_id}\"\n   _json[\"variant_type\"] = \"causative\"\n    return json.dumps(v_json)\n\n\n@app.route(\"/export/variants\", methods=[\"POST\"])\ndef export_variants():\n    \"\"\"Export-variants for a collaborator in.vcf format\"\"\"\n    data = request.get_json()\n    collaborator = data[\"collaborator\"]\n    document_id = data[\"document_id\"]\n    case_id", "solution": "\n    LOG.info(\"Running scout export variants\")\n    adapter = context.obj['adapter']\n    collaborator = collaborator or 'cust000'\n\n    variants = export_variants(\n        adapter,\n        collaborator,\n        document_id=document_id,\n        case_id=case_id\n    )\n\n    if json:\n        click.echo(dumps([var for var in variants]))\n        return\n\n    vcf_header = VCF_HEADER\n\n    #If case_id is given, print more complete vcf entries, with INFO,\n    #and genotypes\n    if case_id:\n        vcf_header[-1] = vcf_header[-1] + \"\\tFORMAT\"\n        case_obj = adapter.case(case_id=case_id)\n        for individual in case_obj['individuals']:\n            vcf_header[-1] = vcf_header[-1] + \"\\t\" + individual['individual_id']\n\n    #print header\n    for line in vcf_header:\n        click.echo(line)\n\n    for variant_obj in variants:\n        variant_string = get_vcf_entry(variant_obj, case_id=case_id)\n        click.echo(variant_string)"}
{"prompt": "def _plot_formatting(title, est_file, algo_ids, last_bound, N, output_file):\n    \"\"\"Formats the plot with the correct axis labels, title, ticks, and\n    so on.\"\"\"", "output": "\n   ___ =_u_v_ =_w_ =_p_ =_q_ =_r_ =_s_ =_t_ =_u_ =_v_ =_w_ = 0\n   _p_ =_q_ =_r_ = 0\n   _s_ = 0\n   _t_ = 0\n   _u_ = 0\n   _v_ = 0\n   _w_ = 0\n\n   _p_ =", "solution": "\n    import matplotlib.pyplot as plt\n    if title is None:\n        title = os.path.basename(est_file).split(\".\")[0]\n    plt.title(title)\n    plt.yticks(np.arange(0, 1, 1 / float(N)) + 1 / (float(N) * 2))\n    plt.gcf().subplots_adjust(bottom=0.22)\n    plt.gca().set_yticklabels(algo_ids)\n    plt.xlabel(\"Time (seconds)\")\n    plt.xlim((0, last_bound))\n    plt.tight_layout()\n    if output_file is not None:\n        plt.savefig(output_file)\n    plt.show()"}
{"prompt": "def f_remove(self, recursive=True, predicate=None):\n        \"\"\"Recursively removes the group and all it's children.\n\n        :param recursive:\n\n            If removal should be applied recursively. If not, node can only be removed\n            if it has no children.\n\n        :param predicate:\n\n            In case of recursive removal, you can selectively remove nodes in the tree.\n            Predicate which can evaluate for each node to ``True`` in order to remove the node or\n            ``False`` if the node should be kept. Leave ``None`` if you want to remove all nodes.\n\n        \"\"\"", "output": "\n        if recursive:\n            for child in self.children:\n                child.f_remove(recursive, predicate)\n        if predicate is None or predicate(self):\n            self.parent.children.remove(self)\n            self.parent = None\n            self.children = []\n            self.parent_id = None\n            self.id = None\n            self.name = None\n            self.description = None\n            self.created_at = None\n            self.updated_at = None\n            self.deleted_at = None\n            self.deleted = True\n            self.save()\n\n    def f_move(self, new_parent", "solution": "\n        parent = self.f_get_parent()\n        parent.f_remove_child(self.v_name, recursive=recursive, predicate=predicate)"}
{"prompt": "def _wrap(text, wrap_at=120, indent=4):\n    \"\"\"\n    Return piece of text, wrapped around if needed.\n\n    :param text: text that may be too long and then needs to be wrapped.\n    :param wrap_at: the maximum line length.\n    :param indent: number of spaces to prepend to all subsequent lines after the first.\n    \"\"\"", "output": "\n    lines = text.split('\\n')\n    wrapped_lines = []\n    for line in lines:\n        if len(line) > wrap_at:\n            wrapped_lines.extend(textwrap.wrap(line, wrap_at))\n        else:\n            wrapped_lines.append(line)\n    return '\\n'.join([indent *'' + line for line in wrapped_lines])\n\n\ndef _get_description(package):\n    \"\"\"\n    Get the description of a package.\n\n    :param package: the package to get the description for.\n    :return: the description of the package.\n    \"\"\"\n   ", "solution": "\n    out = \"\"\n    curr_line_length = indent\n    space_needed = False\n    for word in text.split():\n        if curr_line_length + len(word) > wrap_at:\n            out += \"\\n\" + \" \" * indent\n            curr_line_length = indent\n            space_needed = False\n        if space_needed:\n            out += \" \"\n            curr_line_length += 1\n        out += word\n        curr_line_length += len(word)\n        space_needed = True\n    return out"}
{"prompt": "def sanitize_for_archive(url, headers, payload):\n        \"\"\"Sanitize payload of a HTTP request by removing the login, password and token information\n        before storing/retrieving archived items\n\n        :param: url: HTTP url request\n        :param: headers: HTTP headers request\n        :param: payload: HTTP payload request\n\n        :returns url, headers and the sanitized payload\n        \"\"\"", "output": "\n        # Remove login, password and token information\n        if 'Authorization' in headers:\n            headers['Authorization'] = 'Bearer'+ 'REDACTED'\n        if 'Authorization' in payload:\n            payload['Authorization'] = 'Bearer'+ 'REDACTED'\n        if 'password' in payload:\n            payload['password'] = 'REDACTED'\n        if 'username' in payload:\n            payload['username'] = 'REDACTED'\n        if 'token' in payload:\n            payload['token'] = 'REDACTED'\n\n        return url, headers, payload\n", "solution": "\n        if BugzillaRESTClient.PBUGZILLA_LOGIN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_LOGIN)\n\n        if BugzillaRESTClient.PBUGZILLA_PASSWORD in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_PASSWORD)\n\n        if BugzillaRESTClient.PBUGZILLA_TOKEN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_TOKEN)\n\n        return url, headers, payload"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"", "output": "\n        while True:\n            try:\n                # Get the with the latest data\n                latest_feed_data = self.data_feed.get_latest_feed_data()\n\n                # Check if the latest feed data is valid\n                if latest_feed_data is None:\n                    # If the latest feed data is None, wait for a while and try again\n                    time.sleep(self.data_feed.retry_interval)\n                    continue\n\n                # Check if the latest feed data is for the same as the current\n                if latest_feed_data.post_id == self.post_id:\n                   ", "solution": "\n        messages = self.poll_datafeeds()\n        notifications = self.process_notifications(messages)\n\n        self.draw_notifications(notifications)"}
{"prompt": "def user_agent():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"", "output": "\n    return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n\ndef get_page_content(url):\n    \"\"\"\n    Get the content of a web page.\n    \"\"\"\n    headers = {'User-Agent': user_agent()}\n    response = requests.get(url, headers=headers)\n    return response.content\n\ndef get_page_title(url):", "solution": "\n    data = {\n        \"installer\": {\"name\": \"pip\", \"version\": pip.__version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == 'CPython':\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'PyPy':\n        if sys.pypy_version_info.releaselevel == 'final':\n            pypy_version_info = sys.pypy_version_info[:3]\n        else:\n            pypy_version_info = sys.pypy_version_info\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == 'Jython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'IronPython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        distro = dict(filter(\n            lambda x: x[1],\n            zip([\"name\", \"version\", \"id\"], platform.linux_distribution()),\n        ))\n        libc = dict(filter(\n            lambda x: x[1],\n            zip([\"lib\", \"version\"], platform.libc_ver()),\n        ))\n        if libc:\n            distro[\"libc\"] = libc\n        if distro:\n            data[\"distro\"] = distro\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"OS X\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )"}
{"prompt": "def headers_present(self, headers):\n        \"\"\"\n        Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .headers_present(['content-type', 'Authorization']))\n        \"\"\"", "output": "\n        self.headers_present = headers\n        return self\n\n    def headers_absent(self, headers):\n        \"\"\"\n        Defines a list of headers that must not be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n               .headers_absent(['content-type', 'Authorization']))\n        \"\"\"\n        self.headers_absent", "solution": "\n        headers = {name: re.compile('(.*)') for name in headers}\n        self.add_matcher(matcher('HeadersMatcher', headers))"}
{"prompt": "def resolve_path(self, path, root_id='me/skydrive', objects=False, listdir_limit=500):\n\t\t\"\"\"Return id (or metadata) of an object, specified by chain\n\t\t\t\t(iterable or fs-style path string) of \"name\" attributes\n\t\t\t\tof its ancestors, or raises DoesNotExists error.\n\n\t\t\tRequires many calls to resolve each name in path, so use with care.\n\t\t\t\troot_id parameter allows to specify path relative to some folder_id\n\t\t\t\t(default: me/skydrive).\"\"\"", "output": "\n\t\tif isinstance(path, str):\n\t\t\tpath = path.split('/')\n\t\tif not path:\n\t\t\traise ValueError('Path must be a non-empty iterable')\n\t\tif not isinstance(path, (list, tuple)):\n\t\t\traise ValueError('Path must be an iterable or a fs-style path string')\n\t\tif not objects:\n\t\t\traise ValueError('Objects parameter must be True')\n\t\tif not isinstance(listdir_limit, int) or listdir_limit <= 0:\n\t\t\traise ValueError('Listdir limit must be a positive integer')\n\n\t\t# Resolve root_id\n\t", "solution": "\n\t\tif path:\n\t\t\tif isinstance(path, types.StringTypes):\n\t\t\t\tif not path.startswith('me/skydrive'):\n\t\t\t\t\t# Split path by both kinds of slashes\n\t\t\t\t\tpath = filter(None, it.chain.from_iterable(p.split('\\\\') for p in path.split('/')))\n\t\t\t\telse: root_id, path = path, None\n\t\t\tif path:\n\t\t\t\ttry:\n\t\t\t\t\tfor i, name in enumerate(path):\n\t\t\t\t\t\toffset = None\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tobj_list = self.listdir(root_id, offset=offset, limit=listdir_limit)\n\t\t\t\t\t\t\ttry: root_id = dict(it.imap(op.itemgetter('name', 'id'), obj_list))[name]\n\t\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\t\tif len(obj_list) < listdir_limit: raise # assuming that it's the last page\n\t\t\t\t\t\t\t\toffset = (offset or 0) + listdir_limit\n\t\t\t\t\t\t\telse: break\n\t\t\t\texcept (KeyError, ProtocolError) as err:\n\t\t\t\t\tif isinstance(err, ProtocolError) and err.code != 404: raise\n\t\t\t\t\traise DoesNotExists(root_id, path[i:])\n\t\treturn root_id if not objects else self.info(root_id)"}
{"prompt": "def require(self, key):\n        \"\"\"\n        Raises an exception if value for ``key`` is empty.\n        \"\"\"", "output": "\n        if not self.has_key(key):\n            raise KeyError(key)\n        value = self[key]\n        if not value:\n            raise ValueError(f\"Value for key '{key}' is empty.\")\n        return value\n\n    def get(self, key, default=None):\n        \"\"\"\n        Returns the value for ``key``, or ``default`` if it is not found.\n        \"\"\"\n        return self.get_or_require(key, default)\n\n    def get_or_require(self, key, default=None):\n        \"\"\"\n        Returns the value for ``key``, or raises a KeyError if it is", "solution": "\n        value = self.get(key)\n        if not value:\n            raise ValueError('\"{}\" is empty.'.format(key))\n        return value"}
{"prompt": "def Rconverter(Robj, dataframe=False):\n    \"\"\"\n    Convert an object in R's namespace to one suitable\n    for ipython's namespace.\n\n    For a data.frame, it tries to return a structured array.\n    It first checks for colnames, then names.\n    If all are NULL, it returns np.asarray(Robj), else\n    it tries to construct a recarray\n\n    Parameters\n    ----------\n\n    Robj: an R object returned from rpy2\n    \"\"\"", "output": "\n    if.is_data_frame(Robj):\n        if.is_null(Robj.colnames):\n            colnames = None\n        else:\n            colnames = Robj.colnames\n        if.is_null(Robj.names):\n            names = None\n        else:\n            names = Robj.names\n        if colnames is None and names is None:\n            return np.asarray(Robj)\n        else:\n            return np.rec.fromrecords(Robj, names=names, dtype=Robj.dtypes,\n                                     formats=Robj.formats)\n    else:", "solution": "\n    is_data_frame = ro.r('is.data.frame')\n    colnames = ro.r('colnames')\n    rownames = ro.r('rownames') # with pandas, these could be used for the index\n    names = ro.r('names')\n\n    if dataframe:\n        as_data_frame = ro.r('as.data.frame')\n        cols = colnames(Robj)\n        _names = names(Robj)\n        if cols != ri.NULL:\n            Robj = as_data_frame(Robj)\n            names = tuple(np.array(cols))\n        elif _names != ri.NULL:\n            names = tuple(np.array(_names))\n        else: # failed to find names\n            return np.asarray(Robj)\n        Robj = np.rec.fromarrays(Robj, names = names)\n    return np.asarray(Robj)"}
{"prompt": "def pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"", "output": "\n    try:\n        os.kill(pid, 0)\n    except OSError:\n        return False\n    else:\n        return True\n\ndef kill_process(pid):\n    \"\"\"Kill a process by its pid.\"\"\"\n    if pid_exists(pid):\n        os.kill(pid, signal.SIGKILL)\n\ndef kill_all_processes():\n    \"\"\"Kill all processes in the current process table.\"\"\"\n    for pid in psutil.pids():\n        kill_process(pid)\n\ndef get_process_info(pid):\n    \"\"\"Get information about a process by its pid.\"\"\"\n    try:\n       ", "solution": "\n    if pid < 0:\n        return False\n    try:\n        os.kill(pid, 0)\n    except OSError as exc:\n        logging.debug(\"No process[%s]: %s\", exc.errno, exc)\n        return exc.errno == errno.EPERM\n    else:\n        p = psutil.Process(pid)\n        return p.status != psutil.STATUS_ZOMBIE"}
{"prompt": "def start_hb(self, callback):\n        \"\"\"Start the heartbeating and call the callback if the kernel dies.\"\"\"", "output": "\n        self.hb_thread = threading.Thread(target=self.heartbeat, args=(callback,))\n        self.hb_thread.start()\n\n    def stop_hb(self):\n        \"\"\"Stop the heartbeating thread.\"\"\"\n        if self.hb_thread:\n            self.hb_thread.join()\n            self.hb_thread = None\n\n    def heartbeat(self, callback):\n        \"\"\"Send a heartbeat to the kernel and call the callback if the kernel dies.\"\"\"\n        while self.hb_thread:\n            try:\n                self.kernel.heartbeat()\n            except Exception as e:\n                self.stop_hb()", "solution": "\n        if not self._beating:\n            self._kernel_alive = True\n\n            def ping_or_dead():\n                self.hb_stream.flush()\n                if self._kernel_alive:\n                    self._kernel_alive = False\n                    self.hb_stream.send(b'ping')\n                    # flush stream to force immediate socket send\n                    self.hb_stream.flush()\n                else:\n                    try:\n                        callback()\n                    except:\n                        pass\n                    finally:\n                        self.stop_hb()\n\n            def beat_received(msg):\n                self._kernel_alive = True\n\n            self.hb_stream.on_recv(beat_received)\n            loop = ioloop.IOLoop.instance()\n            self._hb_periodic_callback = ioloop.PeriodicCallback(ping_or_dead, self.time_to_dead*1000, loop)\n            loop.add_timeout(time.time()+self.first_beat, self._really_start_hb)\n            self._beating= True"}
{"prompt": "def _chunk_noise(noise):\n    \"\"\"Chunk input noise data into valid Touchstone file rows.\"\"\"", "output": "\n   _rows = []\n   _cols = []\n   _data = []\n   _ = []\n   __ = []\n   ____ = []\n   ____ = []\n   _ rows_ = []\n   _ cols_ = []\n   _ data_ = []\n   __ rows__ = []\n    me_ cols__ = []\n    me_ data__ = []\n    me_ rows___ = []\n    me_ cols___ = []\n    me_ data___ = []\n   _ rows____ = []\n   _ cols____ = []\n   ", "solution": "\n    data = zip(\n        noise[\"freq\"],\n        noise[\"nf\"],\n        np.abs(noise[\"rc\"]),\n        np.angle(noise[\"rc\"]),\n        noise[\"res\"],\n    )\n    for freq, nf, rcmag, rcangle, res in data:\n        yield freq, nf, rcmag, rcangle, res"}
{"prompt": "def is_lesser(a, b):\n    \"\"\"\n    Verify that an item *a* is <= then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"", "output": "\n    if a is None:\n        return True\n    if b is None:\n        return False\n    return a <= b\n\n\ndef is_greater(a, b):\n    \"\"\"\n    Verify that an item *a* is >= then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"\n    if a is None:\n        return False\n    if b is None:\n        return True\n    return a >= b\n\n\ndef is_equal(a, b):\n    \"\"\"\n    Verify that an item *a* is == then an item *b*\n    \n   ", "solution": "\n\n    if type(a) != type(b):\n        return False\n\n    if isinstance(a, str) and isinstance(b, str):\n        return a == b\n    elif isinstance(a, bool) and isinstance(b, bool):\n        return a == b\n    elif isinstance(a, list) and isinstance(b, list):\n        for element in a:\n            flag = 0\n            for e in b:\n                if is_lesser(element, e):\n                    flag = 1\n                    break\n            if not flag:\n                return False\n        return True\n    elif isinstance(a, dict) and isinstance(b, dict):\n        if is_lesser(list(a.keys()), list(b.keys())):\n            for key, val in a.items():\n                if not is_lesser(val, b[key]):\n                    return False\n            return True\n        return False\n    elif isinstance(a, int) and isinstance(b, int):\n        return a <= b\n    elif isinstance(a, float) and isinstance(b, float):\n        return a <= b\n\n    return False"}
{"prompt": "def isreshapeable(new, old):\n    \"\"\"\n    Check to see if a proposed tuple of axes is a valid reshaping of\n    the old axes by ensuring that they can be factored.\n\n    Parameters\n    ----------\n    new : tuple\n        tuple of proposed axes\n\n    old : tuple\n        tuple of old axes\n    \"\"\"", "output": "2\n    Returns\n    -------\n    bool\n        True if the proposed tuple of axes can be factored into a valid\n        reshaping of the old axes, False otherwise.\n    \"\"\"\n    # Check that the axes are valid\n   _axes = set(new) - set(old)\n    if not_axes:\n        return True\n    # Check that the axes are notlyly dependent on axes\n   _axes =_axes.intersection(old)\n   _axes_set = set(zip(new, old))\n   _axes_set.discard(tuple(sorted(new", "solution": "\n\n    new, old = tupleize(new), tupleize(old)\n\n    if not prod(new) == prod(old):\n        raise ValueError(\"Total size of new keys must remain unchanged\")"}
{"prompt": "def _simple_ast_generator(gen_ast):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"", "output": "\n    def wrapper(node):\n        return GeneratedPyAST(gen_ast(node))\n    return wrapper\n\n\ndef generate_ast(node):\n    \"\"\"Generate a Python AST from a Python node.\"\"\"\n    if isinstance(node, ast.Module):\n        return _simple_ast_generator(generate_ast)(node)\n    elif isinstance(node, ast.FunctionDef):\n        return _simple_ast_generator(generate_ast)(node)\n    elif isinstance(node, ast.ClassDef):\n        return _simple_ast_generator(generate_ast)(node)\n    elif isinstance(node, ast.Assign):", "solution": "\n\n    @wraps(gen_ast)\n    def wrapped_ast_generator(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:\n        return GeneratedPyAST(node=gen_ast(ctx, form))\n\n    return wrapped_ast_generator"}
{"prompt": "def next_opcode(code, offset):\n    \"\"\"Return the next opcode and offset as a tuple. Tuple (-100,\n    -1000) is returned when reaching the end.\"\"\"", "output": "\n    if offset >= len(code):\n        return (-100, -1000)\n    return (code[offset], offset + 1)\n\n\ndef parse_code(code):\n    \"\"\"Parse thecode code into a list of commands.\"\"\"\n   code = []\n    offset = 0\n    while True:\n        opcode, new_offset = next_opcode(code, offset)\n        if opcode == -100:\n            break\n        if opcode == 0:\n            # Push constant\n            value = int(code[offset + 1:offset + 5], 16)", "solution": "\n    n = len(code)\n    while offset < n:\n        op = code[offset]\n        offset += 1\n        if op >= HAVE_ARGUMENT:\n            offset += 2\n            pass\n        yield op, offset\n        pass\n    yield -100, -1000\n    pass"}
{"prompt": "def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,\n                        name=None):\n  \"\"\"Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):\n    output = tf.reduce_sum(x)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"", "output": " not met.\n    summarize:  A string that specifies how to summarize the tensors.\n    message:  Optional string to print if the condition is not met.\n    name:  Optional name for the operation.\n\n  Returns:\n    A scalar `Tensor` with value 1.\n  \"\"\"\n  with tf.name_scope(name, \"assert_rank_at_most\", [x, rank]):\n    x_rank = tf.rank(x)\n    condition = tf.less_equal(x_rank, rank)\n    if data is not None:\n      tf.logging.info(\"Condition not met: %s", "solution": "\n  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):\n    return tf.compat.v1.assert_less_equal(\n        tf.rank(x), rank, data=data, summarize=summarize, message=message)"}
{"prompt": "def insert_instance_template(self, body, request_id=None, project_id=None):\n        \"\"\"\n        Inserts instance template using body specified\n        Must be called with keyword arguments rather than positional.\n\n        :param body: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again)\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"", "output": " result in duplicate resource creation)\n        :type request_id: str\n        :param project_id: Optional, project ID where the instance template should be created\n        :type project_id: str\n        :return: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :rtype: dict\n        \"\"\"\n        return self._insert_resource(\n            'instanceTemplates',\n            body,\n            request_id=request_id,\n            project_id=project_id\n        )\n\n    def insert_disk(self, body, request_id", "solution": "\n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"}
{"prompt": "def _process_op_err(self, e):\n        \"\"\"\n        Process errors which occured while reading or parsing\n        the protocol. If allow_reconnect is enabled it will\n        try to switch the server to which it is currently connected\n        otherwise it will disconnect.\n        \"\"\"", "output": "\n        if self.allow_reconnect:\n            self.reconnect()\n        else:\n            self.disconnect()\n        self.logger.error(f\"Error while processing operation: {e}\")\n\n    def _process_data(self, data):\n        \"\"\"\n        Process data received from the server.\n        \"\"\"\n        try:\n            data = json.loads(data)\n        except json.JSONDecodeError:\n            self.logger.error(\"Invalid JSON received from the server\")\n            return\n\n        if \"type\" in data:\n            if data[\"type\"] == \"error\":\n                self._process_op_err(data[\"error\"])", "solution": "\n        if self.is_connecting or self.is_closed or self.is_reconnecting:\n            return\n\n        if self.options[\"allow_reconnect\"] and self.is_connected:\n            self._status = Client.RECONNECTING\n            self._ps.reset()\n\n            if self._reconnection_task is not None and not self._reconnection_task.cancelled():\n                # Cancel the previous task in case it may still be running.\n                self._reconnection_task.cancel()\n\n            self._reconnection_task = self._loop.create_task(self._attempt_reconnect())\n        else:\n            self._process_disconnect()\n            self._err = e\n            yield from self._close(Client.CLOSED, True)"}
{"prompt": "def get_generator(self, path, *args, **kw_args):\n        \"\"\"\n        Get a generator that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the generator one row at a time.\n        Unlike the direct access queue, this generator also returns the remainder elements.\n        Additional arguments are forwarded to get_queue.\n        See the get_queue method for documentation of these parameters.\n\n        :param path:\n        :return: A generator that iterates over the rows in the dataset.\n        \"\"\"", "output": "\n        return self.get_queue(path, *args, **kw_args)\n\n    def get_queue(self, path, batch_size=None, buffer_size=None, shuffle=False, repeat=False,\n                  prefetch=None, num_parallel_calls=None, **kw_args):\n        \"\"\"\n        Get a queue that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the queue one row at a time.\n        Additional arguments are forwarded to tf.data.Dataset.batch.\n        See the tf.data.Dataset.batch method for documentation of these parameters", "solution": "\n        q = self.get_queue(path=path, *args, **kw_args)\n\n        try:\n            # This generator just implements a standard access pattern for the direct access queue.\n            for guard in q.iter():\n                with guard as batch:\n                    batch_copy = batch.copy()\n\n                for row in batch_copy:\n                    yield row\n\n            last_batch = self.get_remainder(path, q.block_size)\n            for row in last_batch:\n                yield row\n\n        finally:\n            q.close()"}
{"prompt": "def running_under_virtualenv():\n    \"\"\"\n    Return True if we're running inside a virtualenv, False otherwise.\n\n    \"\"\"", "output": "\n    try:\n        import virtualenv\n    except ImportError:\n        return False\n    return virtualenv.is_virtualenv()\n\n\ndef get_virtualenv_path():\n    \"\"\"\n    Return the path to the virtualenv if we're running inside a virtualenv,\n    None otherwise.\n\n    \"\"\"\n    if running_under_virtualenv():\n        return virtualenv.get_env_folder()\n    else:\n        return None\n\n\ndef get_virtualenv_python():\n    \"\"\"\n    Return the Python executable path for the virtualenv if we're running\n    inside a virtualenv, None otherwise.\n\n    \"\"\"\n    if running_under_virtualenv():\n        return os.path.join(get", "solution": "\n    if hasattr(sys, 'real_prefix'):\n        return True\n    elif sys.prefix != getattr(sys, \"base_prefix\", sys.prefix):\n        return True\n\n    return False"}
{"prompt": "def build(self, pre=None, shortest=False):\n        \"\"\"Build the String instance\n\n        :param list pre: The prerequisites list (optional, default=None)\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"", "output": "\n        if pre is None:\n            pre = []\n\n        # Initialize and_\n        self. = set()\n        self. visited = set()\n        self. = set()\n        self._ = set()\n\n        # Initialize and_\n        self. = set()\n        self._ = set()\n\n        # Initialize and_\n        self. = set()\n        self._ = set()\n\n        # Initialize and_\n        self. = set()\n        self._ = set()\n\n        # Initialize and_\n        self. = set()", "solution": "\n        if pre is None:\n            pre = []\n\n        if self.value is not None and rand.maybe():\n            return utils.val(self.value, pre, shortest=shortest)\n\n        length = super(String, self).build(pre, shortest=shortest)\n        res = rand.data(length, self.charset)\n        return res"}
{"prompt": "def chimera_block_quotient(G, blocks):\n    \"\"\"\n    Extract the blocks from a graph, and returns a\n    block-quotient graph according to the acceptability\n    functions block_good and eblock_good\n\n    Inputs:\n        G: a networkx graph\n        blocks: a tuple of tuples\n\n    \"\"\"", "output": "\n   _blocks = []\n    for,_blocks_ in blocks:\n       _blocks.extend(zip(range(len(G.nodes())), [b for, b in_blocks_est for in_blocks_est if!=_blocks_est[0][0]]))\n   _blocks = set(tuple(sorted(b)) for b in_blocks)\n   _blocks = sorted(list(set(tuple(sorted(b)) for b in_blocks)))\n\n   _blocks_dict = {b: [] for b in_", "solution": "\n    from networkx import Graph\n    from itertools import product\n\n    BG = Graph()\n    blockid = {}\n    for i, b in enumerate(blocks):\n        BG.add_node(i)\n        if not b or not all(G.has_node(x) for x in b):\n            continue\n        for q in b:\n            if q in blockid:\n                raise(RuntimeError, \"two blocks overlap\")\n            blockid[q] = i\n\n    for q, u in blockid.items():\n        ublock = blocks[u]\n        for p in G[q]:\n            if p not in blockid:\n                continue\n            v = blockid[p]\n            if BG.has_edge(u, v) or u == v:\n                continue\n            vblock = blocks[v]\n\n            if ublock[0][2] == vblock[0][2]:\n                block_edges = zip(ublock, vblock)\n            else:\n                block_edges = product(ublock, vblock)\n\n            if all(G.has_edge(x, y) for x, y in block_edges):\n                BG.add_edge(u, v)\n\n    return BG"}
{"prompt": "def process_commands(self):\n        \"\"\"Handle debugger commands.\"\"\"", "output": "\n        while True:\n            command = input(\"Debugger> \")\n            if command == \"help\":\n                print(\"Available commands:\")\n                print(\"- help: print this message\")\n                print(\"- continue: continue execution\")\n                print(\"- step: step over the next line of code\")\n                print(\"- next: step over the next line of code and execute the next line of code\")\n                print(\"- back: step back to the previous line of code\")\n                print(\"- quit: exit the debugger\")\n            elif command == \"continue\":\n                self.continue_execution()\n            elif command == \"step\":\n                self.step_over()\n            elif", "solution": "\n        if self.core.execution_status != 'No program':\n            self.setup()\n            self.location()\n            pass\n        leave_loop = run_hooks(self, self.preloop_hooks)\n        self.continue_running = False\n\n        while not leave_loop:\n            try:\n                run_hooks(self, self.precmd_hooks)\n                # bdb had a True return to leave loop.\n                # A more straight-forward way is to set\n                # instance variable self.continue_running.\n                leave_loop = self.process_command()\n                if leave_loop or self.continue_running: break\n            except EOFError:\n                # If we have stacked interfaces, pop to the next\n                # one.  If this is the last one however, we'll\n                # just stick with that.  FIXME: Possibly we should\n                # check to see if we are interactive.  and not\n                # leave if that's the case. Is this the right\n                # thing?  investigate and fix.\n                if len(self.debugger.intf) > 1:\n                    del self.debugger.intf[-1]\n                    self.last_command = ''\n                else:\n                    if self.debugger.intf[-1].output:\n                        self.debugger.intf[-1].output.writeline('Leaving')\n                        raise Mexcept.DebuggerQuit\n                        pass\n                    break\n                pass\n            pass\n        return run_hooks(self, self.postcmd_hooks)"}
{"prompt": "def table(name, auth=None, eager=True):\n    \"\"\"Returns a given table for the given user.\"\"\"", "output": "\n    if auth is None:\n        auth = current_user.is_authenticated\n\n    if not auth:\n        raise Unauthorized(\"You must be authenticated to access this resource.\")\n\n    if eager:\n        return.tables[name]\n    else:\n        return.tables.get(name)\n\n\ndef get_table(name, auth=None, eager=True):\n    \"\"\"Returns a given table for the given user, or None if not found.\"\"\"\n    table = table(name, auth, eager)\n    return table if table else None\n\n\ndef get_tables(auth=None, eager=True):\n    \"\"\"", "solution": "\n    auth = auth or []\n    dynamodb = boto.connect_dynamodb(*auth)\n\n    table = dynamodb.get_table(name)\n    return Table(table=table, eager=eager)"}
{"prompt": "def _is_raising(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node raise an exception\"\"\"", "output": "\n    if len(body) == 0:\n        return False\n    if not isinstance(body[0], ast.Raise):\n        return False\n    return True\n\n\ndef _is_returning(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node returns a value\"\"\"\n    if len(body) == 0:\n        return False\n    if not isinstance(body[0], ast.Return):\n        return False\n    return True\n\n\ndef _is_assigning(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node assigns a value to a variable", "solution": "\n    for node in body:\n        if isinstance(node, astroid.Raise):\n            return True\n    return False"}
{"prompt": "def get_assembly_size(assembly_file):\n    \"\"\"Returns the number of nucleotides and the size per contig for the\n    provided assembly file path\n\n    Parameters\n    ----------\n    assembly_file : str\n        Path to assembly file.\n\n    Returns\n    -------\n    assembly_size : int\n        Size of the assembly in nucleotides\n    contig_size : dict\n        Length of each contig (contig name as key and length as value)\n\n    \"\"\"", "output": "\n    with open(assembly_file, 'r') as f:\n        lines = f.readlines()\n\n    assembly_size = 0\n    contig_size = {}\n\n    for line in lines:\n        if line.startswith('>'):\n            contig_name = line.strip()[1:]\n            contig_size[contig_name] = 0\n        else:\n            assembly_size += len(line.strip())\n\n    return assembly_size, contig_size\n\n\ndef get_assembly_stats(assembly_file,_contigs=None):\n    \"\"\"Returns the and", "solution": "\n\n    assembly_size = 0\n    contig_size = {}\n    header = \"\"\n\n    with open(assembly_file) as fh:\n        for line in fh:\n\n            # Skip empty lines\n            if line.strip() == \"\":\n                continue\n\n            if line.startswith(\">\"):\n                header = line.strip()[1:]\n                contig_size[header] = 0\n\n            else:\n                line_len = len(line.strip())\n                assembly_size += line_len\n                contig_size[header] += line_len\n\n    return assembly_size, contig_size"}
{"prompt": "def compute_index_key(self, to_instance):\n        \"\"\"\n        Compute the index key that can be used to identify an instance\n        on the link.\n        \"\"\"", "output": "\n        raise NotImplementedError\n\n    def get_link_type(self):\n        \"\"\"\n        Return the type of link that this class represents.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_link_name(self):\n        \"\"\"\n        Return the name of the link that this class represents.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_link_model(self):\n        \"\"\"\n        Return the Django model that represents the link.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_link_model_name(self):\n        \"\"\"\n        Return the name of the Django model that represents the link.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_link_model_fields", "solution": "\n        kwargs = dict()\n        for attr in self.key_map.values():\n            if _is_null(to_instance, attr):\n                return None\n            \n            if attr in to_instance.__dict__:\n                kwargs[attr] = to_instance.__dict__[attr]\n            else:\n                kwargs[attr] = getattr(to_instance, attr)\n\n        return frozenset(tuple(kwargs.items()))"}
{"prompt": "def p_boolean_expression(self, p):\n        \"\"\"\n        expression : expression LE          expression\n                   | expression LESSTHAN    expression\n                   | expression DOUBLEEQUAL expression\n                   | expression NOTEQUAL    expression\n                   | expression GE          expression\n                   | expression GT          expression\n                   | expression AND         expression\n                   | expression OR          expression\n        \"\"\"", "output": "\n        if p[2] == '<':\n            p[0] = p[1] < p[3]\n        elif p[2] == '<=':\n            p[0] = p[1] <= p[3]\n        elif p[2] == '>':\n            p[0] = p[1] > p[3]\n        elif p[2] == '>=':\n            p[0] = p[1] >= p[3]\n        elif p[2] == '==':\n            p[0] = p[1] == p[3]\n        elif p[2]", "solution": "\n        p[0] = BinaryOperationNode(left=p[1],\n                                   operator=p[2],\n                                   right=p[3])"}
{"prompt": "def pairwise_reproducibility(df, plot=False):\n    \"\"\"\n    Calculate the reproducibility of LA-ICPMS based on unique pairs of repeat analyses.\n    \n    Pairwise differences are fit with a half-Cauchy distribution, and the median and \n    95% confidence limits are returned for each analyte.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataset\n    \n    plot : bool\n        Whether or not to plot the resulting error distributions.\n    \n    Returns\n    -------\n    pdiffs : pandas.DataFrame\n        Unique pairwise differences for all analytes.\n    rep_dists : dict of scipy.stats.halfcauchy\n        Half-Cauchy distribution objects fitted to the\n        differences.\n    rep_stats : dict of tuples\n        The 50% and 95% quantiles of the half-cauchy\n        distribution.\n    (fig, axs) : matplotlib objects\n        The figure. If not made, returnes (None, None) placeholder\n    \n    \"\"\"", "output": ".distributions\n        Distributions for-sample-group-replicate-pairwise-pairwise-pairwise-pairwise-pairwise-pairwise-pairwise-pairwise-pairwise-pairwise\n    \"\"\"\n   _groups = df.groupby('group').groups\n   _replicates = df.groupby(['group','replicate']).groups\n   _pairs = df.groupby(['group','replicate', 'pair']).groups\n    \n   _groups_list = [list(g) for g in_groups.values()]\n   ", "solution": "\n    \n    ans = df.columns.values\n    pdifs = []\n    \n    # calculate differences between unique pairs\n    for ind, d in df.groupby(level=0):\n        d.index = d.index.droplevel(0)\n\n        difs = []\n        for i, r in d.iterrows():\n            t = d.loc[i+1:, :]\n            difs.append(t[ans] - r[ans])\n\n        pdifs.append(pd.concat(difs))\n    pdifs = pd.concat(pdifs).abs()\n\n    # calculate stats\n    rep_stats = {}\n    rep_dists = {}\n    errfn = stats.halfcauchy\n    \n    for a in ans:\n        d = pdifs.loc[:, a].dropna().values\n        hdist = errfn.fit(d, floc=0)\n        rep_dists[a] = errfn(*hdist)\n        rep_stats[a] = rep_dists[a].ppf((0.5, 0.95))\n    \n    # make plot\n    if not plot:\n        return pdifs, rep_dists, rep_stats, (None, None)\n    \n    fig, axs = plt.subplots(1, len(ans), figsize=[len(ans) * 2, 2])\n    for a, ax in zip(ans, axs):\n        d = pdifs.loc[:, a].dropna().values\n        hist, edges, _ = ax.hist(d, 30)\n        ax.plot(edges, rep_dists[a].pdf(edges) * (sum(hist) * np.mean(np.diff(edges))))\n        ax.set_title(a, loc='left')\n\n    return pdifs, rep_dists, rep_stats, (fig, axs)"}
{"prompt": "def remove_redundant_nodes(self, preserve_lengths=True):\n        \"\"\"\n        Remove all nodes which have only a single child, and attach their\n        grandchildren to their parent.  The resulting tree has the minimum\n        number of internal nodes required for the number of leaves.\n        :param preserve_lengths: If true, branch lengths of removed nodes are \\\n        added to those of their children.\n        \"\"\"", "output": "\n        # First, find all nodes with only a single child\n        single_child_nodes = []\n        for node in self.nodes:\n            if len(node.children) == 1:\n                single_child_nodes.append(node)\n\n        # Now, for each node with only a single child, find the grandchildren\n        # and attach them to the parent\n        for node in single_child_nodes:\n            grandchildren = [child for child in node.children[0].children]\n            node.children[0].children = []\n            for grandchild in grandchildren:\n                node.children[0].", "solution": "\n        for n in self.walk(mode='postorder'):\n            while n.ancestor and len(n.ancestor.descendants) == 1:\n                grandfather = n.ancestor.ancestor\n                father = n.ancestor\n                if preserve_lengths:\n                    n.length += father.length\n\n                if grandfather:\n                    for i, child in enumerate(grandfather.descendants):\n                        if child is father:\n                            del grandfather.descendants[i]\n                    grandfather.add_descendant(n)\n                    father.ancestor = None\n                else:\n                    self.descendants = n.descendants\n                    if preserve_lengths:\n                        self.length = n.length"}
{"prompt": "def _set_start(self):\n        \"\"\" Sets the start timestamp and formatted time to the current time. \"\"\"", "output": "\n        self.start_time = datetime.datetime.now()\n        self.start_time_str = self.start_time.strftime('%Y-%m-%d %H:%M:%S')\n\n    def _set_end(self):\n        \"\"\" Sets the end timestamp and formatted time to the current time. \"\"\"\n        self.end_time = datetime.datetime.now()\n        self.end_time_str = self.end_time.strftime('%Y-%m-%d %H:%M:%S')\n\n    def _set_duration(self):\n        \"\"\" Calculates the duration of the run based on the", "solution": "\n        init_time = time.time()\n        formatted_time = datetime.datetime.fromtimestamp(init_time).strftime('%Y_%m_%d_%Hh%Mm%Ss')\n        run_info_dict = self._run_information[self.v_crun]\n        run_info_dict['timestamp'] = init_time\n        run_info_dict['time'] = formatted_time\n        if self._environment_hexsha is not None:\n            run_info_dict['short_environment_hexsha'] = self._environment_hexsha[0:7]"}
{"prompt": "def return_type(type_name, formatter=None):\n    \"\"\"Specify that this function returns a typed value.\n\n    Args:\n        type_name (str): A type name known to the global typedargs type system\n        formatter (str): An optional name of a formatting function specified\n            for the type given in type_name.\n    \"\"\"", "output": "\n    def decorator(func):\n        func.__typedargs__ = (type_name, formatter)\n        return func\n    return decorator\n\ndef typed_args(func):\n    \"\"\"Decorator that adds type information to a function's signature.\n\n    This decorator adds type information to a function's signature, allowing\n    the type system to enforce type checking and conversion of arguments and\n    return values.\n\n    The decorator should be applied to a function that takes typed arguments\n    and returns a typed value. The type information is specified using the\n    `@return_type` decorator.\n\n    Args:\n        func (function): A function that takes typed arguments and", "solution": "\n\n    def _returns(func):\n        annotated(func)\n        func.metadata.typed_returnvalue(type_name, formatter)\n        return func\n\n    return _returns"}
{"prompt": "def load_metamodel(resource=None, load_globals=True):\n    \"\"\"\n    Load and return a metamodel expressed in ooaofooa from a *resource*.\n    The resource may be either a filename, a path, or a list of filenames\n    and/or paths.\n    \"\"\"", "output": "\n    if resource is None:\n        raise ValueError(\"No resource specified\")\n\n    if isinstance(resource, str):\n        resource = [resource]\n\n    metamodel = None\n    for filename in resource:\n        if not os.path.exists(filename):\n            raise ValueError(\"File not found: {}\".format(filename))\n\n        try:\n            with open(filename, 'r') as f:\n                metamodel = load_metamodel_from_file(f, load_globals)\n        except Exception as e:\n            raise ValueError(\"Error loading metamodel from file: {}\".format(filename))\n\n        if metamodel is not None:\n           ", "solution": "\n    loader = _mk_loader(resource, load_globals)\n    return loader.build_metamodel()"}
{"prompt": "def set_s3_credentials(secret_key_id, secret_access_key):\n    \"\"\"Creates a new Amazon S3 client internally with specified credentials.\n    There are no validations done to the credentials. Incorrect credentials are thus revealed with first S3 import call.\n    \n    secretKeyId Amazon S3 Secret Key ID (provided by Amazon)\n    secretAccessKey Amazon S3 Secret Access Key (provided by Amazon)\n    \"\"\"", "output": "\n    s3 = boto3.client('s3', aws_access_key_id=secret_key_id, aws_secret_access_key=secret_access_key)\n    return s3\n\ndef get_s3_bucket_list(s3_client, bucket_name_prefix):\n    \"\"\"Returns a list of S3 buckets that start with the specified prefix.\n    \n    s3_client: An Amazon S3 client object\n    bucket_name_prefix: The prefix to search for (e.g.'my-bucket-')\n    \"\"\"\n    paginator = s3_client", "solution": "\n    if(secret_key_id is None):\n        raise H2OValueError(\"Secret key ID must be specified\")\n\n    if(secret_access_key is None):\n        raise H2OValueError(\"Secret access key must be specified\")\n    \n    if(not secret_key_id):\n        raise H2OValueError(\"Secret key ID must not be empty\")\n    \n    if(not secret_access_key):\n        raise H2OValueError(\"Secret access key must not be empty\")\n    \n    \n    params = {\"secret_key_id\": secret_key_id,\n              \"secret_access_key\": secret_access_key\n              }\n    \n    h2o.api(endpoint=\"POST /3/PersistS3\", data=params)\n    print(\"Credentials successfully set.\")"}
{"prompt": "def tar_and_s3_upload(self, path, key, bucket):\n        \"\"\"\n        Tar the local file or directory and upload to s3\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str\n        :param bucket: s3 bucket\n        :type bucket: str\n        :return: None\n        \"\"\"", "output": "\n        if os.path.isdir(path):\n            tar_path = os.path.join(path, f\"{path}.tar.gz\")\n            with tarfile.open(tar_path, \"w:gz\") as tar:\n                tar.add(path, arcname=os.path.basename(path))\n        else:\n            tar_path = path\n\n        s3 = boto3.resource('s3')\n        s3.Bucket(bucket).upload_file(tar_path, key)\n        os.remove(tar_path)\n", "solution": "\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)"}
{"prompt": "def _make_default_operation_costs(self, allow_spaces=False):\n        \"\"\"\n        sets 1.0 cost for every replacement, insertion, deletion and transposition\n        \"\"\"", "output": "\n        if allow_spaces:\n            self.operation_costs = {\n                \"replace\": 1.0,\n                \"insert\": 1.0,\n                \"delete\": 1.0,\n                \"transpose\": 1.0,\n                \"space\": 1.0\n            }\n        else:\n            self.operation_costs = {\n                \"replace\": 1.0,\n                \"insert\": 1.0,\n                \"delete\": 1.0,\n                \"transpose\": 1.0\n            }\n\n    def _make_custom_operation_costs(self, operation_costs):\n        \"\"\"\n       ", "solution": "\n        self.operation_costs = dict()\n        self.operation_costs[\"\"] = {c: 1.0 for c in list(self.alphabet) + [' ']}\n        for a in self.alphabet:\n            current_costs = {c: 1.0 for c in self.alphabet}\n            current_costs[a] = 0.0\n            current_costs[\"\"] = 1.0\n            if allow_spaces:\n                current_costs[\" \"] = 1.0\n            self.operation_costs[a] = current_costs\n        # \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0437\u0438\u0446\u0438\u0438\n        for a, b in itertools.permutations(self.alphabet, 2):\n            self.operation_costs[a + b] = {b + a: 1.0}\n        # \u043f\u0440\u043e\u0431\u0435\u043b\u044b\n        if allow_spaces:\n            self.operation_costs[\" \"] = {c: 1.0 for c in self.alphabet}\n            self.operation_costs[\" \"][\"\"] = 1.0"}
{"prompt": "def _dir_exists(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of dir_exists.\n\n    Expects a db-style path name.\n    \"\"\"", "output": "\n    return db.exists(user_id, db_dirname)\n\n\ndef dir_exists(path):\n    \"\"\"\n    Check if a directory exists in the database.\n\n    Args:\n        path (str): The path to the directory to check.\n\n    Returns:\n        bool: True if the directory exists, False otherwise.\n    \"\"\"\n    return _dir_exists(get_db(), get_user_id(), path)\n\n\ndef _create_dir(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of create_dir.\n\n    Expects a db-style path name.\n    \"\"\"\n   ", "solution": "\n    return db.execute(\n        select(\n            [func.count(directories.c.name)],\n        ).where(\n            and_(\n                directories.c.user_id == user_id,\n                directories.c.name == db_dirname,\n            ),\n        )\n    ).scalar() != 0"}
{"prompt": "def create(self, name, description=None, color=None):\n        \"\"\"\n        Creates a new label and returns the response\n\n        :param name: The label name\n        :type name: str\n\n        :param description: An optional description for the label. The name is\n            used if no description is provided.\n        :type description: str\n\n        :param color: The hex color for the label (ex: 'ff0000' for red). If no\n            color is provided, a random one will be assigned.\n        :type color: str\n\n        :returns: The response of your post\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": " \"\"\"\n        data = {\n            'name': name,\n            'description': description or name,\n            'color': color or self.generate_random_color()\n        }\n\n        response = self.post('labels', data=data)\n        return response\n\n    def update(self, label_id, name=None, description=None, color=None):\n        \"\"\"\n        Updates an existing label and returns the response\n\n        :param label_id: The ID of the label to update\n        :type label_id: int\n\n        :param name: An optional new name for the label. The old name is used if\n            no new", "solution": "\n        data = {\n            'name': name,\n            'title': name,\n            'description': description or name,\n            'appearance': {\n                'color': color or random_color()\n            }\n        }\n        # Yes, it's confusing. the `/tags/` endpoint is used for labels\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.TAGS.value,\n            params=data\n        )"}
{"prompt": "def create(self, teamId, personId=None, personEmail=None,\n               isModerator=False, **request_parameters):\n        \"\"\"Add someone to a team by Person ID or email address.\n\n        Add someone to a team by Person ID or email address; optionally making\n        them a moderator.\n\n        Args:\n            teamId(basestring): The team ID.\n            personId(basestring): The person ID.\n            personEmail(basestring): The email address of the person.\n            isModerator(bool): Set to True to make the person a team moderator.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            TeamMembership: A TeamMembership object with the details of the\n            created team membership.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": ": Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            dict: JSON response from the API.\n        \"\"\"\n        url = '/teams/{teamId}/members'.format(teamId=teamId)\n        data = {}\n\n        if personId:\n            data['personId'] = personId\n        elif personEmail:\n            data['personEmail'] = personEmail\n\n        if isModerator:\n            data['isModerator'] = True\n\n        response = self._client.post(url, data=data, **request_parameters)\n        return response.json()\n\n    def delete(", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(personId, basestring)\n        check_type(personEmail, basestring)\n        check_type(isModerator, bool)\n\n        post_data = dict_from_items_with_values(\n            request_parameters,\n            teamId=teamId,\n            personId=personId,\n            personEmail=personEmail,\n            isModerator=isModerator,\n        )\n\n        # API request\n        json_data = self._session.post(API_ENDPOINT, json=post_data)\n\n        # Return a team membership object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def visit(self, visit_uri):\n        \"\"\"\n        Navigate to the given URL. The URL can either be a relative URL or an absolute URL. The\n        behavior of either depends on the driver. ::\n\n            session.visit(\"/foo\")\n            session.visit(\"http://google.com\")\n\n        For drivers which can run against an external application, such as the Selenium driver,\n        giving an absolute URL will navigate to that page. This allows testing applications running\n        on remote servers. For these drivers, setting :data:`capybara.app_host` will make the\n        remote server the default. For example::\n\n            capybara.app_host = \"http://google.com\"\n            session.visit(\"/\")  # visits the Google homepage\n\n        Args:\n            visit_uri (str): The URL to navigate to.\n        \"\"\"", "output": " session.visit(\"https://example.com\")\n            session.visit(\"https://example.com/login\")\n\n        This will navigate to the login page on the example.com domain.\n        \"\"\"\n        if self.app_host:\n            self.visit_url(self.app_host + visit_uri)\n        else:\n            self.visit_url(visit_uri)\n\n    def visit_url(self, url):\n        \"\"\"\n        Navigate to the given URL. This method is used by both :meth:`visit` and\n        :meth:`visit_relative_url` to navigate to a URL.\n        \"\"\"", "solution": "\n\n        self.raise_server_error()\n\n        visit_uri = urlparse(visit_uri)\n\n        if capybara.app_host:\n            uri_base = urlparse(capybara.app_host)\n        elif self.server:\n            uri_base = urlparse(\"http://{}:{}\".format(self.server.host, self.server.port))\n        else:\n            uri_base = None\n\n        visit_uri = ParseResult(\n            scheme=visit_uri.scheme or (uri_base.scheme if uri_base else \"\"),\n            netloc=visit_uri.netloc or (uri_base.netloc if uri_base else \"\"),\n            path=visit_uri.path,\n            params=visit_uri.params,\n            query=visit_uri.query,\n            fragment=visit_uri.fragment)\n\n        self.driver.visit(visit_uri.geturl())"}
{"prompt": "def from_join(cls, join: Join) -> 'ConditionalJoin':\n        \"\"\"Creates a new :see:ConditionalJoin from the\n        specified :see:Join object.\n\n        Arguments:\n            join:\n                The :see:Join object to create the\n                :see:ConditionalJoin object from.\n\n        Returns:\n            A :see:ConditionalJoin object created from\n            the :see:Join object.\n        \"\"\"", "output": "\n        return cls(join.left, join.right, join.condition)\n\n    def __init__(self, left: Column, right: Column, condition: Optional[str] = None):\n        \"\"\"Initializes a new :see:ConditionalJoin object.\n\n        Arguments:\n            left:\n                The left column of the join.\n\n            right:\n                The right column of the join.\n\n            condition:\n                An optional condition string to use for the join.\n        \"\"\"\n        self.left = left\n        self.right = right\n        self.condition = condition\n\n    def __str__(self) -> str:\n        \"\"\"Returns a string representation", "solution": "\n\n        return cls(\n            join.table_name,\n            join.parent_alias,\n            join.table_alias,\n            join.join_type,\n            join.join_field,\n            join.nullable\n        )"}
{"prompt": "def process_tokens(self, tokens):\n        \"\"\"process tokens from the current module to search for module/block\n        level options\n        \"\"\"", "output": "\n        for token in tokens:\n            if token.type == tokenize.NAME and token.string =='module':\n                self.module_level = True\n            elif token.type == tokenize.NAME and token.string == 'block':\n                self.block_level = True\n            elif token.type == tokenize.OP and token.string == ':':\n                if self.module_level:\n                    self.module_level = False\n                elif self.block_level:\n                    self.block_level = False\n            elif self.module_level and token.type == tokenize.NAME:\n                self.module_options.append", "solution": "\n        control_pragmas = {\"disable\", \"enable\"}\n        for (tok_type, content, start, _, _) in tokens:\n            if tok_type != tokenize.COMMENT:\n                continue\n            match = OPTION_RGX.search(content)\n            if match is None:\n                continue\n\n            first_group = match.group(1)\n            if (\n                first_group.strip() == \"disable-all\"\n                or first_group.strip() == \"skip-file\"\n            ):\n                if first_group.strip() == \"disable-all\":\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(\"disable-all\", \"skip-file\"),\n                    )\n                self.add_message(\"file-ignored\", line=start[0])\n                self._ignore_file = True\n                return\n            try:\n                opt, value = first_group.split(\"=\", 1)\n            except ValueError:\n                self.add_message(\n                    \"bad-inline-option\", args=first_group.strip(), line=start[0]\n                )\n                continue\n            opt = opt.strip()\n            if opt in self._options_methods or opt in self._bw_options_methods:\n                try:\n                    meth = self._options_methods[opt]\n                except KeyError:\n                    meth = self._bw_options_methods[opt]\n                    # found a \"(dis|en)able-msg\" pragma deprecated suppression\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(opt, opt.replace(\"-msg\", \"\")),\n                    )\n                for msgid in utils._splitstrip(value):\n                    # Add the line where a control pragma was encountered.\n                    if opt in control_pragmas:\n                        self._pragma_lineno[msgid] = start[0]\n\n                    try:\n                        if (opt, msgid) == (\"disable\", \"all\"):\n                            self.add_message(\n                                \"deprecated-pragma\",\n                                line=start[0],\n                                args=(\"disable=all\", \"skip-file\"),\n                            )\n                            self.add_message(\"file-ignored\", line=start[0])\n                            self._ignore_file = True\n                            return\n                        meth(msgid, \"module\", start[0])\n                    except exceptions.UnknownMessageError:\n                        self.add_message(\"bad-option-value\", args=msgid, line=start[0])\n            else:\n                self.add_message(\"unrecognized-inline-option\", args=opt, line=start[0])"}
{"prompt": "def _flags_changed(self, name, old, new):\n        \"\"\"ensure flags dict is valid\"\"\"", "output": "\n        if name == 'flags':\n            if not isinstance(new, dict):\n                raise ValueError('flags must be a dictionary')\n            for flag in new:\n                if not isinstance(flag, str):\n                    raise ValueError('flags must be strings')\n                if not_flags.is_valid_flag(flag):\n                    raise ValueError('invalid flag: {}'.format(flag))\n            self._flags = new\n        else:\n            super()._flags_changed(name, old, new)\n\n    def _get_flag_value(self, flag):\n        \"\"\"get the value of a flag\"\"\"\n        if flag in self._", "solution": "\n        for key,value in new.iteritems():\n            assert len(value) == 2, \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[0], (dict, Config)), \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[1], basestring), \"Bad flag: %r:%s\"%(key,value)"}
{"prompt": "def images(self, query=None):\n    \"\"\"List local images in the database, optionally with a query.\n\n       Paramters\n       =========\n       query: a string to search for in the container or collection name|tag|uri\n\n    \"\"\"", "output": "\n    images = []\n    for image in self.client.images(all=True):\n        if query is None or query.lower() in image['RepoTags'][0].lower():\n            images.append(image)\n    return images\n\n\ndef containers(self, query=None):\n    \"\"\"List local containers in the database, optionally with a query.\n\n       Paramters\n       =========\n       query: a string to search for in the container name|image|tag|command\n\n    \"\"\"\n    containers = []\n    for container in self.client.containers(all=True):\n        if query is None or query.lower() in container", "solution": "\n    from sregistry.database.models import Collection, Container\n\n    rows = []\n    if query is not None:   \n        like = \"%\" + query + \"%\"\n        containers = Container.query.filter(or_(Container.name == query,\n                                                Container.tag.like(like),\n                                                Container.uri.like(like),\n                                                Container.name.like(like))).all() \n    else:\n        containers = Container.query.all()\n\n    if len(containers) > 0:\n        message = \"  [date]   [client]\\t[uri]\"\n        bot.custom(prefix='Containers:', message=message, color=\"RED\")\n        for c in containers:\n            uri = c.get_uri()\n            created_at = c.created_at.strftime('%B %d, %Y')\n            rows.append([created_at, \"   [%s]\" %c.client, uri])\n        bot.table(rows) \n    return containers"}
{"prompt": "def load_tiff_multipage(tiff_filename, dtype='float32'):\n    \"\"\"\n    Load a multipage tiff into a single variable in x,y,z format.\n\n    Arguments:\n        tiff_filename:     Filename of source data\n        dtype:             data type to use for the returned tensor\n\n    Returns:\n        Array containing contents from input tiff file in xyz order\n    \"\"\"", "output": "\n    # Load the data\n    with tiff.TiffFile(tiff_filename) as tif:\n       _data = np.array(tif.pages[0].asarray(), dtype=dtype)\n\n    # Reshape the data to_data.shape[0] * 3,_data.shape[1])\n   _data = np.reshape(np.transpose(", "solution": "\n    if not os.path.isfile(tiff_filename):\n        raise RuntimeError('could not find file \"%s\"' % tiff_filename)\n\n    # load the data from multi-layer TIF files\n    data = tiff.imread(tiff_filename)\n\n    im = []\n\n    while True:\n\n        Xi = numpy.array(data, dtype=dtype)\n        if Xi.ndim == 2:\n            Xi = Xi[numpy.newaxis, ...]  # add slice dimension\n        im.append(Xi)\n\n        try:\n            data.seek(data.tell()+1)\n        except EOFError:\n            break  # this just means hit end of file (not really an error)\n\n    im = numpy.concatenate(im, axis=0)  # list of 2d -> tensor\n    im = numpy.rollaxis(im, 1)\n    im = numpy.rollaxis(im, 2)\n\n    return im"}
{"prompt": "def with_setup(setup=None, teardown=None):\n    \"\"\"Decorator to add setup and/or teardown methods to a test function::\n\n      @with_setup(setup, teardown)\n      def test_something():\n          \" ... \"\n\n    Note that `with_setup` is useful *only* for test functions, not for test\n    methods or inside of TestCase subclasses.\n    \"\"\"", "output": "\n    def decorator(func):\n        if setup is not None:\n            func.setup = setup\n        if teardown is not None:\n            func.teardown = teardown\n        return func\n    return decorator\n\ndef with_temp_dir(func):\n    \"\"\"Decorator to create a temporary directory and set the current working directory to it.\n\n    This decorator is useful for tests that need to create temporary files or directories.\n    It creates a temporary directory using the `tempfile.mkdtemp` function, sets the current\n    working directory to it, and then calls the decorated function.\n\n    Example usage:\n\n        @with_temp_dir", "solution": "\n    def decorate(func, setup=setup, teardown=teardown):\n        if setup:\n            if hasattr(func, 'setup'):\n                _old_s = func.setup\n                def _s():\n                    setup()\n                    _old_s()\n                func.setup = _s\n            else:\n                func.setup = setup\n        if teardown:\n            if hasattr(func, 'teardown'):\n                _old_t = func.teardown\n                def _t():\n                    _old_t()\n                    teardown()\n                func.teardown = _t\n            else:\n                func.teardown = teardown\n        return func\n    return decorate"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idx : np.array(N)\n            Estimated indeces for the segment boundaries in frames.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n       ___labels = self._labels\n       __labels = self. test_labels\n       ___labels = self. test_labels\n       _ est_idx = self. est_idx\n       _ est_idx = self. est_idx\n\n       _ est_idx =_ est_idx.astype(int)\n       _ est_labels =_ est_labels.astype(int)\n       _ test_labels =_ test_labels.astype(int)\n       _ test_labels =_ test_labels.", "solution": "\n        # Preprocess to obtain features, times, and input boundary indeces\n        F = self._preprocess()\n\n        # Normalize\n        F = U.normalize(F, norm_type=self.config[\"label_norm_feats\"],\n                        floor=self.config[\"label_norm_floor\"],\n                        min_db=self.config[\"label_norm_min_db\"])\n\n        # Find the labels using 2D-FMCs\n        est_labels = compute_similarity(F, self.in_bound_idxs,\n                                        dirichlet=self.config[\"dirichlet\"],\n                                        xmeans=self.config[\"xmeans\"],\n                                        k=self.config[\"k\"],\n                                        offset=self.config[\"2dfmc_offset\"])\n\n        # Post process estimations\n        self.in_bound_idxs, est_labels = self._postprocess(self.in_bound_idxs,\n                                                           est_labels)\n\n        return self.in_bound_idxs, est_labels"}
{"prompt": "def find_handfile(names=None):\n    \"\"\"\n    \u5c1d\u8bd5\u5b9a\u4f4d ``handfile`` \u6587\u4ef6\uff0c\u660e\u786e\u6307\u5b9a\u6216\u9010\u7ea7\u641c\u7d22\u7236\u8def\u5f84\n\n    :param str names: \u53ef\u9009\uff0c\u5f85\u67e5\u627e\u7684\u6587\u4ef6\u540d\uff0c\u4e3b\u8981\u7528\u4e8e\u8c03\u8bd5\uff0c\u9ed8\u8ba4\u4f7f\u7528\u7ec8\u7aef\u4f20\u5165\u7684\u914d\u7f6e\n    :return: ``handfile`` \u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a None\n    :rtype: str\n    \"\"\"", "output": "\n    if names is None:\n        names = os.environ.get('HANDFILE_NAMES', '')\n\n    for name in names.split(':'):\n        path = os.path.join(os.getcwd(), name)\n        if os.path.exists(path):\n            return os.path.abspath(path)\n\n    return None\n\n\ndef load_handfile(names=None):\n    \"\"\"\n    \u52a0\u8f7d ``handfile`` \u6587\u4ef6\uff0c\u4f18\u5148\u4f7f\u7528 ``handfile`` \u6587\u4ef6 ``HANDFILE_NAMES`` \u73af\u5883\u53d8\u91cf\u6307\u5b9a\u7684\u8def\u5f84\n\n    :param str names: \u53ef\u9009\uff0c\u5f85\u67e5\u627e\u7684\u6587\u4ef6\u540d\uff0c\u4e3b\u8981\u7528\u4e8e\u8c03\u8bd5\uff0c", "solution": "\n    # \u5982\u679c\u6ca1\u6709\u660e\u786e\u6307\u5b9a\uff0c\u5219\u5305\u542b env \u4e2d\u7684\u503c\n    names = names or [env.handfile]\n\n    # \u82e5\u65e0 ``.py`` \u6269\u5c55\u540d\uff0c\u5219\u4f5c\u4e3a\u5f85\u67e5\u8be2\u540d\u79f0\uff0c\u8ffd\u52a0\u5230 names \u672b\u5c3e\n    if not names[0].endswith('.py'):\n        names += [names[0] + '.py']\n\n    # name \u4e2d\u662f\u5426\u5305\u542b\u8def\u5f84\u5143\u7d20\n    if os.path.dirname(names[0]):\n        # \u82e5\u5b58\u5728\uff0c\u5219\u6269\u5c55 Home \u8def\u5f84\u6807\u5fd7\uff0c\u5e76\u6d4b\u8bd5\u662f\u5426\u5b58\u5728\n        for name in names:\n            expanded = os.path.expanduser(name)\n            if os.path.exists(expanded):\n                if name.endswith('.py') or _is_package(expanded):\n                    return os.path.abspath(expanded)\n    else:\n        # \u5426\u5219\uff0c\u9010\u7ea7\u5411\u4e0a\u641c\u7d22\uff0c\u76f4\u5230\u6839\u8def\u5f84\n        path = '.'\n\n        # \u5728\u5230\u7cfb\u7edf\u6839\u8def\u5f84\u4e4b\u524d\u505c\u6b62\n        while os.path.split(os.path.abspath(path))[1]:\n            for name in names:\n                joined = os.path.join(path, name)\n                if os.path.exists(joined):\n                    if name.endswith('.py') or _is_package(joined):\n                        return os.path.abspath(joined)\n            path = os.path.join('..', path)\n\n    return None"}
{"prompt": "def remote_upload(self, remote_url, folder_id=None, headers=None):\n        \"\"\"Used to make a remote file upload to openload.co\n\n        Note:\n            If folder_id is not provided, the file will be uploaded to ``Home`` folder.\n\n        Args:\n            remote_url (str): direct link of file to be remotely downloaded.\n            folder_id (:obj:`str`, optional): folder-ID to upload to.\n            headers (:obj:`dict`, optional): additional HTTP headers (e.g. Cookies or HTTP Basic-Auth)\n\n        Returns:\n            dict: dictionary containing (\"id\": uploaded file id, \"folderid\"). ::\n\n                {\n                    \"id\": \"12\",\n                    \"folderid\": \"4248\"\n                }\n\n        \"\"\"", "output": " the response data from the API.\n        \"\"\"\n        if not headers:\n            headers = {}\n\n        if not folder_id:\n            folder_id = \"0\"\n\n        data = {\n            \"file\": remote_url,\n            \"folder_id\": folder_id,\n        }\n\n        response = self.session.post(self.upload_url, headers=headers, data=data)\n        response.raise_for_status()\n\n        return response.json()\n\n    def remote_download(self, remote_url, local_path, headers=None):\n        \"\"\"Used to make a remote file download from openload.", "solution": "\n\n        kwargs = {'folder': folder_id, 'headers': headers}\n        params = {'url': remote_url}\n        params.update({key: value for key, value in kwargs.items() if value})\n\n        return self._get('remotedl/add', params=params)"}
{"prompt": "def list(self, **request_parameters):\n        \"\"\"List all roles.\n\n        Args:\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            GeneratorContainer: A GeneratorContainer which, when iterated,\n            yields the roles returned by the Webex Teams query.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        if not isinstance(request_parameters, dict):\n            raise TypeError(\"request_parameters must be a dictionary\")\n\n        url = \"{}/roles\".format(self.api_url)\n        response = self._request(\"GET\", url, params=request_parameters)\n        return GeneratorContainer(response)\n\n    def create(self, role, **request_parameters):\n        \"\"\"Create a new role.\n\n        Args:\n            role (dict): The role to create.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            dict: The created role.", "solution": "\n        # API request - get items\n        items = self._session.get_items(\n            API_ENDPOINT,\n            params=request_parameters\n        )\n\n        # Yield role objects created from the returned JSON objects\n        for item in items:\n            yield self._object_factory(OBJECT_TYPE, item)"}
{"prompt": "def delete_all_checkpoints(self, path):\n        \"\"\"Delete all checkpoints for the given path.\"\"\"", "output": "\n        checkpoints = os.listdir(path)\n        for checkpoint in checkpoints:\n            if checkpoint.endswith('.pth.tar'):\n                os.remove(os.path.join(path, checkpoint))\n\n    def load_checkpoint(self, path, epoch=None):\n        \"\"\"Load the-level model from the given path.\"\"\"\n        if epoch is None:\n            epoch = self.best_epoch\n        checkpoint = torch.load(os.path.join(path, f'epoch{epoch}.pth.tar'), map_location=self.device)\n        self.model.load_state_dict(checkpoint", "solution": "\n        with self.engine.begin() as db:\n            delete_remote_checkpoints(db, self.user_id, path)"}
{"prompt": "def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"", "output": "\n        self._hdl_procs_to_run.append((trigger, proc))\n\n    def _runHdlProcs(self) -> None:\n        \"\"\"\n        Run all HDL processes in the execution queue\n        \"\"\"\n        for trigger, proc in self._hdl_procs_to_run:\n            if trigger.triggered:\n                try:\n                    proc()\n                except Exception as e:\n                    self.error(f\"Error running HDL process: {e}\")\n\n    def _runSimProcs(self) -> None:\n        \"\"\"\n        Run all simulation processes in the execution queue\n        \"\"\"\n        for proc in self._sim", "solution": "\n        # first process in time has to plan executing of apply values on the\n        # end of this time\n        if not self._applyValPlaned:\n            # (apply on end of this time to minimalize process reevaluation)\n            self._scheduleApplyValues()\n\n        if isEvDependentOn(trigger, proc):\n            if self.now == 0:\n                return  # pass event dependent on startup\n            self._seqProcsToRun.append(proc)\n        else:\n            self._combProcsToRun.append(proc)"}
{"prompt": "def convert(self, amount, currency, new_currency='EUR', date=None):\n        \"\"\"Convert amount from a currency to another one.\n\n        :param float amount: The amount of `currency` to convert.\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :param datetime.date date: Use the conversion rate of this date. If this\n            is not given, the most recent rate is used.\n\n        :return: The value of `amount` in `new_currency`.\n        :rtype: float\n\n        >>> from datetime import date\n        >>> c = CurrencyConverter()\n        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n        72.67...\n        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n        Traceback (most recent call last):\n        RateNotFoundError: BGN has no rate for 2010-11-21\n        \"\"\"", "output": "\n        >>> converter = CurrencyConverter()\n        >>> converter.convert(100, 'USD', 'EUR', date(2021, 1, 1))\n        99.31\n        \"\"\"\n        if date is None:\n            date = datetime.date.today()\n        rates = self.get_rates(date)\n        rate = rates.get(currency, rates.get(new_currency, None))\n        if rate is None:\n            raise ValueError(f\"No conversion rate found for {currency} to {new_currency} on {date}\")\n        return amount * rate\n\n    def get_", "solution": "\n        for c in currency, new_currency:\n            if c not in self.currencies:\n                raise ValueError('{0} is not a supported currency'.format(c))\n\n        if date is None:\n            date = self.bounds[currency].last_date\n        else:\n            try:\n                date = date.date()  # fallback if input was a datetime object\n            except AttributeError:\n                pass\n\n        r0 = self._get_rate(currency, date)\n        r1 = self._get_rate(new_currency, date)\n\n        return float(amount) / r0 * r1"}
{"prompt": "def user_config_file(self):\n        \"\"\"Get the absolute path to the user config file.\"\"\"", "output": "\n        return os.path.join(self.config_dir, 'user_config.yml')\n\n    def system_config_file(self):\n        \"\"\"Get the absolute path to the system config file.\"\"\"\n        return os.path.join(self.config_dir,'system_config.yml')\n\n    def load_user_config(self):\n        \"\"\"Load the user config file.\"\"\"\n        if os.path.exists(self.user_config_file()):\n            with open(self.user_config_file(), 'r') as f:\n                return yaml.safe_load(f)\n        else", "solution": "\n        return os.path.join(\n            get_user_config_dir(self.app_name, self.app_author),\n            self.filename)"}
{"prompt": "def build_url_field(self, field_name, model_class):\n        \"\"\"\n        This is needed due to DRF's model serializer uses the queryset to build url name\n\n        # TODO: Move this to own serializer mixin or fix problem elsewhere?\n        \"\"\"", "output": "\n        try:\n            field = model_class._meta.get_field(field_name)\n            if isinstance(field, models.URLField):\n                return field.url_name\n        except FieldDoesNotExist:\n            pass\n        return None\n\n    def build_url_name(self, field_name, model_class):\n        \"\"\"\n        This method builds the url name for the given field name and model class.\n        \"\"\"\n        url_name = self.build_url_field(field_name, model_class)\n        if url_name:\n            return url_name\n\n        # If the field is not a URLField", "solution": "\n        field, kwargs = super().build_url_field(field_name, model_class)\n\n        view = self.root.context[\"view\"]\n        kwargs[\"view_name\"] = view.get_url_name(\"detail\")\n\n        return field, kwargs"}
{"prompt": "def list_templates(self, extensions=None, filter_func=None):\n        \"\"\"Returns a list of templates for this environment.  This requires\n        that the loader supports the loader's\n        :meth:`~BaseLoader.list_templates` method.\n\n        If there are other files in the template folder besides the\n        actual templates, the returned list can be filtered.  There are two\n        ways: either `extensions` is set to a list of file extensions for\n        templates, or a `filter_func` can be provided which is a callable that\n        is passed a template name and should return `True` if it should end up\n        in the result list.\n\n        If the loader does not support that, a :exc:`TypeError` is raised.\n\n        .. versionadded:: 2.4\n        \"\"\"", "output": " should be\n        included in the list.\n\n        :param extensions: A list of file extensions for templates, or `None`\n            if no filtering is required.\n        :param filter_func: A callable that is passed a template name and\n            should return `True` if it should be included in the list.\n        :return: A list of template names.\n        \"\"\"\n        if extensions is None and filter_func is None:\n            raise ValueError(\"Either extensions or filter_func must be set.\")\n        if extensions is not None and filter_func is not None:\n            raise ValueError(\"Only one of extensions or filter_func can", "solution": "\n        x = self.loader.list_templates()\n        if extensions is not None:\n            if filter_func is not None:\n                raise TypeError('either extensions or filter_func '\n                                'can be passed, but not both')\n            filter_func = lambda x: '.' in x and \\\n                                    x.rsplit('.', 1)[1] in extensions\n        if filter_func is not None:\n            x = ifilter(filter_func, x)\n        return x"}
{"prompt": "def find_debugged_frame(frame):\n    \"\"\"Find the first frame that is a debugged frame. We do this\n    Generally we want traceback information without polluting it with\n    debugger frames. We can tell these because those are frames on the\n    top which don't have f_trace set. So we'll look back from the top\n    to find the fist frame where f_trace is set.\n    \"\"\"", "output": "\n    while frame:\n        if frame.f_trace is not None:\n            return frame\n        frame = frame.f_back\n    return None\n\n\ndef get_traceback_info(exc_info):\n    \"\"\"Extract the traceback information from the given exception info.\n    \"\"\"\n    tb = exc_info[2]\n    tb_list = []\n    while tb:\n        tb_list.append(tb)\n        tb = tb.tb_next\n    return tb_list\n\n\ndef get_debugged_frame(exc_info):\n    \"\"\"Extract the debugged frame from the given exception info.\n    \"\"\"\n    frame", "solution": "\n    f_prev = f = frame\n    while f is not None and f.f_trace is None:\n        f_prev = f\n        f = f.f_back\n        pass\n    if f_prev:\n        val = f_prev.f_locals.get('tracer_func_frame')\n        if val == f_prev:\n            if f_prev.f_back:\n                f_prev = f_prev.f_back\n                pass\n            pass\n        pass\n    else:\n        return frame\n    return f_prev"}
{"prompt": "def parent_frame_arguments():\n  \"\"\"Returns parent frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's function\n  arguments. These are positional arguments and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function argument names are overloaded before invoking\n  this method, then values will reflect the overloaded value. For this reason,\n  we recommend calling `parent_frame_arguments` at the beginning of the\n  function.\n  \"\"\"", "output": "\n  frame = inspect.currentframe().f_back\n  if frame is None:\n    return {}\n  args, _, _, values = inspect.getargvalues(frame)\n  return dict(zip(args, values))\n\n\ndef get_caller_function_name():\n  \"\"\"Returns the name of the caller function.\"\"\"\n  frame = inspect.currentframe().f_back\n  if frame is None:\n    return None\n  return frame.f_code.co_name\n\n\ndef get_caller_function_filename():\n  \"\"\"Returns the filename of the caller function.\"\"\"\n  frame = inspect.currentframe().f_", "solution": "\n  # All arguments and the names used for *varargs, and **kwargs\n  arg_names, variable_arg_name, keyword_arg_name, local_vars = (\n      tf_inspect._inspect.getargvalues(  # pylint: disable=protected-access\n          # Get the first frame of the caller of this method.\n          tf_inspect._inspect.stack()[1][0]))  # pylint: disable=protected-access\n\n  # Remove the *varargs, and flatten the **kwargs. Both are\n  # nested lists.\n  local_vars.pop(variable_arg_name, {})\n  keyword_args = local_vars.pop(keyword_arg_name, {})\n\n  final_args = {}\n  # Copy over arguments and their values. In general, local_vars\n  # may contain more than just the arguments, since this method\n  # can be called anywhere in a function.\n  for arg_name in arg_names:\n    final_args[arg_name] = local_vars.pop(arg_name)\n  final_args.update(keyword_args)\n\n  return final_args"}
{"prompt": "def _process_bit_id(self, node):\n        \"\"\"Process an Id or IndexedId node as a bit or register type.\n\n        Return a list of tuples (Register,index).\n        \"\"\"", "output": "\n        if isinstance(node, ast.Id):\n            return [(node.id, None)]\n        elif isinstance(node, ast.IndexedId):\n            return [(node.id, node.index)]\n        else:\n            raise ValueError(\"Invalid node type: {}\".format(type(node)))\n\n    def _process_bit_slice(self, node):\n        \"\"\"Process a BitSlice node as a bit slice type.\n\n        Return a tuple (Register,start,end).\n        \"\"\"\n        if isinstance(node.lower, ast.Id):\n            start = node.lower.id\n        elif isinstance(node.lower, ast.", "solution": "\n        # pylint: disable=inconsistent-return-statements\n        reg = None\n        if node.name in self.dag.qregs:\n            reg = self.dag.qregs[node.name]\n        elif node.name in self.dag.cregs:\n            reg = self.dag.cregs[node.name]\n        else:\n            raise QiskitError(\"expected qreg or creg name:\",\n                              \"line=%s\" % node.line,\n                              \"file=%s\" % node.file)\n\n        if node.type == \"indexed_id\":\n            # An indexed bit or qubit\n            return [(reg, node.index)]\n        elif node.type == \"id\":\n            # A qubit or qreg or creg\n            if not self.bit_stack[-1]:\n                # Global scope\n                return [(reg, j) for j in range(reg.size)]\n            else:\n                # local scope\n                if node.name in self.bit_stack[-1]:\n                    return [self.bit_stack[-1][node.name]]\n                raise QiskitError(\"expected local bit name:\",\n                                  \"line=%s\" % node.line,\n                                  \"file=%s\" % node.file)\n        return None"}
{"prompt": "def _backup(self):\n        \"\"\"\n        Save the current database into the inactive-db.json file.\n        \"\"\"", "output": "\n        with open(self.inactive_db_path, 'w') as f:\n            json.dump(self.db, f)\n\n    def _restore(self):\n        \"\"\"\n        Restore the inactive-db.json file into the active-db.json file.\n        \"\"\"\n        if os.path.exists(self.inactive_db_path):\n            with open(self.inactive_db_path, 'r') as f:\n                self.db = json.load(f)\n                os.remove(self.inactive_db_path)\n        else:\n            print(\"No inactive-db.json", "solution": "\n\n        if PyFunceble.CONFIGURATION[\"inactive_database\"]:\n            # The database subsystem is activated.\n\n            # We save the current database state into the database file.\n            Dict(PyFunceble.INTERN[\"inactive_db\"]).to_json(self.inactive_db_path)"}
{"prompt": "def get_pandas_df(self, hql, schema='default'):\n        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype: DataFrame\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> len(df.index)\n        100\n\n        :return: pandas.DateFrame\n        \"\"\"", "output": " df.head()\n        \"\"\"\n        if isinstance(hql, list):\n            hql = '\\n'.join(hql)\n        return self.get_pandas_df_from_query(hql, schema)\n\n    def get_pandas_df_from_query(self, hql, schema='default'):\n        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype", "solution": "\n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df"}
{"prompt": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"", "output": "\n        raise NotImplementedError\n\n    def get_item_content(self, identifier):\n        \"\"\"Return content of the item identified by the given identifier.\n\n        :param identifier: item identifier\n        :returns: content of the item\n        \"\"\"\n        raise NotImplementedError\n\n    def set_item_content(self, identifier, content):\n        \"\"\"Set content of the item identified by the given identifier.\n\n        :param identifier: item identifier\n        :param content: content to be set\n        \"\"\"\n        raise NotImplementedError\n\n    def delete_item(self, identifier):\n        \"\"\"Delete the item identified by the given identifier.\n\n        :param identifier: item identifier\n        \"\"\"\n        raise", "solution": "\n        admin_metadata = self.get_admin_metadata()\n        uuid = admin_metadata[\"uuid\"]\n        # Create directory for the specific dataset.\n        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)\n        mkdir_parents(dataset_cache_abspath)\n\n        bucket_fpath = self.data_key_prefix + identifier\n        obj = self.s3resource.Object(self.bucket, bucket_fpath)\n        relpath = obj.get()['Metadata']['handle']\n        _, ext = os.path.splitext(relpath)\n\n        local_item_abspath = os.path.join(\n            dataset_cache_abspath,\n            identifier + ext\n        )\n        if not os.path.isfile(local_item_abspath):\n\n            tmp_local_item_abspath = local_item_abspath + \".tmp\"\n            self.s3resource.Bucket(self.bucket).download_file(\n                bucket_fpath,\n                tmp_local_item_abspath\n            )\n            os.rename(tmp_local_item_abspath, local_item_abspath)\n\n        return local_item_abspath"}
{"prompt": "def record(self, func):\n        \"\"\"Registers a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.\n        \"\"\"", "output": "\n        self._recorders.append(func)\n\n    def _register_blueprint(self, app, state):\n        for recorder in self._recorders:\n            recorder(state)\n\n    def make_setup_state(self):\n        \"\"\"Returns a state object that can be passed to the\n        :meth:`register_blueprint` method to register the blueprint on the\n        application.\n        \"\"\"\n        return {}\n\n\nclass FlaskBlueprint(object):\n    \"\"\"A base class for Flask blueprints.  This class provides a\n    convenient way to define a blueprint with a set of routes and a set of\n    recorders.  The record", "solution": "\n        if self._got_registered_once and self.warn_on_modifications:\n            from warnings import warn\n            warn(Warning('The blueprint was already registered once '\n                         'but is getting modified now.  These changes '\n                         'will not show up.'))\n        self.deferred_functions.append(func)"}
{"prompt": "def find_sourcemap_comment(filepath, block_size=100):\n    \"\"\"\n    Seeks and removes the sourcemap comment. If found, the sourcemap line is\n    returned.\n\n    Bundled output files can have massive amounts of lines, and the sourceMap\n    comment is always at the end. So, to extract it efficiently, we read out the\n    lines of the file starting from the end. We look back at most 2 lines.\n\n    :param:filepath: path to output bundle file containing the sourcemap comment\n    :param:blocksize: integer saying how many bytes to read at once\n    :return:string with the sourcemap comment or None\n    \"\"\"", "output": "\n    :return: the sourcemap comment line, or None if not found\n    \"\"\"\n    with open(filepath, 'rb') as f:\n        # read out the last 2 lines of the file\n        lines = f.readlines()[-2:]\n        # join the lines into a single string\n        content = ''.join(lines)\n        # seek to the end of the file\n        f.seek(0, 2)\n        # read out the last block_size bytes\n        content += f.read(block_size)\n        # search for the sourcemap comment\n        for line in content.split('\\n'):\n            if", "solution": "\n\n    MAX_TRACKBACK = 2  # look back at most 2 lines, catching potential blank line at the end\n\n    block_number = -1\n    # blocks of size block_size, in reverse order starting from the end of the file\n    blocks = []\n    sourcemap = None\n\n    try:\n        # open file in binary read+write mode, so we can seek with negative offsets\n        of = io.open(filepath, 'br+')\n        # figure out what's the end byte\n        of.seek(0, os.SEEK_END)\n        block_end_byte = of.tell()\n\n        # track back for maximum MAX_TRACKBACK lines and while we can track back\n        while block_end_byte > 0 and MAX_TRACKBACK > 0:\n            if (block_end_byte - block_size > 0):\n                # read the last block we haven't yet read\n                of.seek(block_number*block_size, os.SEEK_END)\n                blocks.append(of.read(block_size))\n            else:\n                # file too small, start from begining\n                of.seek(0, os.SEEK_SET)\n                # only read what was not read\n                blocks = [of.read(block_end_byte)]\n\n            # update variables that control while loop\n            content = b''.join(reversed(blocks))\n            lines_found = content.count(b'\\n')\n            MAX_TRACKBACK -= lines_found\n            block_end_byte -= block_size\n            block_number -= 1\n\n            # early check and bail out if we found the sourcemap comment\n            if SOURCEMAPPING_URL_COMMENT in content:\n                offset = 0\n                # splitlines eats the last \\n if its followed by a blank line\n                lines = content.split(b'\\n')\n                for i, line in enumerate(lines):\n                    if line.startswith(SOURCEMAPPING_URL_COMMENT):\n                        offset = len(line)\n                        sourcemap = line\n                        break\n                while i+1 < len(lines):\n                    offset += 1  # for the newline char\n                    offset += len(lines[i+1])\n                    i += 1\n                # track back until the start of the comment, and truncate the comment\n                if sourcemap:\n                    offset += 1  # for the newline before the sourcemap comment\n                    of.seek(-offset, os.SEEK_END)\n                    of.truncate()\n                return force_text(sourcemap)\n    finally:\n        of.close()\n    return sourcemap"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        if default is None:\n            prompt += \" (y/n): \"\n        else:\n            prompt += \" ({}/n): \".format(default)\n        while True:\n            response = input(prompt).lower()\n            if response in ('y', 'yes'):\n                return True\n            elif response in ('n', 'no'):\n                return False\n            else:\n                print(\"Please enter 'y' or 'n'.\")\n", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def delete(self, tag_id):\n        \"\"\"\n        Delete the specified InactivityAlert\n\n        :param tag_id: The tag ID to delete\n        :type tag_id: str\n\n        :raises: This will raise a\n            :class:`ServerException <logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        response = self._client.delete(f\"/inactivity_alerts/{tag_id}\")\n        if response.status_code!= 204:\n            raise ServerException(response.text)\n\n    def list(self, limit=None, offset=None, sort_by=None, sort_order=None):\n        \"\"\"\n        List all InactivityAlerts\n\n        :param limit: The maximum number of InactivityAlerts to return\n        :type limit: int\n\n        :param offset: The number of InactivityAlerts to skip before returning\n        :type offset: int\n\n        :param sort_by: The field to sort", "solution": "\n        tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}'\n\n        self._api_delete(\n            url=tag_url.format(\n                account_id=self.account_id,\n                tag_id=tag_id\n            )\n        )"}
{"prompt": "def normal_left_dclick(self, event):\n        \"\"\" Handles the left mouse button being double-clicked when the tool\n        is in the 'normal' state.\n\n        If the event occurred on this tool's component (or any contained\n        component of that component), the method opens a Traits UI view on the\n        object referenced by the 'element' trait of the component that was\n        double-clicked, setting the tool as the active tool for the duration\n        of the view.\n\n        \"\"\"", "output": "\n        if event.inaxes == self.axes:\n            for artist in self.artists:\n                if artist.contains(event)[0]:\n                    element = getattr(artist, 'element', None)\n                    if element is not None:\n                        view = self.ui_view(element)\n                        if view is not None:\n                            view.edit_traits()\n                            self.active_tool = self\n                            break\n\n    def ui_view(self, element):\n        \"\"\" Returns a Traits UI view for the given object, or None if no view\n        can be created for the object.\n\n        \"\"\"\n        return None\n\n    def _active_", "solution": "\n        x = event.x\n        y = event.y\n\n        # First determine what component or components we are going to hittest\n        # on.  If our component is a container, then we add its non-container\n        # components to the list of candidates.\n#        candidates = []\n        component = self.component\n#        if isinstance(component, Container):\n#            candidates = get_nested_components(self.component)\n#        else:\n#            # We don't support clicking on unrecognized components\n#            return\n#\n#        # Hittest against all the candidate and take the first one\n#        item = None\n#        for candidate, offset in candidates:\n#            if candidate.is_in(x-offset[0], y-offset[1]):\n#                item = candidate\n#                break\n\n        if hasattr(component, \"element\"):\n            if component.element is not None:\n                component.active_tool = self\n                component.element.edit_traits(kind=\"livemodal\")\n                event.handled = True\n                component.active_tool = None\n                component.request_redraw()\n        return"}
{"prompt": "def _read_compressed_points_data(self, laszip_vlr, point_format):\n        \"\"\" reads the compressed point record\n        \"\"\"", "output": "\n        # readl header\n       l_header = laszip_vlr.read(10)\n       l_header_dict = dict(zip(self.header_keys, struct.unpack(self.header_format, er.decode(er.encode(er.decode(er.encode(er.decode(er.encode(er.decode(er.encode(er.decode(er.encode(er.decode(er.encode(er.decode(er.encode(er.decode(er.encode(er.decode(er.encode(er.decode(er", "solution": "\n        offset_to_chunk_table = struct.unpack(\"<q\", self.stream.read(8))[0]\n        size_of_point_data = offset_to_chunk_table - self.stream.tell()\n\n        if offset_to_chunk_table <= 0:\n            logger.warning(\n                \"Strange offset to chunk table: {}, ignoring it..\".format(\n                    offset_to_chunk_table\n                )\n            )\n            size_of_point_data = -1  # Read everything\n\n        points = record.PackedPointRecord.from_compressed_buffer(\n            self.stream.read(size_of_point_data),\n            point_format,\n            self.header.point_count,\n            laszip_vlr,\n        )\n        return points"}
{"prompt": "def projector(state, flatten=False):\n    \"\"\"\n    maps a pure state to a state matrix\n\n    Args:\n        state (ndarray): the number of qubits\n        flatten (bool): determine if state matrix of column work\n    Returns:\n        ndarray:  state_mat(2**num, 2**num) if flatten is false\n        ndarray:  state_mat(4**num) if flatten is true stacked on by the column\n    \"\"\"", "output": "\n    =_matrix(state)\n    =.reshape(2**len(state), 2**len(state))\n    if flatten:\n        = np.hstack([np.vstack([np.kron(np.eye(2**i), np.eye(2**j)) for j in range(2**i)]) for i in range(len(state))])\n    return melt(melt(melt(melt(melt(melt(melt(melt(melt(melt(melt(melt(melt(me", "solution": "\n    density_matrix = np.outer(state.conjugate(), state)\n    if flatten:\n        return density_matrix.flatten(order='F')\n    return density_matrix"}
{"prompt": "def export_transcripts(adapter, build='37'):\n    \"\"\"Export all transcripts from the database\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        build(str)\n    \n    Yields:\n        transcript(scout.models.Transcript)\n    \"\"\"", "output": "\n    for transcript in adapter.export_transcripts(build=build):\n        yield transcript\n", "solution": "\n    LOG.info(\"Exporting all transcripts\")\n\n    for tx_obj in adapter.transcripts(build=build):\n        yield tx_obj"}
{"prompt": "def write_table(self, table):\n        \"\"\"Send DDL to create the specified `table`\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n\n        Returns None\n        \"\"\"", "output": "\n        self.write_ddl(table.get_ddl())\n\n    def write_ddl(self, ddl):\n        \"\"\"Send DDL to create the specified `ddl`\n\n        :Parameters:\n          - `ddl`: a string representing the DDL to be executed.\n\n        Returns None\n        \"\"\"\n        self.execute(ddl)\n\n    def execute(self, query):\n        \"\"\"Execute a query on the database\n\n        :Parameters:\n          - `query`: a string representing the query to be executed.\n\n        Returns None\n        \"\"\"\n        self.cursor.execute(query)\n        self.connection.commit()\n\n    def fetch_all(", "solution": "\n        table_sql, serial_key_sql = super(PostgresDbWriter, self).write_table(table)\n        for sql in serial_key_sql + table_sql:\n            self.execute(sql)"}
{"prompt": "def visit_module(self, node):\n        \"\"\"visit module : update consumption analysis variable\n        checks globals doesn't overrides builtins\n        \"\"\"", "output": "\n        if node.name in self.builtins:\n            raise SyntaxError(f\"Module '{node.name}' overrides built-in '{self.builtins[node.name]}'\")\n        self.module_name = node.name\n        self.consumption_analysis = ConsumptionAnalysis()\n\n    def visit_classdef(self, node):\n        \"\"\"visit classdef : update consumption analysis variable\n        checks class doesn't overrides built-in\n        \"\"\"\n        if node.name in self.builtins:\n            raise SyntaxError(f\"Class '{node.name}' overrides built-in '{self.builtins[node.name]}'\")", "solution": "\n        self._to_consume = [NamesConsumer(node, \"module\")]\n        self._postponed_evaluation_enabled = is_postponed_evaluation_enabled(node)\n\n        for name, stmts in node.locals.items():\n            if utils.is_builtin(name) and not utils.is_inside_except(stmts[0]):\n                if self._should_ignore_redefined_builtin(stmts[0]) or name == \"__doc__\":\n                    continue\n                self.add_message(\"redefined-builtin\", args=name, node=stmts[0])"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n      , n = self.size\n       _prime = n\n       _prime_prime = n\n       _prime_prime_prime = n\n        =_prime_prime_prime = 1\n       _prime = 1\n       _prime_prime = 1\n       _prime_prime_prime = 1\n       _prime_prime_prime_prime = 1\n       _prime_prime_prime_prime_prime = 1\n       _prime_prime_prime_prime_prime_prime = 1\n       _prime_prime_prime_prime_", "solution": "\n        return SuperOp(\n            np.transpose(self._data),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def _prepare_hiveconf(d):\n        \"\"\"\n        This function prepares a list of hiveconf params\n        from a dictionary of key value pairs.\n\n        :param d:\n        :type d: dict\n\n        >>> hh = HiveCliHook()\n        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n        >>> hh._prepare_hiveconf(hive_conf)\n        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\\n \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n        \"\"\"", "output": "dynamic.partition.mode=nonstrict\"]\n        \"\"\"\n        hiveconf_params = []\n        for key, value in d.items():\n            hiveconf_params.append(\"-hiveconf\")\n            hiveconf_params.append(\"{}={}\".format(key, value))\n        return hiveconf_params\n\n\nclass HiveCliHook(BaseHook):\n    \"\"\"\n    A hook for executing Hive queries using the Hive CLI.\n\n    :param hive_cli_path: The path to the Hive CLI executable.\n    :type hive_cli_path: str\n    :param hive_conf_dir:", "solution": "\n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )"}
{"prompt": "def trimmed_pred_default(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"", "output": "\n  return node.pred\n\ndef trimmed__default(node, parent):\n  \"\"\"The default-unit used in Node.trimmed.\"\"\"\n  return node.unit\n\ndef trimmed__default(node, parent):\n  \"\"\"The default-unit used in Node.trimmed.\"\"\"\n  return node.unit\n\ndef trimmed__default(node, parent):\n  \"\"\"The default-unit used in Node.trimmed.\"\"\"\n  return node.unit\n\ndef trimmed__default(node, parent):\n  \"\"\"The default-unit used in Node.trimmed.\"\"\"\n  return node.", "solution": "\n  return isinstance(node, ParseNode) and (node.is_empty or node.is_type(ParseNodeType.terminal))"}
{"prompt": "def _verify_compatibility(wave_a, wave_b, check_dep_units=True):\n    \"\"\"Verify that two waveforms can be combined with various mathematical functions.\"\"\"", "output": "\n    if wave_a.dtype!= wave_b.dtype:\n        raise ValueError(\"Waveforms must have the same data type.\")\n\n    if wave_a.ndim!= 1:\n        raise ValueError(\"Waveforms must be 1D.\")\n\n    if wave_a.shape[0]!= wave_b.shape[0]:\n        raise ValueError(\"Waveforms must have the same number of samples.\")\n\n    if check_dep_units:\n        if wave_a.units!= wave_b.units:\n            raise ValueError(\"Waveforms must have the same units.\")\n\n    return True\n\n\ndef combine_waves(wave", "solution": "\n    exobj = pexdoc.exh.addex(RuntimeError, \"Waveforms are not compatible\")\n    ctuple = (\n        bool(wave_a.indep_scale != wave_b.indep_scale),\n        bool(wave_a.dep_scale != wave_b.dep_scale),\n        bool(wave_a.indep_units != wave_b.indep_units),\n        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),\n        bool(wave_a.interp != wave_b.interp),\n    )\n    exobj(any(ctuple))"}
{"prompt": "def flowwrite(flow, filename, quantize=False, concat_axis=0, *args, **kwargs):\n    \"\"\"Write optical flow to file.\n\n    If the flow is not quantized, it will be saved as a .flo file losslessly,\n    otherwise a jpeg image which is lossy but of much smaller size. (dx and dy\n    will be concatenated horizontally into a single image if quantize is True.)\n\n    Args:\n        flow (ndarray): (h, w, 2) array of optical flow.\n        filename (str): Output filepath.\n        quantize (bool): Whether to quantize the flow and save it to 2 jpeg\n            images. If set to True, remaining args will be passed to\n            :func:`quantize_flow`.\n        concat_axis (int): The axis that dx and dy are concatenated,\n            can be either 0 or 1. Ignored if quantize is False.\n    \"\"\"", "output": " quantize the.\n        concat_axis (int): Axis to concatenate along.\n        *args: Additional arguments to be passed to cv2.imwrite.\n        **kwargs: Additional keyword arguments to be passed to cv2.imwrite.\n    \"\"\"\n    if quantize:\n        flow = np.concatenate([flow[..., 0], flow[..., 1]], axis=concat_axis)\n        flow =.flow_to_image(flow)\n        cv2.imwrite(filename, flow, *args, **kwargs)\n    else:\n        cv2.imwrite(filename, flow[..., 0],", "solution": "\n    if not quantize:\n        with open(filename, 'wb') as f:\n            f.write('PIEH'.encode('utf-8'))\n            np.array([flow.shape[1], flow.shape[0]], dtype=np.int32).tofile(f)\n            flow = flow.astype(np.float32)\n            flow.tofile(f)\n            f.flush()\n    else:\n        assert concat_axis in [0, 1]\n        dx, dy = quantize_flow(flow, *args, **kwargs)\n        dxdy = np.concatenate((dx, dy), axis=concat_axis)\n        imwrite(dxdy, filename)"}
{"prompt": "def get_singularity_version(singularity_version=None):\n    \"\"\"get_singularity_version will determine the singularity version for a\n       build first, an environmental variable is looked at, followed by \n       using the system version.\n\n       Parameters\n       ==========\n       singularity_version: if not defined, look for in environment. If still\n       not find, try finding via executing --version to Singularity. Only return\n       None if not set in environment or installed.\n    \"\"\"", "output": "\n    if singularity_version is not None:\n        return singularity_version\n\n    # Check for environmental variable\n    singularity_version = os.environ.get('SINGULARITY_VERSION')\n    if singularity_version is not None:\n        return singularity_version\n\n    # Check for installed version\n    try:\n        output = subprocess.check_output(['singularity', '--version'])\n        return output.decode().strip().split()[1]\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        return None\n\n\ndef get_singularity_build(singularity_version=None):\n    \"\"\"get", "solution": "\n\n    if singularity_version is None:        \n        singularity_version = os.environ.get(\"SINGULARITY_VERSION\")\n        \n    if singularity_version is None:\n        try:\n            cmd = ['singularity','--version']\n            output = run_command(cmd)\n\n            if isinstance(output['message'],bytes):\n                output['message'] = output['message'].decode('utf-8')\n            singularity_version = output['message'].strip('\\n')\n            bot.info(\"Singularity %s being used.\" % singularity_version)\n            \n        except:\n            singularity_version = None\n            bot.warning(\"Singularity version not found, so it's likely not installed.\")\n\n    return singularity_version"}
{"prompt": "def sign(pkey, data, digest):\n    \"\"\"\n    Sign a data string using the given key and message digest.\n\n    :param pkey: PKey to sign with\n    :param data: data to be signed\n    :param digest: message digest to use\n    :return: signature\n\n    .. versionadded:: 0.11\n    \"\"\"", "output": "\n    return pkey.sign(data, digest)\n\n\ndef verify(pkey, signature, data, digest):\n    \"\"\"\n    Verify a signature using the given key and message digest.\n\n    :param pkey: PKey to verify with\n    :param signature: signature to be verified\n    :param data: data to be signed\n    :param digest: message digest to use\n    :return: True if the signature is valid, False otherwise\n\n   .. versionadded:: 0.11\n    \"\"\"\n    return pkey.verify(signature, data, digest)\n\n\ndef generate_key(key_type, bits, passphrase", "solution": "\n    data = _text_to_bytes_and_warn(\"data\", data)\n\n    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))\n    if digest_obj == _ffi.NULL:\n        raise ValueError(\"No such digest method\")\n\n    md_ctx = _lib.Cryptography_EVP_MD_CTX_new()\n    md_ctx = _ffi.gc(md_ctx, _lib.Cryptography_EVP_MD_CTX_free)\n\n    _lib.EVP_SignInit(md_ctx, digest_obj)\n    _lib.EVP_SignUpdate(md_ctx, data, len(data))\n\n    length = _lib.EVP_PKEY_size(pkey._pkey)\n    _openssl_assert(length > 0)\n    signature_buffer = _ffi.new(\"unsigned char[]\", length)\n    signature_length = _ffi.new(\"unsigned int *\")\n    final_result = _lib.EVP_SignFinal(\n        md_ctx, signature_buffer, signature_length, pkey._pkey)\n    _openssl_assert(final_result == 1)\n\n    return _ffi.buffer(signature_buffer, signature_length[0])[:]"}
{"prompt": "def p_navigation_step_2(self, p):\n        \"\"\"navigation_step : ARROW identifier LSQBR identifier DOT phrase RSQBR\"\"\"", "output": "\n        self.add_navigation_step(p[1], p[2], p[4], p[5], p[6])\n\n    def p_navigation_step_3(self, p):\n        \"\"\"navigation_step : ARROW identifier LSQBR identifier RSQBR\"\"\"\n        self.add_navigation_step(p[1], p[2], p[4], None, None)\n\n    def p_navigation_step_4(self, p):\n        \"\"\"navigation_step : ARROW identifier LSQBR identifier DOT identifier RSQBR\"\"\"\n        self.add_navigation_step(p", "solution": "\n        p[0] = NavigationStepNode(key_letter=p[2],\n                                  rel_id=p[4],\n                                  phrase=p[6])"}
{"prompt": "def _get_request(self, endpoint):\n        \"\"\"Do actual GET request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for GET request\n        \"\"\"", "output": "\n        url = self.api_url + endpoint\n        response = requests.get(url, headers=self.headers)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception(f\"Error {response.status_code}: {response.text}\")\n\n    def _post_request(self, endpoint, data):\n        \"\"\"Do actual POST request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for POST request\n        data -- data to be sent in POST request\n        \"\"\"\n        url = self.api_url + endpoint", "solution": "\n        try:\n            response = requests.get(endpoint)\n        except requests.exceptions.RequestException:\n            raise GoldenCheetahNotAvailable(endpoint)\n        \n        if response.text.startswith('unknown athlete'):\n            match = re.match(\n                pattern='unknown athlete (?P<athlete>.+)',\n                string=response.text)\n            raise AthleteDoesNotExist(\n                athlete=match.groupdict()['athlete'])\n\n        elif response.text == 'file not found':\n            match = re.match(\n                pattern='.+/activity/(?P<filename>.+)',\n                string=endpoint)\n            raise ActivityDoesNotExist(\n                filename=match.groupdict()['filename'])\n\n        return response"}
{"prompt": "def _line_segment_with_colons(linefmt, align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\"\"\"", "output": "\n    if align == 'l':\n        return linefmt.ljust(colwidth)\n    elif align == 'r':\n        return linefmt.rjust(colwidth)\n    elif align == 'c':\n        return linefmt.center(colwidth)\n    else:\n        raise ValueError(\"Invalid alignment: %s\" % align)\n\n\ndef _format_line(line, colwidth,int,frac,exp,,,,,,,,, up, up, up, up, up, up, up, up, up, up,", "solution": "\n    fill = linefmt.hline\n    w = colwidth\n    if align in [\"right\", \"decimal\"]:\n        return (fill[0] * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + (fill[0] * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + (fill[0] * (w - 1))\n    else:\n        return fill[0] * w"}
{"prompt": "def report(self):\n        \"\"\"\n        Print network statistics.\n        \"\"\"", "output": "\n       _count = self._count\n       _count =_count if t_count > 0 else 1\n       _count = min(self.max_count, max(1, int(t_count /_count)))\n       _count =_count if t_count > 0 else 1\n       _count = min(self.max_count, max(1, int(t_count /_count)))\n\n       _count =_count if t_count > 0 else 1\n       _count = min(self.max", "solution": "\n        logging.info(\"network inputs: %s\", \" \".join(map(str, self.input_variables)))\n        logging.info(\"network targets: %s\", \" \".join(map(str, self.target_variables)))\n        logging.info(\"network parameters: %s\", \" \".join(map(str, self.all_parameters)))\n        logging.info(\"parameter count: %d\", self.parameter_count)"}
{"prompt": "def init_handlers(self):\n        \"\"\"Create the default handlers.\"\"\"", "output": "\n        self.handlers = {\n            'default': self.default_handler,\n            'info': self.info_handler,\n            'warning': self.warning_handler,\n            'error': self.error_handler,\n            'critical': self.critical_handler,\n        }\n\n    def add_handler(self, name, handler):\n        \"\"\"Add a custom handler.\"\"\"\n        self.handlers[name] = handler\n\n    def remove_handler(self, name):\n        \"\"\"Remove a custom handler.\"\"\"\n        if name in self.handlers:\n            del self.handlers[name]\n\n    def default_handler(self, record", "solution": "\n        self._handlers = {}\n        self._esc_handlers = {}\n        for handler in _default_handlers:\n            handler(\n                shell=self.shell, prefilter_manager=self, config=self.config\n            )"}
{"prompt": "def average_data(counts, observable):\n    \"\"\"Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n        counts (dict): a dict of outcomes from an experiment\n        observable (dict or matrix or list): The observable to be averaged over.\n        As an example, ZZ on qubits can be given as:\n        * dict: {\"00\": 1, \"11\": 1, \"01\": -1, \"10\": -1}\n        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]\n        * matrix diagonal (list): [1, -1, -1, 1]\n\n    Returns:\n        Double: Average of the observable\n    \"\"\"", "output": "1, \"01\": 0, \"10\": 0}\n        * list: [1, 1, 0, 0]\n        * matrix: [[1, 0], [0, 0], [0, 1], [0, 0]]\n\n    Returns:\n        float: The average value of the observable.\n    \"\"\"\n    if isinstance(observable, dict):\n       _obs = observable\n    elif isinstance(observable, list):\n        if len(observable) == 2**len(counts):\n            # Convert list to matrix\n            observable = np.matrix(observable)\n        else", "solution": "\n    if not isinstance(observable, dict):\n        observable = make_dict_observable(observable)\n    temp = 0\n    tot = sum(counts.values())\n    for key in counts:\n        if key in observable:\n            temp += counts[key] * observable[key] / tot\n    return temp"}
{"prompt": "def _get_object_name(self, line):\n        \"\"\" Get second token in line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_object_name(\"  def func():  \")\n        'func'\n        >>> docwriter._get_object_name(\"  class Klass(object):  \")\n        'Klass'\n        >>> docwriter._get_object_name(\"  class Klass:  \")\n        'Klass'\n        \"\"\"", "output": "\n        tokens = line.split()\n        if len(tokens) < 2:\n            return None\n        return tokens[1]\n\n    def _get_object_type(self, line):\n        \"\"\" Get first token in line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_object_type(\"  def func():  \")\n        'function'\n        >>> docwriter._get_object_type(\"  class Klass(object):  \")\n        'class'\n        >>> docwriter._get_object_type(\"  class Klass:  \")\n        'class'\n        \"\"\"\n        tokens", "solution": "\n        name = line.split()[1].split('(')[0].strip()\n        # in case we have classes which are not derived from object\n        # ie. old style classes\n        return name.rstrip(':')"}
{"prompt": "def _build_metrics(func_name, namespace):\n    \"\"\"\n    Builds metrics dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics\n    \"\"\"", "output": "\n    metrics = {}\n    if namespace.dag_id:\n        metrics[\"dag_id\"] = namespace.dag_id\n    if namespace.task_id:\n        metrics[\"task_id\"] = namespace.task_id\n    if namespace.execution_date:\n        metrics[\"execution_date\"] = namespace.execution_date\n    metrics[\"function_name\"] = func_name\n    return metrics\n\n\ndef _get_metrics_from_args(args):\n    \"\"\"\n    Extracts metrics from args and returns them as a dict.\n\n    :param args: list of arguments from argparse\n    :return: dict with metrics\n   ", "solution": "\n\n    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\n               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\n\n    assert isinstance(namespace, Namespace)\n    tmp_dic = vars(namespace)\n    metrics['dag_id'] = tmp_dic.get('dag_id')\n    metrics['task_id'] = tmp_dic.get('task_id')\n    metrics['execution_date'] = tmp_dic.get('execution_date')\n    metrics['host_name'] = socket.gethostname()\n\n    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))\n    log = Log(\n        event='cli_{}'.format(func_name),\n        task_instance=None,\n        owner=metrics['user'],\n        extra=extra,\n        task_id=metrics.get('task_id'),\n        dag_id=metrics.get('dag_id'),\n        execution_date=metrics.get('execution_date'))\n    metrics['log'] = log\n    return metrics"}
{"prompt": "def get_func_sourcecode(func):\n    \"\"\"\n    Try to get sourcecode using standard inspect.getsource().\n    If the function comes from a module which has been created dynamically\n    (not from the filesystem), then it tries to read the sourcecode on the\n    filesystem anyway.\n    WARNING: can do weird things if the filesystem code slightly differs from\n             the original module code.\n    \"\"\"", "output": "\n    try:\n        # Try to get sourcecode using standard inspect.getsource().\n        sourcecode = inspect.getsource(func)\n    except IOError:\n        # If the function comes from a module which has been created dynamically\n        # (not from the filesystem), then it tries to read the sourcecode on the\n        # filesystem anyway.\n        if hasattr(func, '__code__'):\n            filename = func.__code__.co_filename\n            if filename.startswith('<'):\n                # The filename is a compiled-in string, so we can't read it.\n                return None\n            with open(filename, 'r') as f", "solution": "\n\n    def getsource(func):\n        lines, lnum = getsourcelines(func)\n        return ''.join(lines)\n\n    def getsourcelines(func):\n        lines, lnum = findsource(func)\n        return inspect.getblock(lines[lnum:]), lnum + 1\n\n    def findsource(func):\n        file = getfile(func)  # file path\n        module = inspect.getmodule(func, file)\n        lines = linecache.getlines(file, module.__dict__)\n        code = func.__code__\n        lnum = code.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]):\n                break\n            lnum = lnum - 1  # pragma: no cover\n        return lines, lnum\n\n    def getfile(func):\n        module = inspect.getmodule(func)\n        return module.__file__\n\n    try:\n        return inspect.getsource(func)\n    except Exception:\n        return getsource(func)"}
{"prompt": "def x_fit(self, test_length):\r\n        \"\"\" Test to see if the line can has enough space for the given length. \"\"\"", "output": "\r\n        = self.y_fit(test_length)\r\n       _ = self.y_fit(test_length + 1)\r\n       _ = self.y_fit(test_length - 1)\r\n       _ = self.y_fit(test_length + 2)\r\n       _ = self.y_fit(test_length - 2)\r\n       _ = self.y_fit(test_length + 3)\r\n       _ = self.y_fit(test_length - 3)\r\n       _ = self.y_fit(test_", "solution": "\r\n        if (self.x + test_length) >= self.xmax:\r\n            return False\r\n        else:\r\n            return True"}
{"prompt": "def list_files(self, extensions=None):\n        \"\"\"\n        List the ports contents by file type or all.\n        :param extensions: string extensions, single string or list of extensions.\n        :return: A list of full path names of each file.\n        \"\"\"", "output": "\n        if extensions is None:\n            extensions = self.extensions\n        elif isinstance(extensions, str):\n            extensions = [extensions]\n        files = []\n        for ext in extensions:\n            for root, dirs, files in os.walk(self.directory):\n                for file in files:\n                    if file.endswith(ext):\n                        files.append(os.path.join(root, file))\n        return files\n\n    def list_ports(self):\n        \"\"\"\n        List the ports in the directory.\n        :return: A list ofports.\n        \"\"\"\n        ports = []\n        for root, dirs, files in os.", "solution": "\n        if self.type.lower() != 'directory':\n            raise ValueError(\"Port type is not == directory\")\n\n        filesystem_location = self.path\n\n        for root, dirs, files in os.walk(filesystem_location):\n            if extensions is None:\n                return [os.path.join(root, f) for f in files]\n            elif not isinstance(extensions, list):\n                extensions = [extensions]\n\n            subset_files = []\n\n            for f in files:\n                for extension in extensions:\n                    if f.lower().endswith(extension.lower()):\n                        subset_files.append(os.path.join(root, f))\n                        break\n            return subset_files"}
{"prompt": "def get_image(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get image list from ImageFrame\n        \"\"\"", "output": "\n       _image = self.image_frame.get_image(float_key=float_key, to_chw=to_chw)\n        return [np.transpose(img, (2, 0, 1)) for img in_image]\n\n    def get_floats(self,_key=\"floats\"):\n        \"\"\"\n        get_key from_frame\n        \"\"\"\n        return self.float_frame.get_floats(out_key=out_key)\n\n    def get___key(self,_key=\"floats\"):\n       ", "solution": "\n        tensors = callBigDlFunc(self.bigdl_type,\n                                   \"localImageFrameToImageTensor\", self.value, float_key, to_chw)\n        return map(lambda tensor: tensor.to_ndarray(), tensors)"}
{"prompt": "def generate_key(self, type, bits):\n        \"\"\"\n        Generate a key pair of the given type, with the given number of bits.\n\n        This generates a key \"into\" the this object.\n\n        :param type: The key type.\n        :type type: :py:data:`TYPE_RSA` or :py:data:`TYPE_DSA`\n        :param bits: The number of bits.\n        :type bits: :py:data:`int` ``>= 0``\n        :raises TypeError: If :py:data:`type` or :py:data:`bits` isn't\n            of the appropriate type.\n        :raises ValueError: If the number of bits isn't an integer of\n            the appropriate size.\n        :return: ``None``\n        \"\"\"", "output": " the correct type or if :py:data:`bits` is less than 0.\n        :raises ValueError: If :py:data:`bits` is not a multiple of 8.\n        \"\"\"\n        if not isinstance(type, int) or type not in (TYPE_RSA, TYPE_DSA):\n            raise TypeError(\"type must be an integer from TYPE_RSA to TYPE_DSA\")\n        if not isinstance(bits, int) or bits < 0:\n            raise TypeError(\"bits must be a non-negative integer\")\n        if bits % 8!= 0:\n            raise ValueError(\"bits must be", "solution": "\n        if not isinstance(type, int):\n            raise TypeError(\"type must be an integer\")\n\n        if not isinstance(bits, int):\n            raise TypeError(\"bits must be an integer\")\n\n        if type == TYPE_RSA:\n            if bits <= 0:\n                raise ValueError(\"Invalid number of bits\")\n\n            # TODO Check error return\n            exponent = _lib.BN_new()\n            exponent = _ffi.gc(exponent, _lib.BN_free)\n            _lib.BN_set_word(exponent, _lib.RSA_F4)\n\n            rsa = _lib.RSA_new()\n\n            result = _lib.RSA_generate_key_ex(rsa, bits, exponent, _ffi.NULL)\n            _openssl_assert(result == 1)\n\n            result = _lib.EVP_PKEY_assign_RSA(self._pkey, rsa)\n            _openssl_assert(result == 1)\n\n        elif type == TYPE_DSA:\n            dsa = _lib.DSA_new()\n            _openssl_assert(dsa != _ffi.NULL)\n\n            dsa = _ffi.gc(dsa, _lib.DSA_free)\n            res = _lib.DSA_generate_parameters_ex(\n                dsa, bits, _ffi.NULL, 0, _ffi.NULL, _ffi.NULL, _ffi.NULL\n            )\n            _openssl_assert(res == 1)\n\n            _openssl_assert(_lib.DSA_generate_key(dsa) == 1)\n            _openssl_assert(_lib.EVP_PKEY_set1_DSA(self._pkey, dsa) == 1)\n        else:\n            raise Error(\"No such key type\")\n\n        self._initialized = True"}
{"prompt": "def v2_runner_on_skipped(self, result, **kwargs):\n        \"\"\"Run when a task is skipped.\"\"\"", "output": "\n        pass\n\n    def v2_runner_on_failed(self, result, **kwargs):\n        \"\"\"Run when a task fails.\"\"\"\n        pass\n\n    def v2_runner_on_error(self, result, **kwargs):\n        \"\"\"Run when a task raises an error.\"\"\"\n        pass\n\n    def v2_runner_on_unreachable(self, result, **kwargs):\n        \"\"\"Run when a task is unreachable.\"\"\"\n        pass\n\n    def v2_runner_on_ok(self, result, **kwargs):\n        \"\"\"Run when a task is successful.\"\"\"\n        pass\n\n    def v2_runner_on_start", "solution": "\n        if self._display.verbosity > 1:\n            self._print_task()\n            self.last_skipped = False\n\n            line_length = 120\n            spaces = \" \" * (31 - len(result._host.name) - 4)\n\n            line = \"  * {}{}- {}\".format(\n                colorize(result._host.name, \"not_so_bold\"),\n                spaces,\n                colorize(\"skipped\", \"skipped\"),\n            )\n\n            reason = result._result.get(\"skipped_reason\", \"\") or result._result.get(\n                \"skip_reason\", \"\"\n            )\n            if len(reason) < 50:\n                line += \" -- {}\".format(reason)\n                print(\"{} {}---------\".format(line, \"-\" * (line_length - len(line))))\n            else:\n                print(\"{} {}\".format(line, \"-\" * (line_length - len(line))))\n                print(self._indent_text(reason, 8))\n                print(reason)"}
{"prompt": "def _estimate_gas(self,\n                      master_copy: str,\n                      initializer: bytes,\n                      funder: str,\n                      payment_token: str) -> int:\n        \"\"\"\n        Gas estimation done using web3 and calling the node\n        Payment cannot be estimated, as no ether is in the address. So we add some gas later.\n        :param master_copy: Master Copy of Gnosis Safe already deployed\n        :param initializer: Data initializer to send to GnosisSafe setup method\n        :param funder: Address that should get the payment (if payment set)\n        :param payment_token: Address if a token is used. If not set, 0x0 will be ether\n        :return: Total gas estimation\n        \"\"\"", "output": " of the token that should be used for payment (if payment set)\n        :return: Estimated gas to be used for the transaction\n        \"\"\"\n        # Estimate gas using web3\n        web3 = Web3(Web3.HTTPProvider(\"http://localhost:8545\"))\n        safe_contract = web3.eth.contract(address=master_copy, abi=GnosisSafe.abi)\n        setup_data = initializer + b'\\x00' * (32 - len(initializer))\n        gas_estimate = safe_contract.constructor(setup_data).estimateGas()\n\n        #", "solution": "\n\n        # Estimate the contract deployment. We cannot estimate the refunding, as the safe address has not any fund\n        gas: int = self._build_proxy_contract_creation_constructor(\n            master_copy,\n            initializer,\n            funder,\n            payment_token,\n            0).estimateGas()\n\n        # We estimate the refund as a new tx\n        if payment_token == NULL_ADDRESS:\n            # Same cost to send 1 ether than 1000\n            gas += self.w3.eth.estimateGas({'to': funder, 'value': 1})\n        else:\n            # Top should be around 52000 when storage is needed (funder no previous owner of token),\n            # we use value 1 as we are simulating an internal call, and in that calls you don't pay for the data.\n            # If it was a new tx sending 5000 tokens would be more expensive than sending 1 because of data costs\n            try:\n                gas += get_erc20_contract(self.w3,\n                                          payment_token).functions.transfer(funder, 1).estimateGas({'from':\n                                                                                                    payment_token})\n            except ValueError as exc:\n                raise InvalidERC20Token from exc\n\n        return gas"}
{"prompt": "def reg_on_abort(self, callable_object, *args, **kwargs):\n        \"\"\" Register a function/method to be called when execution is aborted\"\"\"", "output": "\n        self.abort_callbacks.append((callable_object, args, kwargs))\n\n    def _execute(self, callable_object, args, kwargs):\n        try:\n            return callable_object(*args, **kwargs)\n        except Exception as e:\n            self.abort_callbacks = []\n            raise e\n\n    def execute(self, callable_object, *args, **kwargs):\n        \"\"\" Execute a function/method and return the result \"\"\"\n        if callable_object in self.abort_callbacks:\n            self.abort_callbacks.remove(callable_object)\n        return self._execute(callable_object, args,", "solution": "\n        persistent = kwargs.pop('persistent', False)\n        event = self._create_event(callable_object, 'abort', persistent, *args, **kwargs)\n        self.abort_callbacks.append(event)\n        return event"}
{"prompt": "def wa(client, event, channel, nick, rest):\n\t\"\"\"\n\tA free-text query resolver by Wolfram|Alpha. Returns the first\n\tresult, if available.\n\t\"\"\"", "output": "\n\tquery =''.join(rest)\n\ttry:\n\t\tresult = client.query(query)\n\t\tif result['@success'] == 'true':\n\t\t\treturn client.say(channel, result['queryresult']['pod'][0]['subpod'][0]['plaintext'])\n\t\telse:\n\t\t\treturn client.say(channel, 'No results found.')\n\texcept Exception as e:\n\t\treturn client.say(channel, 'An error occurred: {}'.format(e))\n\ndef main(client, event, channel, nick, rest):\n\t\"\"\"\n\tMain function for", "solution": "\n\tclient = wolframalpha.Client(pmxbot.config['Wolfram|Alpha API key'])\n\tres = client.query(rest)\n\treturn next(res.results).text"}
{"prompt": "def update_(self, sct_dict, conf_arg=True):\n        \"\"\"Update values of configuration section with dict.\n\n        Args:\n            sct_dict (dict): dict indexed with option names. Undefined\n                options are discarded.\n            conf_arg (bool): if True, only options that can be set in a config\n                file are updated.\n        \"\"\"", "output": "\n        for opt, val in sct_dict.items():\n            if conf_arg and not self.can_set(opt):\n                continue\n            self[opt] = val\n\n    def can_set(self, opt):\n        \"\"\"Check if option can be set in a config file.\n\n        Args:\n            opt (str): option name.\n\n        Returns:\n            bool: True if option can be set in a config file, False otherwise.\n        \"\"\"\n        return opt in self.config_options\n\n    def __setitem__(self, key, value):\n        if key not in self.config_options:\n            raise KeyError", "solution": "\n        for opt, val in sct_dict.items():\n            if opt not in self.def_:\n                continue\n            if not conf_arg or self.def_[opt].conf_arg:\n                self[opt] = val"}
{"prompt": "def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"", "output": "\n        for root, dirs, files in os.walk(self.directory):\n            for file in files:\n                if re.search(self.regex, file):\n                    self.files.append(os.path.join(root, file))\n        return self.files\n\n    def get_files(self):\n        \"\"\"\n        returns the list of matching files\n\n        :return: list of matching files\n        \"\"\"\n        return self.files\n\n    def get_directory(self):\n        \"\"\"\n        returns the directory where the search was performed\n\n        :return: directory where the search was performed\n        \"\"\"\n        return self.directory\n\n    def set_", "solution": "\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)"}
{"prompt": "def breadcrumbs(self, tree_alias, context):\n        \"\"\"Builds and returns breadcrumb trail structure for 'sitetree_breadcrumbs' tag.\n\n        :param str|unicode tree_alias:\n        :param Context context:\n        :rtype: list|str\n        \"\"\"", "output": "\n        if not hasattr(self, '_breadcrumbs'):\n            self._breadcrumbs = []\n            self._build_breadcrumbs(tree_alias, context)\n        return self._breadcrumbs\n\n    def _build_breadcrumbs(self, tree_alias, context):\n        \"\"\"Builds breadcrumb trail structure for'sitetree_breadcrumbs' tag.\n\n        :param str|unicode tree_alias:\n        :param Context context:\n        \"\"\"\n        tree = self.get_tree(tree_alias, context)\n        if tree:\n            for node in tree.get_descendants():\n                if node.is_active:\n                    self._breadcrumbs.append(", "solution": "\n        tree_alias, sitetree_items = self.init_tree(tree_alias, context)\n\n        if not sitetree_items:\n            return ''\n\n        current_item = self.get_tree_current_item(tree_alias)\n\n        breadcrumbs = []\n\n        if current_item is not None:\n\n            context_ = self.current_page_context\n            check_access = self.check_access\n            get_item_by_id = self.get_item_by_id\n\n            def climb(base_item):\n                "}
{"prompt": "def getCSV(self):\n        \"\"\"\n        Returns\n        -------\n        filename: str\n        \"\"\"", "output": "\n        return self.filename\n\n    def setCSV(self, filename):\n        \"\"\"\n        Parameters\n        ----------\n        filename: str\n        \"\"\"\n        self.filename = filename\n\n    def getCSVHeader(self):\n        \"\"\"\n        Returns\n        -------\n        header: list\n        \"\"\"\n        return self.header\n\n    def setCSVHeader(self, header):\n        \"\"\"\n        Parameters\n        ----------\n        header: list\n        \"\"\"\n        self.header = header\n\n    def getCSVData(self):\n        \"\"\"\n        Returns\n        -------\n        data: list\n        \"\"\"\n        return self.data\n\n    def setCSVData(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data", "solution": "\n        import getpass\n        import gspread\n\n        user = raw_input(\"Insert Google username:\")\n        password = getpass.getpass(prompt=\"Insert password:\")\n        name = raw_input(\"SpreadSheet filename on Drive:\")\n        sheet = raw_input(\"Sheet name (first sheet is default):\")\n\n        cl = gspread.login(user, password)\n        sh = cl.open(name)\n\n        if not (sheet.strip()):\n            ws = sh.sheet1\n            sheet = \"1\"\n        else:\n            ws = sh.worksheet(sheet)\n\n        filename = name + '-worksheet_' + sheet + '.csv'\n        with open(filename, 'wb') as f:\n            writer = UnicodeWriter(f)\n            writer.writerows(ws.get_all_values())\n\n        return filename"}
{"prompt": "def hunt_repeated_yaml_keys(data):\n    \"\"\"Parses yaml and returns a list of repeated variables and\n       the line on which they occur\n    \"\"\"", "output": "\n    result = []\n    for key, value in data.items():\n        if isinstance(value, dict):\n            result.extend(hunt_repeated_yaml_keys(value))\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, dict):\n                    result.extend(hunt_repeated_yaml_keys(item))\n                elif isinstance(item, str):\n                    if key in result:\n                        result.append((key, item, data[key]))\n    return result\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Finds repeated keys in a YAML", "solution": "\n    loader = yaml.Loader(data)\n\n    def compose_node(parent, index):\n        # the line number where the previous token has ended (plus empty lines)\n        line = loader.line\n        node = Composer.compose_node(loader, parent, index)\n        node.__line__ = line + 1\n        return node\n\n    def construct_mapping(node, deep=False):\n        mapping = dict()\n        errors = dict()\n        for key_node, value_node in node.value:\n            key = key_node.value\n            if key in mapping:\n                if key in errors:\n                    errors[key].append(key_node.__line__)\n                else:\n                    errors[key] = [mapping[key], key_node.__line__]\n\n            mapping[key] = key_node.__line__\n\n        return errors\n\n    loader.compose_node = compose_node\n    loader.construct_mapping = construct_mapping\n    data = loader.get_single_data()\n    return data"}
{"prompt": "def deserialize_dataframe(reader, data_type_id):\n    \"\"\"\n    Deserialize a dataframe.\n\n    Parameters\n    ----------\n    reader : file\n        File-like object to read from. Must be opened in binary mode.\n    data_type_id : dict\n        Serialization format of the raw data.\n        See the azureml.DataTypeIds class for constants.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Dataframe object.\n    \"\"\"", "output": "\n    if data_type_id == DataTypeIds.PANDAS_DATAFRAME:\n        import pandas as pd\n        return pd.read_pickle(reader)\n    elif data_type_id == DataTypeIds.CSV:\n        import pandas as pd\n        return pd.read_csv(reader)\n    else:\n        raise ValueError(\"Unsupported data type ID: {}\".format(data_type_id))\n\n\ndef serialize_dataframe(df, data_type_id):\n    \"\"\"\n    Serialize a dataframe.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        Dataframe object to serialize.\n    data_type_id :", "solution": "\n    _not_none('reader', reader)\n    _not_none_or_empty('data_type_id', data_type_id)\n\n    serializer = _SERIALIZERS.get(data_type_id)\n    if serializer is None:\n        raise UnsupportedDatasetTypeError(data_type_id)\n    return serializer[1](reader=reader)"}
{"prompt": "def asin(wave):\n    r\"\"\"\n    Return the arc sine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.asin\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": "Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform is not a valid waveform)\n\n     * ValueError (Waveform", "solution": "\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"asin\", \"rad\", np.arcsin)"}
{"prompt": "def _on_parent_event_dependent(self):\n        \"\"\"\n        After parrent statement become event dependent\n        propagate event dependency flag to child statements\n        \"\"\"", "output": "\n        for child in self.children:\n            child.event_dependent = self.event_dependent\n\n    def _on_parent_event_independent(self):\n        \"\"\"\n        After parrent statement become event independent\n        propagate event dependency flag to child statements\n        \"\"\"\n        for child in self.children:\n            child.event_dependent = self.event_dependent\n\n    def _on_parent_event_dependent_(self):\n        \"\"\"\n        After parrent statement become event dependent_union\n        propagate event dependency flag to child statements\n        \"\"\"\n        for child in self.children:\n            child.event_dependent_union = self", "solution": "\n        if not self._is_completly_event_dependent:\n            self._is_completly_event_dependent = True\n            for stm in self._iter_stms():\n                stm._on_parent_event_dependent()"}
{"prompt": "def allclose(a, b):\n    \"\"\"\n    Test that a and b are close and match in shape.\n\n    Parameters\n    ----------\n    a : ndarray\n        First array to check\n\n    b : ndarray\n        First array to check\n    \"\"\"", "output": "\n    if a.shape!= b.shape:\n        return False\n    return np.allclose(a, b)\n\n\ndef_to_(x):\n    \"\"\"\n    Convertels tot.\n\n    Parameters\n    ----------\n    x : float\n        Value inels\n\n    Returns\n    -------\n    float\n        Value in meters\n    \"\"\"\n    return x * 0.3048\n\n\ndef_to_ft(x):\n    \"\"\"\n    Convert meters to feet.\n\n    Parameters\n    ----------\n    x : float\n        Value in meters\n\n    Returns\n    -------\n    float\n        Value in feet\n    \"\"\"\n    return x * ", "solution": "\n    from numpy import allclose\n    return (a.shape == b.shape) and allclose(a, b)"}
{"prompt": "def signFix(val, width):\n    \"\"\"\n    Convert negative int to positive int which has same bits set\n    \"\"\"", "output": "\n    if val < 0:\n        val = -val\n        val = val | (1 << (width - 1))\n    return val\n\ndef signExtend(val, width):\n    \"\"\"\n    Sign extend a value to a given width\n    \"\"\"\n    if val & (1 << (width - 1)):\n        val = val - (1 << width)\n    return val\n\ndef get_____bits(val, width):\n    \"\"\"\n    Get the bits of a value\n    \"\"\"\n    return val & ((1 << width) - 1)\n\ndef get_zero", "solution": "\n    if val > 0:\n        msb = 1 << (width - 1)\n        if val & msb:\n            val -= mask(width) + 1\n    return val"}
{"prompt": "def list(self):\n        \"\"\"\n        Get all current labels\n\n        :return: The Logentries API response\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        response = self._request('GET', '/labels')\n        return response.json()\n\n    def create(self, label):\n        \"\"\"\n        Create a new label\n\n        :param label: The label to create\n        :type label: str\n\n        :return: The Logentries API response\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        response = self._request('POST', '/labels', data=json.dumps(label))\n        return response.json()\n\n    def delete(", "solution": "\n        return self._post(\n            request='list',\n            uri=ApiUri.TAGS.value,\n        ).get('tags')"}
{"prompt": "def add_menu_action(self, menu, action, defer_shortcut=False):\n        \"\"\"Add action to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will avoid conflict with shortcuts already bound to the\n        widgets themselves.\n        \"\"\"", "output": "\n        if notable_menu(menu):\n            return\n        \n        if not isinstance(action, QAction):\n            raise TypeError(\"action must be a QAction instance\")\n        \n        if not isinstance(menu, QMenu):\n            raise TypeError(\"menu must be a QMenu instance\")\n        \n        if action.menu() is not None:\n            raise ValueError(\"action already has a menu\")\n        \n        if action.parent() is not None:\n            raise ValueError(\"action already has a parent\")\n        \n        if action.shortcutContext() == QAction.WidgetShortcut:\n            action.setShortcutContext(QAction.WidgetShortcut)\n        elif not defer_", "solution": "\n        menu.addAction(action)\n        self.addAction(action)\n\n        if defer_shortcut:\n            action.setShortcutContext(QtCore.Qt.WidgetShortcut)"}
{"prompt": "def from_string(bnf: str, entry=None, *optional_inherit) -> Grammar:\n    \"\"\"\n    Create a Grammar from a string\n    \"\"\"", "output": "\n    =m.Parser(bnf)\n    =m.Grammar(entry)\n    for inherit in optional_inherit:\n        if isinstance(inherit, str):\n            inherit =.get_rule(inherit)\n        if not isinstance(inherit, Rule):\n            raise ValueError(f\"Invalid inherit type: {type(inherit)}\")\n        if inherit.name in entry.rules:\n            raise ValueError(f\"Rule {inherit.name} already exists in the entry grammar\")\n        entry.rules[inherit.name] = inherit\n    for rule in entry.rules.values():\n        if rule.name", "solution": "\n    inherit = [Grammar] + list(optional_inherit)\n    scope = {'grammar': bnf, 'entry': entry}\n    return build_grammar(tuple(inherit), scope)"}
{"prompt": "def parse_statement(self):\n    \"\"\"Parse a single statement.\n\n    Returns:\n      Either a `BindingStatement`, `ImportStatement`, `IncludeStatement`, or\n      `None` if no more statements can be parsed (EOF reached).\n    \"\"\"", "output": "\n    if self.peek_token().type == TokenType.IMPORT:\n      return self.parse_import_statement()\n    elif self.peek_token().type == TokenType.INCLUDE:\n      return self.parse_include_statement()\n    elif self.peek_token().type == TokenType.BINDING:\n      return self.parse_binding_statement()\n    else:\n      self.error('Expected import, include, or binding statement.')\n      return None\n\n  def parse_import_statement(self):\n    \"\"\"Parse an import statement.\n\n    Returns:\n      A `ImportStatement` object.\n    \"\"\"\n    self.expect_token", "solution": "\n    self._skip_whitespace_and_comments()\n    if self._current_token.kind == tokenize.ENDMARKER:\n      return None\n\n    # Save off location, but ignore char_num for any statement-level errors.\n    stmt_loc = self._current_location(ignore_char_num=True)\n    binding_key_or_keyword = self._parse_selector()\n    statement = None\n    if self._current_token.value != '=':\n      if binding_key_or_keyword == 'import':\n        module = self._parse_selector(scoped=False)\n        statement = ImportStatement(module, stmt_loc)\n      elif binding_key_or_keyword == 'include':\n        str_loc = self._current_location()\n        success, filename = self._maybe_parse_basic_type()\n        if not success or not isinstance(filename, str):\n          self._raise_syntax_error('Expected file path as string.', str_loc)\n        statement = IncludeStatement(filename, stmt_loc)\n      else:\n        self._raise_syntax_error(\"Expected '='.\")\n    else:  # We saw an '='.\n      self._advance_one_token()\n      value = self.parse_value()\n      scope, selector, arg_name = parse_binding_key(binding_key_or_keyword)\n      statement = BindingStatement(scope, selector, arg_name, value, stmt_loc)\n\n    assert statement, 'Internal parsing error.'\n\n    if (self._current_token.kind != tokenize.NEWLINE and\n        self._current_token.kind != tokenize.ENDMARKER):\n      self._raise_syntax_error('Expected newline.')\n    elif self._current_token.kind == tokenize.NEWLINE:\n      self._advance_one_token()\n\n    return statement"}
{"prompt": "def add_xpaths_to_stream_item(si):\n    \"\"\"Mutably tag tokens with xpath offsets.\n\n    Given some stream item, this will tag all tokens from all taggings\n    in the document that contain character offsets. Note that some\n    tokens may not have computable xpath offsets, so an xpath offset\n    for those tokens will not be set. (See the documentation and\n    comments for ``char_offsets_to_xpaths`` for what it means for a\n    token to have a computable xpath.)\n\n    If a token can have its xpath offset computed, it is added to its\n    set of offsets with a ``OffsetType.XPATH_CHARS`` key.\n    \"\"\"", "output": ". If a token cannot have its xpath offset computed,\n    its xpath offset is set to None.\n\n    Args:\n        si (StreamItem): The stream item to add xpath offsets to.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the document and of the stream item\n    doc = si.document\n   _doc = si.document.get_root_element()\n\n    # Get the tokens and taggings\n    tokens = doc.get_tokens()\n    taggings = doc.get_taggings()\n\n    # Get the offsets for each token\n    char_offsets = [token.char_", "solution": "\n    def sentences_to_xpaths(sentences):\n        tokens = sentences_to_char_tokens(sentences)\n        offsets = char_tokens_to_char_offsets(tokens)\n        return char_offsets_to_xpaths(html, offsets)\n\n    def xprange_to_offset(xprange):\n        return Offset(type=OffsetType.XPATH_CHARS,\n                      first=xprange.start_offset, length=0,\n                      xpath=xprange.start_xpath,\n                      content_form='clean_html', value=None,\n                      xpath_end=xprange.end_xpath,\n                      xpath_end_offset=xprange.end_offset)\n\n    html = unicode(si.body.clean_html, 'utf-8')\n    for sentences in si.body.sentences.itervalues():\n        tokens = sentences_to_char_tokens(sentences)\n        for token, xprange in izip(tokens, sentences_to_xpaths(sentences)):\n            if xprange is None:\n                continue\n            offset = xprange_to_offset(xprange)\n            token.offsets[OffsetType.XPATH_CHARS] = offset"}
{"prompt": "def fill_parameters(self, path, blocks, exclude_free_params=False, check_parameters=False):\n        \"\"\"\n        Load parameters from file to fill all blocks sequentially.\n        :type blocks: list of deepy.layers.Block\n        \"\"\"", "output": "\n        with open(path, 'r') as f:\n            lines = f.readlines()\n\n        for i, block in enumerate(blocks):\n            if exclude_free_params and block.is_free_param:\n                continue\n\n            for j, line in enumerate(lines):\n                if line.startswith(block.name):\n                    if check_parameters:\n                        block.check_parameters(line)\n                    block.fill_parameters(line)\n                    break\n\n            if j == len(lines) - 1:\n                raise ValueError(\"Block {} not found in file {}\".format(block.name, path))\n\n    def save_", "solution": "\n        if not os.path.exists(path):\n            raise Exception(\"model {} does not exist\".format(path))\n        # Decide which parameters to load\n        normal_params = sum([nn.parameters for nn in blocks], [])\n        all_params = sum([nn.all_parameters for nn in blocks], [])\n        # Load parameters\n        if path.endswith(\".gz\"):\n            opener = gzip.open if path.lower().endswith('.gz') else open\n            handle = opener(path, 'rb')\n            saved_params = pickle.load(handle)\n            handle.close()\n            # Write parameters\n            if len(all_params) != len(saved_params):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(saved_params)))\n            for target, source in zip(all_params, saved_params):\n                if not exclude_free_params or target not in normal_params:\n                    target.set_value(source)\n        elif path.endswith(\".npz\"):\n            arrs = np.load(path)\n            # Write parameters\n            if len(all_params) != len(arrs.keys()):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(arrs.keys())))\n            for target, idx in zip(all_params, range(len(arrs.keys()))):\n                if not exclude_free_params or target not in normal_params:\n                    source = arrs['arr_%d' % idx]\n                    target.set_value(source)\n        else:\n            raise Exception(\"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path)"}
{"prompt": "def get_endpoint_path(self, endpoint_id):\n    \"\"\"return the first fullpath to a folder in the endpoint based on\n       expanding the user's home from the globus config file. This\n       function is fragile but I don't see any other way to do it.\n    \n       Parameters\n       ==========\n       endpoint_id: the endpoint id to look up the path for\n\n    \"\"\"", "output": "\n    import globus_sdk\n    from globus_sdk.exc import GlobusAPIError\n\n    # get theid for the endpoint\n    endpoint_info = globus_sdk.TransferClient(\n        config_file=self.config_file).endpoint_search(endpoint_id)\n    if not endpoint_info:\n        raise ValueError(\"Endpoint not found\")\n    endpoint_info = endpoint_info[0]\n    if'verifiable_identity_document' in endpoint_info:\n        verifiable_identity_document = endpoint_info['verifiable_identity_document']\n        if 'user_id'", "solution": "\n    config = os.path.expanduser(\"~/.globusonline/lta/config-paths\")\n    if not os.path.exists(config):\n        bot.error('%s not found for a local Globus endpoint.')\n        sys.exit(1)\n\n    path = None\n\n    # Read in the config and get the root path\n\n    config = [x.split(',')[0] for x in read_file(config)]\n    for path in config:\n        if os.path.exists(path):\n            break\n\n    # If we don't have an existing path, exit\n\n    if path is None:\n        bot.error('No path was found for a local Globus endpoint.')\n        sys.exit(1)\n\n    return path"}
{"prompt": "def _request_activity_list(self, athlete):\n        \"\"\"Actually do the request for activity list\n        This call is slow and therefore this method is memory cached.\n\n        Keyword arguments:\n        athlete -- Full name of athlete\n        \"\"\"", "output": "\n        # Check if we have already requested this athlete's activity list\n        if athlete in self._activity_list_cache:\n            return self._activity_list_cache[athlete]\n\n        # Make the request\n        url = 'https://www.strava.com/athletes/{}/activities'.format(athlete)\n        response = requests.get(url)\n\n        # Parse the response\n        soup = BeautifulSoup(response.text, 'html.parser')\n        activities = []\n        for activity in soup.find_all('div', class_='activity-item'):\n            activity_id = activity.find('", "solution": "\n        response = self._get_request(self._athlete_endpoint(athlete))\n        response_buffer = StringIO(response.text)\n\n        activity_list = pd.read_csv(\n            filepath_or_buffer=response_buffer,\n            parse_dates={'datetime': ['date', 'time']},\n            sep=',\\s*',\n            engine='python'\n        )\n        activity_list.rename(columns=lambda x: x.lower(), inplace=True)\n        activity_list.rename(\n            columns=lambda x: '_' + x if x[0].isdigit() else x, inplace=True)\n\n        activity_list['has_hr'] = activity_list.average_heart_rate.map(bool)\n        activity_list['has_spd'] = activity_list.average_speed.map(bool)\n        activity_list['has_pwr'] = activity_list.average_power.map(bool)\n        activity_list['has_cad'] = activity_list.average_heart_rate.map(bool)\n        activity_list['data'] = pd.Series(dtype=np.dtype(\"object\"))\n        return activity_list"}
{"prompt": "def reconnectUnitSignalsToModel(synthesisedUnitOrIntf, modelCls):\n    \"\"\"\n    Reconnect model signals to unit to run simulation with simulation model\n    but use original unit interfaces for communication\n\n    :param synthesisedUnitOrIntf: interface where should be signals\n        replaced from signals from modelCls\n    :param modelCls: simulation model form where signals\n        for synthesisedUnitOrIntf should be taken\n    \"\"\"", "output": "\n   _signals = modelCls.getSignals()\n    for signal in_signals:\n        signal_name = signal.name\n        signal_value = getattr(synthesisedUnitOrIntf, signal_name)\n        signal.setValue(signal_value)\n", "solution": "\n    obj = synthesisedUnitOrIntf\n    subInterfaces = obj._interfaces\n\n\n    if subInterfaces:\n        for intf in subInterfaces:\n            # proxies are destroyed on original interfaces and only proxies on\n            # array items will remain\n            reconnectUnitSignalsToModel(intf, modelCls)\n    else:\n        # reconnect signal from model\n        s = synthesisedUnitOrIntf\n        s._sigInside = getattr(modelCls, s._sigInside.name)"}
{"prompt": "def lines_without_stdlib(self):\n        \"\"\"Filters code from standard library from self.lines.\"\"\"", "output": "\n        self.lines = [line for line in self.lines if not line.startswith('import ')]\n\n    def remove_comments(self):\n        \"\"\"Removes comments from self.lines.\"\"\"\n        self.lines = [line for line in self.lines if not line.startswith('#')]\n\n    def remove_empty_lines(self):\n        \"\"\"Removes empty lines from self.lines.\"\"\"\n        self.lines = [line for line in self.lines if line.strip()]\n\n    def remove_docstrings(self):\n        \"\"\"Removes docstrings from self.lines.\"\"\"\n        self.lines = [line for line", "solution": "\n        prev_line = None\n        current_module_path = inspect.getabsfile(inspect.currentframe())\n        for module_path, lineno, runtime in self.lines:\n            module_abspath = os.path.abspath(module_path)\n            if not prev_line:\n                prev_line = [module_abspath, lineno, runtime]\n            else:\n                if (not check_standard_dir(module_path) and\n                        module_abspath != current_module_path):\n                    yield prev_line\n                    prev_line = [module_abspath, lineno, runtime]\n                else:\n                    prev_line[2] += runtime\n        yield prev_line"}
{"prompt": "def from_ascii(path, seperator=None, names=True, skip_lines=0, skip_after=0, **kwargs):\n    \"\"\"\n    Create an in memory DataFrame from an ascii file (whitespace seperated by default).\n\n    >>> ds = vx.from_ascii(\"table.asc\")\n    >>> ds = vx.from_ascii(\"table.csv\", seperator=\",\", names=[\"x\", \"y\", \"z\"])\n\n    :param path: file path\n    :param seperator: value seperator, by default whitespace, use \",\" for comma seperated values.\n    :param names: If True, the first line is used for the column names, otherwise provide a list of strings with names\n    :param skip_lines: skip lines at the start of the file\n    :param skip_after: skip lines at the end of the file\n    :param kwargs:\n    :rtype: DataFrame\n    \"\"\"", "output": ", the first line of the file will be used as column names.\n    :param skip_lines: number of lines to skip before reading the data.\n    :param skip_after: number of lines to skip after reading the data.\n    :param kwargs: additional arguments passed to pandas.read_csv.\n    :return: DataFrame\n    \"\"\"\n    if seperator is None:\n        seperator = \" \"\n    if names:\n        with open(path, \"r\") as f:\n            first_line = f.readline()\n            if seperator in first_line:\n                names = first_line.strip().split(", "solution": "\n\n    import vaex.ext.readcol as rc\n    ds = vaex.dataframe.DataFrameArrays(path)\n    if names not in [True, False]:\n        namelist = names\n        names = False\n    else:\n        namelist = None\n    data = rc.readcol(path, fsep=seperator, asdict=namelist is None, names=names, skipline=skip_lines, skipafter=skip_after, **kwargs)\n    if namelist:\n        for name, array in zip(namelist, data.T):\n            ds.add_column(name, array)\n    else:\n        for name, array in data.items():\n            ds.add_column(name, array)\n    return ds"}
{"prompt": "def ndtr(x, name=\"ndtr\"):\n  \"\"\"Normal distribution function.\n\n  Returns the area under the Gaussian probability density function, integrated\n  from minus infinity to x:\n\n  ```\n                    1       / x\n     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt\n                sqrt(2 pi)  /-inf\n\n              = 0.5 (1 + erf(x / sqrt(2)))\n              = 0.5 erfc(x / sqrt(2))\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtr\").\n\n  Returns:\n    ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"", "output": "64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same type as `x`.\n  \"\"\"\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(x, name=\"x\")\n    return 0.5 * (1.0 + tf.math.erfc(x / tf.math.sqrt(2.0)))\n\ndef log_ndtr(x, name=\"log_ndtr\"):\n  \"\"\"Natural logarithm of the normal distribution function.\n\n  Returns the natural logarithm of the", "solution": "\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"x.dtype=%s is not handled, see docstring for supported types.\"\n          % x.dtype)\n    return _ndtr(x)"}
{"prompt": "def _all_dirs(base_path):\n        \"\"\"\n        Return all dirs in base_path, relative to base_path\n        \"\"\"", "output": "\n        return [os.path.relpath(d, base_path) for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n\n\ndef _all_files(base_path):\n    \"\"\"\n    Return all files in base_path, relative to base_path\n    \"\"\"\n    return [os.path.relpath(f, base_path) for f in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, f))]\n\n\ndef _get_", "solution": "\n        for root, dirs, files in os.walk(base_path, followlinks=True):\n            for dir in dirs:\n                yield os.path.relpath(os.path.join(root, dir), base_path)"}
{"prompt": "def dump_encoding( file, encoding_name, encoding_list ):\n  \"\"\"dump a given encoding\"\"\"", "output": "\n  file.write( \"Encoding: %s\\n\" % encoding_name )\n  for encoding in encoding_list:\n    file.write( \"  %s\\n\" % encoding )\n\ndef dump_encoding_list( file, encoding_list ):\n  \"\"\"dump a list of encodings\"\"\"\n  file.write( \"Encodings:\\n\" )\n  for encoding_name, encoding_list in encoding_list.items():\n    dump_encoding( file, encoding_name, encoding_list )\n\ndef main():\n  \"\"\"main function\"\"\"\n  parser = argparse.ArgumentParser( description=\"dump", "solution": "\n\n  write = file.write\n  write( \"  /* the following are indices into the SID name table */\\n\" )\n  write( \"  static const unsigned short  \" + encoding_name +\n         \"[\" + repr( len( encoding_list ) ) + \"] =\\n\" )\n  write( \"  {\\n\" )\n\n  line  = \"    \"\n  comma = \"\"\n  col   = 0\n  for value in encoding_list:\n    line += comma\n    line += \"%3d\" % value\n    comma = \",\"\n    col  += 1\n    if col == 16:\n      col = 0\n      comma = \",\\n    \"\n\n  write( line + \"\\n  };\\n\\n\\n\" )"}
{"prompt": "def _set_kernel_manager(self, kernel_manager):\n        \"\"\" Disconnect from the current kernel manager (if any) and set a new\n            kernel manager.\n        \"\"\"", "output": "\n        if self._kernel_manager is not None:\n            self._kernel_manager.kernel_connected.disconnect(self._kernel_connected_callback)\n            self._kernel_manager.kernel_disconnected.disconnect(self._kernel_disconnected_callback)\n            self._kernel_manager.kernel_restarting.disconnect(self._kernel_restarting_callback)\n            self._kernel_manager.kernel_restarted.disconnect(self._kernel_restarted_callback)\n            self._kernel_manager.kernel_restarted.disconnect(self._kernel_restarted_callback)\n            self._kernel_manager", "solution": "\n        # Disconnect the old kernel manager, if necessary.\n        old_manager = self._kernel_manager\n        if old_manager is not None:\n            old_manager.started_kernel.disconnect(self._started_kernel)\n            old_manager.started_channels.disconnect(self._started_channels)\n            old_manager.stopped_channels.disconnect(self._stopped_channels)\n\n            # Disconnect the old kernel manager's channels.\n            old_manager.sub_channel.message_received.disconnect(self._dispatch)\n            old_manager.shell_channel.message_received.disconnect(self._dispatch)\n            old_manager.stdin_channel.message_received.disconnect(self._dispatch)\n            old_manager.hb_channel.kernel_died.disconnect(\n                self._handle_kernel_died)\n\n            # Handle the case where the old kernel manager is still listening.\n            if old_manager.channels_running:\n                self._stopped_channels()\n\n        # Set the new kernel manager.\n        self._kernel_manager = kernel_manager\n        if kernel_manager is None:\n            return\n\n        # Connect the new kernel manager.\n        kernel_manager.started_kernel.connect(self._started_kernel)\n        kernel_manager.started_channels.connect(self._started_channels)\n        kernel_manager.stopped_channels.connect(self._stopped_channels)\n\n        # Connect the new kernel manager's channels.\n        kernel_manager.sub_channel.message_received.connect(self._dispatch)\n        kernel_manager.shell_channel.message_received.connect(self._dispatch)\n        kernel_manager.stdin_channel.message_received.connect(self._dispatch)\n        kernel_manager.hb_channel.kernel_died.connect(self._handle_kernel_died)\n\n        # Handle the case where the kernel manager started channels before\n        # we connected.\n        if kernel_manager.channels_running:\n            self._started_channels()"}
{"prompt": "def subscribe_to_trades(self, pair, **kwargs):\n        \"\"\"Subscribe to the passed pair's trades channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"", "output": "\n        self.ws.subscribe(f'trades_{pair}', **kwargs)\n\n    def subscribe_to_orderbook(self, pair, **kwargs):\n        \"\"\"Subscribe to the passed pair's orderbook channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"\n        self.ws.subscribe(f'orderbook_{pair}', **kwargs)\n\n    def subscribe_to_ticker(self, pair, **kwargs):\n        \"\"\"Subscribe to the passed pair's ticker channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param", "solution": "\n        identifier = ('trades', pair)\n        self._subscribe('trades', identifier, symbol=pair, **kwargs)"}
{"prompt": "def interact(self, display_banner=None):\n        \"\"\"Closely emulate the interactive Python console.\"\"\"", "output": "\n        if display_banner:\n            print(display_banner)\n        try:\n            while True:\n                code = input('>>> ')\n                if code == 'exit()':\n                    break\n                exec(code, self.locals)\n        except KeyboardInterrupt:\n            print('\\nExiting.')\n\n    def run(self, code, display_banner=None):\n        \"\"\"Run a code snippet in the current namespace.\"\"\"\n        if display_banner:\n            print(display_banner)\n        exec(code, self.locals)\n\n    def runfile(self, filename, display_banner=None):\n        \"\"\"Run a Python script file in", "solution": "\n\n        # batch run -> do not interact\n        if self.exit_now:\n            return\n\n        if display_banner is None:\n            display_banner = self.display_banner\n\n        if isinstance(display_banner, basestring):\n            self.show_banner(display_banner)\n        elif display_banner:\n            self.show_banner()\n\n        more = False\n\n        if self.has_readline:\n            self.readline_startup_hook(self.pre_readline)\n            hlen_b4_cell = self.readline.get_current_history_length()\n        else:\n            hlen_b4_cell = 0\n        # exit_now is set by a call to %Exit or %Quit, through the\n        # ask_exit callback.\n\n        while not self.exit_now:\n            self.hooks.pre_prompt_hook()\n            if more:\n                try:\n                    prompt = self.prompt_manager.render('in2')\n                except:\n                    self.showtraceback()\n                if self.autoindent:\n                    self.rl_do_indent = True\n\n            else:\n                try:\n                    prompt = self.separate_in + self.prompt_manager.render('in')\n                except:\n                    self.showtraceback()\n            try:\n                line = self.raw_input(prompt)\n                if self.exit_now:\n                    # quick exit on sys.std[in|out] close\n                    break\n                if self.autoindent:\n                    self.rl_do_indent = False\n\n            except KeyboardInterrupt:\n                #double-guard against keyboardinterrupts during kbdint handling\n                try:\n                    self.write('\\nKeyboardInterrupt\\n')\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n                    more = False\n                except KeyboardInterrupt:\n                    pass\n            except EOFError:\n                if self.autoindent:\n                    self.rl_do_indent = False\n                    if self.has_readline:\n                        self.readline_startup_hook(None)\n                self.write('\\n')\n                self.exit()\n            except bdb.BdbQuit:\n                warn('The Python debugger has exited with a BdbQuit exception.\\n'\n                     'Because of how pdb handles the stack, it is impossible\\n'\n                     'for IPython to properly format this particular exception.\\n'\n                     'IPython will resume normal operation.')\n            except:\n                # exceptions here are VERY RARE, but they can be triggered\n                # asynchronously by signal handlers, for example.\n                self.showtraceback()\n            else:\n                self.input_splitter.push(line)\n                more = self.input_splitter.push_accepts_more()\n                if (self.SyntaxTB.last_syntax_error and\n                    self.autoedit_syntax):\n                    self.edit_syntax_error()\n                if not more:\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    self.run_cell(source_raw, store_history=True)\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n\n        # Turn off the exit flag, so the mainloop can be restarted if desired\n        self.exit_now = False"}
{"prompt": "async def add(self, dialog: Dialog):\n        \"\"\"\n        Adds a new dialog to the set and returns the added dialog.\n        :param dialog: The dialog to add.\n        \"\"\"", "output": "\n        self.dialogs.append(dialog)\n        return dialog\n\n    async def remove(self, dialog: Dialog):\n        \"\"\"\n        Removes a dialog from the set and returns the removed dialog.\n        :param dialog: The dialog to remove.\n        \"\"\"\n        self.dialogs.remove(dialog)\n        return dialog\n\n    async def get_dialogs(self) -> List[Dialog]:\n        \"\"\"\n        Returns a list of all the dialogs in the set.\n        \"\"\"\n        return self.dialogs\n", "solution": "\n        if dialog is None or not isinstance(dialog, Dialog):\n            raise TypeError('DialogSet.add(): dialog cannot be None and must be a Dialog or derived class.')\n\n        if dialog.id in self._dialogs:\n            raise TypeError(\"DialogSet.add(): A dialog with an id of '%s' already added.\" % dialog.id)\n\n        # dialog.telemetry_client = this._telemetry_client;\n        self._dialogs[dialog.id] = dialog\n\n        return self"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Request one batch of events from Skype, calling :meth:`onEvent` with each event in turn.\n\n        Subclasses may override this method to alter loop functionality.\n        \"\"\"", "output": "\n        while True:\n            try:\n                event = self.skype.getNextEvent()\n                self.onEvent(event)\n            except SkypeError as e:\n                self.onError(e)\n            except Exception as e:\n                self.onError(e)\n\n    def onEvent(self, event):\n        \"\"\"\n        Called when a new event is received from Skype.\n\n        Subclasses may override this method to handle specific event types.\n        \"\"\"\n        pass\n\n    def onError(self, error):\n        \"\"\"\n        Called when an error occurs during event processing.\n\n        Subclasses may override this method to handle specific errors.\n        \"\"\"\n       ", "solution": "\n        try:\n            events = self.getEvents()\n        except requests.ConnectionError:\n            return\n        for event in events:\n            self.onEvent(event)\n            if self.autoAck:\n                event.ack()"}
{"prompt": "def _maybe_validate_perm(perm, validate_args, name=None):\n  \"\"\"Checks that `perm` is valid.\"\"\"", "output": "\n  if perm is None:\n    return\n  if not_api.validate_perm(perm, validate_args):\n    raise ValueError(f\"Invalid permission: {perm}\")\n\ndef _maybe_validate_perms(perms, validate_args, name=None):\n  \"\"\"Checks that `perms` is a list of valid permissions.\"\"\"\n  if perms is None:\n    return\n  for perm in perms:\n    _maybe_validate_perm(perm, validate_args, name)\n\ndef _maybe_validate_args(args, validate_args, name=None):\n  \"\"\"Checks that `args`", "solution": "\n  with tf.name_scope(name or 'maybe_validate_perm'):\n    assertions = []\n    if not dtype_util.is_integer(perm.dtype):\n      raise TypeError('`perm` must be integer type')\n\n    msg = '`perm` must be a vector.'\n    if tensorshape_util.rank(perm.shape) is not None:\n      if tensorshape_util.rank(perm.shape) != 1:\n        raise ValueError(\n            msg[:-1] +\n            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(perm, 1, message=msg)]\n\n    perm_ = tf.get_static_value(perm)\n    msg = '`perm` must be a valid permutation vector.'\n    if perm_ is not None:\n      if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)):\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_equal(\n              tf.sort(perm), tf.range(tf.size(input=perm)), message=msg)\n      ]\n\n    return assertions"}
{"prompt": "def create_cookie(self, delete=None):\n        \"\"\"\n        Creates the value for ``Set-Cookie`` HTTP header.\n\n        :param bool delete:\n            If ``True`` the cookie value will be ``deleted`` and the\n            Expires value will be ``Thu, 01-Jan-1970 00:00:01 GMT``.\n\n        \"\"\"", "output": "\n        if delete:\n            value = 'deleted'\n            expires = 'Thu, 01-Jan-1970 00:00:01 GMT'\n        else:\n            value = self.value\n            expires = self.expires\n\n        return '{}; expires={}; path={}; domain={}; secure; httponly'.format(\n            value, expires, self.path, self.domain\n        )\n\n    def __repr__(self):\n        return '<Cookie {}>'.format(self.name)\n\n\nclass SimpleCookie(dict):\n    \"\"\"\n    A simple implementation of the `Cookie` class that uses a", "solution": "\n        value = 'deleted' if delete else self._serialize(self.data)\n        split_url = parse.urlsplit(self.adapter.url)\n        domain = split_url.netloc.split(':')[0]\n\n        # Work-around for issue #11, failure of WebKit-based browsers to accept\n        # cookies set as part of a redirect response in some circumstances.\n        if '.' not in domain:\n            template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}'\n        else:\n            template = ('{name}={value}; Domain={domain}; Path={path}; '\n                        'HttpOnly{secure}{expires}')\n\n        return template.format(\n            name=self.name,\n            value=value,\n            domain=domain,\n            path=split_url.path,\n            secure='; Secure' if self.secure else '',\n            expires='; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else ''\n        )"}
{"prompt": "def derivative(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the numerical derivative of a waveform's dependent variable vector.\n\n    The method used is the `backwards differences\n    <https://en.wikipedia.org/wiki/\n    Finite_difference#Forward.2C_backward.2C_and_central_differences>`_ method\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.derivative\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"", "output": "min: float, optional\n\n    :param indep_max: Independent vector end point of computation\n    :type  indep_max: float, optional\n\n    :return: Numerical derivative of the dependent variable vector\n    :rtype: numpy.ndarray\n    \"\"\"\n    if indep_min is None:\n        indep_min = wave.indep_min\n    if indep_max is None:\n        indep_max = wave.indep_max\n\n    = wave.dependent_variable\n   _min = wave.dependent_variable_min\n   _max = wave.dependent_variable_max\n\n   ", "solution": "\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    delta_indep = np.diff(ret._indep_vector)\n    delta_dep = np.diff(ret._dep_vector)\n    delta_indep = np.concatenate((np.array([delta_indep[0]]), delta_indep))\n    delta_dep = np.concatenate((np.array([delta_dep[0]]), delta_dep))\n    ret._dep_vector = np.divide(delta_dep, delta_indep)\n    ret.dep_name = \"derivative({0})\".format(ret._dep_name)\n    ret.dep_units = _build_units(ret.indep_units, ret.dep_units, \"/\")\n    return ret"}
{"prompt": "def publish_to_target(self, target_arn, message):\n        \"\"\"\n        Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str\n        \"\"\"", "output": "\n        if target_arn.startswith(\"arn:aws:sns\"):\n            client = boto3.client(\"sns\")\n            response = client.publish(TopicArn=target_arn, Message=message)\n            return response\n        elif target_arn.startswith(\"arn:aws:sqs\"):\n            client = boto3.client(\"sqs\")\n            response = client.send_message(QueueUrl=target_arn, MessageBody=message)\n            return response\n        else:\n            raise ValueError(\"Invalid target ARN\")\n", "solution": "\n\n        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )"}
{"prompt": "def has_delete_permission(self, request, obj=None):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_delete_permission, but also passes the obj parameter in.\n        \"\"\"", "output": "\n        return self.has_change_permission(request, obj)\n\n    def has_add_permission(self, request):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_add_permission, but also passes the obj parameter in.\n        \"\"\"\n        return self.has_change_permission(request)\n\n    def has_change_or_delete_permission(self, request, obj=None):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_change_or_delete_permission, but also passes the", "solution": "\n        if settings.TREE_EDITOR_OBJECT_PERMISSIONS:\n            opts = self.opts\n            r = request.user.has_perm(opts.app_label + '.' + opts.get_delete_permission(), obj)\n        else:\n            r = True\n\n        return r and super(TreeEditor, self).has_delete_permission(request, obj)"}
{"prompt": "def _clean(self, rmConnetions=True, lockNonExternal=True):\n        \"\"\"\n        Remove all signals from this interface (used after unit is synthesized\n        and its parent is connecting its interface to this unit)\n        \"\"\"", "output": "\n        if rmConnetions:\n            self.disconnectAll()\n        if lockNonExternal:\n            self.lockNonExternal()\n\n    def lockNonExternal(self):\n        \"\"\"\n        Lock all signals that are not external (i.e., not connected to)\n        \"\"\"\n        for sig in self.signals:\n            if sig.isExternal():\n                continue\n            sig.lock()\n\n    def disconnectAll(self):\n        \"\"\"\n        Disconnect all signals from this interface\n        \"\"\"\n        for sig in self.signals:\n            sig.disconnect()\n\n    def connect(self,,,_=None,", "solution": "\n\n        if self._interfaces:\n            for i in self._interfaces:\n                i._clean(rmConnetions=rmConnetions,\n                         lockNonExternal=lockNonExternal)\n        else:\n            self._sigInside = self._sig\n            del self._sig\n\n        if lockNonExternal and not self._isExtern:\n            self._isAccessible = False"}
{"prompt": "def bytesize(self, byteorder='@'):\n        \"\"\"Compute the byte size after serialization.\n\n        Args:\n            byteorder (str, optional): This is byte order of the serialized data. Use one\n                of the `byte order characters\n                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:\n                ``@``, ``=``, ``<``, ``>``, and ``!``.\n                Default is ``@`` -- the native order.\n\n        Returns:\n            int: Size in number of bytes after serialization.\n        \"\"\"", "output": "\n        return struct.calcsize(self.format_string(byteorder))\n\n    def pack(self, data, byteorder='@'):\n        \"\"\"Pack the data into a byte string according to the format string.\n\n        Args:\n            data (object): The data to be packed.\n            byteorder (str, optional): This is byte order of the serialized data. Use one\n                of the `byte order characters\n                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:\n                ``@``, ``=``, ``<``, ``>``, and ``", "solution": "\n        # Use 8 bytes to store the seed integer\n        seed_size = struct.calcsize(byteorder+'q')\n        # Use 4 bytes to store the number of hash values\n        length_size = struct.calcsize(byteorder+'i')\n        # Use 4 bytes to store each hash value as we are using the lower 32 bit\n        hashvalue_size = struct.calcsize(byteorder+'I')\n        return seed_size + length_size + len(self) * hashvalue_size"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a FTP connection object\n        \"\"\"", "output": "\n        if self.conn is None:\n            self.conn = ftplib.FTP(self.host, self.port)\n            self.conn.login(self.user, self.password)\n        return self.conn\n\n    def close_conn(self):\n        \"\"\"\n        Closes the FTP connection object\n        \"\"\"\n        if self.conn is not None:\n            self.conn.quit()\n            self.conn = None\n\n    def get_file(self, remote_file, local_file):\n        \"\"\"\n        Downloads a file from the FTP server\n        \"\"\"\n        with open(local_file, 'wb') as f", "solution": "\n        if self.conn is None:\n            params = self.get_connection(self.ftp_conn_id)\n            pasv = params.extra_dejson.get(\"passive\", True)\n            self.conn = ftplib.FTP(params.host, params.login, params.password)\n            self.conn.set_pasv(pasv)\n\n        return self.conn"}
{"prompt": "def commonprefix(items):\n    \"\"\"Get common prefix for completions\n\n    Return the longest common prefix of a list of strings, but with special\n    treatment of escape characters that might precede commands in IPython,\n    such as %magic functions. Used in tab completion.\n\n    For a more general function, see os.path.commonprefix\n    \"\"\"", "output": "\n    if not items:\n        return \"\"\n    prefix = items[0]\n    for item in items[1:]:\n        while item.startswith(prefix) and item!= prefix:\n            prefix = item\n    return prefix\n\n\ndef common_prefix(items):\n    \"\"\"Get common prefix for completions\n\n    Return the longest common prefix of a list of strings, but with special\n    treatment of escape characters that might precede commands in IPython,\n    such as %magic functions. Used in tab completion.\n\n    For a more general function, see os.path.commonprefix\n    \"\"\"\n    if not items:\n        return \"\"\n    prefix =", "solution": "\n    # the last item will always have the least leading % symbol\n    # min / max are first/last in alphabetical order\n    first_match  = ESCAPE_RE.match(min(items))\n    last_match  = ESCAPE_RE.match(max(items))\n    # common suffix is (common prefix of reversed items) reversed\n    if first_match and last_match:\n        prefix = os.path.commonprefix((first_match.group(0)[::-1], last_match.group(0)[::-1]))[::-1]\n    else:\n        prefix = ''\n\n    items = [s.lstrip(ESCAPE_CHARS) for s in items]\n    return prefix+os.path.commonprefix(items)"}
{"prompt": "def _graph_wrap(func, graph):\n    \"\"\"Constructs function encapsulated in the graph.\"\"\"", "output": "\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return wrapper\n\ndef_graph(func):\n    \"\"\"Decorator for wrapping functions with a TensorFlow graph.\"\"\"\n    return _graph_wrap(func,_graph.graph)\n\ndef_graph(func):\n    \"\"\"Decorator for wrapping functions with a TensorFlow graph.\"\"\"\n    return _graph_wrap(func, tf.get_default_graph())\n\ndef_graph(func):\n    \"\"\"Decorator for wrapping functions with a TensorFlow graph.\"\"\"", "solution": "\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return _wrapped"}
{"prompt": "def avail_archs(self):\n        \"\"\" Initialize the dictionary of architectures for disassembling via capstone\"\"\"", "output": "\n        = Cs(CS_ARCH_X86, CS_MODE_32)\n        for arch in.list_arch():\n            self.arch_dict[arch] = {}\n            for mode in [CS_MODE_32, CS_MODE_64]:\n                self.arch_dict[arch][mode] = {}\n                for detail in [CS_OPT_DETAIL, CS_OPT_ONELINE, CS_OPT_SYNTAX]:\n                    self.arch_dict[arch][mode][detail] = {}\n                    for syntax in [CS_OPT_SYNTAX_INTEL", "solution": "\n\n        return {\n            ARM32:   (CS_ARCH_ARM,   CS_MODE_ARM),\n            ARM64:   (CS_ARCH_ARM64, CS_MODE_LITTLE_ENDIAN),\n            ARM_TB:  (CS_ARCH_ARM,   CS_MODE_THUMB),\n            MIPS32:  (CS_ARCH_MIPS,  CS_MODE_MIPS32),\n            MIPS64:  (CS_ARCH_MIPS,  CS_MODE_MIPS64),\n            SPARC32: (CS_ARCH_SPARC, CS_MODE_BIG_ENDIAN),\n            SPARC64: (CS_ARCH_SPARC, CS_MODE_V9),\n            SYSTEMZ: (CS_ARCH_SYSZ,  CS_MODE_BIG_ENDIAN),\n            X86_16:  (CS_ARCH_X86,   CS_MODE_16),\n            X86_32:  (CS_ARCH_X86,   CS_MODE_32),\n            X86_64:  (CS_ARCH_X86,   CS_MODE_64),\n        }"}
{"prompt": "def html_to_xhtml(html):\n    \"\"\"Convert all tags in an HTML tree to XHTML by moving them to the\n    XHTML namespace.\n    \"\"\"", "output": "\n    from lxml import etree\n\n    def tag_to_xhtml(tag):\n        \"\"\"Convert a tag to XHTML by moving it to the XHTML namespace.\"\"\"\n        xhtml_tag = tag.tag.replace('html:', 'xhtml:')\n        if xhtml_tag == 'html':\n            xhtml_tag = 'html:html'\n        tag.tag = xhtml_tag\n        for attr in tag.attrib:\n            if attr.startswith('xmlns:'):\n                tag.attrib[attr] = 'xmlns:xhtml'\n        for child in tag:\n            tag_to_xhtml(child)\n\n    root = et", "solution": "\n    try:\n        html = html.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    for el in html.iter(etree.Element):\n        tag = el.tag\n        if tag[0] != '{':\n            el.tag = prefix + tag"}
{"prompt": "def p_gate_op_2(self, program):\n        \"\"\"\n        gate_op : id id_list ';'\n        \"\"\"", "output": "\n        gate_name = program[1]\n        id_list = program[2]\n        program.append(('gate', gate_name, id_list))\n\n    def p_gate_op_3(self, program):\n        \"\"\"\n        gate_op : id_list ';'\n        \"\"\"\n        gate_name = program[1]\n        id_list = program[2]\n        program.append(('gate', gate_name, id_list))\n\n    def p_gate_op_4(self, program):\n        \"\"\"\n        gate_op : id_list ';'\n        \"\"\"\n        gate_name = program", "solution": "\n        program[0] = node.CustomUnitary([program[1], program[2]])\n        # To verify:\n        # 1. id is declared as a gate in global scope\n        # 2. everything in the id_list is declared as a bit in local scope\n        self.verify_as_gate(program[1], program[2])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def insert_and_get(self, **fields):\n        \"\"\"Creates a new record in the database and then gets\n        the entire row.\n\n        This allows specifying custom conflict behavior using .on_conflict().\n        If no special behavior was specified, this uses the normal Django create(..)\n\n        Arguments:\n            fields:\n                The fields of the row to create.\n\n        Returns:\n            The model instance representing the row that was created.\n        \"\"\"", "output": "\n        # Create the row\n        instance = self.model(**fields)\n        instance.save(using=self._db)\n\n        # Get the row\n        return self.get(pk=instance.pk)\n\n    def on_conflict(self, conflict_type, **kwargs):\n        \"\"\"Sets the conflict resolution behavior for the insert_and_get() method.\n\n        Arguments:\n            conflict_type:\n                The type of conflict to handle. One of:\n                - 'ignore': Ignore the conflict and continue with the insert.\n                - 'update': Update the existing row with the new values.\n                - 'raise': Raise a", "solution": "\n\n        if not self.conflict_target and not self.conflict_action:\n            # no special action required, use the standard Django create(..)\n            return super().create(**fields)\n\n        compiler = self._build_insert_compiler([fields])\n        rows = compiler.execute_sql(return_id=False)\n\n        columns = rows[0]\n\n        # get a list of columns that are officially part of the model and preserve the fact that the attribute name\n        # might be different than the database column name\n        model_columns = {}\n        for field in self.model._meta.local_concrete_fields:\n            model_columns[field.column] = field.attname\n\n        # strip out any columns/fields returned by the db that\n        # are not present in the model\n        model_init_fields = {}\n        for column_name, column_value in columns.items():\n            try:\n                model_init_fields[model_columns[column_name]] = column_value\n            except KeyError:\n                pass\n\n        return self.model(**model_init_fields)"}
{"prompt": "def strip_email_quotes(text):\n    \"\"\"Strip leading email quotation characters ('>').\n\n    Removes any combination of leading '>' interspersed with whitespace that\n    appears *identically* in all lines of the input text.\n\n    Parameters\n    ----------\n    text : str\n\n    Examples\n    --------\n\n    Simple uses::\n\n        In [2]: strip_email_quotes('> > text')\n        Out[2]: 'text'\n\n        In [3]: strip_email_quotes('> > text\\\\n> > more')\n        Out[3]: 'text\\\\nmore'\n\n    Note how only the common prefix that appears in all lines is stripped::\n\n        In [4]: strip_email_quotes('> > text\\\\n> > more\\\\n> more...')\n        Out[4]: '> text\\\\n> more\\\\nmore...'\n\n    So if any line has no quote marks ('>') , then none are stripped from any\n    of them ::\n    \n        In [5]: strip_email_quotes('> > text\\\\n> > more\\\\nlast different')\n        Out[5]: '> > text\\\\n> > more\\\\nlast different'\n    \"\"\"", "output": " in all lines is stripped.\n    \"\"\"\n    lines = text.split('\\n')\n    common_prefix = lines[0].lstrip('>')\n    for line in lines[1:]:\n        if line.lstrip('>')!= common_prefix:\n            raise ValueError('Input text is not well-formed')\n    return common_prefix\n\n\ndef strip_email_quotes_in_file(input_file, output_file):\n    \"\"\"Strip leading email quotation characters ('>') in a file.\n\n    Removes any combination of leading '>' interspersed with whitespace that\n    appears *identically* in all lines", "solution": "\n    lines = text.splitlines()\n    matches = set()\n    for line in lines:\n        prefix = re.match(r'^(\\s*>[ >]*)', line)\n        if prefix:\n            matches.add(prefix.group(1))\n        else:\n            break\n    else:\n        prefix = long_substr(list(matches))\n        if prefix:\n            strip = len(prefix)\n            text = '\\n'.join([ ln[strip:] for ln in lines])\n    return text"}
{"prompt": "def delete_vm_image(self, vm_image_name, delete_vhd=False):\n        \"\"\"\n        Deletes the specified VM Image from the image repository that is\n        associated with the specified subscription.\n\n        vm_image_name:\n            The name of the image.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        \"\"\"", "output": "\n        subscription_id = self.get_subscription_id()\n        resource_group_name = self.get_resource_group_name()\n        image_repository_name = self.get_image_repository_name()\n\n        # Get the image object\n        image_client = self.get_image_client()\n        image = image_client.get(resource_group_name, subscription_id, image_repository_name, vm_image_name)\n\n        # Delete the image\n        image_client.delete(resource_group_name, subscription_id, image_repository_name, vm_", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        path = self._get_vm_image_path(vm_image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)"}
{"prompt": "def _add_column_kwargs(self, kwargs, column):\n        \"\"\"Add keyword arguments to kwargs (in-place) based on the passed in\n        `Column <sqlalchemy.schema.Column>`.\n        \"\"\"", "output": "\n        if column.primary_key:\n            kwargs['primary_key'] = True\n        if column.unique:\n            kwargs['unique'] = True\n        if column.index:\n            kwargs['index'] = True\n        if column.nullable:\n            kwargs['nullable'] = True\n        if column.default is not None:\n            kwargs['default'] = column.default\n        if column.server_default is not None:\n            kwargs['server_default'] = column.server_default\n        if column.type is not None:\n            kwargs['type'] = column.type\n\n    def _add_column_kwargs_to_table", "solution": "\n        if column.nullable:\n            kwargs[\"allow_none\"] = True\n        kwargs[\"required\"] = not column.nullable and not _has_default(column)\n\n        if hasattr(column.type, \"enums\"):\n            kwargs[\"validate\"].append(validate.OneOf(choices=column.type.enums))\n\n        # Add a length validator if a max length is set on the column\n        # Skip UUID columns\n        # (see https://github.com/marshmallow-code/marshmallow-sqlalchemy/issues/54)\n        if hasattr(column.type, \"length\"):\n            try:\n                python_type = column.type.python_type\n            except (AttributeError, NotImplementedError):\n                python_type = None\n            if not python_type or not issubclass(python_type, uuid.UUID):\n                kwargs[\"validate\"].append(validate.Length(max=column.type.length))\n\n        if hasattr(column.type, \"scale\"):\n            kwargs[\"places\"] = getattr(column.type, \"scale\", None)"}
{"prompt": "def check_for_old_config(ipython_dir=None):\n    \"\"\"Check for old config files, and present a warning if they exist.\n\n    A link to the docs of the new config is included in the message.\n\n    This should mitigate confusion with the transition to the new\n    config system in 0.11.\n    \"\"\"", "output": "\n    if ipython_dir is None:\n        ipython_dir =_config_dir()\n    old_config_files = [\n        os.path.join(ipython_dir, 'profile_default.py'),\n        os.path.join(ipython_dir, 'ipython_config.py'),\n        os.path.join(ipython_dir, 'ipython_config.json'),\n    ]\n    for f in old_config_files:\n        if os.path.exists(f):\n            print(\n                \"WARNING: The following config file(s) exist in the \"\n                \"ipython directory and", "solution": "\n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n\n    old_configs = ['ipy_user_conf.py', 'ipythonrc', 'ipython_config.py']\n    warned = False\n    for cfg in old_configs:\n        f = os.path.join(ipython_dir, cfg)\n        if os.path.exists(f):\n            if filehash(f) == old_config_md5.get(cfg, ''):\n                os.unlink(f)\n            else:\n                warnings.warn(\"Found old IPython config file %r (modified by user)\"%f)\n                warned = True\n\n    if warned:\n        warnings.warn("}
{"prompt": "def getField(self, fld_name):\n        \"\"\" Return :class:`~ekmmeters.Field` content, scaled and formatted.\n\n        Args:\n            fld_name (str): A :class:`~ekmmeters.Field` value which is on your meter.\n\n        Returns:\n            str: String value (scaled if numeric) for the field.\n        \"\"\"", "output": "\n        fld = self.fields.get(fld_name)\n        if fld is None:\n            raise ValueError(f\"Field '{fld_name}' not found.\")\n\n        if fld.is_numeric:\n            return self._format_numeric_field(fld)\n        else:\n            return fld.value\n\n    def _format_numeric_field(self, fld):\n        if fld.unit == \"kWh\":\n            return f\"{fld.value:.2f} kWh\"\n        elif fld.unit == \"kW\":\n            return f\"{fld.value:.2f} kW\"\n        elif fld.unit", "solution": "\n        result = \"\"\n        if fld_name in self.m_req:\n            result = self.m_req[fld_name][MeterData.StringValue]\n        else:\n            ekm_log(\"Requested nonexistent field: \" + fld_name)\n\n        return result"}
{"prompt": "def _mk_index(self):\n        \"\"\"create the index for this set\"\"\"", "output": "\n        = self.get_set()\n      .create_index([(\"name\", pymongo.ASCENDING)], unique=True)\n\n    def add_item(self, item):\n        \"\"\"add an item to the set\"\"\"\n        self.get_set().insert_one(item)\n        self._mk_index()\n\n    def remove_item(self, item):\n        \"\"\"remove an item from the set\"\"\"\n        self.get_set().delete_one({\"name\": item.name})\n        self._mk_index()\n\n    def get_item(self, name):\n        \"\"\"get an item from", "solution": "\n        index = defaultdict(list)\n        for line_no, line in enumerate(self._stripped_lines):\n            if line:\n                index[line].append(line_no)\n        return index"}
{"prompt": "def _find_blob_start(self):\n        \"\"\"Find first blob from selection.\n        \"\"\"", "output": "\n        start = self.view.sel()[0].begin()\n        while start > 0 and self.view.substr(start - 1) not in self.blob_chars:\n            start -= 1\n        return start\n\n    def _find_blob_end(self):\n        \"\"\"Find last blob from selection.\n        \"\"\"\n        end = self.view.sel()[0].end()\n        while end < self.view.size() and self.view.substr(end) not in self.blob_chars:\n            end += 1\n        return end\n\n    def _find_blob_text(self):\n       ", "solution": "\n\n        # Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n\n        # Check which is the blob time offset\n        blob_time_start = self.t_start\n\n        # Check which is the blob frequency offset (in channels)\n        blob_freq_start = self.chan_start_idx\n\n        blob_start = blob_time_start * self.n_channels_in_file + blob_freq_start\n\n        return blob_start"}
{"prompt": "def update(self, ava):\n        \"\"\"\n        Implements the dict.update() method\n        \"\"\"", "output": "\n        for key, value in ava.items():\n            self[key] = value\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Implements the dict.__setitem__() method\n        \"\"\"\n        if key in self:\n            self.data[key] = value\n        else:\n            self.data[key] = value\n            self.keys.append(key)\n\n    def __getitem__(self, key):\n        \"\"\"\n        Implements the dict.__getitem__() method\n        \"\"\"\n        return self.data[key]\n\n    def __delitem__(self, key):\n        \"\"\"\n        Implements the dict.__delitem__()", "solution": "\n        for key, val in ava.items():\n            self[key] = val"}
{"prompt": "def multiqc(institute_id, case_name):\n    \"\"\"Load multiqc report for the case.\"\"\"", "output": "\n    report_path = os.path.join(multiqc_dir, institute_id, case_name, \"multiqc_report.html\")\n    if not os.path.exists(report_path):\n        return None\n\n    with open(report_path, \"r\") as f:\n        report_html = f.read()\n\n    return report_html\n\n\ndef get_multiqc_data(institute_id, case_name):\n    \"\"\"Get_multiqc_data for the case.\"\"\"\n    report_html = multiqc(institute_id, case_name)\n    if report_", "solution": "\n    data = controllers.multiqc(store, institute_id, case_name)\n    if data['case'].get('multiqc') is None:\n        return abort(404)\n    out_dir = os.path.abspath(os.path.dirname(data['case']['multiqc']))\n    filename = os.path.basename(data['case']['multiqc'])\n    return send_from_directory(out_dir, filename)"}
{"prompt": "def get_user_token(user, purpose, minutes_valid):\n    \"\"\"Return login token info for given user.\"\"\"", "output": "\n    token = jwt.encode(\n        {\n            \"user_id\": user.id,\n            \"purpose\": purpose,\n            \"exp\": datetime.utcnow() + timedelta(minutes=minutes_valid),\n        },\n        current_app.config[\"SECRET_KEY\"],\n        algorithm=\"HS256\",\n    )\n    return token\n\n\ndef verify_user_token(token):\n    \"\"\"Verify login token.\"\"\"\n    try:\n        payload = jwt.decode(token, current_app.config[\"SECRET_KEY\"], algorithms=[\"HS256\"])\n        user_id = payload[\"user_id\"]\n        purpose =", "solution": "\n    token = ''.join(\n        dumps([\n            user.get_username(),\n            get_auth_hash(user, purpose),\n        ]).encode('base64').split('\\n')\n    )\n    return {\n        'id': get_meteor_id(user),\n        'token': token,\n        'tokenExpires': calc_expiry_time(minutes_valid),\n    }"}
{"prompt": "def write_meta(self):\n        \"\"\"ucds, descriptions and units are written as attributes in the hdf5 file, instead of a seperate file as\n         the default :func:`Dataset.write_meta`.\n         \"\"\"", "output": "\n        self.ucds = self.attrs.get('ucds', None)\n        self.descriptions = self.attrs.get('descriptions', None)\n        self.units = self.attrs.get('units', None)\n\n    def read_meta(self):\n        \"\"\"ucds, descriptions and units are read as attributes in the hdf5 file, instead of a seperate file as\n         the default :func:`Dataset.read_meta`.\n         \"\"\"\n        self.attrs['ucds'] = self.ucds\n        self.attrs['descriptions'] = self.descriptions\n        self.attrs['units'] = self", "solution": "\n        with h5py.File(self.filename, \"r+\") as h5file_output:\n            h5table_root = h5file_output[self.h5table_root_name]\n            if self.description is not None:\n                h5table_root.attrs[\"description\"] = self.description\n            h5columns = h5table_root if self._version == 1 else h5table_root['columns']\n            for column_name in self.columns.keys():\n                h5dataset = None\n                if column_name in h5columns:\n                    h5dataset = h5columns[column_name]\n                else:\n                    for group in h5columns.values():\n                        if 'type' in group.attrs:\n                            if group.attrs['type'] in ['csr_matrix']: \n                                for name, column in group.items():\n                                    if name == column_name:\n                                        h5dataset = column\n                if h5dataset is None:\n                    raise ValueError('column {} not found'.format(column_name))\n                for name, values in [(\"ucd\", self.ucds), (\"unit\", self.units), (\"description\", self.descriptions)]:\n                    if column_name in values:\n                        value = ensure_string(values[column_name], cast=True)\n                        h5dataset.attrs[name] = value\n                    else:\n                        if name in h5columns.attrs:\n                            del h5dataset.attrs[name]"}
{"prompt": "def rename(self, columns=None):\n        \"\"\"\n        Change names of columns in the frame.\n\n        Dict key is an index or name of the column whose name is to be set.\n        Dict value is the new name of the column.\n\n        :param columns: dict-like transformations to apply to the column names\n        \"\"\"", "output": "\n        if columns is None:\n            columns = {}\n\n        for old_name, new_name in columns.items():\n            if isinstance(old_name, int):\n                self.columns[new_name] = self.columns.pop(old_name)\n            elif isinstance(old_name, str):\n                self.columns[new_name] = self.columns.pop(old_name)\n            else:\n                raise ValueError(\"Old column name must be an index or name\")\n\n    def drop(self, columns):\n        \"\"\"\n        Drop columns from the frame.\n\n        :param columns: column names or indices to", "solution": "\n        assert_is_type(columns, None, dict)\n        new_names = self.names\n        ncols = self.ncols\n\n        for col, name in columns.items():\n            col_index = None\n            if is_type(col, int) and (-ncols <= col < ncols):\n                col_index = (col + ncols) % ncols  # handle negative indices\n            elif is_type(col, str) and col in self.names:\n                col_index = self.names.index(col)  # lookup the name\n\n            if col_index is not None:\n                new_names[col_index] = name\n\n        return self.set_names(new_names)"}
{"prompt": "def get_root_schema_dict(record):\n    \"\"\"Return a root jsonschema for a given record\n\n    A root schema includes the $schema attribute and all sub-record\n    schemas and definitions.\n    \"\"\"", "output": "\n    root_schema = {\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n        \"type\": \"object\",\n        \"properties\": {},\n        \"required\": [],\n        \"additionalProperties\": False,\n        \"definitions\": {},\n    }\n\n    for field in record.fields:\n        if field.is_subrecord:\n            sub_schema = get_subrecord_schema_dict(field.subrecord)\n            root_schema[\"properties\"][field.name] = sub_schema\n            root_schema[\"required\"].append(field.name)\n        elif field.is_", "solution": "\n    state = SchemaGeneratorState()\n    schema = get_schema_dict(record, state)\n    del state.record_schemas[record._schema_name]\n    if state.record_schemas:\n        schema['definitions'] = dict()\n        for name, sub_schema in state.record_schemas.iteritems():\n            schema['definitions'][name] = sub_schema\n    return schema"}
{"prompt": "def _check_multi_statement_line(self, node, line):\n        \"\"\"Check for lines containing multiple statements.\"\"\"", "output": "\n        if line.strip().startswith('if ') or line.strip().startswith('while '):\n            return True\n        return False\n\n    def _check_single_statement_line(self, node, line):\n        \"\"\"Check for lines containing a single statement.\"\"\"\n        if line.strip().startswith('return '):\n            return True\n        return False\n\n    def _check_single_expression_line(self, node, line):\n        \"\"\"Check for lines containing a single expression.\"\"\"\n        if line.strip().startswith('return ') or line.strip().startswith('raise '):\n            return True\n        return False\n\n    def _check_", "solution": "\n        # Do not warn about multiple nested context managers\n        # in with statements.\n        if isinstance(node, nodes.With):\n            return\n        # For try... except... finally..., the two nodes\n        # appear to be on the same line due to how the AST is built.\n        if isinstance(node, nodes.TryExcept) and isinstance(\n            node.parent, nodes.TryFinally\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.If)\n            and not node.parent.orelse\n            and self.config.single_line_if_stmt\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.ClassDef)\n            and len(node.parent.body) == 1\n            and self.config.single_line_class_stmt\n        ):\n            return\n        self.add_message(\"multiple-statements\", node=node)\n        self._visited_lines[line] = 2"}
{"prompt": "def _validate(value, optdict, name=\"\"):\n    \"\"\"return a validated value for an option according to its type\n\n    optional argument name is only used for error message formatting\n    \"\"\"", "output": "\n    if isinstance(value, bool):\n        return value\n    elif isinstance(value, int):\n        return value\n    elif isinstance(value, float):\n        return value\n    elif isinstance(value, str):\n        if value.lower() in optdict:\n            return optdict[value.lower()]\n        else:\n            raise ValueError(f\"invalid value for option {name}: {value}\")\n    else:\n        raise TypeError(f\"invalid type for option {name}: {type(value)}\")\n\n\ndef parse_options(options, optdict):\n    \"\"\"parse a list of options and return a dictionary of validated options\n\n   ", "solution": "\n    try:\n        _type = optdict[\"type\"]\n    except KeyError:\n        # FIXME\n        return value\n    return _call_validator(_type, optdict, name, value)"}
{"prompt": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"", "output": "200 status codes\n        :type extra_options: dict\n        :return: the response object\n        :rtype: requests.Response\n        \"\"\"\n        if extra_options is None:\n            extra_options = {}\n\n        if data is None:\n            data = {}\n\n        if headers is None:\n            headers = {}\n\n        url = self.base_url + endpoint\n        response = self.session.request(\n            method=self.method,\n            url=url,\n            data=data,\n            headers=headers,\n            **extra_options\n        )\n\n        if not extra_options.get('check_response', True):\n            return", "solution": "\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)"}
{"prompt": "def print_annotation(self):\n        \"\"\"Print annotation \"key: value\" pairs to standard output.\"\"\"", "output": "\n        for key, value in self.items():\n            print(f\"{key}: {value}\")\n\n    def __str__(self):\n        return self.to_json()\n\n    def to_json(self):\n        \"\"\"Return annotation as a JSON string.\"\"\"\n        return json.dumps(self, default=lambda o: o.__dict__)\n\n    @classmethod\n    def from_json(cls, json_str):\n        \"\"\"Return annotation from a JSON string.\"\"\"\n        return cls(json.loads(json_str))\n\n\nclass AnnotationSet(dict):\n    \"\"\"A set of annotations, with methods for merging and filtering.\"\"\"\n\n   ", "solution": "\n        for path, ann in self.annotation.items():\n            print(\"{}: {}\".format(path, ann['value']))"}
{"prompt": "def info(self, **kwargs):\n        \"\"\"\n        Get the detailed information about a particular credit record. This is \n        currently only supported with the new credit model found in TV. These \n        ids can be found from any TV credit response as well as the tv_credits \n        and combined_credits methods for people.\n\n        The episodes object returns a list of episodes and are generally going \n        to be guest stars. The season array will return a list of season \n        numbers.  Season credits are credits that were marked with the \n        \"add to every season\" option in the editing interface and are \n        assumed to be \"season regulars\".\n\n        Args:\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "\" credits.\n\n        Args:\n            id (int): The id of the credit record to get information about.\n           _name (str): The name of the to get information about.\n           _id (int): The id of the to get information about.\n\n        Returns:\n            dict: A dictionary containing the detailed information about the credit record.\n        \"\"\"\n        if_name and not_id:\n           _id =_name_to_id(self.client,_name)\n\n        if_id and_name:\n            raise ValueError(\"Both_name and", "solution": "\n        path = self._get_credit_id_path('info')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def parse_options():\n    \"\"\"\n    Commandline options arguments parsing.\n\n    :return: parsed commandline arguments.\n    :rtype: optparse.Values.\n    \"\"\"", "output": "\n    parser = optparse.OptionParser()\n    parser.add_option(\"-i\", \"--input\", dest=\"input_file\", help=\"Inputml file\")\n    parser.add_option(\"-o\", \"--output\", dest=\"output_file\", help=\"Outputml file\")\n    parser.add_option(\"-v\", \"--verbose\", action=\"store_true\", dest=\"verbose\", help=\"Verbose output\")\n    parser.add_option(\"-d\", \"--debug\", action=\"store_true\", dest=\"debug\", help=\"Debug output\")\n    parser.add_option(\"-t\", \"--test\", action=\"store", "solution": "\n\n    version = \"%%prog {version}\".format(version=__version__)\n    parser = OptionParser(version=version)\n    parser.add_option(\n        \"-s\", \"--server\", action=\"store\", dest=\"server\",\n        type=\"string\", default=\"\", metavar=\"SERVER\",\n        help=\"server name, IP address or unix socket path\"\n    )\n    parser.add_option(\n        \"-p\", \"--port\", action=\"store\", type=\"int\", dest=\"port\",\n        default=9001, metavar=\"PORT\", help=\"port number\"\n    )\n    parser.add_option(\n        \"-P\", \"--programs\", action=\"store\", dest=\"programs\", type=\"string\", default=\"\",\n        metavar=\"PROGRAMS\", help=\"comma separated programs list, or empty for all programs in supervisord response\"\n    )\n    parser.add_option(\n        \"-u\", \"--username\", action=\"store\", dest=\"username\", type=\"string\", default=\"\",\n        metavar=\"USERNAME\", help=\"supervisord user\"\n    )\n    parser.add_option(\n        \"-S\", \"--password\", action=\"store\", dest=\"password\", type=\"string\", default=\"\",\n        metavar=\"PASSWORD\", help=\"supervisord user password\"\n    )\n    parser.add_option(\n        \"-q\", \"--quiet\", metavar=\"QUIET\", action=\"store_true\", default=False, dest=\"quiet\", help=\"be quiet\"\n    )\n    parser.add_option(\n        \"--stopped-state\", action=\"store\", dest=\"stopped_state\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_OK,\n        metavar=\"STOPPED_STATE\", help=\"stopped state\"\n    )\n    parser.add_option(\n        \"--network-errors-exit-code\", action=\"store\", dest=\"network_errors_exit_code\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_UNKNOWN,\n        metavar=\"NETWORK_ERRORS_EXIT_CODE\", help=\"network errors exit code\"\n    )\n\n    options = parser.parse_args(sys.argv)[0]\n    STATE2TEMPLATE[STATE_STOPPED] = options.stopped_state  # update stopped state value from command line argument\n\n    # check mandatory command line options supplied\n    if not options.server:\n        parser.error(\"Required server address option missing\")\n    if options.username and not options.password:\n        parser.error(\"Required supervisord user password\")\n\n    return options"}
{"prompt": "def _create_hstore_unique(self, model, field, keys):\n        \"\"\"Creates a UNIQUE constraint for the specified hstore keys.\"\"\"", "output": "\n        if not keys:\n            return\n\n        # Create a list ofable keys\n        keys = [key.replace('\"', '\"\"') for key in keys]\n\n        # Create the constraint\n        constraint_name = f\"unique_{model}_{field}_{'_'.join(keys)}\"\n        constraint = f\"CREATE UNIQUE INDEX {constraint_name} ON {model} USING hstore ({', '.join(keys)});\"\n\n        # Add the constraint to the model\n        model.add_index(fields=keys, name=constraint_name, unique=True)\n\n        # Log the constraint creation\n        self.logger.info(f", "solution": "\n\n        name = self._unique_constraint_name(\n            model._meta.db_table, field, keys)\n        columns = [\n            '(%s->\\'%s\\')' % (field.column, key)\n            for key in keys\n        ]\n        sql = self.sql_hstore_unique_create.format(\n            name=self.quote_name(name),\n            table=self.quote_name(model._meta.db_table),\n            columns=','.join(columns)\n        )\n        self.execute(sql)"}
{"prompt": "def create_trace(\n        turn_activity: Activity,\n        name: str,\n        value: object = None,\n        value_type: str = None,\n        label: str = None,\n    ) -> Activity:\n        \"\"\"Creates a trace activity based on this activity.\n\n        :param turn_activity:\n        :type turn_activity: Activity\n        :param name: The value to assign to the trace activity's <see cref=\"Activity.name\"/> property.\n        :type name: str\n        :param value: The value to assign to the trace activity's <see cref=\"Activity.value\"/> property., defaults to None\n        :param value: object, optional\n        :param value_type: The value to assign to the trace activity's <see cref=\"Activity.value_type\"/> property, defaults to None\n        :param value_type: str, optional\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property, defaults to None\n        :param label: str, optional\n        :return: The created trace activity.\n        :rtype: Activity\n        \"\"\"", "output": "\n        :type value: object, optional\n        :param value_type: The value to assign to the trace activity's <see cref=\"Activity.valueType\"/> property., defaults to None\n        :type value_type: str, optional\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property., defaults to None\n        :type label: str, optional\n        :return: The created trace activity.\n        :rtype: Activity\n        \"\"\"\n        trace_activity = Activity(\n            name=name,\n            value=value,\n            valueType=value_type,\n            label=label,", "solution": "\n\n        from_property = (\n            ChannelAccount(\n                id=turn_activity.recipient.id, name=turn_activity.recipient.name\n            )\n            if turn_activity.recipient is not None\n            else ChannelAccount()\n        )\n        if value_type is None and value is not None:\n            value_type = type(value).__name__\n\n        reply = Activity(\n            type=ActivityTypes.trace,\n            timestamp=datetime.utcnow(),\n            from_property=from_property,\n            recipient=ChannelAccount(\n                id=turn_activity.from_property.id, name=turn_activity.from_property.name\n            ),\n            reply_to_id=turn_activity.id,\n            service_url=turn_activity.service_url,\n            channel_id=turn_activity.channel_id,\n            conversation=ConversationAccount(\n                is_group=turn_activity.conversation.is_group,\n                id=turn_activity.conversation.id,\n                name=turn_activity.conversation.name,\n            ),\n            name=name,\n            label=label,\n            value_type=value_type,\n            value=value,\n        )\n        return reply"}
{"prompt": "def voucher_code(request):\n    \"\"\" A view *just* for entering a voucher form. \"\"\"", "output": "\n    if request.method == 'POST':\n        form = VoucherForm(request.POST)\n        if form.is_valid():\n            # Do something with the voucher code\n            return render(request, 'voucher_success.html', {'form': form})\n    else:\n        form = VoucherForm()\n    return render(request, 'voucher_form.html', {'form': form})\n```\n\nThis view uses a `VoucherForm` to validate the voucher code entered by the user. If the voucher code is valid, it redirects the user to a success page with the voucher code", "solution": "\n\n    VOUCHERS_FORM_PREFIX = \"vouchers\"\n\n    # Handle the voucher form *before* listing products.\n    # Products can change as vouchers are entered.\n    v = _handle_voucher(request, VOUCHERS_FORM_PREFIX)\n    voucher_form, voucher_handled = v\n\n    if voucher_handled:\n        messages.success(request, \"Your voucher code was accepted.\")\n        return redirect(\"dashboard\")\n\n    data = {\n        \"voucher_form\": voucher_form,\n    }\n\n    return render(request, \"registrasion/voucher_code.html\", data)"}
{"prompt": "def create(cls, name_value, name_type):\n        \"\"\"\n            Returns a Name object, populated with the given value and type\n        \"\"\"", "output": "\n        if name_type ==_name.NAME_TYPE__NAME:\n            return cls(name_value, name_type)\n        elif name_type ==_name.NAME_TYPE__NAME:\n            return cls(name_value, name_type)\n        elif name_type ==_name.NAME_TYPE__NAME:\n            return cls(name_value, name_type)\n        elif name_type ==_name.NAME_TYPE___NAME:\n            return cls(name_value, name_type)\n        else:\n            raise", "solution": "\n        if isinstance(name_value, Name.NameValue):\n            value = name_value\n        elif isinstance(name_value, str):\n            value = cls.NameValue(name_value)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_value'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_value', type(Name.NameValue),\n                                       type(name_value)))\n\n        if isinstance(name_type, Name.NameType):\n            n_type = name_type\n        elif isinstance(name_type, Enum):\n            n_type = cls.NameType(name_type)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_type'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_type', type(Name.NameType),\n                                       type(name_type)))\n\n        return Name(name_value=value,\n                    name_type=n_type)"}
{"prompt": "def loop_gtk(kernel):\n    \"\"\"Start the kernel, coordinating with the GTK event loop\"\"\"", "output": "\n    kernel.start()\n    gtk_main()\n    kernel.shutdown()\n\ndef start_kernel(argv=None):\n    \"\"\"Start the IPython kernel and run the GTK event loop\"\"\"\n    kernel = IPython.start_kernel(argv=argv)\n    loop_gtk(kernel)\n\nif __name__ == '__main__':\n    start_kernel()", "solution": "\n    from .gui.gtkembed import GTKEmbed\n\n    gtk_kernel = GTKEmbed(kernel)\n    gtk_kernel.start()"}
{"prompt": "def single_request_timeout(self, value):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"", "output": "\n        self._single_request_timeout = value\n\n    @property\n    def_timeout(self):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"\n        return self._single_request_timeout\n\n    @property\n    def_timeout(self):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"\n        return self._single_request_timeout\n\n    @property\n    def_timeout(self):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"\n        return self._single_request_timeout\n\n    @property\n    def_timeout(self):\n        \"\"\"", "solution": "\n        check_type(value, int)\n        assert value is None or value > 0\n        self._single_request_timeout = value"}
{"prompt": "def search(self, pattern=\"*\", raw=True, search_raw=True,\n                                                        output=False):\n        \"\"\"Search the database using unix glob-style matching (wildcards\n        * and ?).\n\n        Parameters\n        ----------\n        pattern : str\n          The wildcarded pattern to match when searching\n        search_raw : bool\n          If True, search the raw input, otherwise, the parsed input\n        raw, output : bool\n          See :meth:`get_range`\n\n        Returns\n        -------\n        Tuples as :meth:`get_range`\n        \"\"\"", "output": "\n        if search_raw:\n            search_func = self.search_raw\n        else:\n            search_func = self.search\n\n        if output:\n            return search_func(pattern, raw)\n        else:\n            return search_func(pattern, raw)[0]\n\n    def search_raw(self, pattern=\"*\", raw=True):\n        \"\"\"Search the raw input using unix glob-style matching (wildcards\n        * and?).\n\n        Parameters\n        ----------\n        pattern : str\n          The wildcarded pattern to match when searching\n        raw : bool\n          If True, search the raw input, otherwise, the parsed input\n\n        Returns", "solution": "\n        tosearch = \"source_raw\" if search_raw else \"source\"\n        if output:\n            tosearch = \"history.\" + tosearch\n        self.writeout_cache()\n        return self._run_sql(\"WHERE %s GLOB ?\" % tosearch, (pattern,),\n                                    raw=raw, output=output)"}
{"prompt": "async def get_bots(self, limit, offset):\n        \"\"\"Gets an object of bots on DBL\"\"\"", "output": "\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"https://discordbots.org/api/bots/{self.client_id}/stats?limit={limit}&offset={offset}\") as response:\n                if response.status!= 200:\n                    raise Exception(f\"Error getting bots from DBL: {response.status}\")\n                data = await response.json()\n                return data[\"bots\"]\n\n    async def get_bot_info(self, bot_id):\n        \"\"\"Gets information about a specific bot on DBL\"\"\"\n        async with aiohttp.Client", "solution": "\n        if limit > 500:\n            limit = 50\n        return await self.request('GET', '{}/bots?limit={}&offset={}'.format(self.BASE, limit, offset))"}
{"prompt": "def group_transactions(self):\n        # type: () -> List[List[Transaction]]\n        \"\"\"\n        Groups transactions in the bundle by address.\n        \"\"\"", "output": "\n        grouped_transactions = defaultdict(list)\n        for transaction in self.transactions:\n            grouped_transactions[transaction.sender].append(transaction)\n        return list(grouped_transactions.values())\n\n    def get_transaction(self, transaction_hash):\n        # type: (str) -> Optional[Transaction]\n        \"\"\"\n        Returns the transaction with the given transaction hash, or None if not found.\n        \"\"\"\n        for transaction in self.transactions:\n            if transaction.transaction_hash == transaction_hash:\n                return transaction\n        return None\n\n    def get_transaction_by_index(self, index):\n        # type:", "solution": "\n        groups = []\n\n        if self:\n            last_txn = self.tail_transaction\n            current_group = [last_txn]\n            for current_txn in self.transactions[1:]:\n                # Transactions are grouped by address, so as long as the\n                # address stays consistent from one transaction to\n                # another, we are still in the same group.\n                if current_txn.address == last_txn.address:\n                    current_group.append(current_txn)\n                else:\n                    groups.append(current_group)\n                    current_group = [current_txn]\n\n                last_txn = current_txn\n\n            if current_group:\n                groups.append(current_group)\n\n        return groups"}
{"prompt": "def authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"", "output": "\n\n        # Build the authentication URL\n        auth_url = self.client.get_authorization_url(\n            redirect_uri=callback_uri,\n            scope=self.scope,\n            state=self.state,\n            ax_attrs=ax_attrs\n        )\n\n        return auth_url\n\n    def authenticate(self, code, callback_uri=None):\n        \"\"\"Exchanges the authorization code for an access token.\n\n        Returns the access token and the user's attributes.\n        \"\"\"\n\n        # Exchange the authorization code for an access token\n        token_response = self.client.get_access_token_by_authorization_code", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def write_byte_data(self, addr, cmd, val):\n        \"\"\"Write a byte of data to the specified cmd register of the device.\n        \"\"\"", "output": "\n       _addr =_addr_to_int(addr)\n       _addr =_addr_to_int(val)\n       _addr =_addr & 0xFF\n       _addr =_addr <<_addr_shift(addr)\n       _addr =_addr &_addr_mask(addr)\n       _addr =_addr |_addr_mask(addr)\n       _addr =_addr &_addr_mask(addr)\n       _addr =_addr |_addr_mask(addr)\n       ", "solution": "\n        assert self._device is not None, 'Bus must be opened before operations are made against it!'\n        # Construct a string of data to send with the command register and byte value.\n        data = bytearray(2)\n        data[0] = cmd & 0xFF\n        data[1] = val & 0xFF\n        # Send the data to the device.\n        self._select_device(addr)\n        self._device.write(data)"}
{"prompt": "def remove_client(self, client):\n        # type: (object) -> None\n        \"\"\"Remove the client from the users of the socket.\n\n        If there are no more clients for the socket, it\n        will close automatically.\n        \"\"\"", "output": "\n        if client in self.clients:\n            self.clients.remove(client)\n            if not self.clients:\n                self.close()\n\n    def handle_client(self, client):\n        # type: (object) -> None\n        \"\"\"Handle a new client by sending a welcome message.\"\"\"\n        client.send(b\"Welcome to the chat server!\")\n        self.clients.append(client)\n\n    def handle_message(self, client, message):\n        # type: (object, bytes) -> None\n        \"\"\"Handle a message from a client.\"\"\"\n        message = message.decode(\"utf-8\")\n        print(", "solution": "\n\n        try:\n            self._clients.remove(id(client))\n        except ValueError:\n            pass\n\n        if len(self._clients) < 1:\n            self.close()"}
{"prompt": "def send(self, data):\n        \"\"\"\n        Send encoded instructions to Guacamole guacd server.\n        \"\"\"", "output": "\n        self.sock.sendall(data)\n\n    def recv(self, size):\n        \"\"\"\n        Receive data from Guacamole guacd server.\n        \"\"\"\n        return self.sock.recv(size)\n\n    def close(self):\n        \"\"\"\n        Close the connection to Guacamole guacd server.\n        \"\"\"\n        self.sock.close()\n", "solution": "\n        self.logger.debug('Sending data: %s' % data)\n        self.client.sendall(data.encode())"}
{"prompt": "def strsplit(self, pattern):\n        \"\"\"\n        Split the strings in the target column on the given regular expression pattern.\n\n        :param str pattern: The split pattern.\n        :returns: H2OFrame containing columns of the split strings.\n        \"\"\"", "output": "\n        return self._run_expr(\"strsplit(target, '{}')\".format(pattern))\n\n    def strsplit_by_pattern(self, pattern, column_name):\n        \"\"\"\n        Split the strings in the target column on the given regular expression pattern and store the results in a new column.\n\n        :param str pattern: The split pattern.\n        :param str column_name: The name of the new column.\n        :returns: H2OFrame containing the original target column and the new column with the split strings.\n        \"\"\"\n        return self._run_expr(\"strsplit_by_pattern(target,", "solution": "\n        fr = H2OFrame._expr(expr=ExprNode(\"strsplit\", self, pattern))\n        fr._ex._cache.nrows = self.nrow\n        return fr"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report gathered analysis data in human readable form.\"\"\"", "output": "\n       _count = len(self.items)\n        if not self.items:\n            return\n\n        output_file.write(f\"Total items: {self.total_items}\\n\")\n        output_file.write(f\"Unique items: {self.unique_items}\\n\")\n        output_file.write(f\"Items withly duplicates: {self.duplicates}\\n\")\n        output_file.write(f\"Items withlylarities: {self.similarities}\\n\")\n        output_file.write(f\"Items withlyly duplicates: {self", "solution": "\n        if self.verbose > 1:\n            with pprint_nosort():\n                pprint.pprint(self.results)\n\n        if self.verbose > 0:\n            print('Runtime (per repetition): {:.2g} s'.format(\n                self.results['Runtime (per repetition) [s]']),\n                file=output_file)\n        if self.verbose > 0:\n            print('Iterations per repetition: {!s}'.format(\n                self.results['Iterations per repetition']),\n                file=output_file)\n        print('Runtime (per cacheline update): {:.2f} cy/CL'.format(\n            self.results['Runtime (per cacheline update) [cy/CL]']),\n            file=output_file)\n        print('MEM volume (per repetition): {:.0f} Byte'.format(\n            self.results['MEM volume (per repetition) [B]']),\n            file=output_file)\n        print('Performance: {:.2f} MFLOP/s'.format(self.results['Performance [MFLOP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} MLUP/s'.format(self.results['Performance [MLUP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} It/s'.format(self.results['Performance [MIt/s]']),\n              file=output_file)\n        if self.verbose > 0:\n            print('MEM bandwidth: {:.2f} MByte/s'.format(self.results['MEM BW [MByte/s]']),\n                  file=output_file)\n        print('', file=output_file)\n\n        if not self.no_phenoecm:\n            print(\"Data Transfers:\")\n            print(\"{:^8} |\".format(\"cache\"), end='')\n            for metrics in self.results['data transfers'].values():\n                for metric_name in sorted(metrics):\n                    print(\" {:^14}\".format(metric_name), end='')\n                print()\n                break\n            for cache, metrics in sorted(self.results['data transfers'].items()):\n                print(\"{!s:^8} |\".format(cache), end='')\n                for k, v in sorted(metrics.items()):\n                    print(\" {!s:^14}\".format(v), end='')\n                print()\n            print()\n\n            print('Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | '\n                  '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL'.format(\n                **{k: float(v) for k, v in self.results['ECM'].items()}),\n                file=output_file)\n            print('T_OL assumes that two loads per cycle may be retiered, which is true for '\n                  '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, '\n                  'BDW, SKL and SKX, but it also depends on AGU availability.',\n                  file=output_file)"}
{"prompt": "def wait_for_kernel(self, timeout=None):\n        \"\"\"method to wait for a kernel to be ready\"\"\"", "output": "\n        start_time = time.time()\n        while not self.kernel_ready:\n            if timeout is not None and time.time() - start_time > timeout:\n                raise TimeoutError(\"Kernel did not start within the specified timeout.\")\n            time.sleep(0.1)\n        return self\n\n    def execute_code(self, code, silent=False):\n        \"\"\"method to execute code in the kernel\"\"\"\n        if not silent:\n            print(f\"Executing code: {code}\")\n        self.kernel_client.execute(code)\n        return self\n\n    def get_output(self, silent=False):", "solution": "\n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n                if timeout is not None and (time.time() - tic) > timeout:\n                    return False\n        return True"}
{"prompt": "def load(self, pkcs11dll_filename=None, *init_string):\n        \"\"\"\n        load a PKCS#11 library\n\n        :type pkcs11dll_filename: string\n        :param pkcs11dll_filename: the library name.\n          If this parameter is not set then the environment variable\n          `PYKCS11LIB` is used instead\n        :returns: a :class:`PyKCS11Lib` object\n        :raises: :class:`PyKCS11Error` (-1): when the load fails\n        \"\"\"", "output": "\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.environ.get('PYKCS11LIB', None)\n            if pkcs11dll_filename is None:\n                raise PyKCS11Error(-1, \"PKCS#11 library filename not set\")\n\n        if not os.path.exists(pkcs11dll_filename):\n            raise PyKCS11Error(-1, \"PKCS#11 library file not found: %s\" % pkcs11dll_filename)\n\n        self.lib = ctypes.CDLL(", "solution": "\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.getenv(\"PYKCS11LIB\")\n            if pkcs11dll_filename is None:\n                raise PyKCS11Error(-1, \"No PKCS11 library specified (set PYKCS11LIB env variable)\")\n        rv = self.lib.Load(pkcs11dll_filename)\n        if rv == 0:\n            raise PyKCS11Error(-1, pkcs11dll_filename)"}
{"prompt": "def load_publickey(type, buffer):\n    \"\"\"\n    Load a public key from a buffer.\n\n    :param type: The file type (one of :data:`FILETYPE_PEM`,\n        :data:`FILETYPE_ASN1`).\n    :param buffer: The buffer the key is stored in.\n    :type buffer: A Python string object, either unicode or bytestring.\n    :return: The PKey object.\n    :rtype: :class:`PKey`\n    \"\"\"", "output": "\n    if type == FILETYPE_PEM:\n        return load_pem_publickey(buffer)\n    elif type == FILETYPE_ASN1:\n        return load_der_publickey(buffer)\n    else:\n        raise ValueError(\"Invalid file type: %s\" % type)\n\n\ndef load_privatekey(type, buffer, passphrase=None):\n    \"\"\"\n    Load a private key from a buffer.\n\n    :param type: The file type (one of :data:`FILETYPE_PEM`,\n        :data:`FILETYPE_ASN1`).\n    :param buffer: The buffer the key is stored in.\n   ", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        evp_pkey = _lib.PEM_read_bio_PUBKEY(\n            bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        evp_pkey = _lib.d2i_PUBKEY_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if evp_pkey == _ffi.NULL:\n        _raise_current_error()\n\n    pkey = PKey.__new__(PKey)\n    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)\n    pkey._only_public = True\n    return pkey"}
{"prompt": "def words(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"", "output": "\n        for word in self.text.split():\n            yield word\n\n    def __len__(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043b\u043e\u0432 \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"\n        return len(self.text.split())\n\n    def __str__(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0431\u043e\u0440\u0430\n        \"\"\"\n        return self.text\n\n    def __repr__(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0431\u043e\u0440\u0430\n        \"\"\"\n        return f\"Text({self.text})\"\n", "solution": "\n        branch, word, indexes = [self.root], [], [0]\n        letters_with_children = [self._get_children_and_letters(self.root)]\n        while len(branch) > 0:\n            if self.is_final(branch[-1]):\n                yield \"\".join(word)\n            while indexes[-1] == len(letters_with_children[-1]):\n                indexes.pop()\n                letters_with_children.pop()\n                branch.pop()\n                if len(indexes) == 0:\n                    raise StopIteration()\n                word.pop()\n            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n            indexes[-1] += 1\n            indexes.append(0)\n            word.append(next_letter)\n            branch.append(next_child)\n            letters_with_children.append(self._get_children_and_letters(branch[-1]))"}
{"prompt": "def parse_forensic_report(feedback_report, sample, msg_date,\n                          nameservers=None, dns_timeout=2.0,\n                          strip_attachment_payloads=False,\n                          parallel=False):\n    \"\"\"\n    Converts a DMARC forensic report and sample to a ``OrderedDict``\n\n    Args:\n        feedback_report (str): A message's feedback report as a string\n        sample (str): The RFC 822 headers or RFC 822 message sample\n        msg_date (str): The message's date header\n        nameservers (list): A list of one or more nameservers to use\n        (Cloudflare's public DNS resolvers by default)\n        dns_timeout (float): Sets the DNS timeout in seconds\n        strip_attachment_payloads (bool): Remove attachment payloads from\n        forensic report results\n        parallel (bool): Parallel processing\n\n    Returns:\n        OrderedDict: A parsed report and sample\n    \"\"\"", "output": " more nameservers to use for DNS lookups\n        dns_timeout (float): The DNS timeout in seconds\n        strip_attachment_payloads (bool): Whether to strip attachment payloads from the sample\n        parallel (bool): Whether to use parallel processing for DNS lookups\n\n    Returns:\n        OrderedDict: An ordered dictionary containing the parsed forensic report and sample\n    \"\"\"\n    import re\n    from collections import OrderedDict\n\n    def parse_header(header):\n        \"\"\"\n        Parses a header string into a dictionary\n\n        Args:\n            header (str): The header string to parse\n\n        Returns:\n            dict: A dictionary containing the header's key-value", "solution": "\n    delivery_results = [\"delivered\", \"spam\", \"policy\", \"reject\", \"other\"]\n\n    try:\n        parsed_report = OrderedDict()\n        report_values = feedback_report_regex.findall(feedback_report)\n        for report_value in report_values:\n            key = report_value[0].lower().replace(\"-\", \"_\")\n            parsed_report[key] = report_value[1]\n\n        if \"arrival_date\" not in parsed_report:\n            if msg_date is None:\n                raise InvalidForensicReport(\n                    \"Forensic sample is not a valid email\")\n            parsed_report[\"arrival_date\"] = msg_date.isoformat()\n\n        if \"version\" not in parsed_report:\n            parsed_report[\"version\"] = 1\n\n        if \"user_agent\" not in parsed_report:\n            parsed_report[\"user_agent\"] = None\n\n        if \"delivery_result\" not in parsed_report:\n            parsed_report[\"delivery_result\"] = None\n        else:\n            for delivery_result in delivery_results:\n                if delivery_result in parsed_report[\"delivery_result\"].lower():\n                    parsed_report[\"delivery_result\"] = delivery_result\n                    break\n        if parsed_report[\"delivery_result\"] not in delivery_results:\n            parsed_report[\"delivery_result\"] = \"other\"\n\n        arrival_utc = human_timestamp_to_datetime(\n            parsed_report[\"arrival_date\"], to_utc=True)\n        arrival_utc = arrival_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n        parsed_report[\"arrival_date_utc\"] = arrival_utc\n\n        ip_address = parsed_report[\"source_ip\"]\n        parsed_report_source = get_ip_address_info(ip_address,\n                                                   nameservers=nameservers,\n                                                   timeout=dns_timeout,\n                                                   parallel=parallel)\n        parsed_report[\"source\"] = parsed_report_source\n        del parsed_report[\"source_ip\"]\n\n        if \"identity_alignment\" not in parsed_report:\n            parsed_report[\"authentication_mechanisms\"] = []\n        elif parsed_report[\"identity_alignment\"] == \"none\":\n            parsed_report[\"authentication_mechanisms\"] = []\n            del parsed_report[\"identity_alignment\"]\n        else:\n            auth_mechanisms = parsed_report[\"identity_alignment\"]\n            auth_mechanisms = auth_mechanisms.split(\",\")\n            parsed_report[\"authentication_mechanisms\"] = auth_mechanisms\n            del parsed_report[\"identity_alignment\"]\n\n        if \"auth_failure\" not in parsed_report:\n            parsed_report[\"auth_failure\"] = \"dmarc\"\n        auth_failure = parsed_report[\"auth_failure\"].split(\",\")\n        parsed_report[\"auth_failure\"] = auth_failure\n\n        optional_fields = [\"original_envelope_id\", \"dkim_domain\",\n                           \"original_mail_from\", \"original_rcpt_to\"]\n        for optional_field in optional_fields:\n            if optional_field not in parsed_report:\n                parsed_report[optional_field] = None\n\n        parsed_sample = parse_email(\n            sample,\n            strip_attachment_payloads=strip_attachment_payloads)\n\n        if \"reported_domain\" not in parsed_report:\n            parsed_report[\"reported_domain\"] = parsed_sample[\"from\"][\"domain\"]\n\n        sample_headers_only = False\n        number_of_attachments = len(parsed_sample[\"attachments\"])\n        if number_of_attachments < 1 and parsed_sample[\"body\"] is None:\n            sample_headers_only = True\n        if sample_headers_only and parsed_sample[\"has_defects\"]:\n            del parsed_sample[\"defects\"]\n            del parsed_sample[\"defects_categories\"]\n            del parsed_sample[\"has_defects\"]\n        parsed_report[\"sample_headers_only\"] = sample_headers_only\n        parsed_report[\"sample\"] = sample\n        parsed_report[\"parsed_sample\"] = parsed_sample\n\n        return parsed_report\n\n    except KeyError as error:\n        raise InvalidForensicReport(\"Missing value: {0}\".format(\n            error.__str__()))\n\n    except Exception as error:\n        raise InvalidForensicReport(\n            \"Unexpected error: {0}\".format(error.__str__()))"}
{"prompt": "def map_peaks_to_image(peaks, r=4, vox_dims=(2, 2, 2), dims=(91, 109, 91),\n                       header=None):\n    \"\"\" Take a set of discrete foci (i.e., 2-D array of xyz coordinates)\n    and generate a corresponding image, convolving each focus with a\n    hard sphere of radius r.\"\"\"", "output": "\n\n    # Check that_dims are valid_dims\n   _dims =_dims_to_vox_dims(dims,_dims=vox_dims)\n   _dims =_dims_to_vox_dims(dims,_dims=vox_dims)\n   _dims =_dims_to_vox_dims(dims,_dims=vox_dims)\n\n    # Check that_dims are valid_dims\n   _dims =_dims_to_vox_dims(dims,_dims=v", "solution": "\n    data = np.zeros(dims)\n    for p in peaks:\n        valid = get_sphere(p, r, vox_dims, dims)\n        valid = valid[:, ::-1]\n        data[tuple(valid.T)] = 1\n    return nifti1.Nifti1Image(data, None, header=header)"}
{"prompt": "def iter_parse(fiql_str):\n    \"\"\"Iterate through the FIQL string. Yield a tuple containing the\n    following FIQL components for each iteration:\n\n      - preamble: Any operator or opening/closing paranthesis preceding a\n        constraint or at the very end of the FIQL string.\n      - selector: The selector portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n      - comparison: The comparison portion of a FIQL constraint or ``None``\n        if yielding the last portion of the string.\n      - argument: The argument portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n\n    For usage see :func:`parse_str_to_expression`.\n\n    Args:\n        fiql_str (string): The FIQL formatted string we want to parse.\n\n    Yields:\n        tuple: Preamble, selector, comparison, argument.\n    \"\"\"", "output": " constraint or ``None`` if\n        yielding the last portion of the string.\n      -: The portion of a FIQL constraint or ``None`` if yielding the\n        last portion of the string.\n      -: The portion of a FIQL constraint or ``None`` if yielding the\n        last portion of the string.\n      -: The entirety of the FIQL string.\n\n    Args:\n      fiql_str (str): The FIQL string to parse.\n\n    Yields:\n      tuple: A tuple containing the FIQL components for each iteration.\n    \"\"\"\n    preamble = []\n   ", "solution": "\n    while len(fiql_str):\n        constraint_match = CONSTRAINT_COMP.split(fiql_str, 1)\n        if len(constraint_match) < 2:\n            yield (constraint_match[0], None, None, None)\n            break\n        yield (\n            constraint_match[0],\n            unquote_plus(constraint_match[1]),\n            constraint_match[4],\n            unquote_plus(constraint_match[6]) \\\n                    if constraint_match[6] else None\n        )\n        fiql_str = constraint_match[8]"}
{"prompt": "def shape(self):\n        \"\"\"Returns the shape of the data.\"\"\"", "output": "\n        return self.data.shape\n\n    def(self,):\n        \"\"\"Returns the value for.\"\"\"\n        return self.data[v]\n\n    def(self, v):\n        \"\"\"Returns the value for v.\"\"\"\n        return self.data[v]\n\n    def(self, v,):\n        \"\"\"Returns the value for test v.\"\"\"\n        return self.data[v, :][:, self.test_data[m]]\n\n    def(self, v, m):\n        \"\"\"Returns the value for test v.\"\"\"\n        return self.data[v,", "solution": "\n        # TODO cache\n        first = self.first().shape\n        shape = self._rdd.map(lambda x: x.shape[0]).sum()\n        return (shape,) + first[1:]"}
{"prompt": "def _wait_job_completion(self):\n        \"\"\"Wait for the cache to be empty before resizing the pool.\"\"\"", "output": "\n        while self.cache.size() > 0:\n            time.sleep(0.1)\n\n    def _resize_pool(self):\n        \"\"\"Resize the pool if necessary.\"\"\"\n        if self.cache.size() < self.pool_size:\n            self.pool_size = self.cache.size()\n            self.logger.info(\"Resizing pool to %d jobs\", self.pool_size)\n            self.pool.resize(self.pool_size)\n\n    def _run_job(self, job):\n        \"\"\"Run a job in the pool.\"\"\"\n        self.logger.info(\"Running", "solution": "\n        # Issue a warning to the user about the bad effect of this usage.\n        if len(self._pending_work_items) > 0:\n            warnings.warn(\"Trying to resize an executor with running jobs: \"\n                          \"waiting for jobs completion before resizing.\",\n                          UserWarning)\n            mp.util.debug(\"Executor {} waiting for jobs completion before\"\n                          \" resizing\".format(self.executor_id))\n        # Wait for the completion of the jobs\n        while len(self._pending_work_items) > 0:\n            time.sleep(1e-3)"}
{"prompt": "def energy(self, state=None):\n        \"\"\"Calculates the length of the route.\"\"\"", "output": "\n        =_from_state(state)\n        =_from_state(self.state)\n       _diff = - v_from_state(state)\n       _diff_ =_diff +_from_state(self.state)\n       _diff_ =_diff_ +_from_state(self.state)\n       _diff_ =_diff_ +_from_state(self.state)\n       _diff_ =_diff_ +_from_state(self.state)\n       _diff_", "solution": "\n        state = self.state if state is None else state\n        route = state\n        e = 0\n        if self.distance_matrix:\n            for i in range(len(route)):\n                e += self.distance_matrix[\"{},{}\".format(route[i-1], route[i])]\n        else:\n            for i in range(len(route)):\n                e += distance(self.cities[route[i-1]], self.cities[route[i]])\n        return e"}
{"prompt": "def recommendations(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"", "output": "\n        url = f\"{self.api_url}/recommendations\"\n        params = {\"api_key\": self.api_key}\n        params.update(kwargs)\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n\n    def movie_details(self, movie_id, **kwargs):\n        \"\"\"\n        Get detailed information about a movie.\n\n        Args:\n            movie_id: The ID of the movie to get details for.\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum", "solution": "\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def get_clinvar_id(self, submission_id):\n        \"\"\"Returns the official Clinvar submission ID for a submission object\n\n            Args:\n                submission_id(str): submission_id(str) : id of the submission\n\n            Returns:\n                clinvar_subm_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n\n        \"\"\"", "output": "\n        clinvar_subm_id = self.clinvar_submission_id_dict.get(submission_id, None)\n        if clinvar_subm_id is None:\n            raise ValueError(f\"No Clinvar submission ID found for submission ID {submission_id}\")\n        return clinvar_subm_id\n\n    def get_clinvar_submission_id(self, clinvar_subm_id):\n        \"\"\"Returns the submission ID for a Clinvar submission ID\n\n            Args:\n                clinvar_subm_id(str): a string with a format: SUB[0-", "solution": "\n        submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n        clinvar_subm_id = submission_obj.get('clinvar_subm_id') # This key does not exist if it was not previously provided by user\n        return clinvar_subm_id"}
{"prompt": "def dict_factory(self, cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        return {k: json.dumps(v) if isinstance(v, (int, float, str, bool)) else v for k, v in zip(cursor.description, row)}\n\n    def get_data(self, query, params=None):\n        \"\"\" Get data from the database.\n\n        Args:\n            query (str): SQL query.\n            params (list, optional): List of parameters for the query. Defaults to None.\n\n        Returns:\n            list: List of dictionaries representing the data.\n        \"\"\"\n        with sqlite3.connect(self.db_file) as conn:\n            conn.row_factory", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp:\n                d[col[0]] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":  # or name == Field.Meter_Time:\n                continue\n            if name not in self.m_all_fields:\n                continue\n            if (str(val) != \"None\") and ((val > 0) or (val < 0)):\n                d[name] = str(val)\n        return d"}
{"prompt": "def streams(self, url, **params):\n        \"\"\"Attempts to find a plugin and extract streams from the *url*.\n\n        *params* are passed to :func:`Plugin.streams`.\n\n        Raises :exc:`NoPluginError` if no plugin is found.\n        \"\"\"", "output": "\n        plugin = self.get_plugin(url)\n        if plugin is None:\n            raise NoPluginError(url)\n        return plugin.streams(url, **params)\n\n    def get_plugin(self, url):\n        \"\"\"Attempts to find a plugin that can handle the *url*.\n\n        Returns the plugin instance or None if no plugin is found.\n        \"\"\"\n        for plugin in self.plugins:\n            if plugin.can_handle(url):\n                return plugin\n        return None\n\n\nclass Plugin:\n    \"\"\"Base class for plugins.\"\"\"\n\n    def __init__(self, name, can_handle):\n        self.name", "solution": "\n\n        plugin = self.resolve_url(url)\n        return plugin.streams(**params)"}
{"prompt": "def map(self, func):\n        \"\"\"\n        Process all data with given function.\n        The scheme of function should be x,y -> x,y.\n        \"\"\"", "output": "\n        for x, y in self.data:\n            x, y = func(x, y)\n            self.data.append((x, y))\n\n    def(self,_id):\n        \"\"\"\n        Get data by its id.\n        \"\"\"\n        for x, y, id in self.data:\n            if id == str(int(id)):\n                return x, y\n        return None, None\n\n    def(self,_id):\n        \"\"\"\n        Get data by its id.\n        \"\"\"\n        for x, y, id, in self.data:\n            if id == str(", "solution": "\n        if self._train_set:\n            self._train_set = map(func, self._train_set)\n        if self._valid_set:\n            self._valid_set = map(func, self._valid_set)\n        if self._test_set:\n            self._test_set = map(func, self._test_set)"}
{"prompt": "def positions_func(inputs, pad=0):\n    \"\"\"\n    A layer filling i-th column of a 2D tensor with\n    1+ln(1+i) when it contains a meaningful symbol\n    and with 0 when it contains PAD\n    \"\"\"", "output": "\n  , _ = inputs.size()\n   _padded =.clone()\n    for i in range(1, + 1):\n        if_padded[i - 1]!= pad:\n           _padded[i - 1] = 1 + math.log1p(i)\n    return_padded\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    A layer that adds positional encoding to the input tensor.\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(", "solution": "\n    position_inputs = kb.cumsum(kb.ones_like(inputs, dtype=\"float32\"), axis=1)\n    position_inputs *= kb.cast(kb.not_equal(inputs, pad), \"float32\")\n    return kb.log(1.0 + position_inputs)"}
{"prompt": "def _with_loc(f: W) -> W:\n    \"\"\"Wrap a reader function in a decorator to supply line and column\n    information along with relevant forms.\"\"\"", "output": "\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except Exception as e:\n            raise e\n    return wrapper\n\n\ndef _parse_form(form: str) -> Tuple[str, str]:\n    \"\"\"Parse a form string into its and parts.\"\"\"\n    if form.startswith(\"(\") and form.endswith(\")\"):\n        return form[1:-1].split(\" \", 1)\n    else:\n        return form, \"\"\n\n\ndef _parse_forms(forms: List[str]) ->", "solution": "\n\n    @functools.wraps(f)\n    def with_lineno_and_col(ctx):\n        meta = lmap.map(\n            {READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col}\n        )\n        v = f(ctx)\n        try:\n            return v.with_meta(meta)  # type: ignore\n        except AttributeError:\n            return v\n\n    return cast(W, with_lineno_and_col)"}
{"prompt": "def p_creg_decl(self, program):\n        \"\"\"\n           creg_decl : CREG indexed_id\n        \"\"\"", "output": "\n        program.add_creg(program.current_line.split()[1], program.current_line.split()[2])\n\n    def p_creg_assign(self, program):\n        \"\"\"\n           creg_assign : CREG ASSIGN expr\n        \"\"\"\n        program.add_creg_assign(program.current_line.split()[1], program.current_line.split()[3])\n\n    def p_creg_read(self, program):\n        \"\"\"\n           creg_read : CREG READ\n        \"\"\"\n        program.add_creg_read(program.current_line.", "solution": "\n        program[0] = node.Creg([program[2]])\n        if program[2].name in self.external_functions:\n            raise QasmError(\"CREG names cannot be reserved words. \"\n                            + \"Received '\" + program[2].name + \"'\")\n        if program[2].index == 0:\n            raise QasmError(\"CREG size must be positive\")\n        self.update_symtab(program[0])"}
{"prompt": "def order_verification(self, institute, case, user, link, variant):\n        \"\"\"Create an event for a variant verification for a variant\n        and an event for a variant verification for a case\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n\n        Returns:\n            updated_variant(dict)\n        \"\"\"", "output": "\n        # Create an event for a variant verification for a variant\n        variant_verification_event = {\n            \"event_type\": \"variant_verification\",\n            \"user_id\": user[\"_id\"],\n            \"institute_id\": institute[\"_id\"],\n            \"case_id\": case[\"_id\"],\n            \"variant_id\": variant[\"_id\"],\n            \"link\": link,\n            \"variant_type\": variant[\"variant_type\"],\n            \"variant_subtype\": variant[\"variant_subtype\"],\n            \"variant_status\": variant[\"variant_status\"],\n            \"variant_classification\": variant[\"variant_", "solution": "\n        LOG.info(\"Creating event for ordering validation for variant\" \\\n                    \" {0}\".format(variant['display_name']))\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {'$set': {'sanger_ordered': True}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        LOG.info(\"Creating event for ordering sanger for case\" \\\n                    \" {0}\".format(case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n        return updated_variant"}
{"prompt": "def update_product_set(\n        self,\n        product_set,\n        location=None,\n        product_set_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator`\n        \"\"\"", "output": "\n        return CloudVisionProductSetUpdateOperator(\n            task_id=self.task_id,\n           _dataset_id=self.dataset_id,\n           _product_set_id=product_set_id,\n           _location=location,\n           _project_id=project_id,\n           _update_mask=update_mask,\n           _retry=retry,\n           _timeout=timeout,\n           _metadata=metadata,\n           _product_set=product_set,\n           _gcp_conn_id=self.gcp_conn_id,\n           ", "solution": "\n        client = self.get_conn()\n        product_set = self.product_set_name_determiner.get_entity_with_name(\n            product_set, product_set_id, location, project_id\n        )\n        self.log.info('Updating ProductSet: %s', product_set.name)\n        response = client.update_product_set(\n            product_set=product_set, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('ProductSet updated: %s', response.name if response else '')\n        self.log.debug('ProductSet updated:\\n%s', response)\n        return MessageToDict(response)"}
{"prompt": "def get_mic(self, message, qop_req=C.GSS_C_QOP_DEFAULT):\n        \"\"\"\n        Calculates a cryptographic message integrity code (MIC) over an application message, and\n        returns that MIC in a token. This is in contrast to :meth:`wrap` which calculates a MIC\n        over a message, optionally encrypts it and returns the original message and the MIC packed\n        into a single token. The peer application can then verify the MIC to ensure the associated\n        message has not been changed in transit.\n\n        :param message: The message to calculate a MIC for\n        :type message: bytes\n        :param qop_req: The quality of protection required. It is recommended to not change this\n            from the default as most GSSAPI implementations do not support it.\n        :returns: A MIC for the message calculated using this security context's cryptographic keys\n        :rtype: bytes\n        \"\"\"", "output": ": bytes\n        :param qop_req: The quality of protection requested for the MIC (default: C.GSS_C_QOP_DEFAULT)\n        :type qop_req: int\n        :return: A token containing the original message and the MIC\n        :rtype: bytes\n        \"\"\"\n      , seq_num = self.get_seq_num()\n       _mic = self.get_mic_func(qop_req)\n        mic =_mic(message, seq_num)\n        return self.pack_mic(message, mic, seq_num)\n\n    def verify_", "solution": "\n        if not (self.flags & C.GSS_C_INTEG_FLAG):\n            raise GSSException(\"No integrity protection negotiated.\")\n        if not (self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)):\n            raise GSSException(\"Protection not yet ready.\")\n\n        minor_status = ffi.new('OM_uint32[1]')\n        output_token_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer[0].length = len(message)\n        c_str_message = ffi.new('char[]', message)\n        message_buffer[0].value = c_str_message\n        retval = C.gss_get_mic(\n            minor_status,\n            self._ctx[0],\n            ffi.cast('gss_qop_t', qop_req),\n            message_buffer,\n            output_token_buffer\n        )\n        try:\n            if GSS_ERROR(retval):\n                if minor_status[0] and self.mech_type:\n                    raise _exception_for_status(retval, minor_status[0], self.mech_type)\n                else:\n                    raise _exception_for_status(retval, minor_status[0])\n\n            output_token = _buf_to_str(output_token_buffer[0])\n            return output_token\n        finally:\n            if output_token_buffer[0].length != 0:\n                C.gss_release_buffer(minor_status, output_token_buffer)"}
{"prompt": "def extractHolidayDate(self, setting_holiday):\n        \"\"\" Read a single holiday date from meter buffer.\n\n        Args:\n            setting_holiday (int):  Holiday from 0-19 or in range(Extents.Holidays)\n\n        Returns:\n            tuple: Holiday tuple, elements are strings.\n\n            =============== ======================\n            Holiday         Holiday 0-19 as string\n            Day             Day 1-31 as string\n            Month           Monty 1-12 as string\n            =============== ======================\n\n        \"\"\"", "output": "\n        if setting_holiday < 0 or setting_holiday > 19:\n            raise ValueError(\"Invalid holiday index: {}\".format(setting_holiday))\n\n        holiday_index = setting_holiday\n        holiday_date = self.buffer[holiday_index]\n        holiday_day = holiday_date[0]\n        holiday_month = holiday_date[1]\n        holiday_year = holiday_date[2]\n\n        return (holiday_day, holiday_month, holiday_year)\n\n    def extractHolidayDates(self, start_index, end_index", "solution": "\n        ret = namedtuple(\"result\", [\"Holiday\", \"Month\", \"Day\"])\n        setting_holiday += 1\n        ret.Holiday = str(setting_holiday)\n\n        if (setting_holiday < 1) or (setting_holiday > Extents.Holidays):\n            ekm_log(\"Out of bounds:  holiday \" + str(setting_holiday))\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n\n        idxday = \"Holiday_\" + str(setting_holiday) + \"_Day\"\n        idxmon = \"Holiday_\" + str(setting_holiday) + \"_Mon\"\n        if idxmon not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        if idxday not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        ret.Day = self.m_hldy[idxday][MeterData.StringValue]\n        ret.Month = self.m_hldy[idxmon][MeterData.StringValue]\n        return ret"}
{"prompt": "def getMaxStmIdForStm(stm):\n    \"\"\"\n    Get maximum _instId from all assigments in statement\n    \"\"\"", "output": "\n    max_id = -1\n    for ass in stm.assignments:\n        if ass._instId > max_id:\n            max_id = ass._instId\n    return max_id\n\n\ndef getStmIdForStm(stm):\n    \"\"\"\n    Get unique _instId for statement\n    \"\"\"\n    max_id = getMaxStmIdForStm(stm)\n    return max_id + 1\n\n\ndef getStmIdForAss(ass):\n    \"\"\"\n    Get unique _instId for assignment\n    \"\"\"\n    max_id = getMaxStmIdForStm(ass.stm", "solution": "\n    maxId = 0\n    if isinstance(stm, Assignment):\n        return stm._instId\n    elif isinstance(stm, WaitStm):\n        return maxId\n    else:\n        for _stm in stm._iter_stms():\n            maxId = max(maxId, getMaxStmIdForStm(_stm))\n        return maxId"}
{"prompt": "def delete(self, endpoint, headers):\n        \"\"\"\n        Method to delete an item or all items\n\n        headers['If-Match'] must contain the _etag identifier of the element to delete\n\n        :param endpoint: endpoint (API URL)\n        :type endpoint: str\n        :param headers: headers (example: Content-Type)\n        :type headers: dict\n        :return: response (deletion information)\n        :rtype: dict\n        \"\"\"", "output": "\n        if 'If-Match' not in headers:\n            raise ValueError('If-Match header is missing')\n\n        response = requests.delete(endpoint, headers=headers)\n        response.raise_for_status()\n\n        return response.json()\n\n    def get(self, endpoint, headers):\n        \"\"\"\n        Method to get an item or all items\n\n        :param endpoint: endpoint (API URL)\n        :type endpoint: str\n        :param headers: headers (example: Content-Type)\n        :type headers: dict\n        :return: response (item or items)\n        :rtype: dict\n        \"\"\"\n        response = requests.", "solution": "\n        response = self.get_response(method='DELETE', endpoint=endpoint, headers=headers)\n\n        logger.debug(\"delete, response: %s\", response)\n        if response.status_code != 204:  # pragma: no cover - should not happen ...\n            resp = self.decode(response=response)\n\n        resp = {\"_status\": \"OK\"}\n        return resp"}
{"prompt": "def _make_safe_pod_id(safe_dag_id, safe_task_id, safe_uuid):\n        \"\"\"\n        Kubernetes pod names must be <= 253 chars and must pass the following regex for\n        validation\n        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n\n        :param safe_dag_id: a dag_id with only alphanumeric characters\n        :param safe_task_id: a task_id with only alphanumeric characters\n        :param random_uuid: a uuid\n        :return: ``str`` valid Pod name of appropriate length\n        \"\"\"", "output": "_id: a task_id with only alphanumeric characters\n        :param safe_uuid: a uuid with only alphanumeric characters\n        :return: a safe pod name\n        \"\"\"\n        safe_dag_id = safe_dag_id.replace(\".\", \"-\")\n        safe_task_id = safe_task_id.replace(\".\", \"-\")\n        safe_uuid = safe_uuid.replace(\"-\", \"\")\n        safe_pod_id = f\"{safe_dag_id}-{safe_task_id}-{safe_uuid}\"\n        return safe_pod_id\n\n\ndef _make_safe_service_name(safe", "solution": "\n        MAX_POD_ID_LEN = 253\n\n        safe_key = safe_dag_id + safe_task_id\n\n        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + \"-\" + safe_uuid\n\n        return safe_pod_id"}
{"prompt": "def run(self, func):\n        \"\"\"Run 'func' under os sandboxing\"\"\"", "output": "\n        if not self.enabled:\n            return func()\n\n        # Create a temporary directory for the sandbox\n        sandbox_dir = tempfile.mkdtemp()\n\n        # Set the environment variables for the sandbox\n        env = os.environ.copy()\n        env['SANDBOX_DIR'] = sandbox_dir\n        env['SANDBOX_________ dir'] = sandbox_dir\n        env['SANDBOX__ dir'] = sandbox_dir\n        env['SANDBOX___ dir_'] = sandbox_dir\n        env['SAN", "solution": "\n        try:\n            self._copy(self)\n            if _file:\n                __builtin__.file = self._file\n            __builtin__.open = self._open\n            self._active = True\n            return func()\n        finally:\n            self._active = False\n            if _file:\n                __builtin__.file = _file\n            __builtin__.open = _open\n            self._copy(_os)"}
{"prompt": "def _isint(string):\n    \"\"\"\n    >>> _isint(\"123\")\n    True\n    >>> _isint(\"123.45\")\n    False\n    \"\"\"", "output": "\n    try:\n        int(string)\n        return True\n    except ValueError:\n        return False\n\n\ndef _isfloat(string):\n    \"\"\"\n    >>> _isfloat(\"123\")\n    False\n    >>> _isfloat(\"123.45\")\n    True\n    \"\"\"\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False\n\n\ndef _isbool(string):\n    \"\"\"\n    >>> _isbool(\"True\")\n    True\n    >>> _isbool(\"False\")\n    True\n    >>> _isbool(\"true\")\n    False\n    \"\"\"\n    return string.lower() in [\"true", "solution": "\n    return type(string) is int or \\\n        (isinstance(string, _binary_type) or\n         isinstance(string, string_types)) and \\\n        _isconvertible(int, string)"}
{"prompt": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Get request payload and decode it into its\n        constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"", "output": "\n        self.kmip_version = kmip_version\n\n        # Read the message header\n        self.message_header = MessageHeader.read(input_stream, kmip_version)\n\n        # Read the request message\n        self.request_message = RequestMessage.read(input_stream, kmip_version)\n\n        # Read the response message\n        self.response_message = ResponseMessage.read(input_stream, kmip_version)\n\n    def write(self, output_stream):\n        \"\"\"\n        Write the data encoding the Get request payload and its constituent\n        parts to the specified output stream.\n\n       ", "solution": "\n        super(GetRequestPayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_FORMAT_TYPE, local_stream):\n            self._key_format_type = primitives.Enumeration(\n                enum=enums.KeyFormatType,\n                tag=enums.Tags.KEY_FORMAT_TYPE\n            )\n            self._key_format_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_COMPRESSION_TYPE, local_stream):\n            self._key_compression_type = primitives.Enumeration(\n                enum=enums.KeyCompressionType,\n                tag=enums.Tags.KEY_COMPRESSION_TYPE\n            )\n            self._key_compression_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(\n                enums.Tags.KEY_WRAPPING_SPECIFICATION,\n                local_stream\n        ):\n            self._key_wrapping_specification = \\\n                objects.KeyWrappingSpecification()\n            self._key_wrapping_specification.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)"}
{"prompt": "def get_latex_maybe_optional_arg(self, pos):\n        \"\"\"\n        Parses the latex content given to the constructor (and stored in `self.s`),\n        starting at position `pos`, to attempt to parse an optional argument.\n\n        Attempts to parse an optional argument. If this is successful, we return\n        a tuple `(node, pos, len)` if success where `node` is a\n        :py:class:`LatexGroupNode`.  Otherwise, this method returns None.\n        \"\"\"", "output": "\n        # Check if we have an optional argument\n        if self.s[pos]!= '[':\n            return None\n\n        # Parse the optional argument\n        start_pos = pos + 1\n        end_pos = self.s.find(']', start_pos)\n        if end_pos == -1:\n            return None\n\n        arg = self.s[start_pos:end_pos]\n        if arg.startswith('{'):\n            arg = arg[1:-1]\n\n        # Parse the content\n        node, pos, len = self.parse_latex(pos + len(arg) + 1)\n        if", "solution": "\n\n        tok = self.get_token(pos, brackets_are_chars=False, environments=False)\n        if (tok.tok == 'brace_open' and tok.arg == '['):\n            return self.get_latex_braced_group(pos, brace_type='[')\n\n        return None"}
{"prompt": "def _label__get(self):\n        \"\"\"\n        Get or set any <label> element associated with this element.\n        \"\"\"", "output": "\n        if self._label is None:\n            self._label = Label(self)\n        return self._label\n\n    def _label__set(self, label):\n        \"\"\"\n        Get or set any <label> element associated with this element.\n        \"\"\"\n        self._label = label\n\n    label = property(_label__get, _label__set)\n\n    def _label_text__get(self):\n        \"\"\"\n        Get or set the text content of the <label> element associated with this element.\n        \"\"\"\n        return self.label.text\n\n    def _label_text__set(self, label_text):\n        \"\"\"", "solution": "\n        id = self.get('id')\n        if not id:\n            return None\n        result = _label_xpath(self, id=id)\n        if not result:\n            return None\n        else:\n            return result[0]"}
{"prompt": "def _split_comma_separated(string):\n    \"\"\"Return a set of strings.\"\"\"", "output": "\n    return set(string.split(','))\n\n\ndef _get_all_tags(tags_string):\n    \"\"\"Return a set of all tags in a comma-separated string.\"\"\"\n    return _split_comma_separated(tags_string)\n\n\ndef _get_tags_from_file(file_path):\n    \"\"\"Return a set of tags from a file.\"\"\"\n    with open(file_path, 'r') as file:\n        tags_string = file.read()\n    return _get_all_tags(tags_string)\n\n\ndef _get_tags_from_repo(repo_", "solution": "\n    return set(text.strip() for text in string.split(',') if text.strip())"}
{"prompt": "def apply_next_patch(self, force=False, quiet=False):\n        \"\"\" Apply next patch in series file \"\"\"", "output": "\n        if not self.series_file:\n            raise ValueError(\"No series file specified\")\n\n        if not self.patch_file:\n            raise ValueError(\"No patch file specified\")\n\n        if not os.path.exists(self.series_file):\n            raise ValueError(\"Series file not found\")\n\n        if not os.path.exists(self.patch_file):\n            raise ValueError(\"Patch file not found\")\n\n        if not os.path.isfile(self.series_file):\n            raise ValueError(\"Series file is not a file\")\n\n        if not os.path.isfile(self.patch_file):\n            raise ValueError", "solution": "\n        self._check()\n        top = self.db.top_patch()\n        if not top:\n            patch = self.series.first_patch()\n        else:\n            patch = self.series.patch_after(top)\n\n        if not patch:\n            raise AllPatchesApplied(self.series, top)\n\n        self.applying(patch)\n\n        self._apply_patch(patch, force, quiet)\n\n        self.db.save()\n\n        self.applied(self.db.top_patch())"}
{"prompt": "def _data_sanity_checks(self, explore_iterable):\n        \"\"\"Checks if data values are  valid.\n\n        Checks if the data values are supported by the parameter and if the values are of the same\n        type as the default value.\n\n        \"\"\"", "output": "\n        for data in explore_iterable:\n            for value in data:\n                if value not in self.supported_values:\n                    raise ValueError(f\"Invalid value '{value}' for parameter '{self.name}'.\")\n\n                if not isinstance(value, self.default_value_type):\n                    raise ValueError(f\"Invalid type '{type(value).__name__}' for parameter '{self.name}'.\")\n\n    def _generate_data(self, explore_iterable):\n        \"\"\"Generates data for the parameter.\n\n        Generates data for the parameter by iterating over the explore iterable and appending the\n        generated data to the data", "solution": "\n        data_list = []\n\n        for val in explore_iterable:\n\n            if not self.f_supports(val):\n                raise TypeError('%s is of not supported type %s.' % (repr(val), str(type(val))))\n\n            if not self._values_of_same_type(val, self._default):\n                raise TypeError(\n                    'Data of `%s` is not of the same type as the original entry value, '\n                    'new type is %s vs old type %s.' %\n                    (self.v_full_name, str(type(val)), str(type(self._default))))\n\n            data_list.append(val)\n\n        if len(data_list) == 0:\n            raise ValueError('Cannot explore an empty list!')\n\n        return data_list"}
{"prompt": "def _cursor_position_changed(self):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"", "output": "\n        if self.tip_label is not None:\n            self.tip_label.set_text(self.get_tip())\n\n    def get_tip(self):\n        \"\"\" Returns the text to display as the tip.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_tip(self, tip):\n        \"\"\" Sets the text to display as the tip.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_cursor_position(self, position):\n        \"\"\" Sets the cursor position to the specified position.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_cursor_position(self):\n        \"\"\" Returns the current cursor position.\n        \"\"\"\n        raise", "solution": "\n        cursor = self._text_edit.textCursor()\n        if cursor.position() <= self._start_position:\n            self.hide()\n        else:\n            position, commas = self._find_parenthesis(self._start_position + 1)\n            if position != -1:\n                self.hide()"}
{"prompt": "def resources(self):\n        \"\"\"\n        Returns list of unique `FragmentResource`s by order of first appearance.\n        \"\"\"", "output": "\n        resources = []\n        for fragment in self.fragments:\n            resources.extend(fragment.resources)\n        return list(set(resources))\n\n    def __str__(self):\n        return f\"Fragment({self.name})\"\n\n\nclass FragmentResource:\n    def __init__(self, name,_type,_count,_price):\n        self.name = name\n        self.item_type = item_type\n        self.item_count = item_count\n        self.item_price = item_price\n\n    def __str__(self):\n        return f\"Resource({self.name})\"\n", "solution": "\n        seen = set()\n        # seen.add always returns None, so 'not seen.add(x)' is always True,\n        # but will only be called if the value is not already in seen (because\n        # 'and' short-circuits)\n        return [x for x in self._resources if x not in seen and not seen.add(x)]"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    def wrapper(*args, **kwargs):\n        =_from_args(args, kwargs)\n        node = f(*args, **kwargs)\n        node.production = t.production_from_node(node)\n        return node\n    return wrapper\n\ndef track_productions(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        t.productions = []\n        node = f(*args, **kwargs)\n        for p in t.productions:\n            node.productions.append(p)\n        return node\n    return wrapper\n", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def s2p(self):\n        \"\"\"Return 2 proton separation energy\"\"\"", "output": "\n       2 = self.s2\n       2 = self.convert_to_(self.s2, 'eV')\n       2p = self.convert_to_proton(self.s2, 'eV')\n        return f\"{self.s2} eV, {self.s2p} proton separation energy\"\n\n    def s3(self):\n        \"\"\"Return 3 proton separation energy\"\"\"\n        s3 = self.s3_eV\n        s3 = self.convert_to_proton(self.s3_eV, 'eV')\n        return f\"{", "solution": "\n        M_P = 7.28897050         # proton mass excess in MeV\n        f = lambda parent, daugther: -parent + daugther + 2 * M_P\n        return self.derived('s2p', (-2, 0), f)"}
{"prompt": "def clear_xcom_data(self, session=None):\n        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"", "output": "\n        if session is None:\n            session = self.get_session()\n\n        session.query(XComData).filter_by(task_id=self.task_id).delete()\n        session.commit()\n\n    def get_xcom_data(self, session=None):\n        \"\"\"\n        Returns all XCom data for the task instance\n        \"\"\"\n        if session is None:\n            session = self.get_session()\n\n        return session.query(XComData).filter_by(task_id=self.task_id).all()\n\n    def set_xcom_data(self, data,", "solution": "\n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()"}
{"prompt": "def as_property_description(self):\n        \"\"\"\n        Get the property description.\n\n        Returns a dictionary describing the property.\n        \"\"\"", "output": "\n        return {\n            'name': self.name,\n            'description': self.description,\n            'type': self.type,\n           'required': self.required,\n            'default': self.default,\n            'choices': self.choices,\n           'minimum': self.minimum,\n           'maximum': self.maximum,\n            'pattern': self.pattern,\n           'min_length': self.min_length,\n           'max_length': self.max_length,\n           'min_items': self.min_items,\n           'max_items': self.max_items,\n            'unique_items", "solution": "\n        description = deepcopy(self.metadata)\n\n        if 'links' not in description:\n            description['links'] = []\n\n        description['links'].append(\n            {\n                'rel': 'property',\n                'href': self.href_prefix + self.href,\n            }\n        )\n        return description"}
{"prompt": "def save(self, fname=None):\n        \"\"\"\n        Download the chart from the URL into a filename as a PNG\n\n        The filename defaults to the chart title (chtt) if any\n        \"\"\"", "output": "\n        if fname is None:\n            fname = self.title\n        url = self.url\n        if not url:\n            raise ValueError(\"No URL found for chart\")\n        response = requests.get(url)\n        if response.status_code!= 200:\n            raise ValueError(f\"Failed to download chart from {url}\")\n        with open(fname + \".png\", \"wb\") as f:\n            f.write(response.content)\n        print(f\"Chart saved as {fname}.png\")\n\n\nclass ChartList(list):\n    \"\"\"\n    A list of Chart objects\n    \"\"\"\n\n    def __init", "solution": "\n        if not fname:\n            fname = self.getname()\n        assert fname != None, 'You must specify a filename to save to'\n        if not fname.endswith('.png'):\n            fname += '.png'\n        try:\n            urlretrieve(self.url, fname)\n        except Exception:\n            raise IOError('Problem saving %s to file'%fname)\n        return fname"}
{"prompt": "def opt(parser: Union[Parser, Sequence[Input]]) -> OptionalParser:\n    \"\"\"Optionally match a parser.\n\n    An ``OptionalParser`` attempts to match ``parser``. If it succeeds, it\n    returns a list of length one with the value returned by the parser as the\n    only element. If it fails, it returns an empty list.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"", "output": "\n    if isinstance(parser, Parser):\n        return OptionalParser(parser)\n    else:\n        return OptionalParser(Literal(parser))\n\n\nclass OptionalParser(Parser):\n    \"\"\"Optionally match a parser.\n\n    An ``OptionalParser`` attempts to match ``parser``. If it succeeds, it\n    returns a list of length one with the value returned by the parser as the\n    only element. If it fails, it returns an empty list.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"\n\n    def __init__(self, parser: Parser):\n        self.parser = parser\n\n    def parse(self, s: str)", "solution": "\n    if isinstance(parser, str):\n        parser = lit(parser)\n    return OptionalParser(parser)"}
{"prompt": "def log(self):\n        \"\"\"\n        Print on screen and on file the percentages for each status.\n        \"\"\"", "output": "\n        =_____loss\n        =_loss_\n        =_loss_loss\n        =_loss_loss_loss\n        =_loss_loss_loss_loss\n        =_loss_loss_loss_loss_loss\n       _ =_loss_loss_loss_loss_loss_\n\n       _loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss_loss", "solution": "\n\n        if (\n            PyFunceble.CONFIGURATION[\"show_percentage\"]\n            and PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0\n        ):\n            # * We are allowed to show the percentage on screen.\n            # and\n            # * The number of tested is greater than 0.\n\n            # We initiate the output file.\n            output = (\n                PyFunceble.OUTPUT_DIRECTORY\n                + PyFunceble.OUTPUTS[\"parent_directory\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"parent\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"percentage\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"filenames\"][\"percentage\"]\n            )\n\n            # We delete the output file if it does exist.\n            File(output).delete()\n\n            # We calculate the percentage of each statuses.\n            self._calculate()\n\n            if not PyFunceble.CONFIGURATION[\"quiet\"]:\n                # The quiet mode is activated.\n\n                # We print a new line.\n                print(\"\\n\")\n\n                # We print the percentage header on file and screen.\n                Prints(None, \"Percentage\", output).header()\n\n                # We construct the different lines/data to print on screen and file.\n                lines_to_print = [\n                    [\n                        PyFunceble.STATUS[\"official\"][\"up\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"up\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"up\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"down\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"down\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"down\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"invalid\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"invalid\"])\n                        + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"invalid\"],\n                    ],\n                ]\n\n                if PyFunceble.CONFIGURATION[\"syntax\"]:\n                    # We are checking for syntax.\n\n                    # We update the denomination of the UP.\n                    lines_to_print[0][0] = PyFunceble.STATUS[\"official\"][\"valid\"]\n\n                    # And we unset the INACTIVE line.\n                    del lines_to_print[1]\n\n                for to_print in lines_to_print:\n                    # We loop throught the different line to print.\n                    # (one line for each status.)\n\n                    # And we print the current status line on file and screen.\n                    Prints(to_print, \"Percentage\", output).data()\n\n        elif PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0:\n            # * We are not allowed to show the percentage on screen.\n            # but\n            # * The number of tested is greater than 0.\n\n            # We run the calculation.\n            # Note: The following is needed, because all counter calculation are\n            # done by this class.\n            self._calculate()"}
{"prompt": "def grid(self, *args):\n        \"\"\"\n        Apply a grid to your chart\n        args are of the form::\n            <x axis step size>,\n            <y axis step size>,\n            <length of line segment>,\n            <length of blank segment>\n            <x offset>,\n            <y offset>\n        APIPARAM: chg\n        \"\"\"", "output": "\n        if len(args) == 7:\n            self._grid_x_step = args[0]\n            self._grid_y_step = args[1]\n            self._grid_line_length = args[2]\n            self._grid_blank_length = args[3]\n            self._grid_x_offset = args[4]\n            self._grid_y_offset = args[5]\n            self._grid_visible = True\n        else:\n            raise ValueError(\"Invalid number of arguments for grid()\")\n\n    def(self, *args):\n        \"\"\"\n        Apply a line to", "solution": "\n        grids =  map(str,map(float,args))\n        self['chg'] = ','.join(grids).replace('None','')\n        return self"}
{"prompt": "def delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"", "output": " to use.\n        :type mongo_db: str\n        :param kwargs: Additional keyword arguments passed to the delete_one method.\n        :type kwargs: dict\n        :return: The result of the delete operation.\n        :rtype: dict\n        \"\"\"\n        if not mongo_db:\n            mongo_db = self.mongo_db\n        return self.mongo_client[mongo_db][mongo_collection].delete_one(filter_doc, **kwargs)\n\n    def delete_many(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes multiple documents in a", "solution": "\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)"}
{"prompt": "def _check_relative_import(\n        self, modnode, importnode, importedmodnode, importedasname\n    ):\n        \"\"\"check relative import. node is either an Import or From node, modname\n        the imported module name.\n        \"\"\"", "output": "\n        if modnode.type == \"Import\":\n            if modnode.module == importedmodnode:\n                if modnode.names:\n                    for name in modnode.names:\n                        if name.name == importedasname:\n                            return True\n        elif modnode.type == \"From\":\n            if modnode.module == importedmodnode:\n                if modnode.names:\n                    for name in modnode.names:\n                        if name.name == importedasname:\n                            return True\n        return False\n\n    def _check_import_from_as(self, modnode, importnode, importedmodnode):\n        \"\"\"", "solution": "\n        if not self.linter.is_message_enabled(\"relative-import\"):\n            return None\n        if importedmodnode.file is None:\n            return False  # built-in module\n        if modnode is importedmodnode:\n            return False  # module importing itself\n        if modnode.absolute_import_activated() or getattr(importnode, \"level\", None):\n            return False\n        if importedmodnode.name != importedasname:\n            # this must be a relative import...\n            self.add_message(\n                \"relative-import\",\n                args=(importedasname, importedmodnode.name),\n                node=importnode,\n            )\n            return None\n        return None"}
{"prompt": "def download(self, bucket_name, object_name, filename=None):\n        \"\"\"\n        Get a file from Google Cloud Storage.\n\n        :param bucket_name: The bucket to fetch from.\n        :type bucket_name: str\n        :param object_name: The object to fetch.\n        :type object_name: str\n        :param filename: If set, a local file path where the file should be written to.\n        :type filename: str\n        \"\"\"", "output": "\n        if filename is None:\n            filename = object_name\n\n        with open(filename, 'wb') as f:\n            self.storage_client.get_bucket(bucket_name).get_blob(object_name).download_to_file(f)\n\n    def upload(self, bucket_name, object_name, filename):\n        \"\"\"\n        Upload a file to Google Cloud Storage.\n\n        :param bucket_name: The bucket to upload to.\n        :type bucket_name: str\n        :param object_name: The object to upload.\n        :type object_name: str\n        :param filename:", "solution": "\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()"}
{"prompt": "def parse(self, s):\n        \"\"\"Parses the input string, and returns a reference to the created AST's root\"\"\"", "output": "\n        self.lexer.input(s)\n        self.current_token = self.lexer.token()\n        self.current_line = self.lexer.lineno\n        self.current_column = self.lexer.lexpos\n\n        self.ast = self.parse_program()\n        return self.ast\n\n    def parse_program(self):\n        \"\"\"Parses the program and returns the root of the AST\"\"\"\n        program = Program()\n        self.parse_statement(program)\n        return program\n\n    def parse_statement(self, parent):\n        \"\"\"Parses a statement and adds it to the parent node\"\"\"\n        if", "solution": "\n        with self.lock:\n            try:\n                return self.parser.parse(s, lexer=self.lexer)\n            except InvalidIEMLObjectArgument as e:\n                raise CannotParse(s, str(e))\n            except CannotParse as e:\n                e.s = s\n                raise e"}
{"prompt": "def allocate_ids(self, partial_keys):\n        \"\"\"\n        Allocate IDs for incomplete keys.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list\n        \"\"\"", "output": "\n        request = {\n            'keys': partial_keys\n        }\n        response = self._service.projects().allocateIds(body=request).execute()\n        return response.get('keys', [])\n\n    def lookup(self, keys, read_options=None):\n        \"\"\"\n        Lookup entities by keys.\n\n       .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup\n\n        :param keys: a list of keys.\n        :type keys: list\n        :param read_options: optional read options.\n        :type read_options: dict\n        :return", "solution": "\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']"}
{"prompt": "def _insert_plain_text_into_buffer(self, cursor, text):\n        \"\"\" Inserts text into the input buffer using the specified cursor (which\n            must be in the input buffer), ensuring that continuation prompts are\n            inserted as necessary.\n        \"\"\"", "output": "\n        # Get the current text in the buffer\n        buffer_text = self.buffer.get(\"1.0\", END)\n\n        # Insert the text at the cursor position\n        self.buffer.insert(cursor, text)\n\n        # Get the current cursor position\n        current_cursor = self.buffer.index(INSERT)\n\n        # Find the position of the first prompt character\n        prompt_start = buffer_text.find(self.prompt_char, 0, current_cursor)\n\n        # If there are no prompt characters, return\n        if prompt_start == -1:\n            return\n\n        # Find the position of the", "solution": "\n        lines = text.splitlines(True)\n        if lines:\n            cursor.beginEditBlock()\n            cursor.insertText(lines[0])\n            for line in lines[1:]:\n                if self._continuation_prompt_html is None:\n                    cursor.insertText(self._continuation_prompt)\n                else:\n                    self._continuation_prompt = \\\n                        self._insert_html_fetching_plain_text(\n                            cursor, self._continuation_prompt_html)\n                cursor.insertText(line)\n            cursor.endEditBlock()"}
{"prompt": "def get_total_contributors(self, repo):\n        \"\"\"\n        Retrieves the number of contributors to a repo in the organization.\n        Also adds to unique contributor list.\n        \"\"\"", "output": "\n        url = f\"{self.api_url}/repos/{repo}/contributors\"\n        response = requests.get(url, headers=self.headers)\n        response.raise_for_status()\n        contributors = response.json()\n        for contributor in contributors:\n            self.unique_contributors.add(contributor[\"login\"])\n        return len(contributors)\n\n    def get_total_commits(self, repo):\n        \"\"\"\n        Retrieves the total number of commits to a repo in the organization.\n        \"\"\"\n        url = f\"{self.api_url}/repos/{repo}/commits\"\n        response = requests.get(url", "solution": "\n        repo_contributors = 0\n        for contributor in repo.iter_contributors():\n            repo_contributors += 1\n            self.unique_contributors[contributor.id].append(repo.name)\n            self.contributors_json[repo.name].append(contributor.to_json())\n        return repo_contributors"}
{"prompt": "def _unreg_event(self, event_list, event):\n        \"\"\" Tries to remove a registered event without triggering it \"\"\"", "output": "\n        try:\n            self._event_handlers[event].remove(event_list)\n        except ValueError:\n            pass\n\n    def _trigger_event(self, event, *args, **kwargs):\n        \"\"\" Triggers a registered event \"\"\"\n        for event_list in self._event_handlers.get(event, []):\n            event_list(*args, **kwargs)\n\n    def _register_event(self, event, event_list):\n        \"\"\" Registers an event \"\"\"\n        self._event_handlers[event] = event_list\n\n    def _unregister_event(self, event):\n        \"\"\" Unregisters an event", "solution": "\n        try:\n            self.log.debug(\"Removing event {0}({1},{2})\".format(event['function'].__name__, event['args'], event['kwargs']))\n        except AttributeError:\n            self.log.debug(\"Removing event {0}\".format(str(event)))\n\n        try:\n            event_list.remove(event)\n        except ValueError:\n            try:\n                self.log.warn(\"Unable to remove event {0}({1},{2}) , not found in list: {3}\".format(event['function'].__name__, event['args'], event['kwargs'], event_list))\n            except AttributeError:\n                self.log.debug(\"Unable to remove event {0}\".format(str(event)))\n            raise KeyError('Unable to unregister the specified event from the signals specified')"}
{"prompt": "def get_system_per_cpu_times():\n    \"\"\"Return system per-CPU times as a list of named tuples.\"\"\"", "output": "\n    times = []\n    for cpu in range(psutil.cpu_count()):\n        times.append(psutil.cpu_times_percent(percpu=True, interval=0))\n    return times\n\n\ndef get_system_memory_info():\n    \"\"\"Return system memory information as a named tuple.\"\"\"\n    return psutil.virtual_memory()\n\n\ndef get_system_disk_usage():\n    \"\"\"Return system disk usage information as a named tuple.\"\"\"\n    return psutil.disk_usage('/')\n\n\ndef get_system_network_io():\n    \"\"\"Return system network I/O information as a", "solution": "\n    ret = []\n    for cpu_t in _psutil_mswindows.get_system_cpu_times():\n        user, system, idle = cpu_t\n        item = _cputimes_ntuple(user, system, idle)\n        ret.append(item)\n    return ret"}
{"prompt": "def queue_startfile(self, cmdfile):\n        \"\"\"Arrange for file of debugger commands to get read in the\n        process-command loop.\"\"\"", "output": "\n        self.cmdfile = cmdfile\n        self.cmdqueue = []\n        self.cmd_running = False\n        self.cmd_thread = threading.Thread(target=self.run_cmd_queue)\n        self.cmd_thread.start()\n\n    def run_cmd_queue(self):\n        \"\"\"Run the command queue in a separate thread.\"\"\"\n        while True:\n            if self.cmd_running:\n                time.sleep(0.1)\n            else:\n                self.cmd_running = True\n                try:\n                    with open(self.cmdfile, 'r') as f:\n                        for line in", "solution": "\n        expanded_cmdfile = os.path.expanduser(cmdfile)\n        is_readable = Mfile.readable(expanded_cmdfile)\n        if is_readable:\n            self.cmd_queue.append('source ' + expanded_cmdfile)\n        elif is_readable is None:\n            self.errmsg(\"source file '%s' doesn't exist\" % expanded_cmdfile)\n        else:\n            self.errmsg(\"source file '%s' is not readable\" %\n                        expanded_cmdfile)\n            pass\n        return"}
{"prompt": "def read_infile(infile: Union[Path, str], from_words=False,\n                word_column: int = WORD_COLUMN, pos_column: int = POS_COLUMN,\n                tag_column: int = TAG_COLUMN, max_sents: int = -1,\n                read_only_words: bool = False) -> List[Tuple[List, Union[List, None]]]:\n    \"\"\"Reads input file in CONLL-U format\n\n    Args:\n        infile: a path to a file\n        word_column: column containing words (default=1)\n        pos_column: column containing part-of-speech labels (default=3)\n        tag_column: column containing fine-grained tags (default=5)\n        max_sents: maximal number of sents to read\n        read_only_words: whether to read only words\n\n    Returns:\n        a list of sentences. Each item contains a word sequence and a tag sequence, which is ``None``\n        in case ``read_only_words = True``\n    \"\"\"", "output": " part-of-speech tags (default=2)\n        tag_column: column containing tags (default=3)\n        max_sents: maximum number of sentences to read (default=-1, read all)\n        read_only_words: if True, only read words (not tags)\n\n    Returns:\n        A list of tuples, where each tuple contains a list of words and a list of tags (or None if read_only_words=True)\n    \"\"\"\n    with open(infile, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n\n    if max_sents", "solution": "\n    answer, curr_word_sent, curr_tag_sent = [], [], []\n    if from_words:\n        word_column, read_only_words = 0, True\n    with open(infile, \"r\", encoding=\"utf8\") as fin:\n        for line in fin:\n            line = line.strip()\n            if line.startswith(\"#\"):\n                continue\n            if line == \"\":\n                if len(curr_word_sent) > 0:\n                    if read_only_words:\n                        curr_tag_sent = None\n                    answer.append((curr_word_sent, curr_tag_sent))\n                curr_tag_sent, curr_word_sent = [], []\n                if len(answer) == max_sents:\n                    break\n                continue\n            splitted = line.split(\"\\t\")\n            index = splitted[0]\n            if not from_words and not index.isdigit():\n                continue\n            curr_word_sent.append(splitted[word_column])\n            if not read_only_words:\n                pos, tag = splitted[pos_column], splitted[tag_column]\n                tag = pos if tag == \"_\" else \"{},{}\".format(pos, tag)\n                curr_tag_sent.append(tag)\n        if len(curr_word_sent) > 0:\n            if read_only_words:\n                curr_tag_sent = None\n            answer.append((curr_word_sent, curr_tag_sent))\n    return answer"}
{"prompt": "def fuzzed(self):\n        \"\"\"\n        Get a printable fuzzed object\n        \"\"\"", "output": "\n        return self.__dict__\n\n    def __str__(self):\n        \"\"\"\n        Get a printable string representation of the object\n        \"\"\"\n        return str(self.__dict__)\n\n    def __repr__(self):\n        \"\"\"\n        Get a printable string representation of the object\n        \"\"\"\n        return str(self.__dict__)\n\n    def __eq__(self, other):\n        \"\"\"\n        Compare two objects for equality\n        \"\"\"\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \"\"\"\n        Compare two objects for inequality\n        \"\"\"\n        return not self.__eq__(other)\n\n    def __hash__(", "solution": "\n        try:\n            if self.config.strong_fuzz:\n                fuzzer = PJFMutators(self.config)\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                    else:\n                        return urllib.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                else:\n                    if type(self.config.json) in [list, dict]:\n                        return fuzzer.fuzz(json.dumps(self.config.json))\n                    else:\n                        return fuzzer.fuzz(self.config.json)\n            else:\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                    else:\n                        return urllib.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                else:\n                    return self.get_fuzzed(self.config.indent, self.config.utf8)\n        except Exception as e:\n            raise PJFBaseException(e.message if hasattr(e, \"message\") else str(e))"}
{"prompt": "def validate(self, request):\n        \"\"\"Validate JSON-RPC request.\n\n        :param request: RPC request object\n        :type request: dict\n\n        \"\"\"", "output": "\n        if not isinstance(request, dict):\n            raise ValueError('Request must be a dictionary')\n\n        if 'jsonrpc' not in request:\n            raise ValueError('Request must have a \"jsonrpc\" key')\n\n        if request['jsonrpc']!= '2.0':\n            raise ValueError('Request must have a \"jsonrpc\" value of \"2.0\"')\n\n        if'method' not in request:\n            raise ValueError('Request must have a \"method\" key')\n\n        if 'params' not in request:\n            raise ValueError('Request must have a \"params\" key')\n\n        if not isinstance(request['method'], str):\n           ", "solution": "\n\n        try:\n            validate_version(request)\n            validate_method(request)\n            validate_params(request)\n            validate_id(request)\n        except (AssertionError, KeyError) as error:\n            invalid_request(error)"}
{"prompt": "def action_notify(self, action):\n        \"\"\"\n        Notify all subscribers of an action status change.\n\n        action -- the action whose status changed\n        \"\"\"", "output": "\n        for subscriber in self.subscribers:\n            subscriber.action_status_changed(action)\n\n    def add_subscriber(self, subscriber):\n        \"\"\"\n        Add a subscriber to the list of subscribers.\n\n        subscriber -- the subscriber to add\n        \"\"\"\n        self.subscribers.append(subscriber)\n\n    def remove_subscriber(self, subscriber):\n        \"\"\"\n        Remove a subscriber from the list of subscribers.\n\n        subscriber -- the subscriber to remove\n        \"\"\"\n        self.subscribers.remove(subscriber)\n", "solution": "\n        message = json.dumps({\n            'messageType': 'actionStatus',\n            'data': action.as_action_description(),\n        })\n\n        for subscriber in list(self.subscribers):\n            try:\n                subscriber.write_message(message)\n            except tornado.websocket.WebSocketClosedError:\n                pass"}
{"prompt": "def update_mongo_compound_variants(self, bulk):\n        \"\"\"Update the compound information for a bulk of variants in the database\n\n            Args:\n                bulk(dict): {'_id': scout.models.Variant}\n\n        \"\"\"", "output": "\n        for variant_id, variant in bulk.items():\n            self.update_mongo_compound_variant(variant_id, variant, bulk)\n\n    def update_mongo_compound_variant(self, variant_id, variant, bulk):\n        \"\"\"Update the compound information for a single variant in the database\n\n            Args:\n                variant_id(str): The variant id\n                variant(dict): The variant object\n                bulk(dict): {'_id': scout.models.Variant}\n\n        \"\"\"\n       _variant = self.mongo_variant(variant_id)\n       _variant['compound_variants'] =", "solution": "\n        requests = []\n        for var_id in bulk:\n            var_obj = bulk[var_id]\n            if not var_obj.get('compounds'):\n                continue\n            # Add a request to update compounds\n            operation = pymongo.UpdateOne(\n                {'_id': var_obj['_id']},\n                {\n                    '$set': {\n                        'compounds': var_obj['compounds']\n                    }\n                })\n            requests.append(operation)\n\n        if not requests:\n            return\n\n        try:\n            self.variant_collection.bulk_write(requests, ordered=False)\n        except BulkWriteError as err:\n            LOG.warning(\"Updating compounds failed\")\n            raise err"}
{"prompt": "def module_list(path):\n    \"\"\"\n    Return the list containing the names of the modules available in the given\n    folder.\n    \"\"\"", "output": "\n    modules = []\n    for file in os.listdir(path):\n        if file.endswith('.py') and not file.startswith('_'):\n            module_name = file[:-3]\n            modules.append(module_name)\n    return modules\n\n\ndef load_module(module_name):\n    \"\"\"\n    Load the module with the given name.\n    \"\"\"\n    module_path = os.path.join(os.path.dirname(__file__),'modules', module_name + '.py')\n    module = __import__(module_name, fromlist=['*'])\n    return module\n\n\ndef load_modules(", "solution": "\n    # sys.path has the cwd as an empty string, but isdir/listdir need it as '.'\n    if path == '':\n        path = '.'\n\n    if os.path.isdir(path):\n        folder_list = os.listdir(path)\n    elif path.endswith('.egg'):\n        try:\n            folder_list = [f for f in zipimporter(path)._files]\n        except:\n            folder_list = []\n    else:\n        folder_list = []\n\n    if not folder_list:\n        return []\n\n    # A few local constants to be used in loops below\n    isfile = os.path.isfile\n    pjoin = os.path.join\n    basename = os.path.basename\n\n    def is_importable_file(path):\n        "}
{"prompt": "def modify_conf():\n    \"\"\"\n    pip install redbaron\n    \"\"\"", "output": "\n    import redbaron\n\n    with open(\"conf.py\", \"r\") as f:\n        conf = f.read()\n\n    # Find_config and_config_dict\n   _config_pattern = re.compile(r\"^m_config\\s*=\\s*dict\\(\\s*(.*?)\\s*\\)\", re.MULTILINE)\n    m_config_dict_pattern = re.compile(r\"^m_config_dict\\s*=\\s*dict\\(\\s*(.*?)\\s*\\)\", re.MULTILINE)\n   _config_pattern =", "solution": "\n    import redbaron\n    import ubelt as ub\n    conf_path = 'docs/conf.py'\n\n    source = ub.readfrom(conf_path)\n    red = redbaron.RedBaron(source)\n\n    # Insert custom extensions\n    extra_extensions = [\n        '\"sphinxcontrib.napoleon\"'\n    ]\n\n    ext_node = red.find('name', value='extensions').parent\n    ext_node.value.value.extend(extra_extensions)\n\n    # Overwrite theme to read-the-docs\n    theme_node = red.find('name', value='html_theme').parent\n    theme_node.value.value = '\"sphinx_rtd_theme\"'\n\n    ub.writeto(conf_path, red.dumps())"}
{"prompt": "def show(self, *args, **kwargs):\n        \"\"\"\n        Shows the chart URL in a webbrowser\n\n        Other arguments passed to webbrowser.open\n        \"\"\"", "output": "\n        webbrowser.open(self.url, *args, **kwargs)\n\n    def save(self, filename, *args, **kwargs):\n        \"\"\"\n        Saves the chart to a file\n\n        Other arguments passed to open()\n        \"\"\"\n        with open(filename, 'wb') as f:\n            f.write(self.content)\n        webbrowser.open(filename, *args, **kwargs)\n\n    def __str__(self):\n        return self.url\n\n\nclass Chart(object):\n    \"\"\"\n    Base class for chart types\n    \"\"\"\n    def __init__(self, data,_axis=None,", "solution": "\n        from webbrowser import open as webopen\n        return webopen(str(self), *args, **kwargs)"}
{"prompt": "def getView(self,name):\n        \"\"\"\n        Returns the view with name ``name``\\ .\n        \n        Raises a :py:exc:`ValueError` if the view does not exist.\n        \"\"\"", "output": "\n        if name not in self._views:\n            raise ValueError(\"View '%s' does not exist.\" % name)\n        return self._views[name]\n    \n    def addView(self,view):\n        \"\"\"\n        Adds a view to the manager.\n        \n        :param view: The view to add.\n        \"\"\"\n        self._views[view.name] = view\n    \n    def removeView(self,name):\n        \"\"\"\n        Removes a view from the manager.\n        \n        :param name: The name of the view to remove.\n        \"\"\"\n        if name in self._views:\n            del self._views[name]\n    \n    def clearViews", "solution": "\n        if name not in self.views:\n            raise ValueError(\"Unknown world view\")\n        return self.views[name]"}
{"prompt": "def _process_execute_error(self, msg):\n        \"\"\" Process a reply for an execution request that resulted in an error.\n        \"\"\"", "output": "\n        if 'error' in msg:\n            self.error_message = msg['error']\n            self.error_code = msg['error_code']\n            self.error_data = msg['error_data']\n            self.error_traceback = msg['error_traceback']\n            self.status = 'error'\n        else:\n            self.error_message = 'Unknown error'\n            self.error_code = 'unknown_error'\n            self.error_data = None\n            self.error_traceback = None\n            self.status = 'error'\n\n    def _process_execute_reply(self, msg", "solution": "\n        content = msg['content']\n        # If a SystemExit is passed along, this means exit() was called - also\n        # all the ipython %exit magic syntax of '-k' to be used to keep\n        # the kernel running\n        if content['ename']=='SystemExit':\n            keepkernel = content['evalue']=='-k' or content['evalue']=='True'\n            self._keep_kernel_on_exit = keepkernel\n            self.exit_requested.emit(self)\n        else:\n            traceback = ''.join(content['traceback'])\n            self._append_plain_text(traceback)"}
{"prompt": "def s_add(self, path, function, method=None, type_cast=None):\n        \"\"\"Function for registering a simple path.\n\n        Args:\n            path (str): Path to be matched.\n            function (function): Function to associate with this path.\n            method (str, optional): Usually used to define one of GET, POST,\n                PUT, DELETE. You may use whatever fits your situation though.\n                Defaults to None.\n            type_cast (dict, optional): Mapping between the param name and\n                one of `int`, `float` or `bool`. The value reflected by the\n                provided param name will than be casted to the given type.\n                Defaults to None.\n        \"\"\"", "output": " will be converted to the corresponding\n                type before passing it to the function. Defaults to None.\n        \"\"\"\n        self.paths[path] = {\n            'function': function,\n           'method': method,\n            'type_cast': type_cast\n        }\n\n    def s_match(self, path, method, params):\n        \"\"\"Function for matching a path and method with a function.\n\n        Args:\n            path (str): Path to be matched.\n            method (str): HTTP method to match.\n            params (dict): Dictionary of parameters to match.\n\n        Returns:\n            function: Function associated with the matched path and method", "solution": "\n        with self._lock:\n            try:\n                path = '^/{}'.format(path.lstrip('/'))\n                path = '{}/$'.format(path.rstrip('/'))\n                path = path.replace('<', '(?P<')\n                path = path.replace('>', '>[^/]*)')\n\n                self.add(path, function, method, type_cast)\n            except Exception:\n                pass"}
{"prompt": "def login(request, template_name='ci/login.html',\n          redirect_field_name=REDIRECT_FIELD_NAME,\n          authentication_form=AuthenticationForm):\n    \"\"\"\n    Displays the login form and handles the login action.\n    \"\"\"", "output": "\n    if request.user.is_authenticated:\n        return redirect('ci:dashboard')\n\n    form = authentication_form(request)\n    if request.method == 'POST':\n        if form.is_valid():\n            user = form.get_user()\n            login(request, user)\n            return redirect('ci:dashboard')\n        else:\n            messages.error(request, 'Invalid username or password.')\n\n    context = {\n        'form': form,\n       'redirect_field_name': redirect_field_name,\n    }\n\n    return render(request, template_name, context)\n\n\ndef logout(request", "solution": "\n    redirect_to = request.POST.get(redirect_field_name,\n                                   request.GET.get(redirect_field_name, ''))\n\n    if request.method == \"POST\":\n        form = authentication_form(request, data=request.POST)\n        if form.is_valid():\n\n            # Ensure the user-originating redirection url is safe.\n            if not is_safe_url(url=redirect_to, host=request.get_host()):\n                redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\n            # Okay, security check complete. Get the user object from auth api.\n            user = form.get_user()\n            request.session['user_token'] = user[\"token\"]\n            request.session['user_email'] = user[\"email\"]\n            request.session['user_permissions'] = user[\"permissions\"]\n            request.session['user_id'] = user[\"id\"]\n            request.session['user_list'] = user[\"user_list\"]\n\n            if not settings.HIDE_DASHBOARDS:\n                # Set user dashboards because they are slow to change\n                dashboards = ciApi.get_user_dashboards(user[\"id\"])\n                dashboard_list = list(dashboards['results'])\n                if len(dashboard_list) > 0:\n                    request.session['user_dashboards'] = \\\n                        dashboard_list[0][\"dashboards\"]\n                    request.session['user_default_dashboard'] = \\\n                        dashboard_list[0][\"default_dashboard\"][\"id\"]\n                else:\n                    request.session['user_dashboards'] = []\n                    request.session['user_default_dashboard'] = None\n\n            # Get the user access tokens too and format for easy access\n            tokens = ciApi.get_user_service_tokens(\n                params={\"user_id\": user[\"id\"]})\n            token_list = list(tokens['results'])\n            user_tokens = {}\n            if len(token_list) > 0:\n                for token in token_list:\n                    user_tokens[token[\"service\"][\"name\"]] = {\n                        \"token\": token[\"token\"],\n                        \"url\": token[\"service\"][\"url\"] + \"/api/v1\"\n                    }\n            request.session['user_tokens'] = user_tokens\n\n            return HttpResponseRedirect(redirect_to)\n    else:\n        form = authentication_form(request)\n\n    current_site = get_current_site(request)\n\n    context = {\n        'form': form,\n        redirect_field_name: redirect_to,\n        'site': current_site,\n        'site_name': current_site.name,\n    }\n\n    return TemplateResponse(request, template_name, context)"}
{"prompt": "def get_io_write_task(self, fileobj, data, offset):\n        \"\"\"Get an IO write task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to write to\n\n        :type data: bytes\n        :param data: The data to write out\n\n        :type offset: integer\n        :param offset: The offset to write the data to in the file-like object\n\n        :returns: An IO task to be used to write data to a file-like object\n        \"\"\"", "output": " to the file-like object\n        \"\"\"\n        return self.io_executor.submit(self._io_write_task, fileobj, data, offset)\n\n    def _io_write_task(self, fileobj, data, offset):\n        \"\"\"Perform the actual IO write task\n\n        This method is called by the IO executor to perform the actual write\n        operation.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to write to\n\n        :type data: bytes\n        :param data: The data to write out\n\n        :type offset: integer\n        :param offset: The", "solution": "\n        return IOWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n                'offset': offset,\n            }\n        )"}
{"prompt": "def __notify_listeners(self, data, stats):\n        \"\"\" notify all listeners about aggregate data and stats \"\"\"", "output": "\n        for listener in self.listeners:\n            listener(data, stats)\n\n    def add_listener(self, listener):\n        \"\"\" add a listener to the list of listeners \"\"\"\n        self.listeners.append(listener)\n\n    def remove_listener(self, listener):\n        \"\"\" remove a listener from the list of listeners \"\"\"\n        self.listeners.remove(listener)\n\n    def start(self):\n        \"\"\" start the loop \"\"\"\n        self.running = True\n        while self.running:\n            try:\n                data, stats = self.get_data()\n                self.__notify_listeners(data, stats)\n            except Exception", "solution": "\n        for listener in self.listeners:\n            listener.on_aggregated_data(data, stats)"}
{"prompt": "def utf8(unicode_str):\n    \"\"\"\n    Return a utf-8 encoded string from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"", "output": "\n    return unicode_str.encode('utf-8')\n\n\ndef is_valid_email(email):\n    \"\"\"\n    Check if a given email is valid.\n\n    :param email: str\n\n    :return: bool\n    \"\"\"\n    try:\n        email_address = email_validator.validate_email(email)\n        return True\n    except email_validator.EmailNotValidError:\n        return False\n\n\ndef is_valid_url(url):\n    \"\"\"\n    Check if a given URL is valid.\n\n    :param url: str\n\n    :return: bool\n    \"\"\"\n    try:\n        url_validator.validate_url", "solution": "\n    if six.PY2 and isinstance(unicode_str, __unicode__):\n        return unicode_str.encode('utf-8')\n\n    return unicode_str"}
{"prompt": "def csoftmax_for_slice(input):\n    \"\"\" It is a implementation of the constrained softmax (csoftmax) for slice.\n        Based on the paper:\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf \"Learning What's Easy: Fully Differentiable Neural Easy-First Taggers\" (page 4)\n    Args:\n        input: A list of [input tensor, cumulative attention].\n    Returns:\n        output: A list of [csoftmax results, masks]\n    \"\"\"", "output": "\n    input_tensor, cumulative_attention = input\n   _tensor = input_tensor\n   _tensor =_tensor.unsqueeze(0)\n   _tensor =_tensor.expand(input_tensor.size(0), -1, -1, -1)\n   _tensor =_tensor.contiguous().view(-1, input_tensor.size(1), input_tensor.size(2))\n   _tensor =_tensor.transpose(0, 1)\n   _tensor =_tensor.contiguous().view(-1, input_tensor.size(1", "solution": "\n\n    [ten, u] = input\n\n    shape_t = ten.shape\n    shape_u = u.shape\n\n    ten -= tf.reduce_mean(ten)\n    q = tf.exp(ten)\n    active = tf.ones_like(u, dtype=tf.int32)\n    mass = tf.constant(0, dtype=tf.float32)\n    found = tf.constant(True, dtype=tf.bool)\n\n    def loop(q_, mask, mass_, found_):\n        q_list = tf.dynamic_partition(q_, mask, 2)\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(q_)[0]), mask, 2)  # 0 element it False,\n        #  1 element if true\n\n        p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])\n        p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])\n\n        # condition verification and mask modification\n        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,\n                                                 2)  # 0 when u is bigger than p, 1 when u is less than p\n\n        split_p_new = tf.dynamic_partition(p_new, less_mask, 2)\n        split_u = tf.dynamic_partition(u, less_mask, 2)\n\n        alpha = tf.dynamic_stitch(condition_indices, [split_p_new[0], split_u[1]])\n        mass_ += tf.reduce_sum(split_u[1])\n\n        mask = mask * (tf.ones_like(less_mask) - less_mask)\n\n        found_ = tf.cond(tf.equal(tf.reduce_sum(less_mask), 0),\n                         lambda: False,\n                         lambda: True)\n\n        alpha = tf.reshape(alpha, q_.shape)\n\n        return alpha, mask, mass_, found_\n\n    (csoft, mask_, _, _) = tf.while_loop(cond=lambda _0, _1, _2, f: f,\n                                         body=loop,\n                                         loop_vars=(q, active, mass, found))\n\n    return [csoft, mask_]"}
{"prompt": "def lazy(function, *args, **kwargs):\n    \"\"\" Produces a callable so that functions can be lazily evaluated in\n    templates.\n\n    Arguments:\n\n        function (callable): The function to call at evaluation time.\n\n        args: Positional arguments, passed directly to ``function``.\n\n        kwargs: Keyword arguments, passed directly to ``function``.\n\n    Return:\n\n        callable: A callable that will evaluate a call to ``function`` with\n            the specified arguments.\n\n    \"\"\"", "output": "\n    def wrapper():\n        return function(*args, **kwargs)\n    return wrapper\n\ndef render_template(template_name, context=None, request=None):\n    \"\"\" Renders a template with the given context.\n\n    Arguments:\n\n        template_name (str): The name of the template to render.\n\n        context (dict): A dictionary of context variables to pass to the\n            template.\n\n        request (HttpRequest): An optional request object to pass to the\n            template.\n\n    Returns:\n\n        str: The rendered template.\n\n    \"\"\"\n    if context is None:\n        context = {}\n    if request is None:\n        request =", "solution": "\n\n    NOT_EVALUATED = object()\n    retval = [NOT_EVALUATED]\n\n    def evaluate():\n        if retval[0] is NOT_EVALUATED:\n            retval[0] = function(*args, **kwargs)\n        return retval[0]\n\n    return evaluate"}
{"prompt": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"", "output": "\n  if_utils.is_singleton(from_structure):\n    new_from_structure =_utils.tile_structure(to_structure, from_structure)\n  else:\n    new_from_structure = from_structure\n  return new_from_structure\n\n\ndef broadcast_structures(from_structures: List[Any], to_structure: Any) -> List[Any]:\n  \"\"\"Broadcasts `from_structures` to `to_structure`.\n\n  If any `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the", "solution": "\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)"}
{"prompt": "def main():\n    \"\"\"Initialize and run command line interface.\"\"\"", "output": "\n    parser = argparse.ArgumentParser(\n        description=\"Generateubmeids fromids andubmeids.\"\n    )\n    parser.add_argument(\n        \"vids\",\n        nargs=\"+\",\n        help=\"One or moreids to generate vubme vids from.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        default=\"vubme_vids.txt\",\n        help=\"Output file for generated vubme vids.\",\n    )\n    args = parser.parse_args()\n\n    vubme_vids = generate_vubme_", "solution": "\n    # Create and populate parser\n    parser = create_parser()\n\n    # Parse given arguments\n    args = parser.parse_args()\n\n    # Checking arguments\n    check_arguments(args, parser)\n\n    # BUSINESS LOGIC IS FOLLOWING\n    run(parser, args)"}
{"prompt": "def from_tuplelist(tuple_list):\n        \"\"\"\n        Populates a Layout from a list containing virtual\n        qubits---(QuantumRegister, int) tuples---, or None.\n\n        Args:\n            tuple_list (list):\n                e.g.: [qr[0], None, qr[2], qr[3]]\n        Returns:\n            Layout: the corresponding Layout object\n        Raises:\n            LayoutError: If the elements are not (Register, integer) or None\n        \"\"\"", "output": "\n        if not all(isinstance(t, tuple) and len(t) == 2 for t in tuple_list):\n            raise LayoutError(\"Each element in the list must be a tuple of (Register, integer) or None\")\n\n       _qubits = []\n        for t in tuple_list:\n            if t[1] is None:\n                qubit = None\n            else:\n                qubit = t[0].qubits[t[1]]\n            if qubit is not None:\n                if qubit in self.qubits:\n                    raise LayoutError(\"Duplicate qubits found in the list\")\n               ", "solution": "\n        out = Layout()\n        for physical, virtual in enumerate(tuple_list):\n            if virtual is None:\n                continue\n            elif Layout.is_virtual(virtual):\n                if virtual in out._v2p:\n                    raise LayoutError('Duplicate values not permitted; Layout is bijective.')\n                out[virtual] = physical\n            else:\n                raise LayoutError(\"The list should contain elements of the form\"\n                                  \" (Register, integer) or None\")\n        return out"}
{"prompt": "def reject_connection(self, reason=None):\n        \"\"\"\n        Reject a pending connection.\n        \"\"\"", "output": "\n        if self.state ==punch.ConnectionState.PENDING:\n            self.state = reason\n            self.reject_reason = reason\n            self.connection_event.set()\n\n    def accept_connection(self, connection):\n        \"\"\"\n        Accept a pending connection.\n        \"\"\"\n        if self.state == ConnectionState.PENDING:\n            self.state = ConnectionState.CONNECTED\n            self.connection = connection\n            self.connection_event.set()\n\n    def __str__(self):\n        return f\"ConnectionState: {self.state}, Connection: {self.connection}, Reject Reason: {self.reject_reason", "solution": "\n        assert self.pending, \"Connection is not pending.\"\n        if reason is None:\n            reason = ConnectionRefusedError()\n        self._accept_d.errback(reason)"}
{"prompt": "def signature(frame):\n    \"\"\"return suitable frame signature to key display expressions off of.\"\"\"", "output": "\n    if_is_active():\n        return_signature(frame)\n    else:\n        return_signature(frame)\n\n\ndef_signature(frame):\n    \"\"\"return suitable frame signature to key display expressions off of.\"\"\"\n    if_is_active():\n        return_signature(frame)\n    else:\n        return_signature(frame)\n\n\ndef_signature(frame):\n    \"\"\"return suitable frame signature to key display expressions off of.\"\"\"\n    if_is_active():\n        return test_signature(frame)\n    else:\n        return test_signature(frame)\n\n\ndef", "solution": "\n    if not frame: return None\n    code = frame.f_code\n    return (code.co_name, code.co_filename, code.co_firstlineno)"}
{"prompt": "async def parse_tag_results(soup):\n    \"\"\"\n    Parse a page of tag or trait results. Same format.\n\n    :param soup: BS4 Class Object\n    :return: A list of tags, Nothing else really useful there\n    \"\"\"", "output": "\n    tags = []\n    for tag in soup.find_all('div', class_='tag-item'):\n        tag_name = tag.find('span', class_='tag-name').text\n        tag_count = tag.find('span', class_='tag-count').text\n        tags.append({'name': tag_name, 'count': tag_count})\n    return tags\n\n\nasync def parse_trait_results(soup):\n    \"\"\"\n    Parse a page of trait results. Same format.\n\n    :param soup: BS4 Class Object\n    :return: A list of traits, Nothing else really", "solution": "\n    soup = soup.find_all('td', class_='tc3')\n    tags = []\n    for item in soup:\n        tags.append(item.a.string)\n    return tags"}
{"prompt": "def mean(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the mean for expression, possibly on a grid defined by binby.\n\n        Example:\n\n        >>> df.mean(\"x\")\n        -0.067131491264005971\n        >>> df.mean(\"(x**2+y**2)**0.5\", binby=\"E\", shape=4)\n        array([  2.43483742,   4.41840721,   8.26742458,  15.53846476])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}\n        \"\"\"", "output": "   2.43483742,   2.43483742,   2.43483742])\n\n        Parameters\n        ----------\n        expression : str\n            The expression to calculate the mean for.\n        binby : list, optional\n            The columns to bin the data by.\n        limits : list, optional\n            The limits for the binning.\n        shape : tuple, optional\n            The shape of the grid.\n        selection : bool, optional\n            Whether to select the data before calculating the mean.\n        delay : bool, optional\n            Whether to delay the calculation of the", "solution": "\n        return self._compute_agg('mean', expression, binby, limits, shape, selection, delay, edges, progress)\n        logger.debug(\"mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r\", expression, binby, limits, shape, selection, delay)\n        expression = _ensure_strings_from_expressions(expression)\n        selection = _ensure_strings_from_expressions(selection)\n        binby = _ensure_strings_from_expressions(binby)\n\n        @delayed\n        def calculate(expression, limits):\n            task = tasks.TaskStatistic(self, binby, shape, limits, weight=expression, op=tasks.OP_ADD_WEIGHT_MOMENTS_01, selection=selection)\n            self.executor.schedule(task)\n            progressbar.add_task(task, \"mean for %s\" % expression)\n            return task\n\n        @delayed\n        def finish(*stats_args):\n            stats = np.array(stats_args)\n            counts = stats[..., 0]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                mean = stats[..., 1] / counts\n            return vaex.utils.unlistify(waslist, mean)\n        waslist, [expressions, ] = vaex.utils.listify(expression)\n        progressbar = vaex.utils.progressbars(progress)\n        limits = self.limits(binby, limits, delay=True)\n        stats = [calculate(expression, limits) for expression in expressions]\n        var = finish(*stats)\n        return self._delay(delay, var)"}
{"prompt": "def build_fake_data(num_examples=10):\n  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"", "output": "\n  data = []\n  for i in range(num_examples):\n    img = np.random.randint(0, 256, size=(28, 28))\n    label = np.random.randint(0, 10)\n    data.append((img, label))\n  return data\n\ndef test_______model():\n  \"\"\"Test_model function.\"\"\"\n  data = build_fake_data()\n _model =_model_fn(__model_params)\n _model =_", "solution": "\n\n  class Dummy(object):\n    pass\n\n  num_examples = 10\n  mnist_data = Dummy()\n  mnist_data.train = Dummy()\n  mnist_data.train.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.train.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.train.num_examples = num_examples\n  mnist_data.validation = Dummy()\n  mnist_data.validation.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.validation.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.validation.num_examples = num_examples\n  return mnist_data"}
{"prompt": "def min(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the minimum for given expressions, possibly on a grid defined by binby.\n\n\n        Example:\n\n        >>> df.min(\"x\")\n        array(-128.293991)\n        >>> df.min([\"x\", \"y\"])\n        array([-128.293991 ,  -71.5523682])\n        >>> df.min(\"x\", binby=\"x\", shape=5, limits=[-10, 10])\n        array([-9.99919128, -5.99972439, -1.99991322,  2.0000093 ,  6.0004878 ])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}, the last dimension is of shape (2)\n        \"\"\"", "output": "y\")\n        array([[-128.293991, -128.293991],\n               [-128.293991, -128.293991],\n               [-128.293991, -128.293991],\n              ...,\n               [-128.293991, -128.293991],\n               [-128.293991, -128.2939", "solution": "\n        return self._compute_agg('min', expression, binby, limits, shape, selection, delay, edges, progress)\n        @delayed\n        def finish(result):\n            return result[..., 0]\n        return self._delay(delay, finish(self.minmax(expression, binby=binby, limits=limits, shape=shape, selection=selection, delay=delay, progress=progress)))"}
{"prompt": "def _create_value(self, *args, **kwargs):\n        \"\"\"\n        Lowest value generator.\n\n        Separated from __call__, because it seems that python\n        cache __call__ reference on module import\n        \"\"\"", "output": "\n        return self.generator(*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Generate a new value for the.\n\n        Args:\n            *args: Positional arguments passed to the generator function.\n            **kwargs: Keyword arguments passed to the generator function.\n\n        Returns:\n            The generated value.\n        \"\"\"\n        return self._create_value(*args, **kwargs)\n\n\nclass RandomValueGenerator(ValueGenerator):\n    \"\"\"\n    A value generator that generates random values.\n\n    Args:\n        generator (function): A function that takes positional and keyword arguments and returns a value.", "solution": "\n        if not len(args):\n            raise TypeError('Object instance is not provided')\n\n        if self.by_instance:\n            field_type = args[0]\n        else:\n            field_type = args[0].__class__\n\n        function = self.registry.get(field_type, self.default)\n\n        if function is None:\n            raise TypeError(\"no match %s\" % field_type)\n\n        return function(*args, **kwargs)"}
{"prompt": "def join_lines(iterator):\n    \"\"\"\n    Joins a line ending in '\\' with the previous line.\n    \"\"\"", "output": "\n    lines = []\n    for line in iterator:\n        if line.endswith('\\\\'):\n            if lines:\n                lines[-1] += line[:-1]\n            else:\n                lines.append(line[:-1])\n        else:\n            lines.append(line)\n    return lines\n\n\ndef split_lines(lines):\n    \"\"\"\n    Splits a list of lines into a list of lines with line endings.\n    \"\"\"\n    return join_lines(iter(lines))\n\n\ndef read_file(filename):\n    \"\"\"\n    Reads a file and returns its contents as a list of lines.\n    \"\"\"\n    with open(filename", "solution": "\n    lines = []\n    for line in iterator:\n        if not line.endswith('\\\\'):\n            if lines:\n                lines.append(line)\n                yield ''.join(lines)\n                lines = []\n            else:\n                yield line\n        else:\n            lines.append(line.strip('\\\\'))"}
{"prompt": "def handle(self):\n        \"\"\"\n        Handle the backend of the given status.\n        \"\"\"", "output": "\n        if self.status ==_pb2.Status.RUNNING:\n            self.status = self.backend.handle(self.request)\n        elif self.status ==_pb2.Status.SUCCEEDED:\n            self.backend.handle(self.request)\n            self.status = self.backend.status\n        elif self.status ==_pb2.Status.FAILED:\n            self.backend.handle(self.request)\n            self.status = self.backend.status\n        else:\n            raise ValueError(\"Invalid status: {}\".format(self.status))\n\n    def __str__(self):\n        return", "solution": "\n\n        # We initiate the source we are going to parse to the Generate class.\n        source = \"URL\"\n\n        if self.catched.lower() not in PyFunceble.STATUS[\"list\"][\"invalid\"]:\n            # The parsed status is not in the list of invalid.\n\n            # We generate the status file with the catched status.\n            Generate(self.catched, source).status_file()\n        else:\n            # The parsed status is in the list of invalid.\n\n            # We generate the status file with the parsed status.\n            Generate(self.catched, \"SYNTAX\").status_file()\n\n        # We return the parsed status.\n        return self.catched"}
{"prompt": "def _segmentPath(self, path):\n        \"\"\"Return list of valid *path* segments.\"\"\"", "output": "\n        segments = []\n        for segment in path.split('/'):\n            if segment:\n                segments.append(segment)\n        return segments\n\n    def _get_segment_value(self, segment):\n        \"\"\"Return value of segment, or None if segment is not defined.\"\"\"\n        if segment in self._segment_values:\n            return self._segment_values[segment]\n        return None\n\n    def _set_segment_value(self, segment, value):\n        \"\"\"Set value of segment.\"\"\"\n        self._segment_values[segment] = value\n\n    def _get_segment_type(self, segment):\n        \"\"\"Return type", "solution": "\n        parts = []\n        model = self._filesystemWidget.model()\n\n        # Separate root path from remainder.\n        remainder = path\n\n        while True:\n            if remainder == model.root.path:\n                break\n\n            if remainder:\n                parts.append(remainder)\n\n            head, tail = os.path.split(remainder)\n            if head == remainder:\n                break\n\n            remainder = head\n\n        parts.append(model.root.path)\n        return parts"}
{"prompt": "def p_gate_op_5(self, program):\n        \"\"\"\n        gate_op : BARRIER id_list ';'\n        \"\"\"", "output": "\n        program.append(('barrier', program.pop(), program.pop()))\n\n    def p_gate_op_6(self, program):\n        \"\"\"\n        gate_op :_gate_op_6\n        \"\"\"\n        program.append(program.pop())\n\n    def p_gate_op_7(self, program):\n        \"\"\"\n        gate_op :_gate_op_7\n        \"\"\"\n        program.append(program.pop())\n\n    def p_gate_op_8(self, program):\n        \"\"\"\n        gate_op :_gate_op_8\n        \"\"\"\n        program.append(", "solution": "\n        program[0] = node.Barrier([program[2]])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def luhn_check(card_number):\n    \"\"\" checks to make sure that the card passes a luhn mod-10 checksum \"\"\"", "output": "\n    def double_every_other(num):\n        return sum(int(digit) * 2 for digit in str(num))\n\n    def sum_digits(num):\n        return sum(int(digit) for digit in str(num))\n\n    def luhn_algorithm(num):\n        return (num // 10) * 9 + double_every_other(num) % 10\n\n    return luhn_algorithm(sum_digits(card_number)) == 0\n\ndef is_valid_card(card_number):\n    \"\"\" checks if the card number is valid according to Lu", "solution": "\n    sum = 0\n    num_digits = len(card_number)\n    oddeven = num_digits & 1\n\n    for count in range(0, num_digits):\n        digit = int(card_number[count])\n\n        if not ((count & 1) ^ oddeven):\n            digit *= 2\n        if digit > 9:\n            digit -= 9\n\n        sum += digit\n\n    return (sum % 10) == 0"}
{"prompt": "def _make_empty_queue_for(k, element):\n  \"\"\"Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.\n\n  For example:\n\n  ```python\n    element = tf.constant([[0., 1., 2., 3., 4.],\n                           [5., 6., 7., 8., 9.]])\n\n    # A queue capable of holding 3 elements.\n    _make_empty_queue_for(3, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: A positive scalar integer, number of elements that each queue will hold.\n    element: A `tf.Tensor`, only its shape and dtype information are relevant.\n\n  Returns:\n    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype\n    as `element`.\n  \"\"\"", "output": " [ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #    [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #    [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  ", "solution": "\n  queue_shape = tf.concat(\n      [[k], distribution_util.prefer_static_shape(element)], axis=0)\n  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)"}
{"prompt": "def join(self):\n        \"\"\"Waits until all there are no processes enqueued.\"\"\"", "output": "\n        while self.queue.qsize() > 0:\n            time.sleep(0.1)\n\n    def run(self):\n        \"\"\"Runs the.\"\"\"\n        self.join()\n       _name = self.name\n       _queue = self.queue\n       _result = self.result\n       _func = self.func\n\n        def worker(task):\n            try:\n                result = task()\n                with lock:\n                    if result is not None:\n                        if_result is None:\n                            raise ValueError(\"Result already set\")\n                        else:\n                            raise ValueError(\"Result already set\")\n                    else:\n                        if", "solution": "\n        while True:\n            with self._processes_lock:\n                if not self._processes and self._processing_semaphore.locked():\n                    return True\n            gevent.sleep(0.1)"}
{"prompt": "def phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype='l',**kwargs):\n    \"\"\"\n    Calculates phase difference between X and Y feeds given U and V (U and Q for circular basis)\n    data from a noise diode measurement on the target\n    \"\"\"", "output": "\n   __ =_ut_v_ut_v(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype,**kwargs)\n   _ut_v_ut_v__ = ut_v_ut_v_ ut_v_(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype,**kwargs)\n   _ut_v_ut_v_ ut_v_ =_ut_v_ut_v_ ut_v_(Id", "solution": "\n    #Fold noise diode data and calculate ON OFF diferences for U and V\n    if feedtype=='l':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        V_OFF,V_ON = foldcal(Vdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Vdiff = V_ON-V_OFF\n        poffset = np.arctan2(-1*Vdiff,Udiff)\n\n    if feedtype=='c':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        Q_OFF,Q_ON = foldcal(Qdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Qdiff = Q_ON-Q_OFF\n        poffset = np.arctan2(Udiff,Qdiff)\n\n    coarse_p =  convert_to_coarse(poffset,chan_per_coarse)\n\n    #Correct for problems created by discontinuity in arctan\n    #Find whether phase offsets have increasing or decreasing slope\n    y = coarse_p[:6]\n    x = np.arange(y.size)\n    m = np.polyfit(x,y,1)[0]\n\n    for i in range(coarse_p.size-3):\n        if (m>0 and coarse_p[i+1]<coarse_p[i]) or (m<0 and coarse_p[i+1]>coarse_p[i]):\n            coarse_p[i+1] = 2*coarse_p[i+2]-coarse_p[i+3]    #Move problem point near the next\n\n    return coarse_p"}
{"prompt": "def filter_on(self, filt=None, analyte=None, samples=None, subset=None, show_status=False):\n        \"\"\"\n        Turns data filters on for particular analytes and samples.\n\n        Parameters\n        ----------\n        filt : optional, str or array_like\n            Name, partial name or list of names of filters. Supports\n            partial matching. i.e. if 'cluster' is specified, all\n            filters with 'cluster' in the name are activated.\n            Defaults to all filters.\n        analyte : optional, str or array_like\n            Name or list of names of analytes. Defaults to all analytes.\n        samples : optional, array_like or None\n            Which samples to apply this filter to. If None, applies to all\n            samples.\n\n        Returns\n        -------\n        None\n        \"\"\"", "output": " to all analytes.\n        samples : optional, str or array_like\n            Name or list of names of samples. Defaults to all samples.\n        subset : optional, str or array_like\n            Name or list of names of or samples. Defaults to all samples.\n        show_status : optional, bool\n            Whether to show the status of the filters being turned on.\n            Defaults to False.\n\n        Returns\n        -------\n        : None\n            This method does not return anything.\n\n        \"\"\"\n        if filt is None:\n            filt = self.filters\n        if analyte is None:\n            analyte = self.analytes", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            try:\n                self.data[s].filt.on(analyte, filt)\n            except:\n                warnings.warn(\"filt.on failure in sample \" + s)\n\n        if show_status:\n            self.filter_status(subset=subset)\n        return"}
{"prompt": "def _add_https(self, q):\n        \"\"\"for push, pull, and other api interactions, the user can optionally\n           define a custom registry. If the registry name doesn't include http\n           or https, add it.\n \n           Parameters\n           ==========\n           q: the parsed image query (names), including the original\n        \"\"\"", "output": "\n        if not q.registry:\n            return q\n\n        if not q.registry.startswith(('http://', 'https://')):\n            q.registry = 'https://' + q.registry\n\n        return q\n\n    def _add_tag(self, q):\n        \"\"\"for pull and other api interactions, the user can optionally\n           define a custom tag. If the tag is not defined, use the latest\n           tag.\n \n           Parameters\n           ==========\n           q: the parsed image query (names), including the original\n        \"\"\"\n        if not q.tag:\n            q.tag = 'latest'\n\n        return q\n\n    def _add_", "solution": "\n\n        # If image uses http or https, add back\n        if not q['registry'].startswith('http'):\n\n            if q['original'].startswith('http:'):\n                q['registry'] = 'http://%s' % q['registry']\n\n            elif q['original'].startswith('https:'):\n                q['registry'] = 'https://%s' % q['registry']\n\n            # Otherwise, guess from the user's environment\n            else:\n\n                prefix = 'https://'\n\n                # The user can set an environment variable to specify nohttps\n                nohttps = os.environ.get('SREGISTRY_REGISTRY_NOHTTPS')\n                if nohttps != None:\n                    prefix = 'http://'\n                q['registry'] = '%s%s' %(prefix, q['registry'])\n\n        return q"}
{"prompt": "def clean_tempfiles():\n  \"\"\"Clean up temp files\"\"\"", "output": "\n  for f in glob.glob(\"*.tmp\"):\n    os.remove(f)\n\ndef main():\n  \"\"\"Main function\"\"\"\n  parser = argparse.ArgumentParser(description=\"Convertcf files totf\")\n  parser.add_argument(\"input_file\", help=\"Input vcf file\")\n  parser.add_argument(\"output_file\", help=\"Output vtf file\")\n  args = parser.parse_args()\n\n  # Clean up temp files\n  clean_tempfiles()\n\n  # Convert vcf to vcf.tmp\n  subprocess.run([\"vcftools\", \"vcf2vcf\", \"-i\",", "solution": "\n  for fn in TEMP_FILES:\n    if os.path.exists(fn):\n      os.unlink(fn)"}
{"prompt": "def get_section_by_sis_id(self, sis_section_id, params={}):\n        \"\"\"\n        Return section resource for given sis id.\n        \"\"\"", "output": "\n        return self.get_resource(f\"sections/{sis_section_id}\", params=params)\n\n    def get_sections_by_course_id(self, course_id, params={}):\n        \"\"\"\n        Return sections for given course id.\n        \"\"\"\n        return self.get_resource(f\"courses/{course_id}/sections\", params=params)\n\n    def get_section_enrollments(self, sis_section_id, params={}):\n        \"\"\"\n        Return enrollments for given section id.\n        \"\"\"\n        return self.get_resource(f\"sections/{sis_section_", "solution": "\n        return self.get_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)"}
{"prompt": "async def get_tracks(self, *, limit=20, offset=0) -> List[Track]:\n        \"\"\"Get a list of the songs saved in the current Spotify user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optional[int]\n            The index of the first item to return. Default: 0\n        \"\"\"", "output": "\n        if limit < 1 or limit > 50:\n            raise ValueError(\"Limit must be between 1 and 50\")\n\n        response = await self._get(f\"/me/tracks?limit={limit}&offset={offset}\")\n        return [Track(self, track) for track in response[\"items\"]]\n\n    async def get_saved_tracks(self) -> List[Track]:\n        \"\"\"Get a list of the songs saved in the current Spotify user\u2019s \u2018Your Music\u2019 library.\n\n        Returns\n        -------\n        List[Track]\n            A list of the songs saved in the current Spotify user\u2019s \u2018", "solution": "\n        data = await self.user.http.saved_tracks(limit=limit, offset=offset)\n\n        return [Track(self.__client, item['track']) for item in data['items']]"}
{"prompt": "def get_direct(self):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be read.\n\n        :return: A guard object that returns the buffer element.\n        \"\"\"", "output": "\n        return self._buffer.get_direct()\n\n    def read(self, n: int) -> bytes:\n        \"\"\"\n        Reads up to n bytes from the buffer.\n        Blocks until there is data that can be read.\n\n        :param n: The maximum number of bytes to read.\n        :return: The bytes read.\n        \"\"\"\n        return self._buffer.read(n)\n\n    def read_exactly(self, n: int) -> bytes:\n        \"\"\"\n        Reads exactly n bytes from the buffer.\n        Blocks until there is data that can be read.\n\n        :param n: The number of bytes to read", "solution": "\n\n        read_idx = self.__get_idx()\n\n        if read_idx is QueueClosed:\n            return QueueClosed\n\n        # Once the guard is released, read_idx will be placed into write_queue.\n        return self.Guard(self.write_queue, self.arys, lambda: read_idx)"}
{"prompt": "def _get_format_from_style(self, token, style):\n        \"\"\" Returns a QTextCharFormat for token by reading a Pygments style.\n        \"\"\"", "output": "\n        if style is None:\n            return None\n        style_dict = style.to_dict()\n        if 'color' in style_dict:\n            color = style_dict['color']\n            if color.startswith('#'):\n                color = color[1:]\n            return QTextCharFormat(QColor(color))\n        elif 'bg_color' in style_dict:\n            bg_color = style_dict['bg_color']\n            if bg_color.startswith('#'):\n                bg_color = bg_color[1:]\n            return QTextCharFormat(QColor(bg_color))\n        else:\n           ", "solution": "\n        result = QtGui.QTextCharFormat()\n        for key, value in style.style_for_token(token).items():\n            if value:\n                if key == 'color':\n                    result.setForeground(self._get_brush(value))\n                elif key == 'bgcolor':\n                    result.setBackground(self._get_brush(value))\n                elif key == 'bold':\n                    result.setFontWeight(QtGui.QFont.Bold)\n                elif key == 'italic':\n                    result.setFontItalic(True)\n                elif key == 'underline':\n                    result.setUnderlineStyle(\n                        QtGui.QTextCharFormat.SingleUnderline)\n                elif key == 'sans':\n                    result.setFontStyleHint(QtGui.QFont.SansSerif)\n                elif key == 'roman':\n                    result.setFontStyleHint(QtGui.QFont.Times)\n                elif key == 'mono':\n                    result.setFontStyleHint(QtGui.QFont.TypeWriter)\n        return result"}
{"prompt": "def p_closed_proposition_list(self, p):\n        \"\"\" closed_proposition_list :  closed_proposition_list SLASH SLASH closed_proposition\n                                    | closed_proposition\"\"\"", "output": "\n        if len(p) == 4:\n            p[0] = [p[1], p[3]]\n        else:\n            p[0] = [p[1]]\n\n    def p_closed_proposition(self, p):\n        \"\"\" closed_proposition :_proposition\n                             | test test_proposition\"\"\"\n        if len(p) == 6:\n            p[0] = [p[1], p[3], p[5]]\n        else:\n            p[0] = [p[1],", "solution": "\n        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[0] = p[1] + [p[4]]"}
{"prompt": "def docker(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"", "output": "\n        future = self.executor.submit(method, *args, **kwargs)\n        return future\n\n    def run_docker(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in the current thread\n\n        returns the result of the method\n        \"\"\"\n        return method(*args, **kwargs)\n\n    def run_docker_async(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"\n        future = self.executor.submit(method, *args, **kwargs)\n        return future\n\n    def run_docker_sync(self,", "solution": "\n        return self.executor.submit(self._docker, method, *args, **kwargs)"}
{"prompt": "def restore_context(self) -> bool:\n        \"\"\"Rollback to previous saved position.\"\"\"", "output": "\n        if not self._saved_positions:\n            return False\n        self._position = self._saved_positions.pop()\n        return True\n\n    def save_context(self) -> None:\n        \"\"\"Save current position for later restoration.\"\"\"\n        self._saved_positions.append(self._position)\n\n    def get_position(self) -> int:\n        \"\"\"Return current position.\"\"\"\n        return self._position\n\n    def set_position(self, position: int) -> None:\n        \"\"\"Set current position.\"\"\"\n        self._position = position\n", "solution": "\n        self._cursor.position = self._contexts.pop()\n        return False"}
{"prompt": "def wave_interp_option(obj):\n    r\"\"\"\n    Validate if an object is a :ref:`WaveInterpOption` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by the name of the argument the\n     contract is attached to\n\n    :rtype: None\n    \"\"\"", "output": "\n    if not isinstance(obj,_types.WaveInterpOption):\n        raise RuntimeError(f\"Argument `{obj}` is not valid. Expected a \"\n                           f\":ref:`WaveInterpOption` pseudo-type object.\")\n\n\ndef___types_option(obj):\n    r\"\"\"\n    Validate if an object is a :ref:`MaxTypesOption` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by", "solution": "\n    exdesc = pexdoc.pcontracts.get_exdesc()\n    if not isinstance(obj, str):\n        raise ValueError(exdesc)\n    if obj.upper() in [\"CONTINUOUS\", \"STAIRCASE\"]:\n        return None\n    raise ValueError(exdesc)"}
{"prompt": "def run(self, dag):\n        \"\"\"\n        Pick a layout by assigning n circuit qubits to device qubits 0, .., n-1.\n\n        Args:\n            dag (DAGCircuit): DAG to find layout for.\n\n        Raises:\n            TranspilerError: if dag wider than self.coupling_map\n        \"\"\"", "output": "\n       _qubits = dag.qubits\n       _clbits = dag.clbits\n       _qubit_map = dag.qubit_map\n       _clbit_map = dag.clbit_map\n\n       _qubits.sort()\n       _clbits.sort()\n\n       _qubit_map = {q: i for i, q in enumerate(dag.qubits)}\n       _clbit_map = {c: i for i, c in enumerate(dag.clbits)}\n\n       _layout = []\n       _mapping = {}\n\n        for i,", "solution": "\n        num_dag_qubits = sum([qreg.size for qreg in dag.qregs.values()])\n        if num_dag_qubits > self.coupling_map.size():\n            raise TranspilerError('Number of qubits greater than device.')\n        self.property_set['layout'] = Layout.generate_trivial_layout(*dag.qregs.values())"}
{"prompt": "def update(self,dt):\n        \"\"\"\n        Should be called regularly to move the actor.\n        \n        This method does nothing if the :py:attr:`enabled` property is set to False.\n        \n        This method is called automatically and should not be called manually.\n        \"\"\"", "output": "\n        if not self.enabled:\n            return\n        \n        # Update-specific\n        self.update_game(dt)\n        \n        # Update-specific\n      .update(dt)\n        \n        # Update-specific\n      .update_game(dt)\n        \n        # Update-specific\n      .update_game_state(dt)\n        \n        # Update-specific\n      .update_game_state_game(dt)\n        \n        # Update-specific\n      .update_game_state_game_state(dt)\n        \n        # Update-specific\n      .update_game", "solution": "\n        if not self.enabled:\n            return\n        dy = self.speed * dt * self.move\n        x,y,z = self.actor._pos\n        newpos = x,dy+y,z\n        self.actor.pos = newpos"}
{"prompt": "def _start_webserver(self, authorize_url=None):\n\t\t\"\"\"\n\t\tStart the webserver that will receive the code\n\t\t\"\"\"", "output": "\n\t\tif not authorize_url:\n\t\t\tauthorize_url = self.authorize_url\n\n\t\tif not self.server:\n\t\t\tself.server = SimpleHTTPServer.SimpleHTTPRequestHandler\n\t\t\tself.server.protocol_version = \"HTTP/1.0\"\n\t\t\tself.server.server_version = \"Python/2.7.3\"\n\t\t\tself.server.extensions_map = {}\n\t\t\tself.server.extensions_map.update(self.extensions_map)\n\t\t\tself.server.extensions_map.update({\n\t\t\t\t\".py\": \"text/plain\",\n\t\t\t", "solution": "\n\t\tserver_address = (SERVER_URL, SERVER_PORT)\n\t\tself.server = HTTPServer(server_address, OAuth2UtilRequestHandler)\n\t\tself.server.response_code = None\n\t\tself.server.authorize_url = authorize_url\n\t\tt = Thread(target=self.server.serve_forever)\n\t\tt.daemon = True\n\t\tt.start()"}
{"prompt": "def transcode_to_stream(input_filename, date_format=None):\n    \"\"\"\n    Read a JSON or CSV file and convert it into a JSON stream, which will\n    be saved in an anonymous temp file.\n    \"\"\"", "output": "\n    if date_format:\n        import datetime\n        date_format = datetime.datetime.strptime(date_format, '%Y-%m-%d %H:%M:%S')\n\n    if input_filename.endswith('.json'):\n        with open(input_filename, 'r') as f:\n            data = json.load(f)\n    elif input_filename.endswith('.csv'):\n        import csv\n        data = []\n        with open(input_filename, 'r') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                data.append(row)\n    else:\n       ", "solution": "\n    tmp = tempfile.TemporaryFile()\n    for entry in open_json_or_csv_somehow(input_filename,\n                                          date_format=date_format):\n        tmp.write(json.dumps(entry, ensure_ascii=False).encode('utf-8'))\n        tmp.write(b'\\n')\n    tmp.seek(0)\n    return tmp"}
{"prompt": "def visit_classdef(self, node):\n        \"\"\"visit an astroid.Class node\n\n         * set the locals_type and instance_attrs_type mappings\n         * set the implements list and build it\n         * optionally tag the node with a unique id\n        \"\"\"", "output": "\n        # set the locals_type and instance_attrs_type mappings\n        self.locals_type = self.instance_attrs_type = None\n        self.visit_locals(node)\n        self.visit_instance_attrs(node)\n\n        # set the implements list and build it\n        self.implements = []\n        for base in node.bases:\n            if isinstance(base, astroid.Class):\n                self.implements.append(base.name)\n            elif isinstance(base, astroid.Instance):\n                self.implements.append(base.name)\n            else:\n                raise ValueError(\"Unsupported base class type", "solution": "\n        if hasattr(node, \"locals_type\"):\n            return\n        node.locals_type = collections.defaultdict(list)\n        if self.tag:\n            node.uid = self.generate_id()\n        # resolve ancestors\n        for baseobj in node.ancestors(recurs=False):\n            specializations = getattr(baseobj, \"specializations\", [])\n            specializations.append(node)\n            baseobj.specializations = specializations\n        # resolve instance attributes\n        node.instance_attrs_type = collections.defaultdict(list)\n        for assignattrs in node.instance_attrs.values():\n            for assignattr in assignattrs:\n                self.handle_assignattr_type(assignattr, node)\n        # resolve implemented interface\n        try:\n            node.implements = list(interfaces(node, self.inherited_interfaces))\n        except astroid.InferenceError:\n            node.implements = ()"}
{"prompt": "def _create_input_transactions(self, addy):\n        # type: (Address) -> None\n        \"\"\"\n        Creates transactions for the specified input address.\n        \"\"\"", "output": "\n       _amount = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "solution": "\n        self._transactions.append(ProposedTransaction(\n            address=addy,\n            tag=self.tag,\n\n            # Spend the entire address balance; if necessary, we will\n            # add a change transaction to the bundle.\n            value=-addy.balance,\n        ))\n\n        # Signatures require additional transactions to store, due to\n        # transaction length limit.\n        # Subtract 1 to account for the transaction we just added.\n        for _ in range(addy.security_level - 1):\n            self._transactions.append(ProposedTransaction(\n                address=addy,\n                tag=self.tag,\n\n                # Note zero value; this is a meta transaction.\n                value=0,\n            ))"}
{"prompt": "def _define(self):\n        \"\"\"Calculate a subcircuit that implements this initialization\n\n        Implements a recursive initialization algorithm, including optimizations,\n        from \"Synthesis of Quantum Logic Circuits\" Shende, Bullock, Markov\n        https://arxiv.org/abs/quant-ph/0406176v5\n\n        Additionally implements some extra optimizations: remove zero rotations and\n        double cnots.\n        \"\"\"", "output": "\n       _circuit = self.circuit\n       _circuit.remove_zero_rotations()\n       _circuit.remove_double_cnots()\n\n       _circuit.add_barrier()\n       _circuit.add_barrier()\n\n       _circuit.add_barrier()\n       _circuit.add_barrier()\n\n       _circuit.add_barrier()\n       _circuit.add_barrier()\n\n       _circuit.add_barrier()\n       _circuit.add_barrier()\n\n       _circuit.add_barrier()\n       _circuit.add_barrier()\n\n       _circuit.", "solution": "\n        # call to generate the circuit that takes the desired vector to zero\n        disentangling_circuit = self.gates_to_uncompute()\n\n        # invert the circuit to create the desired vector from zero (assuming\n        # the qubits are in the zero state)\n        initialize_instr = disentangling_circuit.to_instruction().inverse()\n\n        q = QuantumRegister(self.num_qubits, 'q')\n        initialize_circuit = QuantumCircuit(q, name='init_def')\n        for qubit in q:\n            initialize_circuit.append(Reset(), [qubit])\n        initialize_circuit.append(initialize_instr, q[:])\n\n        self.definition = initialize_circuit.data"}
{"prompt": "def generate_query(command):\n    \"\"\"Add header, checksum and footer to command data.\"\"\"", "output": "\n    header = b\"QSTP\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00", "solution": "\n    data = bytearray(command)\n    c = checksum(data)\n    data.append(c >> 8)\n    data.append(c & 0xFF)\n    data.replace(b'\\xFE', b'\\xFE\\xF0')\n\n    data = bytearray.fromhex(\"FEFE\") + data + bytearray.fromhex(\"FE0D\")\n    return data"}
{"prompt": "def copy(self):\n        \"\"\"\n        :returns: datasketch.MinHash -- A copy of this MinHash by exporting its state.\n        \"\"\"", "output": "\n        return MinHash(self.export())\n\n    def export(self):\n        \"\"\"\n        :returns: dict -- A dictionary representation of the MinHash state.\n        \"\"\"\n        return {\n            'hash_size': self.hash_size,\n            'num_perm': self.num_perm,\n            'num_buckets': self.num_buckets,\n            '_': self.num_,\n            '_sum': self.num_sum,\n            'hash_table': self.hash_table\n        }\n\n    def import_state(self, state):\n        \"\"\"\n        :param state: dict", "solution": "\n        return MinHash(seed=self.seed, hashfunc=self.hashfunc,\n                hashvalues=self.digest(),\n                permutations=self.permutations)"}
{"prompt": "def publish(\n            self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Publish runbook draft.\n\n        :param resource_group_name: Name of an Azure Resource group.\n        :type resource_group_name: str\n        :param automation_account_name: The name of the automation account.\n        :type automation_account_name: str\n        :param runbook_name: The parameters supplied to the publish runbook\n         operation.\n        :type runbook_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n        \"\"\"", "output": " str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns either Runbook\n         :class:`<Runbook>` or ClientRawResponse if raw=true\n        :rtype: ~msrestazure.azure_operation.LROPoller[~azure.mgmt.automation.models.Runbook]\n         or ~msrest.pipeline.Client", "solution": "\n        raw_result = self._publish_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'location': 'str',\n                })\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _is_async(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async keyword.\"\"\"", "output": "\n    return o.get(\"async\", False)\n\n\ndef _is_coroutine(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async and :coroutine keywords.\"\"\"\n    return _is_async(o) and o.get(\"coroutine\", False)\n\n\ndef _is_sync(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :sync keyword.\"\"\"\n    return o.get(\"sync\", False)\n\n\ndef _is_sync_or_async(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :sync or", "solution": "\n    return (  # type: ignore\n        Maybe(o.meta)\n        .map(lambda m: m.entry(SYM_ASYNC_META_KEY, None))\n        .or_else_get(False)\n    )"}
{"prompt": "def add_value(self, name, value):\n        \"\"\"\n        Add a new value to the list.\n\n        :param str name: name of the value that is being parsed\n        :param str value: value that is being parsed\n        :raises ietfparse.errors.MalformedLinkValue:\n            if *strict mode* is enabled and a validation error\n            is detected\n\n        This method implements most of the validation mentioned in\n        sections 5.3 and 5.4 of :rfc:`5988`.  The ``_rfc_values``\n        dictionary contains the appropriate values for the attributes\n        that get special handling.  If *strict mode* is enabled, then\n        only values that are acceptable will be added to ``_values``.\n\n        \"\"\"", "output": ".\n        \"\"\"\n        if name in self._rfc_values:\n            self._values[name] = value\n        else:\n            raise ietfparse.errors.MalformedLinkValue(\n                \"Invalid link value: %s\" % name\n            )\n\n        if self._strict_mode:\n            if name == \"rel\":\n                if not self._validate_rel(value):\n                    raise ietfparse.errors.MalformedLinkValue(\n                        \"Invalid rel value: %s\" % value\n                    )\n            elif name == \"rev\":\n                if not self._validate_rev(value):\n                    raise ietfparse.", "solution": "\n        try:\n            if self._rfc_values[name] is None:\n                self._rfc_values[name] = value\n            elif self.strict:\n                if name in ('media', 'type'):\n                    raise errors.MalformedLinkValue(\n                        'More than one {} parameter present'.format(name))\n                return\n        except KeyError:\n            pass\n\n        if self.strict and name in ('title', 'title*'):\n            return\n\n        self._values.append((name, value))"}
{"prompt": "def sp_search_query(query):\n    \"\"\"Translate a Mopidy search query to a Spotify search query\"\"\"", "output": "\n    # Replace with:\n    query = query.replace(\" \", \" \")\n    query = query.replace(\"the\", \"the \")\n    query = query.replace(\"of\", \"of \")\n    query = query.replace(\"and\", \"and \")\n    query = query.replace(\"a\", \"a \")\n    query = query.replace(\"an\", \"an \")\n    query = query.replace(\"in\", \"in \")\n    query = query.replace(\"on\", \"on \")\n    query = query.replace(\"at\", \"at \")\n    query = query.replace(\"to\", \"to \")\n    query =", "solution": "\n\n    result = []\n\n    for (field, values) in query.items():\n        field = SEARCH_FIELD_MAP.get(field, field)\n        if field is None:\n            continue\n\n        for value in values:\n            if field == 'year':\n                value = _transform_year(value)\n                if value is not None:\n                    result.append('%s:%d' % (field, value))\n            elif field == 'any':\n                result.append('\"%s\"' % value)\n            else:\n                result.append('%s:\"%s\"' % (field, value))\n\n    return ' '.join(result)"}
{"prompt": "def summary(self, fullpath=False):\n        \"\"\"Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files.\n\n        \"\"\"", "output": "\n       lobj = self.get_source_object()\n       lobj.load_data()\n       lobj.calculate_coverage()\n       lobj.calculate_totals()\n       lobj.calculate_missing()\n       lobj.calculate_partial()\n       lobj.calculate_partial_missing()\n       lobj.calculate_partial_missing_lines()\n       lobj.calculate_partial_missing_lines_percent()\n       lobj.calculate_partial_missing_lines_percent_lobj.calculate_partial_missing_lines_percent_ed", "solution": "\n        summ = {}\n        if fullpath:\n            filename_fn = lambda f: f\n        else:\n            filename_fn = os.path.basename\n        for filename, lines in iitems(self.lines):\n            summ[filename_fn(filename)] = len(lines)\n        return summ"}
{"prompt": "def read_hex_integer(self) -> bool:\n    \"\"\"\n    read a hexadecimal number\n    Read the following BNF rule else return False::\n\n        readHexInteger = [\n            [ '0'..'9' | 'a'..'f' | 'A'..'F' ]+\n        ]\n    \"\"\"", "output": "\n    if not self.match_char('0'):\n      return False\n    if not self.match_char('x'):\n      return False\n    if not self.match_char('0'):\n      return False\n    if not self.match_char('x'):\n      return False\n    if not self.match_char('0'):\n      return False\n    if not self.match_char('x'):\n      return False\n    if not self.match_char('0'):\n      return False\n    if not self.match_char('x'):\n      return False\n    if not self.match_char('0'):\n      return False\n    if", "solution": "\n    if self.read_eof():\n        return False\n    self._stream.save_context()\n    c = self._stream.peek_char\n    if c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f'):\n        self._stream.incpos()\n        while not self.read_eof():\n            c = self._stream.peek_char\n            if not (c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f')):\n                break\n            self._stream.incpos()\n        return self._stream.validate_context()\n    return self._stream.restore_context()"}
{"prompt": "def init_app(self, app):\n        \"\"\"\n        Initializes the Flask-Allows object against the provided application\n        \"\"\"", "output": "\n        self.app = app\n        self.init_app(app)\n\n    def init_app(self, app):\n        \"\"\"\n        Initializes the Flask-Allows object against the provided application\n        \"\"\"\n        self.app = app\n        self.init_app(app)\n\n    def init_app(self, app):\n        \"\"\"\n        Initializes the Flask-Allows object against the provided application\n        \"\"\"\n        self.app = app\n        self.init_app(app)\n\n    def init_app(self, app):\n        \"\"\"\n        Initializes the Flask-Allows object against the provided application\n        \"\"\"\n        self.app =", "solution": "\n        if not hasattr(app, \"extensions\"):  # pragma: no cover\n            app.extensions = {}\n        app.extensions[\"allows\"] = self\n\n        @app.before_request\n        def start_context(*a, **k):\n            self.overrides.push(Override())\n            self.additional.push(Additional())\n\n        @app.after_request\n        def cleanup(response):\n            self.clear_all_overrides()\n            self.clear_all_additional()\n            return response"}
{"prompt": "def raw_dict_factory(cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types, including raw read hex strings.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        return {k: sqlite3.Binary(v) if isinstance(v, bytes) else v for k, v in zip(cursor.description, row)}\n\n    def get_data(self, query, params=None):\n        \"\"\" Execute a query and return the result as a list of dictionaries.\n\n        Args:\n            query (str): SQL query to execute.\n            params (list, optional): List of parameters to bind to the query. Defaults to None.\n\n        Returns:\n            list: List of dictionaries representing the result set.\n        \"\"\"\n        with sqlite3.connect(self.db_path) as conn:", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp or name == Field.Meter_Address:\n                d[name] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":\n                d[name] = str(val)\n                continue\n        return d"}
{"prompt": "def _onSelectItem(self, selection, previousSelection):\n        \"\"\"Handle selection of item in listing.\"\"\"", "output": "\n        if selection is not None:\n            self.itemSelected.emit(selection)\n\n    def _onShowItem(self, item):\n        \"\"\"Handle showing of item in listing.\"\"\"\n        self.itemShown.emit(item)\n\n    def _onHideItem(self, item):\n        \"\"\"Handle hiding of item in listing.\"\"\"\n        self.itemHidden.emit(item)\n\n    def _onUpdateItem(self, item):\n        \"\"\"Handle updating of item in listing.\"\"\"\n        self.itemUpdated.emit(item)\n\n    def _onClearSelection(self):\n        \"\"\"Handle clearing of selection in listing.\"\"\"\n        self.", "solution": "\n        self._acceptButton.setEnabled(True)\n        del self._selected[:]\n        item = self._filesystemWidget.model().item(selection)\n        self._selected.append(item.path)"}
{"prompt": "def _srvc_set_config(self, trajectory):\n        \"\"\"Sets a config value to the Trajectory or changes it if the trajectory was loaded\n        a the settings no longer match\"\"\"", "output": "\n        if self.trajectory is not None:\n            if self.trajectory.config is None:\n                self.trajectory.config = {}\n            if trajectory.config is not None:\n                for key, value in trajectory.config.items():\n                    if key not in self.trajectory.config:\n                        self.trajectory.config[key] = value\n                    elif self.trajectory.config[key]!= value:\n                        self.trajectory.config[key] = value\n        else:\n            self.trajectory = trajectory\n            self.trajectory.config = {}\n\n    def _srvc_set____config(self", "solution": "\n        def _set_config(name, value, comment):\n            if not trajectory.f_contains('config.'+name, shortcuts=False):\n                trajectory.f_add_config(Parameter, name, value, comment=comment)\n\n        for attr_name in HDF5StorageService.NAME_TABLE_MAPPING:\n            table_name = HDF5StorageService.NAME_TABLE_MAPPING[attr_name]\n            value = getattr(self, attr_name)\n            _set_config('hdf5.overview.' + table_name,\n                         value,\n                         comment='Whether or not to have an overview '\n                                 'table with that name')\n\n        _set_config('hdf5.purge_duplicate_comments',\n                    self._purge_duplicate_comments,\n                    comment='Whether comments of results and'\n                            ' derived parameters should only'\n                            ' be stored for the very first instance.'\n                            ' Works only if the summary tables are'\n                            ' active.')\n\n        _set_config('hdf5.results_per_run', self._results_per_run,\n                    comment='Expected number of results per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.derived_parameters_per_run',\n                    self._derived_parameters_per_run,\n                    comment='Expected number of derived parameters per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.complevel', self._complevel,\n                    comment='Compression Level (0 no compression '\n                            'to 9 highest compression)')\n\n        _set_config('hdf5.complib', self._complib,\n                    comment='Compression Algorithm')\n\n        _set_config('hdf5.encoding', self._encoding,\n                    comment='Encoding for unicode characters')\n\n        _set_config('hdf5.fletcher32', self._fletcher32,\n                    comment='Whether to use fletcher 32 checksum')\n\n        _set_config('hdf5.shuffle', self._shuffle,\n                    comment='Whether to use shuffle filtering.')\n\n        _set_config('hdf5.pandas_format', self._pandas_format,\n                    comment="}
{"prompt": "def draw(self):\n        \"\"\"\n        Draws all layers of this LayeredWidget.\n        \n        This should normally be unneccessary, since it is recommended that layers use Vertex Lists instead of OpenGL Immediate Mode.\n        \"\"\"", "output": "\n        for layer in self.layers:\n            layer.draw()\n    \n    def add_layer(self, layer):\n        \"\"\"\n        Adds a layer to this LayeredWidget.\n        \n        This should normally be unneccessary, since it is recommended that layers use Vertex Lists instead of OpenGL Immediate Mode.\n        \"\"\"\n        self.layers.append(layer)\n    \n    def remove_layer(self, layer):\n        \"\"\"\n        Removes a layer from this LayeredWidget.\n        \n        This should normally be unneccessary, since it is recommended that layers use Vertex Lists instead of OpenGL Immediate Mode.\n        \"\"\"\n        self.layers.remove(layer", "solution": "\n        super(LayeredWidget,self).draw()\n        for layer,_ in self.layers:\n            layer._draw()"}
{"prompt": "def get_exitcodes_terminated_worker(processes):\n    \"\"\"Return a formated string with the exitcodes of terminated workers.\n\n    If necessary, wait (up to .25s) for the system to correctly set the\n    exitcode of one terminated worker.\n    \"\"\"", "output": "\n    def get_exitcode(process):\n        try:\n            return process.exitcode\n        except OSError:\n            return None\n\n    terminated_workers = [p for p in processes if p.exitcode is None]\n    if not terminated_workers:\n        return \"No terminated workers found.\"\n\n    # Wait for the system to correctly set the exitcode of one terminated worker.\n    for _ in range(10):\n        if all(get_exitcode(p) is not None for p in terminated_workers):\n            break\n        time.sleep(.25)\n\n    # Format the exitcodes and return them.\n   ", "solution": "\n    patience = 5\n\n    # Catch the exitcode of the terminated workers. There should at least be\n    # one. If not, wait a bit for the system to correctly set the exitcode of\n    # the terminated worker.\n    exitcodes = [p.exitcode for p in list(processes.values())\n                 if p.exitcode is not None]\n    while len(exitcodes) == 0 and patience > 0:\n        patience -= 1\n        exitcodes = [p.exitcode for p in list(processes.values())\n                     if p.exitcode is not None]\n        time.sleep(.05)\n\n    return _format_exitcodes(exitcodes)"}
{"prompt": "def eintr_retry(exc_type, f, *args, **kwargs):\n    \"\"\"Calls a function.  If an error of the given exception type with\n    interrupted system call (EINTR) occurs calls the function again.\n    \"\"\"", "output": "\n    while True:\n        try:\n            return f(*args, **kwargs)\n        except exc_type as e:\n            if e.errno == errno.EINTR:\n                continue\n            else:\n                raise\n\ndef get_ip_address(interface):\n    \"\"\"Returns the IP address of the given interface.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as sock:\n        return sock.getsockname()[0]\n\ndef get_mac_address(interface):\n    \"\"\"Returns the MAC address of the given interface.\"\"\"\n    with socket.socket(socket.AF_", "solution": "\n    while True:\n        try:\n            return f(*args, **kwargs)\n        except exc_type as exc:\n            if exc.errno != EINTR:\n                raise\n        else:\n            break"}
{"prompt": "def _colorify(self, data):\n        \"\"\"\n        Retun colored string.\n\n        :param data: The string to colorify.\n        :type data: str\n\n        :return: A colored string.\n        :rtype: str\n        \"\"\"", "output": "\n        return colored(data, self.color)\n\n    def _get_color(self, level):\n        \"\"\"\n        Get the color for a log level.\n\n        :param level: The log level.\n        :type level: int\n\n        :return: The color for the log level.\n        :rtype: str\n        \"\"\"\n        if level == logging.DEBUG:\n            return self.debug_color\n        elif level == logging.INFO:\n            return self.info_color\n        elif level == logging.WARNING:\n            return self.warning_color\n        elif level == logging.ERROR:\n            return self.error_color\n        elif level", "solution": "\n\n        if self.template in [\"Generic\", \"Less\"]:\n            # The template is in the list of template that need the coloration.\n\n            if (\n                self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"up\"]\n                or self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"valid\"]\n            ):\n                # The status is in the list of up status.\n\n                # We print the data with a green background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.GREEN + data\n            elif self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"down\"]:\n                # The status is in the list of down status.\n\n                # We print the data with a red background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.RED + data\n            else:\n                # The status is not in the list of up and down status.\n\n                # We print the data with a cyan background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.CYAN + data\n\n        # We return the data.\n        return data"}
{"prompt": "def __eof_qubit(rho):\n    \"\"\"\n    Compute the Entanglement of Formation of a 2-qubit density matrix.\n\n    Args:\n        rho ((array_like): (4,4) array_like, input density matrix.\n\n    Returns:\n        float: The entanglement of formation.\n    \"\"\"", "output": "\n    # Compute the trace of the density matrix\n    trace = np.trace(rho)\n\n    # Compute the fidelity\n    fidelity = trace ** 2\n\n    # Compute thestate\n   state =state_from_rho(rho)\n\n    # Compute thestate\n   state_ =state_from_rho(rho.T)\n\n    # Compute thestate fidelity\n   state_fidelity = np.abs(np.dot(rho, np.conj(rho.T))) ** 2\n\n    # Compute thestatestate fidelity\n   state", "solution": "\n    c = concurrence(rho)\n    c = 0.5 + 0.5 * np.sqrt(1 - c * c)\n    return shannon_entropy([c, 1 - c])"}
{"prompt": "def compile_sympy_accesses(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"", "output": "\n       _accesses = self.get_accesses()\n       _accesses = [a for a in self.get_accesses() if a.source in sources and a.destination in destinations]\n       _accesses.sort(key=lambda x: (x.source, x.destination))\n\n       _accesses = {}\n        for a in self.get_accesses():\n            if a.source not in sources or a.destination not in destinations:\n                continue\n            if a.source not in test_accesses:\n                test_accesses[a.source] = []", "solution": "\n        sympy_accesses = defaultdict(list)\n        # Compile sympy accesses\n        for var_name in self.variables:\n            if sources:\n                for r in self.sources.get(var_name, []):\n                    if r is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))\n            if destinations:\n                for w in self.destinations.get(var_name, []):\n                    if w is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))\n\n        return sympy_accesses"}
{"prompt": "def deserialize(bstr):\n    \"\"\"\n    Attempts to deserialize a bytestring into an audiosegment.\n\n    :param bstr: The bytestring serialized via an audiosegment's serialize() method.\n    :returns: An AudioSegment object deserialized from `bstr`.\n    \"\"\"", "output": "\n    try:\n        return AudioSegment.from_bytes(bstr)\n    except Exception as e:\n        raise DeserializationError(f\"Failed to deserialize audiosegment: {e}\") from e\n\n\nclass DeserializationError(Exception):\n    pass\n", "solution": "\n    d = pickle.loads(bstr)\n    seg = pickle.loads(d['seg'])\n    return AudioSegment(seg, d['name'])"}
{"prompt": "def create_validator(data_struct_dict, name=None):\n    \"\"\"\n    create a Validator instance from data_struct_dict\n\n    :param data_struct_dict: a dict describe validator's fields, like the dict `to_dict()` method returned.\n    :param name: name of Validator class \n\n    :return: Validator instance\n    \"\"\"", "output": "\n    if name is None:\n        name = data_struct_dict[\"name\"]\n\n    if data_struct_dict[\"type\"] == \"object\":\n        validator_class = ObjectValidator\n    elif data_struct_dict[\"type\"] == \"array\":\n        validator_class = ArrayValidator\n    elif data_struct_dict[\"type\"] == \"string\":\n        validator_class = StringValidator\n    elif data_struct_dict[\"type\"] == \"number\":\n        validator_class = NumberValidator\n    elif data_struct_dict[\"type\"] == \"boolean\":\n        validator_class = BooleanValidator\n    elif data_struct_", "solution": "\n\n    if name is None:\n        name = 'FromDictValidator'\n    attrs = {}\n    for field_name, field_info in six.iteritems(data_struct_dict):\n        field_type = field_info['type']\n        if field_type == DictField.FIELD_TYPE_NAME and isinstance(field_info.get('validator'), dict):\n            field_info['validator'] = create_validator(field_info['validator'])\n        attrs[field_name] = create_field(field_info)\n    name = force_str(name)\n    return type(name, (Validator, ), attrs)"}
{"prompt": "def _store(self, lines, buffer=None, store='source'):\n        \"\"\"Store one or more lines of input.\n\n        If input lines are not newline-terminated, a newline is automatically\n        appended.\"\"\"", "output": "\n        if buffer is None:\n            buffer = []\n        for line in lines:\n            if not line.endswith('\\n'):\n                line += '\\n'\n            buffer.append(line)\n        if store =='source':\n            self.source.extend(buffer)\n        elif store == 'buffer':\n            self.buffer.extend(buffer)\n        elif store == 'both':\n            self.source.extend(buffer)\n            self.buffer.extend(buffer)\n        else:\n            raise ValueError('Invalid store option: {}'.format(store))\n\n    def _get_line(self, index):\n        \"\"\"Return the line at", "solution": "\n        \n        if buffer is None:\n            buffer = self._buffer\n            \n        if lines.endswith('\\n'):\n            buffer.append(lines)\n        else:\n            buffer.append(lines+'\\n')\n        setattr(self, store, self._set_source(buffer))"}
{"prompt": "def selection_redo(self, name=\"default\", executor=None):\n        \"\"\"Redo selection, for the name.\"\"\"", "output": "\n        if executor is None:\n            executor = self.executor\n        executor.redo(name)\n\n    def selection_undo(self, name=\"default\", executor=None):\n        \"\"\"Undo selection, for the name.\"\"\"\n        if executor is None:\n            executor = self.executor\n        executor.undo(name)\n\n    def selection_clear(self, name=\"default\", executor=None):\n        \"\"\"Clear selection, for the name.\"\"\"\n        if executor is None:\n            executor = self.executor\n        executor.clear(name)\n\n    def selection_toggle(self, name=\"default\", executor=None):\n        \"\"\"Toggle selection,", "solution": "\n        logger.debug(\"redo\")\n        executor = executor or self.executor\n        assert self.selection_can_redo(name=name)\n        selection_history = self.selection_histories[name]\n        index = self.selection_history_indices[name]\n        next = selection_history[index + 1]\n        self.selection_history_indices[name] += 1\n        self.signal_selection_changed.emit(self)\n        logger.debug(\"redo: selection history is %r, index is %r\", selection_history, index)"}
{"prompt": "def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET):\n        \"\"\"Fetch questions from the Kitsune url.\n\n        :param category: the category of items to fetch\n        :offset: obtain questions after offset\n        :returns: a generator of questions\n        \"\"\"", "output": "\n        url = self.url + category + '?offset=' + str(offset)\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            for item in data['results']:\n                yield Question(item)\n        else:\n            raise Exception('Failed to fetch questions from Kitsune')\n\n    def fetch_all(self):\n        \"\"\"Fetch all questions from the Kitsune url.\n\n        :returns: a generator of all questions\n        \"\"\"\n        for category in CATEGORIES:\n            for offset in range(0, MAX_OFFSET, OFFSET", "solution": "\n        if not offset:\n            offset = DEFAULT_OFFSET\n\n        kwargs = {\"offset\": offset}\n        items = super().fetch(category, **kwargs)\n\n        return items"}
{"prompt": "def make_reports(self, stats, old_stats):\n        \"\"\"render registered reports\"\"\"", "output": "\n        for report in self.reports:\n            report.render(stats, old_stats)\n\n    def get_report_data(self,):\n        \"\"\"get report data\"\"\"\n        report_data = {}\n        for report in self.reports:\n            report_data[report.name] = report.get_data(self.request, self.user, self.project, self.stats, self.old_stats)\n        return report_data\n\n    def get_report_template_data(self, report_data):\n        \"\"\"get report template data\"\"\"\n        report_template_data = {}\n        for report", "solution": "\n        sect = Section(\"Report\", \"%s statements analysed.\" % (self.stats[\"statement\"]))\n        for checker in self.report_order():\n            for reportid, r_title, r_cb in self._reports[checker]:\n                if not self.report_is_enabled(reportid):\n                    continue\n                report_sect = Section(r_title)\n                try:\n                    r_cb(report_sect, stats, old_stats)\n                except EmptyReportError:\n                    continue\n                report_sect.report_id = reportid\n                sect.append(report_sect)\n        return sect"}
{"prompt": "def format_lines(statements, lines):\n    \"\"\"Nicely format a list of line numbers.\n\n    Format a list of line numbers for printing by coalescing groups of lines as\n    long as the lines represent consecutive statements.  This will coalesce\n    even if there are gaps between statements.\n\n    For example, if `statements` is [1,2,3,4,5,10,11,12,13,14] and\n    `lines` is [1,2,5,10,11,13,14] then the result will be \"1-2, 5-11, 13-14\".\n\n    \"\"\"", "output": " [1,2,3,4,5,10,11,12,13,14].\n\n    If `statements` is [1,2,3,4,5,6,7,8,9,10,11,12,13,14] and\n    `lines` is [1,2,5,10,11,13,14] then the result will be [1,2,3,4,5,6,7,8,9,10,11,12,13,1", "solution": "\n    pairs = []\n    i = 0\n    j = 0\n    start = None\n    statements = sorted(statements)\n    lines = sorted(lines)\n    while i < len(statements) and j < len(lines):\n        if statements[i] == lines[j]:\n            if start == None:\n                start = lines[j]\n            end = lines[j]\n            j += 1\n        elif start:\n            pairs.append((start, end))\n            start = None\n        i += 1\n    if start:\n        pairs.append((start, end))\n    ret = ', '.join(map(nice_pair, pairs))\n    return ret"}
{"prompt": "def _check_function(self):\n        \"\"\" make some basic checks on the function to make sure it is valid\"\"\"", "output": "\n        if not callable(self.function):\n            raise ValueError(\"The function must be a callable object\")\n\n    def _check_args(self):\n        \"\"\" make sure the arguments are valid\"\"\"\n        if not isinstance(self.args, tuple):\n            raise ValueError(\"The args must be a tuple\")\n        if not all(isinstance(arg, (int, float, str)) for arg in self.args):\n            raise ValueError(\"All arguments must be numeric or string\")\n\n    def _check_kwargs(self):\n        \"\"\" make sure the keyword arguments are valid\"\"\"\n        if not isinstance(self.kwargs, dict):\n            raise", "solution": "\n        # note, callable is valid for Python 2 and Python 3.2 onwards but\n        # not inbetween\n        if not callable(self._function):\n            raise RuntimeError(\n                \"provided function '{0}' is not callable\".\n                format(str(self._function)))\n        from inspect import getargspec\n        arg_info = getargspec(self._function)\n        if len(arg_info.args) != 1:\n            print str(arg_info)\n            raise RuntimeError(\n                \"provided function should have one argument but found \"\n                \"{0}\".format(len(arg_info.args)))"}
{"prompt": "def span(self, index):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"", "output": "\n        return self.ranges[index]\n\n    def(self, index):\n        \"\"\"Give thelaue of a tuple at a given index\n        \"\"\"\n        return self.values[index]\n\n    def(self, index):\n        \"\"\"Give the mnemonic of a tuple at a given index\n        \"\"\"\n        return self.mnemonics[index]\n\n    def(self, index):\n        \"\"\"Give the explanation of a tuple at a given index\n        \"\"\"\n        return self.explanations[index]\n\n    def(self, index):\n        \"\"\"Give the of a tuple at a given index\n        \"\"\"\n        return", "solution": "\n        lower = self.value0+sum(1<<x for x in self.extraTable[:index])\n        upper = lower+(1<<self.extraTable[index])\n        return lower, upper-1"}
{"prompt": "def paid_invoices_by_date(request, form):\n    \"\"\" Shows the number of paid invoices containing given products or\n    categories per day. \"\"\"", "output": "\n   voices = Invoice.objects.filter(status=Invoice.PAID)\n    if form.cleaned_data['product']:\n        invoices = invoices.filter(products__in=form.cleaned_data['product'])\n    if form.cleaned_data['category']:\n        invoices = invoices.filter(categories__in=form.cleaned_data['category'])\n    invoices = invoices.annotate(date=TruncDate('date'))\n    invoices = invoices.values('date').annotate(count=Count('id'))\n    return invoices\n\n\n@register.inclusion_tag('", "solution": "\n\n    products = form.cleaned_data[\"product\"]\n    categories = form.cleaned_data[\"category\"]\n\n    invoices = commerce.Invoice.objects.filter(\n        (\n            Q(lineitem__product__in=products) |\n            Q(lineitem__product__category__in=categories)\n        ),\n        status=commerce.Invoice.STATUS_PAID,\n    )\n\n    # Invoices with payments will be paid at the time of their latest payment\n    payments = commerce.PaymentBase.objects.all()\n    payments = payments.filter(\n        invoice__in=invoices,\n    )\n    payments = payments.order_by(\"invoice\")\n    invoice_max_time = payments.values(\"invoice\").annotate(\n        max_time=Max(\"time\")\n    )\n\n    # Zero-value invoices will have no payments, so they're paid at issue time\n    zero_value_invoices = invoices.filter(value=0)\n\n    times = itertools.chain(\n        (line[\"max_time\"] for line in invoice_max_time),\n        (invoice.issue_time for invoice in zero_value_invoices),\n    )\n\n    by_date = collections.defaultdict(int)\n    for time in times:\n        date = datetime.datetime(\n            year=time.year, month=time.month, day=time.day\n        )\n        by_date[date] += 1\n\n    data = [(date_, count) for date_, count in sorted(by_date.items())]\n    data = [(date_.strftime(\"%Y-%m-%d\"), count) for date_, count in data]\n\n    return ListReport(\n        \"Paid Invoices By Date\",\n        [\"date\", \"count\"],\n        data,\n    )"}
{"prompt": "def get(m, k, default=None):\n    \"\"\"Return the value of k in m. Return default if k not found in m.\"\"\"", "output": "\n    return m.get(k, default)\n\n\ndef set(m, k, v):\n    \"\"\"Set the value of k in m to v.\"\"\"\n    m[k] = v\n\n\ndef update(m, *args, **kwargs):\n    \"\"\"Update the dictionary m with the key-value pairs from the given arguments and keyword arguments.\"\"\"\n    m.update(*args, **kwargs)\n\n\ndef keys(m):\n    \"\"\"Return a list of all keys in m.\"\"\"\n    return list(m.keys())\n\n\ndef values(m):\n    \"\"\"Return a list of all values in m.\"\"\"\n   ", "solution": "\n    if isinstance(m, IAssociative):\n        return m.entry(k, default=default)\n\n    try:\n        return m[k]\n    except (KeyError, IndexError, TypeError) as e:\n        logger.debug(\"Ignored %s: %s\", type(e).__name__, e)\n        return default"}
{"prompt": "def _send_coroutine():\n    \"\"\"\n    Creates a running coroutine to receive message instances and send\n    them in a futures executor.\n    \"\"\"", "output": "\n    while True:\n        message = yield\n        if message is None:\n            break\n        with futures.ThreadPoolExecutor() as executor:\n            future = executor.submit(send_message, message)\n            future.add_done_callback(lambda f: print(f.result()))\n\n\ndef send_message(message):\n    \"\"\"\n    Sends a message to a remote endpoint.\n    \"\"\"\n    print(f\"Sending message: {message}\")\n    # TODO: Implement actual sending logic here\n    pass\n\n\ndef main():\n    \"\"\"\n    Starts the message sender coroutine.\n    \"\"\"\n    coroutine = _send_coroutine()\n    next(coroutine", "solution": "\n    with PoolExecutor() as executor:\n        while True:\n            msg = yield\n            future = executor.submit(msg.send)\n            future.add_done_callback(_exception_handler)"}
{"prompt": "def cb_list_groups(self, *args, **kwargs):\n        \"\"\"List all the check groups that pylint knows about\n\n        These should be useful to know what check groups someone can disable\n        or enable.\n        \"\"\"", "output": "\n        for group in.check_groups:\n            print(group)\n\n    def cb_list_checks(self, *args, **kwargs):\n        \"\"\"List all the checks that pylint knows about\n\n        These should be useful to know what checks someone can disable\n        or enable.\n        \"\"\"\n        for check in.check_classes:\n            print(check.__name__)\n\n    def cb_disable_check(self, *args, **kwargs):\n        \"\"\"Disable a specific check\n\n        This command takes a check name as an argument and disables it.\n        \"\"\"\n        check_name = args[0]\n        for check in", "solution": "\n        for check in self.linter.get_checker_names():\n            print(check)\n        sys.exit(0)"}
{"prompt": "def add_column(self, name, data):\n        \"\"\"Add a column to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data\n        \"\"\"", "output": "\n        self.data[name] = data\n\n    def add_columns(self, **kwargs):\n        \"\"\"Add multiple columns to the DataFrame\n\n        :param kwargs: keyword arguments with column names as keys and numpy arrays as values\n        \"\"\"\n        for name, data in kwargs.items():\n            self.add_column(name, data)\n\n    def drop_column(self, name):\n        \"\"\"Drop a column from the DataFrame\n\n        :param str name: name of column to drop\n        \"\"\"\n        del self.data[name]\n\n    def drop_columns(self, *names):\n        \"\"\"Drop multiple columns from the DataFrame\n\n        :", "solution": "\n        # assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type)\n        # self._length = len(data)\n        # if self._length_unfiltered is None:\n        #     self._length_unfiltered = len(data)\n        #     self._length_original = len(data)\n        #     self._index_end = self._length_unfiltered\n        super(DataFrameArrays, self).add_column(name, data)\n        self._length_unfiltered = int(round(self._length_original * self._active_fraction))"}
{"prompt": "def get_objects_from_form(variant_ids, form_fields, object_type):\n    \"\"\"Extract the objects to be saved in the clinvar database collection.\n       object_type param specifies if these objects are variant or casedata objects\n\n       Args:\n        variant_ids(list): list of database variant ids\n        form_fields(dict): it's the submission form dictionary. Keys have the same names as CLINVAR_HEADER and CASEDATA_HEADER\n        object_type(str): either 'variant' or 'case_data'\n\n       Returns:\n        submission_objects(list): list of submission objects of either type 'variant' or 'casedata'\n    \"\"\"", "output": " the specified type\n    \"\"\"\n    submission_objects = []\n    for variant_id in variant_ids:\n        submission_object = {}\n        for field in form_fields:\n            if field in CLINVAR_HEADER:\n                submission_object[field] = form_fields[field]\n            elif field in CASEDATA_HEADER:\n                submission_object[field] = form_fields[field]\n            else:\n                submission_object[field] = None\n        submission_object['variant_id'] = variant_id\n        submission_object['object_type'] = object_type\n        submission_objects.append(submission", "solution": "\n    submission_fields = []\n    if object_type == 'variant':\n        submission_fields = CLINVAR_HEADER\n    else: #collect casedata objects\n        submission_fields = CASEDATA_HEADER\n\n    # A list of objects (variants of casedata info) to be saved into clinvar database collection\n    submission_objects = []\n\n    # Loop over the form fields and collect the data:\n    for variant_id in variant_ids: # loop over the variants\n\n        subm_obj = {} # A new submission object for each\n\n        # Don't included casedata for a variant unless specified by user\n        if object_type == 'casedata' and 'casedata_'+variant_id not in form_fields:\n            continue\n\n        subm_obj['csv_type'] = object_type\n        subm_obj['case_id'] = form_fields.get('case_id')\n        subm_obj['category'] = form_fields.get('category@'+variant_id)\n\n        for key, values in submission_fields.items(): # loop over the form info fields\n            field_value = form_fields.get(key+'@'+variant_id)\n            if field_value and not field_value == '-':\n                if key == 'ref_seq': # split this field into\n                    refseq_raw = field_value.split('|')\n                    subm_obj['ref_seq'] = refseq_raw[0]\n                    subm_obj['hgvs'] = refseq_raw[1]\n                else:\n                    subm_obj[key] = field_value\n\n        # Create a unique ID for the database\n        # For casedata : = caseID_sampleID_variantID\n        # For variants : ID = caseID_variantID\n        if object_type == 'casedata':\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id + '_' + str(subm_obj['individual_id'])\n        else:\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id\n\n        submission_objects.append(subm_obj)\n\n    return submission_objects"}
{"prompt": "def evaluate(self, x, y=None, batch_size=32):\n        \"\"\"\n        Evaluate a model on a given dataset in distributed mode.\n\n        # Arguments\n        x: Input data. A Numpy array or RDD of Sample.\n        y: Labels. A Numpy array. Default is None if x is already RDD of Sample.\n        batch_size: Number of samples per gradient update.\n        \"\"\"", "output": "\n        if y is None:\n            y = np.zeros(x.shape[0])\n\n       _data = x\n       _label = y\n\n       _rdd = self.spark_context.parallelize(zip(np.array(x), np.array(y)))\n       _data,_label = self.transform_data(train_data=None, train_label=None, test_data=None, test_label=None,_data=None,_label=None,_rdd=None)\n\n       _rdd = self.spark_context.", "solution": "\n        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n            evaluation_data = to_sample_rdd(x, y)\n        elif isinstance(x, RDD) and not y:\n            evaluation_data = x\n        else:\n            raise TypeError(\"Unsupported evaluation data type: %s\" % type(x))\n        return callBigDlFunc(self.bigdl_type, \"evaluate\",\n                             self.value,\n                             evaluation_data,\n                             batch_size)"}
{"prompt": "def _loop_to_py_ast(ctx: GeneratorContext, node: Loop) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `loop*` expression.\"\"\"", "output": "\n    # TODO: Implement this function\n    raise NotImplementedError()\n\n\ndef _loop_to_py_ast_for(ctx: GeneratorContext, node: For) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `for` loop.\"\"\"\n    # TODO: Implement this function\n    raise NotImplementedError()\n\n\ndef _loop_to_py_ast_while(ctx: GeneratorContext, node: While) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `while` loop.\"\"\"\n    # TODO: Implement this function\n    raise NotImplementedError()\n\n\ndef _loop_to_py_ast", "solution": "\n    assert node.op == NodeOp.LOOP\n\n    with ctx.new_symbol_table(\"loop\"):\n        binding_names = []\n        init_bindings: List[ast.AST] = []\n        for binding in node.bindings:\n            init_node = binding.init\n            assert init_node is not None\n            init_ast = gen_py_ast(ctx, init_node)\n            init_bindings.extend(init_ast.dependencies)\n            binding_name = genname(munge(binding.name))\n            binding_names.append(binding_name)\n            init_bindings.append(\n                ast.Assign(\n                    targets=[ast.Name(id=binding_name, ctx=ast.Store())],\n                    value=init_ast.node,\n                )\n            )\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), binding_name, LocalType.LOOP\n            )\n\n        loop_result_name = genname(\"loop\")\n        with ctx.new_recur_point(\n            node.loop_id, RecurType.LOOP, binding_names=binding_names\n        ):\n            loop_body_ast: List[ast.AST] = []\n            body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n            loop_body_ast.extend(body_ast.dependencies)\n            loop_body_ast.append(\n                ast.Assign(\n                    targets=[ast.Name(id=loop_result_name, ctx=ast.Store())],\n                    value=body_ast.node,\n                )\n            )\n            loop_body_ast.append(ast.Break())\n\n            return GeneratedPyAST(\n                node=_load_attr(loop_result_name),\n                dependencies=list(\n                    chain(\n                        [\n                            ast.Assign(\n                                targets=[\n                                    ast.Name(id=loop_result_name, ctx=ast.Store())\n                                ],\n                                value=ast.NameConstant(None),\n                            )\n                        ],\n                        init_bindings,\n                        [\n                            ast.While(\n                                test=ast.NameConstant(True),\n                                body=loop_body_ast,\n                                orelse=[],\n                            )\n                        ],\n                    )\n                ),\n            )"}
{"prompt": "def get_model(self, model_name):\n        \"\"\"\n        TODO: Need to validate model name has 2x '.' chars\n        \"\"\"", "output": "\n        model_name = model_name.replace('.', '')\n        model_name = model_name.replace('_', '')\n        model_name = model_name.replace('-', '')\n        model_name = model_name.replace(' ', '')\n        model_name = model_name.lower()\n        model_name = model_name.replace('__', '')\n        model_name = model_name.replace('__', '')\n        model_name = model_name.replace('__', '')\n        model_name = model_name.replace('__', '')\n        model_name = model_name.replace", "solution": "\n        klass = None\n        try:\n            module_name, class_name = model_name.rsplit('.', 1)\n            mod = __import__(module_name, fromlist=[class_name])\n            klass = getattr(mod, class_name)\n        except ImportError, e:\n            self.error('Cannot find app %s %s' % (model_name, e))\n\n        return klass"}
{"prompt": "def compare(self, cost_map):\n        \"\"\"\n        Compare to previous records and return whether the given cost is a new best.\n        :return: True if the given cost is a new best\n        \"\"\"", "output": "\n      , w = self.size\n       _prev, w_prev = self.prev_size\n       _prev, w_prev =_to_(self.prev_size),_to_ w(self.prev_size)\n      , w =_to_ (self.size),_to_ w(self.size)\n\n       _diff =_prev -\n        w_diff = w_prev - w\n\n       _diff_ = 0\n        w_diff_min = 0\n        for i in range(1,_diff", "solution": "\n        cri_val = cost_map[self._criteria]\n        if self._best_criteria is None:\n            self._best_criteria = cri_val\n            return True\n        else:\n            if self._smaller_is_better and cri_val < self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            elif not self._smaller_is_better and cri_val > self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            else:\n                return False"}
{"prompt": "def execute(self):\n        \"\"\"\n        Execute R script\n        \"\"\"", "output": "\n        self.logger.info(\"Executing R script\")\n        self.r_script_path = os.path.join(self.working_dir, \"r_script.R\")\n        self.r_script_content = self.r_script.replace(\"{{input_file}}\", self.input_file)\n        with open(self.r_script_path, \"w\") as f:\n            f.write(self.r_script_content)\n        self.logger.info(\"R script written to {}\".format(self.r_script_path))\n        self.run_command(\"", "solution": "\n        rprocess = OrderedDict()\n        commands = OrderedDict([\n            (self.file, ['Rscript', self.file] + self.cmd),\n        ])\n        for cmd_name, cmd in commands.items():\n            rprocess[cmd_name] = self.run_command_under_r_root(cmd)\n        \n        return self.decode_cmd_out(completed_cmd=rprocess[self.file])"}
{"prompt": "def _duplicated_isinstance_types(node):\n        \"\"\"Get the duplicated types from the underlying isinstance calls.\n\n        :param astroid.BoolOp node: Node which should contain a bunch of isinstance calls.\n        :returns: Dictionary of the comparison objects from the isinstance calls,\n                  to duplicate values from consecutive calls.\n        :rtype: dict\n        \"\"\"", "output": "\n        comparisons = {}\n        for child in node.values:\n            if isinstance(child, astroid.Call) and child.func.attr == 'isinstance':\n                if child.args[0] in comparisons:\n                    comparisons[child.args[0]].append(child.args[1])\n                else:\n                    comparisons[child.args[0]] = [child.args[1]]\n        return comparisons\n\n    def _get_duplicate_values(comparison_objects):\n        \"\"\"Get the duplicate values from the comparison objects.\n\n        :param dict comparison_objects: Dictionary of the comparison objects from the\n                                        isinstance calls.\n       ", "solution": "\n        duplicated_objects = set()\n        all_types = collections.defaultdict(set)\n\n        for call in node.values:\n            if not isinstance(call, astroid.Call) or len(call.args) != 2:\n                continue\n\n            inferred = utils.safe_infer(call.func)\n            if not inferred or not utils.is_builtin_object(inferred):\n                continue\n\n            if inferred.name != \"isinstance\":\n                continue\n\n            isinstance_object = call.args[0].as_string()\n            isinstance_types = call.args[1]\n\n            if isinstance_object in all_types:\n                duplicated_objects.add(isinstance_object)\n\n            if isinstance(isinstance_types, astroid.Tuple):\n                elems = [\n                    class_type.as_string() for class_type in isinstance_types.itered()\n                ]\n            else:\n                elems = [isinstance_types.as_string()]\n            all_types[isinstance_object].update(elems)\n\n        # Remove all keys which not duplicated\n        return {\n            key: value for key, value in all_types.items() if key in duplicated_objects\n        }"}
{"prompt": "def digest(self, data, mecha=MechanismSHA1):\n        \"\"\"\n        C_DigestInit/C_Digest\n\n        :param data: the data to be digested\n        :type data:  (binary) sring or list/tuple of bytes\n        :param mecha: the digesting mechanism to be used\n          (use `MechanismSHA1` for `CKM_SHA_1`)\n        :type mecha: :class:`Mechanism`\n        :return: the computed digest\n        :rtype: list of bytes\n\n        :note: the returned value is an istance of :class:`ckbytelist`.\n          You can easly convert it to a binary string with:\n          ``bytes(ckbytelistDigest)``\n          or, for Python 2:\n          ``''.join(chr(i) for i in ckbytelistDigest)``\n\n        \"\"\"", "output": "terange.ByteRange`\n        \"\"\"\n        if isinstance(data, bytes):\n            data = [data]\n        elif not isinstance(data, list):\n            raise TypeError(\"data must be a byte string or a list/tuple of bytes\")\n\n        if not isinstance(mecha, Mechanism):\n            raise TypeError(\"mecha must be an instance of Mechanism\")\n\n        if mecha.mechanism_type!= MechanismType.DIGEST:\n            raise ValueError(\"mecha must be a DIGEST mechanism\")\n\n        if mecha.mechanism_class!= MechanismClass.:\n            raise ValueError(\"mecha must be", "solution": "\n        digest = ckbytelist()\n        m = mecha.to_native()\n        data1 = ckbytelist(data)\n        rv = self.lib.C_DigestInit(self.session, m)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # first call get digest size\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # second call get actual digest data\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return digest"}
{"prompt": "def file_hash(self, source, cu):\n        \"\"\"Compute a hash that changes if the file needs to be re-reported.\"\"\"", "output": "\n        # TODO: Implement a more robust hash function that takes into account\n        # the contents of the file, not just the filename.\n        return hashlib.sha1(source.encode('utf-8')).hexdigest()\n\n    def report_file(self, source, cu):\n        \"\"\"Report a file that needs to be re-reported.\"\"\"\n        # TODO: Implement a more robust reporting mechanism that takes into account\n        # the contents of the file, not just the filename.\n        print(\"Re-reporting file:\", source)\n\n\nclass FileReporter(Reporter):\n    \"\"\"A reporter that reports files that need to be re-", "solution": "\n        m = Hasher()\n        m.update(source)\n        self.coverage.data.add_to_hash(cu.filename, m)\n        return m.digest()"}
{"prompt": "def voronoi(script, hole_num=50, target_layer=None, sample_layer=None, thickness=0.5, backward=True):\n    \"\"\" Turn a model into a surface with Voronoi style holes in it\n\n    References:\n    http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html\n    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html\n\n    Requires FilterScript object\n\n    Args:\n        script: the FilterScript object to write the filter to. Does not\n            work with a script filename.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"", "output": "\n        hole_num: the number ofonoi holes to create\n        target_layer: the layer to create theonoi holes in\n        sample_layer: the layer to sampleonoi holes from\n        thickness: theonoithickeness\n        backward: if True,onoi holes are created in reverse order\n\n    Returns:\n        None\n    \"\"\"\n   onoi_layer = script.get_layer(target_layer)\n    sample_layer = script.get_layer(sample_layer)\n\n   onoi_layer.clear()\n   onoi_layer.set_name", "solution": "\n\n    if target_layer is None:\n        target_layer = script.current_layer()\n    if sample_layer is None:\n        # Current layer is currently not changed after poisson_disk is run\n        sampling.poisson_disk(script, sample_num=hole_num)\n        sample_layer = script.last_layer()\n\n    vert_color.voronoi(script, target_layer=target_layer, source_layer=sample_layer, backward=backward)\n    select.vert_quality(script, min_quality=0.0, max_quality=thickness)\n    if backward:\n        select.invert(script)\n    delete.selected(script)\n    smooth.laplacian(script, iterations=3)\n\n    return None"}
{"prompt": "def indices_to_global_iterator(self, indices):\n        \"\"\"\n        Transform a dictionary of indices to a global iterator integer.\n\n        Inverse of global_iterator_to_indices().\n        \"\"\"", "output": "\n       lobj = self.global_iterator_to_indices(indices)\n       lobj =lobj.reshape(self.shape)\n        return np.ravel_multi_index(tuple(np.nonzero(mlobj)[::-1]), self.shape)\n\n    deflobj_to_indices(self, mlobj):\n        \"\"\"\n        Transform a multi-dimensional numpy array of locations to a dictionary of locations.\n\n        The locations are represented as", "solution": "\n        global_iterator = self.subs_consts(self.global_iterator().subs(indices))\n        return global_iterator"}
{"prompt": "def _value(self, dtype=None, name=None, as_ref=False):  # pylint: disable=g-doc-args\n  \"\"\"Get the value returned by `tf.convert_to_tensor(distribution)`.\n\n  Note: this function may mutate the distribution instance state by caching\n  the concretized `Tensor` value.\n\n  Args:\n    dtype: Must return a `Tensor` with the given `dtype` if specified.\n    name: If the conversion function creates a new `Tensor`, it should use the\n      given `name` if specified.\n    as_ref: `as_ref` is true, the function must return a `Tensor` reference,\n      such as a `Variable`.\n  Returns:\n    concretized_distribution_value: `Tensor` identical to\n    `tf.convert_to_tensor(distribution)`.\n\n  #### Examples\n\n  ```python\n  tfd = tfp.distributions\n  x = tfd.Normal(0.5, 1).set_tensor_conversion(tfd.Distribution.mean)\n\n  x._value()\n  # ==> tf.convert_to_tensor(x) ==> 0.5\n\n  x._value() + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n\n  x + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n  ```\n\n  \"\"\"", "output": " is a boolean flag that indicates whether the returned\n      `Tensor` should be a reference or a copy of the original tensor.\n\n  Returns:\n    A `Tensor` with the same `dtype` as the distribution's `Tensor` value.\n    If `as_ref` is `True`, the returned `Tensor` will be a reference to the\n    original tensor. Otherwise, the returned `Tensor` will be a copy of the\n    original tensor.\n\n  Raises:\n    ValueError: If the distribution's `Tensor` value cannot be converted to a\n      `Tensor` with the specified `dtype`.\n  \"\"\"\n  if not_distribution", "solution": "\n  # pylint: disable=protected-access\n  if as_ref:\n    raise NotImplementedError(\n        'Cannot convert a `Distribution` to a reference '\n        '(e.g., `tf.Variable`).')\n  if self._concrete_value is None:\n    if self._convert_to_tensor_fn is None:\n      raise NotImplementedError(\n          'Failed to convert object of type {} to Tensor. Contents: {}. '\n          'Call `distribution.set_tensor_conversion(lambda self: ...)` to '\n          'enable `tf.convert_to_tensor` capability. For example: '\n          '`x = tfd.Normal(0,1).set_tensor_conversion(tfd.Distribution.mean)`'\n          ' results in `tf.convert_to_tensor(x)` being identical to '\n          '`x.mean()`.'.format(type(self), self))\n    with self._name_scope('value'):\n      self._concrete_value = (self._convert_to_tensor_fn(self)\n                              if callable(self._convert_to_tensor_fn)\n                              else self._convert_to_tensor_fn)\n      if not tf.is_tensor(self._concrete_value):\n        self._concrete_value = tfd._convert_to_tensor(\n            value=self._concrete_value,\n            name=name or 'concrete_value',\n            dtype=dtype,\n            dtype_hint=self.dtype)\n  return self._concrete_value"}
{"prompt": "def url_equal(first, second, ignore_scheme=False, ignore_netloc=False, ignore_path=False, ignore_params=False,\n              ignore_query=False, ignore_fragment=False):\n    \"\"\"\n    Compare two URLs and return True if they are equal, some parts of the URLs can be ignored\n    :param first: URL\n    :param second: URL\n    :param ignore_scheme: ignore the scheme\n    :param ignore_netloc: ignore the netloc\n    :param ignore_path: ignore the path\n    :param ignore_params: ignore the params\n    :param ignore_query: ignore the query string\n    :param ignore_fragment: ignore the fragment\n    :return: result of comparison\n    \"\"\"", "output": "query: ignore the query\n    :param ignore_fragment: ignore the fragment\n    :return: True if the URLs are equal, False otherwise\n    \"\"\"\n    if ignore_scheme:\n        first = first.split('://', 1)[1]\n    if ignore_netloc:\n        first = first.split('/', 1)[0]\n    if ignore_path:\n        first = first.split('?', 1)[0]\n    if ignore_params:\n        first = first.split('#', 1)[0]\n    if ignore_query:\n        first = first.split('?', 1)[0]\n    if", "solution": "\n    # <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\n    firstp = urlparse(first)\n    secondp = urlparse(second)\n\n    return ((firstp.scheme == secondp.scheme or ignore_scheme) and\n            (firstp.netloc == secondp.netloc or ignore_netloc) and\n            (firstp.path == secondp.path or ignore_path) and\n            (firstp.params == secondp.params or ignore_params) and\n            (firstp.query == secondp.query or ignore_query) and\n            (firstp.fragment == secondp.fragment or ignore_fragment))"}
{"prompt": "def get_users_for_course(self, course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given course id.\n        \"\"\"", "output": "\n        url = self._build_url(f\"courses/{course_id}/users\", params)\n        response = self._get(url)\n        return response.json()\n\n    def get_user_details(self, user_id):\n        \"\"\"\n        Returns the details of the given user id.\n        \"\"\"\n        url = self._build_url(f\"users/{user_id}\")\n        response = self._get(url)\n        return response.json()\n\n    def update_user_details(self, user_id, data):\n        \"\"\"\n        Updates the details of the given user id.\n        \"\"\"\n        url", "solution": "\n        url = COURSES_API.format(course_id) + \"/users\"\n        data = self._get_paged_resource(url, params=params)\n        users = []\n        for datum in data:\n            users.append(CanvasUser(data=datum))\n        return users"}
{"prompt": "def make_signing_service(config, entity_id):\n    \"\"\"\n    Given configuration initiate a SigningService instance\n\n    :param config: The signing service configuration\n    :param entity_id: The entity identifier\n    :return: A SigningService instance\n    \"\"\"", "output": "\n    if config['type'] == 'default':\n        return DefaultSigningService(config, entity_id)\n    elif config['type'] == 'azure':\n        return AzureSigningService(config, entity_id)\n    else:\n        raise ValueError(f\"Unsupported signing service type: {config['type']}\")\n\n\nclass SigningService:\n    \"\"\"\n    Abstract base class for signing service implementations\n    \"\"\"\n\n    def __init__(self, config, entity_id):\n        self.config = config\n        self.entity_id = entity_id\n\n    def sign(self, data):\n        \"\"\"\n        Sign the given data\n\n       ", "solution": "\n\n    _args = dict([(k, v) for k, v in config.items() if k in KJ_SPECS])\n    _kj = init_key_jar(**_args)\n\n    if config['type'] == 'internal':\n        signer = InternalSigningService(entity_id, _kj)\n    elif config['type'] == 'web':\n        _kj.issuer_keys[config['iss']] = _kj.issuer_keys['']\n        del _kj.issuer_keys['']\n        signer = WebSigningServiceClient(config['iss'], config['url'],\n                                         entity_id, _kj)\n    else:\n        raise ValueError('Unknown signer type: {}'.format(config['type']))\n\n    return signer"}
{"prompt": "def delete(filename, delete_v1=True, delete_v2=True):\n    \"\"\"Remove tags from a file.\n\n    Keyword arguments:\n\n    * delete_v1 -- delete any ID3v1 tag\n    * delete_v2 -- delete any ID3v2 tag\n    \"\"\"", "output": "\n    if delete_v1:\n        id3v1.delete(filename)\n    if delete_v2:\n        id3v2.delete(filename)\n", "solution": "\n\n    f = open(filename, 'rb+')\n\n    if delete_v1:\n        try:\n            f.seek(-128, 2)\n        except IOError:\n            pass\n        else:\n            if f.read(3) == b'TAG':\n                f.seek(-128, 2)\n                f.truncate()\n\n    # technically an insize=0 tag is invalid, but we delete it anyway\n    # (primarily because we used to write it)\n    if delete_v2:\n        f.seek(0, 0)\n        idata = f.read(10)\n        try:\n            id3, vmaj, vrev, flags, insize = unpack('>3sBBB4s', idata)\n        except struct.error:\n            id3, insize = b'', -1\n        insize = BitPaddedInt(insize)\n        if id3 == b'ID3' and insize >= 0:\n            delete_bytes(f, insize + 10, 0)"}
{"prompt": "def state(self):\n        \"\"\"Return internal state, useful for testing.\"\"\"", "output": "\n        return self._state\n\n    def set_state(self, state):\n        \"\"\"Set internal state, useful for testing.\"\"\"\n        self._state = state\n\n    def reset(self):\n        \"\"\"Reset internal state.\"\"\"\n        self._state = self.initial_state\n\n    def step(self, action):\n        \"\"\"Perform a step in the environment.\n\n        Args:\n            action (int): The action to take.\n\n        Returns:\n            tuple: (observation, reward, done, info)\n                observation (object): The observation of the environment after the action.\n                reward (float): The reward received for taking the action.\n                done", "solution": "\n        return {'c': self.c, 's0': self.s0, 's1': self.s1, 's2': self.s2}"}
{"prompt": "def get_mint_tree(tokens_stream):\n    \"\"\"\n    This function is wrapper to normal parsers (tag_parser, block_parser, etc.).\n    Returns mint tree.\n    \"\"\"", "output": "\n   _tokens = list(tokens_stream)\n   _tokens.append(('EOF', ''))\n   _tokens =_tokens[1:]\n   _tokens =_tokens[:-1]\n\n   _tokens =_tokens[::-1]\n   _tokens = list(filter(lambda x: x[0]!= 'EOF', reversed(list(map(tag_parser, reversed(me_tokens))))))\n    return parse_mint_tree(me_tokens)\n\n\ndef parse_mint_tree(tokens):\n    \"\"\"\n    This function is_tokens to mint tree.", "solution": "\n    smart_stack = RecursiveStack()\n    block_parser.parse(tokens_stream, smart_stack)\n    return MintTemplate(body=smart_stack.stack)"}
{"prompt": "def from_yaml(cls, defaults, **kwargs):\n        \"\"\"Creates a new instance of a rule by merging two dictionaries.\n\n        This allows for independant configuration files to be merged\n        into the defaults.\"\"\"", "output": "\n        merged = merge_dicts(defaults, kwargs)\n        return cls(**merged)\n\n    def __init__(self,,_type,_id,_text,_tags,_category,\n                _difficulty,_duration,_language,_source,\n                _url,_license,_author,_publisher,\n                _description,_keywords,_version,\n                _release_date,_last_update,\n                _rating,_votes,_downloads,\n                 ip_address,_status,", "solution": "\n        # TODO: I hate myself for this. Fix it later mmkay?\n        if \"token\" not in defaults:\n            kwargs[\"token\"] = None\n\n        defaults = copy.deepcopy(defaults)\n        return cls(\n            defaults=defaults,\n            token=kwargs.pop(\"token\"),\n            directory=kwargs.pop(\"directory\"),\n            **kwargs\n        )"}
{"prompt": "def getPhysicalInterface(self, physicalInterfaceId, draft=False):\n        \"\"\"\n        Get a physical interface.\n        Parameters:\n          - physicalInterfaceId (string)\n          - draft (boolean)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        if not physicalInterfaceId:\n            raise APIException(\"Invalid parameter: physicalInterfaceId cannot be empty\")\n\n        resourcePath = \"/physicalInterfaces/{physicalInterfaceId}\"\n        resourcePath = resourcePath.replace(\"{format}\", \"json\")\n        method = \"GET\"\n\n        queryParams = {}\n        headerParams = {}\n        formParams = {}\n        files = {}\n        bodyParam = None\n\n        headerParams[\"Authorization\"] = self.configuration.authorization\n        headerParams[\"Content-Type\"] = self.configuration.contentType\n\n        if draft:\n            queryParams[\"draft\"] = True\n\n        pathParams = {}\n        if \"physicalInterfaceId\" in params", "solution": "\n        if draft:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"/draft\", physicalInterfaceId)\n        else:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"\", physicalInterfaceId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"physical interface retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting a physical interface\", resp)\n        return resp.json()"}
{"prompt": "def get(cls):\n        \"\"\"\n        Get the status while testing for an IP or domain.\n\n        .. note::\n            We consider that the domain or IP we are currently testing\n            is into :code:`PyFunceble.INTERN[\"to_test\"]`.\n        \"\"\"", "output": "\n\n        if cls.is_ip():\n            return cls.get_ip_status()\n        elif cls.is_domain():\n            return cls.get_domain_status()\n        else:\n            raise ValueError(\"The given string is not an IP or a domain.\")\n\n    @classmethod\n    def get_ip_status(cls):\n        \"\"\"\n        Get the status while testing for an IP.\n\n       .. note::\n            We consider that the IP we are currently testing is into\n            :code:`PyFunceble.INTERN[\"to_test\"]`.\n        \"\"\"\n\n        ip_status = cls.get_status(\n            PyF", "solution": "\n\n        if \"to_test\" in PyFunceble.INTERN and PyFunceble.INTERN[\"to_test\"]:\n            expiration_date = ExpirationDate().get()\n\n            if expiration_date is False:\n                return cls.handle(status=\"invalid\")\n\n            if expiration_date == PyFunceble.STATUS[\"official\"][\"up\"]:\n                return expiration_date, \"WHOIS\"\n\n            return cls.handle(status=\"inactive\")\n\n        raise NotImplementedError(\"We expect `INTERN['to_test']` to be set.\")"}
{"prompt": "def execute(option):\n    \"\"\"A script that melody calls with each valid set of options. This\n    script runs the required code and returns the results.\"\"\"", "output": "\n   _options = [\n        # 1 1\n        {\n            \"melody\": [\n                \"C\",\n                \"E\",\n                \"G\",\n                \"A\",\n                \"C\",\n                \"E\",\n                \"G\",\n                \"A\",\n                \"C\",\n                \"E\",\n                \"G\",\n                \"A\",\n                \"C\",\n                \"E\",\n                \"G\",\n                \"A\",\n                \"C\",\n                \"E\",\n                \"G\",\n                \"A\",\n                \"C\",\n                \"E\",\n                \"G\",\n                \"A\",\n                \"C\",\n                \"E\",", "solution": "\n\n    namelist_option = []\n    makefile_option = []\n    flags = \"\"\n    for entry in option:\n        key = entry.keys()[0]\n        if key == \"Problem Size\":\n            namelist_option.append({\"SIZE\": entry[key]})\n        elif key == \"F90\":\n            makefile_option.append(entry)\n        else:\n            flags += entry[key] + \" \"\n    makefile_option.append({\"F90FLAGS\": flags})\n\n    namelist = create_input(namelist_option, \"namelist\",\n                            template_location=\"templates\")\n\n    makefile_include = create_input(makefile_option, \"Makefile.include\",\n                                    template_location=\"templates\")\n\n    benchmark_base = \"shallow\"\n\n    # save the input files in the appropriate place\n    location = benchmark_base + \"/original/namelist\"\n    my_file = open(location, 'w')\n    my_file.write(namelist)\n    my_file.flush()\n\n    location = benchmark_base + \"/common/Makefile.include\"\n    my_file = open(location, 'w')\n    my_file.write(makefile_include)\n    my_file.flush()\n\n    # compile shallow if required\n    base_path = benchmark_base + \"/original\"\n    import subprocess\n    make_process = subprocess.Popen([\"make\", \"clean\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    make_process = subprocess.Popen([\"make\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    # run shallow\n    make_process = subprocess.Popen([\"./shallow_base\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n    # _ = make_process.stderr.read()\n    stdout = make_process.stdout.read()\n\n    # determine if the results are correct. We will need to look at\n    # the results from stdout but for the moment we assume they are\n    # correct\n\n    # extract the required outputs\n    for line in stdout.split(\"\\n\"):\n        if \"Time-stepping\" in line:\n            total_time = line.split()[2]\n\n    return True, total_time"}
{"prompt": "def _read_next(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901\n    \"\"\"Read the next full form from the input stream.\"\"\"", "output": "\n    start_pos = ctx.pos\n    form = ctx.read_form()\n    if form is None:\n        raise LispReaderError(f\"Unexpected end of input at position {start_pos}\")\n    return form\n\n\ndef _read_next_form(ctx: ReaderContext) -> LispReaderForm:\n    \"\"\"Read the next form from the input stream, including comments.\"\"\"\n    form = _read_next(ctx)\n    while form.is_comment:\n        form = _read_next(ctx)\n    return form\n\n\ndef _read_next_form_list(ctx: ReaderContext", "solution": "\n    reader = ctx.reader\n    token = reader.peek()\n    if token == \"(\":\n        return _read_list(ctx)\n    elif token == \"[\":\n        return _read_vector(ctx)\n    elif token == \"{\":\n        return _read_map(ctx)\n    elif begin_num_chars.match(token):\n        return _read_num(ctx)\n    elif whitespace_chars.match(token):\n        reader.next_token()\n        return _read_next(ctx)\n    elif token == \":\":\n        return _read_kw(ctx)\n    elif token == '\"':\n        return _read_str(ctx)\n    elif token == \"'\":\n        return _read_quoted(ctx)\n    elif token == \"\\\\\":\n        return _read_character(ctx)\n    elif ns_name_chars.match(token):\n        return _read_sym(ctx)\n    elif token == \"#\":\n        return _read_reader_macro(ctx)\n    elif token == \"^\":\n        return _read_meta(ctx)  # type: ignore\n    elif token == \";\":\n        return _read_comment(ctx)\n    elif token == \"`\":\n        return _read_syntax_quoted(ctx)\n    elif token == \"~\":\n        return _read_unquote(ctx)\n    elif token == \"@\":\n        return _read_deref(ctx)\n    elif token == \"\":\n        return ctx.eof\n    else:\n        raise SyntaxError(\"Unexpected token '{token}'\".format(token=token))"}
{"prompt": "def __tomo_linear_inv(freqs, ops, weights=None, trace=None):\n    \"\"\"\n    Reconstruct a matrix through linear inversion.\n\n    Args:\n        freqs (list[float]): list of observed frequences.\n        ops (list[np.array]): list of corresponding projectors.\n        weights (list[float] or array_like):\n            weights to be used for weighted fitting.\n        trace (float or None): trace of returned operator.\n\n    Returns:\n        numpy.array: A numpy array of the reconstructed operator.\n    \"\"\"", "output": "\n   _ops =_ops_from_freqs(freqs,_ops=ops)\n   _weights =_weights_from_freqs(freqs,_weights=weights)\n   _trace =_trace_from_freqs(freqs,_trace=trace)\n\n   _ops =_ops.reshape(-1, 2, 2)\n   _weights =_weights.reshape(-1, 2, 2)\n\n   _ops_inv = np.linalg.inv(np.matmul(np.matmul(", "solution": "\n    # get weights matrix\n    if weights is not None:\n        W = np.array(weights)\n        if W.ndim == 1:\n            W = np.diag(W)\n\n    # Get basis S matrix\n    S = np.array([vectorize(m).conj()\n                  for m in ops]).reshape(len(ops), ops[0].size)\n    if weights is not None:\n        S = np.dot(W, S)  # W.S\n\n    # get frequencies vec\n    v = np.array(freqs)  # |f>\n    if weights is not None:\n        v = np.dot(W, freqs)  # W.|f>\n    Sdg = S.T.conj()  # S^*.W^*\n    inv = np.linalg.pinv(np.dot(Sdg, S))  # (S^*.W^*.W.S)^-1\n\n    # linear inversion of freqs\n    ret = devectorize(np.dot(inv, np.dot(Sdg, v)))\n    # renormalize to input trace value\n    if trace is not None:\n        ret = trace * ret / np.trace(ret)\n    return ret"}
{"prompt": "def check_upload_status(self, video_id):\n        \"\"\"\n        Checks the video upload status\n        Newly uploaded videos may be in the processing state\n\n        Authentication is required\n\n        Returns:\n            True if video is available\n            otherwise a dict containes upload_state and detailed message\n            i.e. {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n        \"\"\"", "output": "\n        url = f\"{self.base_url}/videos/{video_id}\"\n        response = self.session.get(url)\n        response.raise_for_status()\n        data = response.json()\n\n        if data[\"upload_state\"] == \"processing\":\n            return {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n        elif data[\"upload_state\"] == \"completed\":\n            return True\n        else:\n            return {\"upload_state\": data[\"upload_state\"], \"detailed_message\": data[\"detailed_message\"]}\n\n    def get_video_info(self, video_id", "solution": "\n        # Raise ApiError if not authenticated\n        if not self.authenticated:\n            raise ApiError(_(\"Authentication is required\"))\n\n        entry = self.fetch_video(video_id)\n        upload_status = Api.yt_service.CheckUploadStatus(entry)\n\n        if upload_status is not None:\n            video_upload_state = upload_status[0]\n            detailed_message = upload_status[1]\n            return {\"upload_state\": video_upload_state, \"detailed_message\": detailed_message}\n        else:\n            return True"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        if self.farthest < status.farthest:\n            self.farthest = status.farthest\n        if self.expected is None:\n            self.expected = status.expected\n        if status.failure_message is not None:\n            self.failure_message = status.failure_message\n        return self\n\n    def __str__(self) -> str:\n        return f\"Status(farthest={self.farthest}, expected={self.expected}, failure_message={self.failure_message})\"\n\n\nclass StatusList(Generic[Input, Output]):\n    \"\"\"A list of statuses", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def stream(self, id, task, type, follow=False, offset=0, origin=\"start\", plain=False):\n        \"\"\" This endpoint streams a task's stderr/stdout logs.\n\n            https://www.nomadproject.io/api/client.html#stream-logs\n\n            arguments:\n              - id: (str) allocation_id required\n              - task: (str) name of the task inside the allocation to stream logs from\n              - type: (str) Specifies the stream to stream. Either \"stderr|stdout\"\n              - follow: (bool) default false\n              - offset: (int) default 0\n              - origin: (str) either start|end, default \"start\"\n              - plain: (bool) Return just the plain text without framing. default False\n            returns: (str) text\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.BadRequestNomadException\n        \"\"\"", "output": " 0\n              - origin: (str) default \"start\"\n              - plain: (bool) default false\n\n            returns:\n              - logs: (list) list of log lines as strings\n        \"\"\"\n        url = f\"/v1/allocations/{id}/tasks/{task}/logs/{type}\"\n        params = {\n            \"follow\": follow,\n            \"offset\": offset,\n            \"origin\": origin,\n            \"plain\": plain,\n        }\n        response = self._request(\"GET\", url, params=params)\n        return response.json()\n\n    def logs(self, id, task, type, follow=False,", "solution": "\n        params = {\n            \"task\": task,\n            \"type\": type,\n            \"follow\": follow,\n            \"offset\": offset,\n            \"origin\": origin,\n            \"plain\": plain\n        }\n        return self.request(id, params=params, method=\"get\").text"}
{"prompt": "def _map_segmentation_mask_to_stft_domain(mask, times, frequencies, stft_times, stft_frequencies):\n    \"\"\"\n    Maps the given `mask`, which is in domain (`frequencies`, `times`) to the new domain (`stft_frequencies`, `stft_times`)\n    and returns the result.\n    \"\"\"", "output": "\n   _mask =_mask_from_segmentation_mask(mask, frequencies, times)\n   _stft_mask =_stft_mask_from_segmentation_mask(mask, stft_frequencies, stft_times)\n   _mask =_mask_to_stft_domain(melspectrogram_from_mask(melspectrogram_from_mask(melspectrogram_from_mask(melspectrogram_from_mask(melspectrogram_from_mask(melspectrogram_from_mask(melspect", "solution": "\n    assert mask.shape == (frequencies.shape[0], times.shape[0]), \"Times is shape {} and frequencies is shape {}, but mask is shaped {}\".format(\n        times.shape, frequencies.shape, mask.shape\n    )\n    result = np.zeros((stft_frequencies.shape[0], stft_times.shape[0]))\n\n    if len(stft_times) > len(times):\n        all_j = [j for j in range(len(stft_times))]\n        idxs  = [int(i) for i in np.linspace(0, len(times) - 1, num=len(stft_times))]\n        all_i = [all_j[idx] for idx in idxs]\n    else:\n        all_i = [i for i in range(len(times))]\n        idxs  = [int(i) for i in np.linspace(0, len(stft_times) - 1, num=len(times))]\n        all_j = [all_i[idx] for idx in idxs]\n\n    for i, j in zip(all_i, all_j):\n        result[:, j] = np.interp(stft_frequencies, frequencies, mask[:, i])\n\n    return result"}
{"prompt": "def sync_labels(self, repo):\n        \"\"\"Creates a local map of github labels/milestones to asana tags.\"\"\"", "output": "\n        labels = repo.get_labels()\n        asana_tags = {}\n        for label in labels:\n            asana_tags[label.name] = label.name\n        return asana_tags\n\n    def create_asana_tags(self, repo, asana_tags):\n        \"\"\"Creates asana tags for the given repo.\"\"\"\n        for tag_name, asana_tag_name in asana_tags.items():\n            if asana_tag_name not in repo.get_tags():\n                repo.create_tag(tag_name, tag_name, message=tag_name)", "solution": "\n\n        logging.info(\"syncing new github.com labels to tags\")\n\n        # create label tag map\n        ltm = self.app.data.get(\"label-tag-map\", {})\n\n        # loop over labels, if they don't have tags, make them\n        for label in repo.get_labels():\n            tag_id = ltm.get(label.name, None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=label.name,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % label.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", label.name, tag['id'])\n                ltm[label.name] = tag['id']\n\n        # loop over milestones, if they don't have tags, make them\n        for ms in repo.get_milestones(state=\"all\"):\n            tag_id = ltm.get(_ms_label(ms.id), None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=ms.title,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % ms.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", ms.title, tag['id'])\n                ltm[_ms_label(ms.id)] = tag['id']\n\n        self.app.data['label-tag-map'] = ltm\n        return ltm"}
{"prompt": "def recurse(self, k, wait=False, wait_index=None, timeout='5m'):\n        \"\"\"Recursively get the tree below the given key\"\"\"", "output": "\n        if wait:\n            if wait_index is None:\n                raise ValueError(\"wait_index must be provided if wait=True\")\n            if wait_index >= len(self.children):\n                raise ValueError(\"wait_index must be less than the number of children\")\n            if self.children[wait_index].key == k:\n                return self.children[wait_index]\n            else:\n                return self.children[wait_index].recurse(k, wait, wait_index+1, timeout)\n        else:\n            for child in self.children:\n                if child.key == k:\n                    return", "solution": "\n        k = k.lstrip('/')\n        url = '{}/{}'.format(self.endpoint, k)\n        params = {}\n        params['recurse'] = 'true'\n        if wait:\n            params['wait'] = timeout\n            if not wait_index:\n                params['index'] = self.index(k, recursive=True)\n            else:\n                params['index'] = wait_index\n        r = requests.get(url, params=params)\n        if r.status_code == 404:\n            raise KeyDoesNotExist(\"Key \" + k + \" does not exist\")\n        if r.status_code != 200:\n            raise KVStoreError('GET returned {}'.format(r.status_code))\n        entries = {} \n        for e in r.json():\n            if e['Value']:\n                entries[e['Key']] = base64.b64decode(e['Value'])\n            else:\n                entries[e['Key']] = ''\n        return entries"}
{"prompt": "def delayed(f):\n    \"\"\"Decorator to transparantly accept delayed computation.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,\n    >>>                   shape=4, delay=True)\n    >>> @vaex.delayed\n    >>> def total_sum(sums):\n    >>>     return sums.sum()\n    >>> sum_of_sums = total_sum(delayed_sum)\n    >>> ds.execute()\n    >>> sum_of_sums.get()\n    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations\n    \"\"\"", "output": "io/en/latest/tutorial/delayed_computation.html\n    \"\"\"\n   _delayed =_delayed_computation(f)\n   _delayed.__name__ = f.__name__\n    return_delayed\n\n\ndef_delayed_computation(f):\n    def wrapper(*args, **kwargs):\n       _delayed =_delayed_computation(f)\n       _delayed(*args, **kwargs)\n       _delayed.delay()\n        return_delayed\n    return wrapper\n\n\ndef_delayed_computation(f):\n    def wrapper(*args, **kwargs):\n       _delayed =", "solution": "\n\n    def wrapped(*args, **kwargs):\n        # print \"calling\", f, \"with\", kwargs\n        # key_values = kwargs.items()\n        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])\n        # key_promise = [(key, promisify(value)) for key, value in key_values]\n        arg_promises = list([promisify(value) for value in args])\n        kwarg_promises = list([promise for key, promise in key_promise])\n        promises = arg_promises + kwarg_promises\n        for promise in promises:\n            def echo_error(exc, promise=promise):\n                print(\"error with \", promise, \"exception is\", exc)\n                # raise exc\n\n            def echo(value, promise=promise):\n                print(\"done with \", repr(promise), \"value is\", value)\n            # promise.then(echo, echo_error)\n\n        # print promises\n        allarguments = aplus.listPromise(*promises)\n\n        def call(_):\n            kwargs_real = {key: promise.get() for key, promise in key_promise}\n            args_real = list([promise.get() for promise in arg_promises])\n            return f(*args_real, **kwargs_real)\n\n        def error(exc):\n            print(\"error\", exc)\n            raise exc\n        return allarguments.then(call, error)\n    return wrapped"}
{"prompt": "def read(cls, path, sc=None, min_partitions=1, bigdl_type=\"float\"):\n        \"\"\"\n        Read images as Image Frame\n        if sc is defined, Read image as DistributedImageFrame from local file system or HDFS\n        if sc is null, Read image as LocalImageFrame from local file system\n        :param path path to read images\n        if sc is defined, path can be local or HDFS. Wildcard character are supported.\n        if sc is null, path is local directory/image file/image file with wildcard character\n        :param sc SparkContext\n        :param min_partitions A suggestion value of the minimal splitting number for input data.\n        :return ImageFrame\n        \"\"\"", "output": " value for the number of partitions in the resulting DataFrame.\n        :param bigdl_type float or double\n        :return ImageFrame\n        \"\"\"\n        if sc is None:\n            return cls.readLocal(path, min_partitions, bigdl_type)\n        else:\n            return cls.readDistributed(path, sc, min_partitions, bigdl_type)\n\n    @staticmethod\n    def readLocal(path, min_partitions, bigdl_type):\n        \"\"\"\n        Read images as Image Frame from local file system\n        :param path path to read images\n        :param min_partitions A suggestion value for the number of partitions", "solution": "\n        return ImageFrame(jvalue=callBigDlFunc(bigdl_type, \"read\", path, sc, min_partitions))"}
{"prompt": "def _is_package(path):\n    \"\"\"\n    \u5224\u65ad\u4f20\u5165\u7684\u8def\u5f84\u662f\u5426\u4e3a\u4e00\u4e2a Python \u6a21\u5757\u5305\n\n    :param str path: \u5f85\u5224\u65ad\u7684\u8def\u5f84\n    :return: \u8fd4\u56de\u662f\uff0c\u5219\u4f20\u5165 path \u4e3a\u4e00\u4e2a Python \u5305\uff0c\u5426\u5219\u4e0d\u662f\n    :rtype: bool\n    \"\"\"", "output": "\n    if not os.path.isdir(path):\n        return False\n\n    if not os.path.exists(os.path.join(path, '__init__.py')):\n        return False\n\n    return True\n\n\ndef _get_package_name(path):\n    \"\"\"\n    \u83b7\u53d6\u4f20\u5165\u8def\u5f84\u4e0b\u7684 Python \u6a21\u5757\u5305\u540d\n\n    :param str path: \u5f85\u83b7\u53d6\u5305\u540d\u7684\u8def\u5f84\n    :return: \u8fd4\u56de\u5305\u540d\n    :rtype: str\n    \"\"\"\n    if not _is_package(path):\n        raise ValueError('The path is not a Python package.')\n\n    return os.path.basename(path)\n\n\ndef", "solution": "\n    def _exists(s):\n        return os.path.exists(os.path.join(path, s))\n\n    return (\n        os.path.isdir(path) and\n        (_exists('__init__.py') or _exists('__init__.pyc'))\n    )"}
{"prompt": "def get_settings(self, client_name=None):\n    \"\"\"get all settings, either for a particular client if a name is provided,\n       or across clients.\n\n       Parameters\n       ==========\n       client_name: the client name to return settings for (optional)\n\n    \"\"\"", "output": "\n    if client_name:\n        return self.client_settings.get(client_name, {})\n    else:\n        return self.client_settings\n\ndef set_settings(self, client_name, settings):\n    \"\"\"set all settings for a particular client.\n\n       Parameters\n       ==========\n       client_name: the client name to set settings for\n       settings: a dictionary of settings to set\n\n    \"\"\"\n    self.client_settings[client_name] = settings\n\ndef get_setting(self, client_name, setting_name, default=None):\n    \"\"\"get a setting for a particular client.\n\n      ", "solution": "\n    settings = read_client_secrets()\n    if client_name is not None and client_name in settings:\n        return settings[client_name]           \n    return settings"}
{"prompt": "def _evolve(self, state, qargs=None):\n        \"\"\"Evolve a quantum state by the operator.\n\n        Args:\n            state (QuantumState): The input statevector or density matrix.\n            qargs (list): a list of QuantumState subsystem positions to apply\n                           the operator on.\n\n        Returns:\n            QuantumState: the output quantum state.\n\n        Raises:\n            QiskitError: if the operator dimension does not match the\n            specified QuantumState subsystem dimensions.\n        \"\"\"", "output": "\n        if qargs is None:\n            qargs = range(len(self.subsystems))\n\n        if not all(qarg in range(len(self.subsystems)) for qarg in qargs):\n            raise QiskitError(\"Invalid qargs: {}\".format(qargs))\n\n       _state = state.copy()\n       _state.evolve(self.operator, qargs)\n        return_state\n\n    def __call__(self, state, qargs=None):\n        \"\"\"Apply the operator to a quantum state.\n\n        Args:\n            state (QuantumState): The input statevector or", "solution": "\n        state = self._format_state(state)\n        if qargs is None:\n            if state.shape[0] != self._input_dim:\n                raise QiskitError(\n                    \"Operator input dimension is not equal to state dimension.\"\n                )\n            if state.ndim == 1:\n                # Return evolved statevector\n                return np.dot(self.data, state)\n            # Return evolved density matrix\n            return np.dot(\n                np.dot(self.data, state), np.transpose(np.conj(self.data)))\n        # Subsystem evolution\n        return self._evolve_subsystem(state, qargs)"}
{"prompt": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the CreateKeyPair response payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data buffer containing encoded object\n                data, supporting a read method.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            InvalidKmipEncoding: Raised if the private key unique identifier or\n                the public key unique identifier is missing from the encoded\n                payload.\n        \"\"\"", "output": " identifier\n                is not present in the encoded data.\n\n        Returns:\n            CreateKeyPairResponse: A CreateKeyPairResponse object containing\n                the decoded data.\n        \"\"\"\n        self._kmip_version = kmip_version\n\n        # Read the status\n        self._read_status(input_buffer)\n\n        # Read the unique identifier\n        self._read_unique_identifier(input_buffer)\n\n        # Read the private key data\n        self._read_private_key_data(input_buffer)\n\n        # Read the public key data\n        self._read_public_key_data(input_buffer)\n\n        # Read", "solution": "\n        super(CreateKeyPairResponsePayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._private_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER\n            )\n            self._private_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"private key unique identifier.\"\n            )\n\n        if self.is_tag_next(\n                enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._public_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER\n            )\n            self._public_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"public key unique identifier.\"\n            )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._private_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._private_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._public_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._public_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n        self.is_oversized(local_buffer)"}
{"prompt": "def validate(self):\n        \"\"\"\n        Error check the attributes of the ActivateRequestPayload object.\n        \"\"\"", "output": "\n        if not self.activation_code:\n            raise ValueError(\"Activation code is required.\")\n        if not self.email:\n            raise ValueError(\"Email is required.\")\n        if not self.password:\n            raise ValueError(\"Password is required.\")\n        if not self.first_name:\n            raise ValueError(\"First name is required.\")\n        if not self.last_name:\n            raise ValueError(\"Last name is required.\")\n        if not self.phone_number:\n            raise ValueError(\"Phone number is required.\")\n        if not self.address:\n            raise ValueError(\"Address is required.\")\n        if not self.city:\n            raise ValueError(\"", "solution": "\n        if self.unique_identifier is not None:\n            if not isinstance(self.unique_identifier,\n                              attributes.UniqueIdentifier):\n                msg = \"invalid unique identifier\"\n                raise TypeError(msg)\n        if self.compromise_occurrence_date is not None:\n            if not isinstance(self.compromise_occurrence_date,\n                              primitives.DateTime):\n                msg = \"invalid compromise time\"\n                raise TypeError(msg)\n        if not isinstance(self.revocation_reason, objects.RevocationReason):\n            msg = \"invalid revocation reason\"\n            raise TypeError(msg)"}
{"prompt": "def receive_ack_renewing(self, pkt):\n        \"\"\"Receive ACK in RENEWING state.\"\"\"", "output": "\n        if_debug:\n            print(\"Received ACK in RENEWING state\")\n        if_debug_:\n            print(\"ACK: \", pkt.ack)\n        if pkt.ack == self.ack_num:\n            if self.state ==_states.ACK_:\n                self.state = user_states.ACK_\n                self.ack_num += 1\n                if self.ack_num == self.ack_limit:\n                    self.state = user_states.ACK__complete\n                    self.ack_num = 0\n                    self.ack_limit = 0", "solution": "\n        logger.debug(\"C3. Received ACK?, in RENEWING state.\")\n        if self.process_received_ack(pkt):\n            logger.debug(\"C3: T. Received ACK, in RENEWING state, \"\n                         \"raise BOUND.\")\n            raise self.BOUND()"}
{"prompt": "def _get_server_info_from_logs(self):\n        \"\"\"\n        Check server's output log, and determine its scheme / IP / port (helper method for `._launch_server()`).\n\n        This method is polled during process startup. It looks at the server output log and checks for a presence of\n        a particular string (\"INFO: Open H2O Flow in your web browser:\") which indicates that the server is\n        up-and-running. If the method detects this string, it extracts the server's scheme, ip and port and returns\n        them; otherwise it returns None.\n\n        :returns: (scheme, ip, port) tuple if the server has already started, None otherwise.\n        \"\"\"", "output": " A tuple containing the server's scheme, ip and port, or None if the server is not up-and-running.\n        \"\"\"\n        # Check if the server is up-and-running\n        if self._server_log_file is None:\n            return None\n\n        # Read the server output log\n        with open(self._server_log_file, 'r') as f:\n            log_lines = f.readlines()\n\n        # Look for the \"INFO: Open H2O Flow in your web browser:\" string\n        for line in log_lines:\n            if \"INFO: Open H2O Flow in your web", "solution": "\n        searchstr = \"INFO: Open H2O Flow in your web browser:\"\n        with open(self._stdout, \"rt\") as f:\n            for line in f:\n                if searchstr in line:\n                    url = line[line.index(searchstr) + len(searchstr):].strip().rstrip(\"/\")\n                    parts = url.split(\":\")\n                    assert len(parts) == 3 and (parts[0] == \"http\" or parts[1] == \"https\") and parts[2].isdigit(), \\\n                        \"Unexpected URL: %s\" % url\n                    return parts[0], parts[1][2:], int(parts[2])\n        return None"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        = f(*args, **kwargs)\n        if is not None:\n          .pos = args[0].pos\n        return f(*args, **kwargs)\n    return wrapper\n\n\nclass Node:\n    def __init__(self, label, children=None,=None,_=None,__=None,_ utm=None,_ utm_=None,_ utm_z=None,_ utm_z_=None,_ utm_z", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def comparison_stats(df, els=['Mg', 'Sr', 'Ba', 'Al', 'Mn']):\n    \"\"\"\n    Compute comparison stats for test and LAtools data.\n    \n    Population-level similarity assessed by a Kolmogorov-Smirnov test.\n    \n    Individual similarity assessed by a pairwise Wilcoxon signed rank test.\n    \n    Trends in residuals assessed by regression analysis, where significance of\n    the slope and intercept is determined by t-tests (both relative to zero).\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataframe containing reference ('X/Ca_r'), test user \n        ('X/Ca_t') and LAtools ('X123') data.\n    els : list\n        list of elements (names only) to plot.\n    \n    Returns\n    -------\n    pandas.DataFrame\n    \n    \"\"\"", "output": "') and ('Y')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')')", "solution": "\n    \n    # get corresponding analyte and ratio names\n    As = []\n    Rs = []\n    analytes = [c for c in df.columns if ('_r' not in c) and ('_t' not in c)]\n    ratios = [c for c in df.columns if ('_r' in c)]\n\n    for e in els:\n        if e == 'Sr':\n            As.append('Sr88')\n        elif e == 'Mg':\n            As.append('Mg24')\n        else:\n            As.append([a for a in analytes if e in a][0])\n        Rs.append([r for r in ratios if e in r][0][:-2])\n        \n    yt_stats = []\n    yl_stats = []\n    \n    for i, (e, a) in enumerate(zip(Rs, As)):\n        if a == 'Ba138':\n            m = 1e3\n            u = '$\\mu$mol/mol'\n        else:\n            m = 1\n            u = 'mmol/mol'\n        \n        x = df.loc[:, e + '_r'].values * m\n        yt = df.loc[:, e + '_t'].values * m\n        yl = df.loc[:, a].values * m\n        \n        yt_stats.append(summary_stats(x, yt, e))\n        yl_stats.append(summary_stats(x, yl, e))\n    \n    yt_stats = pd.concat(yt_stats).T\n    yl_stats = pd.concat(yl_stats).T\n    \n    return pd.concat([yt_stats, yl_stats], keys=['Test User', 'LAtools']).T"}
{"prompt": "def get_pk_value_on_save(self, instance):\n        \"\"\"Generate ID if required.\"\"\"", "output": "\n        if not instance.pk:\n            instance.pk = self.generate_pk(instance)\n        return instance.pk\n\n    def generate_pk(self, instance):\n        \"\"\"Generate a unique ID for the given instance.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n\n    def get_pk_value_on_update(self, instance, old_pk):\n        \"\"\"Generate a new ID if the old ID is not available.\"\"\"\n        if not instance.pk:\n            instance.pk = self.generate_pk(instance)\n        return instance.pk\n\n    def get_pk_value_on_delete(", "solution": "\n        value = super(AleaIdField, self).get_pk_value_on_save(instance)\n        if not value:\n            value = self.get_seeded_value(instance)\n        return value"}
{"prompt": "def _send(self, data, content_type):\n        \"\"\"\n        Sends a GCM message with the given content type\n        \"\"\"", "output": "\n        headers = {'Authorization': 'key={}'.format(self.api_key), 'Content-Type': content_type}\n        response = requests.post(self.url, data=data, headers=headers)\n        if response.status_code!= 200:\n            raise Exception('GCM message sending failed: {}'.format(response.text))\n\n    def send_message(self, registration_ids, data, content_type='application/json'):\n        \"\"\"\n        Sends a GCM message to the given registration IDs with the given data and content type\n        \"\"\"\n        data = json.dumps(data)", "solution": "\n\n        headers = {\n            \"Content-Type\": content_type,\n            \"Authorization\": \"key=%s\" % (self.api_key),\n            \"Content-Length\": str(len(data))\n        }\n\n        request = Request(self.api_url, data, headers)\n        return urlopen(request).read().decode(self.encoding)"}
{"prompt": "def make_clean_visible(_html, tag_replacement_char=' '):\n    \"\"\"\n    Takes an HTML-like Unicode string as input and returns a UTF-8\n    encoded string with all tags replaced by whitespace. In particular,\n    all Unicode characters inside HTML are replaced with a single\n    whitespace character.\n\n    This does not detect comments, style, script, link.  It also does\n    do anything with HTML-escaped characters.  All of these are\n    handled by the clean_html pre-cursor step.\n\n    Pre-existing whitespace of any kind (newlines, tabs) is converted\n    to single spaces ' ', which has the same byte length (and\n    character length).\n\n    This is a simple state machine iterator without regexes\n    \"\"\"", "output": " and all other whitespace is removed.\n\n    Args:\n        _html (str): The HTML-like Unicode string to clean.\n        tag_replacement_char (str): The character to replace tags with.\n            Defaults to a single space.\n\n    Returns:\n        str: The cleaned HTML-like Unicode string.\n    \"\"\"\n    # Convert the input string to a list of Unicode characters\n    char_list = list(_html)\n\n    # Iterate over the list of characters\n    for i, char in enumerate(char_list):\n        # Check if the character is a tag\n        if char in ('<', '>'):\n            # Replace the tag", "solution": "\n    def non_tag_chars(html):\n        n = 0\n        while n < len(html):\n            angle = html.find('<', n)\n            if angle == -1:\n                yield html[n:]\n                n = len(html)\n                break\n            yield html[n:angle]\n            n = angle\n\n            while n < len(html):\n                nl = html.find('\\n', n)\n                angle = html.find('>', n)\n                if angle == -1:\n                    yield ' ' * (len(html) - n)\n                    n = len(html)\n                    break\n                elif nl == -1 or angle < nl:\n                    yield ' ' * (angle + 1 - n)\n                    n = angle + 1\n                    break\n                else:\n                    yield ' ' * (nl - n) + '\\n'\n                    n = nl + 1\n                    # do not break\n\n    if not isinstance(_html, unicode):\n        _html = unicode(_html, 'utf-8')\n\n    # Protect emails by substituting with unique key\n    _html = fix_emails(_html)\n\n    #Strip tags with previous logic\n    non_tag = ''.join(non_tag_chars(_html))\n\n    return non_tag.encode('utf-8')"}
{"prompt": "def match(self, request):\n        \"\"\"\n        Matches a given Request instance contract against the registered mocks.\n\n        If a mock passes all the matchers, its response will be returned.\n\n        Arguments:\n            request (pook.Request): Request contract to match.\n\n        Raises:\n            pook.PookNoMatches: if networking is disabled and no mock matches\n                with the given request contract.\n\n        Returns:\n            pook.Response: the mock response to be used by the interceptor.\n        \"\"\"", "output": "\n        if not self.enabled:\n            raise PookNoMatches(\"Networking is disabled.\")\n\n        for mock in self.mocks:\n            if mock.match(request):\n                return mock.response\n\n        raise PookNoMatches(\"No mock matches the given request contract.\")\n\n    def add_mock(self, mock):\n        \"\"\"\n        Adds a mock to the list of registered mocks.\n\n        Arguments:\n            mock (pook.Mock): Mock to add.\n        \"\"\"\n        self.mocks.append(mock)\n\n    def remove_mock(self, mock):\n        \"\"\"\n        Removes a mock from the list of registered mocks.\n\n        Arguments", "solution": "\n        # Trigger engine-level request filters\n        for test in self.filters:\n            if not test(request, self):\n                return False\n\n        # Trigger engine-level request mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Store list of mock matching errors for further debugging\n        match_errors = []\n\n        # Try to match the request against registered mock definitions\n        for mock in self.mocks[:]:\n            try:\n                # Return the first matched HTTP request mock\n                matches, errors = mock.match(request.copy())\n                if len(errors):\n                    match_errors += errors\n                if matches:\n                    return mock\n            except PookExpiredMock:\n                # Remove the mock if already expired\n                self.mocks.remove(mock)\n\n        # Validate that we have a mock\n        if not self.should_use_network(request):\n            msg = 'pook error!\\n\\n'\n\n            msg += (\n                '=> Cannot match any mock for the '\n                'following request:\\n{}'.format(request)\n            )\n\n            # Compose unmatch error details, if debug mode is enabled\n            if self.debug:\n                err = '\\n\\n'.join([str(err) for err in match_errors])\n                if err:\n                    msg += '\\n\\n=> Detailed matching errors:\\n{}\\n'.format(err)\n\n            # Raise no matches exception\n            raise PookNoMatches(msg)\n\n        # Register unmatched request\n        self.unmatched_reqs.append(request)"}
{"prompt": "def discrete_best_alpha(data, alpharangemults=(0.9,1.1), n_alpha=201, approximate=True, verbose=True):\n    \"\"\"\n    Use the maximum L to determine the most likely value of alpha\n\n    *alpharangemults* [ 2-tuple ]\n        Pair of values indicating multiplicative factors above and below the\n        approximate alpha from the MLE alpha to use when determining the\n        \"exact\" alpha (by directly maximizing the likelihood function)\n    \"\"\"", "output": "\n   _alpha =_alpha_from_data(data, approximate=approximate, verbose=verbose)\n   _alpha_ =_alpha_from_data(data,_alpha, approximate=approximate, verbose=verbose)\n   _alpha_neg =_alpha_from_data(data,_alpha*alpharangemults[0], approximate=approximate, verbose=verbose)\n   _alpha_pos =_alpha_from_data(data,_alpha*alpharangemults[1], approximate=approximate, verbose=verbose)", "solution": "\n\n    xmins = np.unique(data)\n    if approximate:\n        alpha_of_xmin = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n    else:\n        alpha_approx = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n        alpharanges = [(0.9*a,1.1*a) for a in alpha_approx]\n        alpha_of_xmin = [ most_likely_alpha(data,xmin,alpharange=ar,n_alpha=n_alpha) for xmin,ar in zip(xmins,alpharanges) ]\n    ksvalues = [ discrete_ksD(data, xmin, alpha) for xmin,alpha in zip(xmins,alpha_of_xmin) ]\n\n    best_index = argmin(ksvalues)\n    best_alpha = alpha_of_xmin[best_index]\n    best_xmin = xmins[best_index]\n    best_ks = ksvalues[best_index]\n    best_likelihood = discrete_likelihood(data, best_xmin, best_alpha)\n\n    if verbose:\n        print(\"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % (\n                best_alpha, best_xmin, best_ks, best_likelihood,\n                (data<best_xmin).sum(), (data>=best_xmin).sum()))\n\n    return best_alpha,best_xmin,best_ks,best_likelihood"}
{"prompt": "def _convert_date(date_string, date_format):\n    \"\"\"\n    Convert a date in a given format to epoch time. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"", "output": "\n    try:\n        return int(datetime.datetime.strptime(date_string, date_format).timestamp())\n    except ValueError:\n        return None\n\n\ndef _convert_datetime(date_string, date_format):\n    \"\"\"\n    Convert a date in a given format to a datetime object. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"\n    try:\n        return datetime.datetime.strptime(date_string, date_format)\n    except ValueError:\n        return None\n\n\ndef _convert_time(time_string, time_format):\n    \"\"\"\n    Convert a time in a given format to epoch", "solution": "\n    if date_format != 'epoch':\n        return datetime.strptime(date_string, date_format).timestamp()\n    else:\n        return float(date_string)"}
{"prompt": "def create_session(token=None):\n    \"\"\"\n    Create a github3.py session connected to GitHub.com\n\n    If token is not provided, will attempt to use the GITHUB_API_TOKEN\n    environment variable if present.\n    \"\"\"", "output": "\n    if token is None:\n        token = os.environ.get('GITHUB_API_TOKEN')\n\n    if token is None:\n        raise ValueError('No token provided and GITHUB_API_TOKEN not set')\n\n    session = github3.login(token=token)\n    return session\n\n\ndef get_repo(session, repo_name):\n    \"\"\"\n    Get a repository object from the session\n\n    :param session: A github3.py session\n    :param repo_name: The name of the repository to get\n    :return: A github3.py repository object\n    \"\"\"\n    repo = session.repository(repo", "solution": "\n    if token is None:\n        token = os.environ.get('GITHUB_API_TOKEN', None)\n\n    gh_session = github3.login(token=token)\n\n    if gh_session is None:\n        raise RuntimeError('Invalid or missing GITHUB_API_TOKEN')\n\n    return gh_session"}
{"prompt": "def load(self, source):\n        \"\"\"\n        Opens the source file.\n        \"\"\"", "output": "\n        self.source = open(source, 'r')\n\n    def close(self):\n        \"\"\"\n        Closes the source file.\n        \"\"\"\n        self.source.close()\n\n    def read(self):\n        \"\"\"\n        Reads the source file and returns its contents as a string.\n        \"\"\"\n        return self.source.read()\n\n    def readline(self):\n        \"\"\"\n        Reads the next line from the source file and returns it as a string.\n        \"\"\"\n        return self.source.readline()\n\n    def readlines(self):\n        \"\"\"\n        Reads all lines from the source file and returns them as a list of strings.\n       ", "solution": "\n        self.source = open(self.source, 'rb')    \n        self.loaded = True"}
{"prompt": "def unregisterObserver(self, observer):\n        \"\"\" Remove an observer from the meter update() chain.\n\n        Args:\n            observer (MeterObserver): Subclassed MeterObserver.\n        \"\"\"", "output": "\n        if observer in self._observers:\n            self._observers.remove(observer)\n\n    def update(self,_batch, target_batch=None):\n        \"\"\" Update the meter with a new batch of data.\n\n        Args:\n            input_batch (Tensor): A batch of input data.\n            target_batch (Tensor, optional): A batch of target data.\n        \"\"\"\n        for observer in self._observers:\n            observer.update(self, input_batch, target_batch)\n", "solution": "\n        if observer in self.m_observers:\n            self.m_observers.remove(observer)\n        pass"}
{"prompt": "def query(self, minhash, size):\n        \"\"\"\n        Giving the MinHash and size of the query set, retrieve\n        keys that references sets with containment with respect to\n        the query set greater than the threshold.\n\n        Args:\n            minhash (datasketch.MinHash): The MinHash of the query set.\n            size (int): The size (number of unique items) of the query set.\n\n        Returns:\n            `iterator` of keys.\n        \"\"\"", "output": "\n       b = self.threshold\n       a = 1 - self.threshold\n       b__size = size * (1 - self.threshold)\n\n       __size = size * self.threshold\n\n       __size__size = size * (1 - self.threshold)\n\n       _size_ = size * (1 - self.threshold)\n\n       _size = size * self.threshold\n\n       _size_ = size * (1 - self.threshold)\n\n       _size_ = size * (1 - self.threshold)\n\n       _size_ = size", "solution": "\n        for i, index in enumerate(self.indexes):\n            u = self.uppers[i]\n            if u is None:\n                continue\n            b, r = self._get_optimal_param(u, size)\n            for key in index[r]._query_b(minhash, b):\n                yield key"}
{"prompt": "def get_security_repository(self):\n        \"\"\" Security repository \"\"\"", "output": "\n        return self._security_repository\n\n    def set_security_repository(self, security_repository):\n        \"\"\" Set the security repository \"\"\"\n        self._security_repository = security_repository\n\n    def get_user_repository(self):\n        \"\"\" User repository \"\"\"\n        return self._user_repository\n\n    def set_user_repository(self, user_repository):\n        \"\"\" Set the user repository \"\"\"\n        self._user_repository = user_repository\n\n    def get_group_repository(self):\n        \"\"\" Group repository \"\"\"\n        return self._group_repository\n\n    def set_group_repository(self, group_repository):\n       ", "solution": "\n        from .repositories import SecurityRepository\n\n        if not self.security_repo:\n            self.security_repo = SecurityRepository(self.session)\n        return self.security_repo"}
{"prompt": "def Counter(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"", "output": "\n    = a + delta\n    = a - delta\n   _prime = (b - a) / (2 * delta)\n   _prime = (b - a) / (2 * delta)\n   _second_prime = (b - a) / (delta ** 2)\n   _second_prime = (b - a) / (delta ** 2)\n   _third_prime = (b - a) / (delta ** 3)\n    return (\n        (b - a)\n        * (\n            (b - a) * (b - a) * (b", "solution": "\n    if b < a:\n        return None \n\n    return (b - a) / float(delta)"}
{"prompt": "def cli_command_restart(self, msg):\n        \"\"\"\\\n        restart the subprocess\n        i. we set our state to RESTARTING - on restarting we still send heartbeat\n        ii. we kill the subprocess\n        iii. we start again\n        iv. if its started we set our state to RUNNING, else we set it to WAITING\n\n        :param msg:\n        :return:\n        \"\"\"", "output": "\n        if self.state == self.RUNNING:\n            self.state = self.RESTARTING\n            self.heartbeat_timer.cancel()\n            self.heartbeat_timer = threading.Timer(self.heartbeat_interval, self.send_heartbeat)\n            self.heartbeat_timer.start()\n            self.subprocess.kill()\n            self.subprocess = None\n            self.state = self.WAITING\n        else:\n            self.state = self.RESTARTING\n            self.heartbeat_timer.cancel()\n            self.heartbeat_timer = threading.Timer(self.heartbeat_interval, self.send_heartbeat)", "solution": "\n        info = ''\n        if self.state == State.RUNNING and self.sprocess and self.sprocess.proc:\n            self.state = State.RESTARTING\n            self.sprocess.set_exit_callback(self.proc_exit_cb_restart)\n            self.sprocess.proc.kill()\n            info = 'killed'\n            # TODO: check if process is really dead etc.\n        return info"}
{"prompt": "def delete_dir(bucket_name, root_path,\n               aws_access_key_id=None, aws_secret_access_key=None,\n               aws_profile=None):\n    \"\"\"Delete all objects in the S3 bucket named ``bucket_name`` that are\n    found in the ``root_path`` directory.\n\n    Parameters\n    ----------\n    bucket_name : `str`\n        Name of an S3 bucket.\n    root_path : `str`\n        Directory in the S3 bucket that will be deleted.\n    aws_access_key_id : `str`\n        The access key for your AWS account. Also set\n        ``aws_secret_access_key``.\n    aws_secret_access_key : `str`\n        The secret key for your AWS account.\n    aws_profile : `str`, optional\n        Name of AWS profile in :file:`~/.aws/credentials`. Use this instead\n        of ``aws_access_key_id`` and ``aws_secret_access_key`` for file-based\n        credentials.\n\n    Raises\n    ------\n    ltdconveyor.s3.S3Error\n        Thrown by any unexpected faults from the S3 API.\n    \"\"\"", "output": " AWS account.\n    aws_secret_access_key : `str`\n        The secret key for your AWS account.\n    aws_profile : `str`\n        The name of an AWS profile to use.\n\n    Returns\n    -------\n    `bool`\n        True if the deletion was successful, False otherwise.\n    \"\"\"\n    s3 = boto3.client('s3',\n                      aws_access_key_id=aws_access_key_id,\n                      aws_secret_access_key=aws_secret_access_key,\n                      aws_profile_name=aws_profile)\n\n    # List all objects in the bucket", "solution": "\n    logger = logging.getLogger(__name__)\n\n    session = boto3.session.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key)\n    s3 = session.resource('s3')\n    client = s3.meta.client\n\n    # Normalize directory path for searching patch prefixes of objects\n    if not root_path.endswith('/'):\n        root_path.rstrip('/')\n\n    paginator = client.get_paginator('list_objects_v2')\n    pages = paginator.paginate(Bucket=bucket_name, Prefix=root_path)\n\n    keys = dict(Objects=[])\n    for item in pages.search('Contents'):\n        try:\n            keys['Objects'].append({'Key': item['Key']})\n        except TypeError:  # item is None; nothing to delete\n            continue\n        # Delete immediately when 1000 objects are listed\n        # the delete_objects method can only take a maximum of 1000 keys\n        if len(keys['Objects']) >= 1000:\n            try:\n                client.delete_objects(Bucket=bucket_name, Delete=keys)\n            except Exception:\n                message = 'Error deleting objects from %r' % root_path\n                logger.exception(message)\n                raise S3Error(message)\n            keys = dict(Objects=[])\n\n    # Delete remaining keys\n    if len(keys['Objects']) > 0:\n        try:\n            client.delete_objects(Bucket=bucket_name, Delete=keys)\n        except Exception:\n            message = 'Error deleting objects from %r' % root_path\n            logger.exception(message)\n            raise S3Error(message)"}
{"prompt": "def find_launcher_class(clsname, kind):\n    \"\"\"Return a launcher for a given clsname and kind.\n\n    Parameters\n    ==========\n    clsname : str\n        The full name of the launcher class, either with or without the\n        module path, or an abbreviation (MPI, SSH, SGE, PBS, LSF,\n        WindowsHPC).\n    kind : str\n        Either 'EngineSet' or 'Controller'.\n    \"\"\"", "output": "\n    if_launcher_class is not None:\n        return_launcher_class\n\n    if_launcher_classes is None:\n        import_launcher_classes\n        from_launcher_classes import_launcher_classes\n\n    if kind == 'EngineSet':\n       _launcher_classes =_launcher_classes.engine_set_launcher_classes\n    elif kind == 'Controller':\n        pass\n    else:\n        raise ValueError('Invalid kind: {}'.format(kind))\n\n    if clsname in_launcher_classes:\n        return_launcher_classes[clsname]\n    else:\n        raise ValueError", "solution": "\n    if '.' not in clsname:\n        # not a module, presume it's the raw name in apps.launcher\n        if kind and kind not in clsname:\n            # doesn't match necessary full class name, assume it's\n            # just 'PBS' or 'MPI' prefix:\n            clsname = clsname + kind + 'Launcher'\n        clsname = 'IPython.parallel.apps.launcher.'+clsname\n    klass = import_item(clsname)\n    return klass"}
{"prompt": "def get_end_of_day(timestamp):\n    \"\"\"\n    Given a date or a datetime, return a datetime at 23:59:59 on that day\n    \"\"\"", "output": "\n    if isinstance(timestamp, datetime):\n        return timestamp.replace(hour=23, minute=59, second=59)\n    else:\n        return datetime.strptime(timestamp, \"%Y-%m-%d\").replace(hour=23, minute=59, second=59)\n\n\ndef get_start_of_day(timestamp):\n    \"\"\"\n    Given a date or a datetime, return a datetime at 00:00:00 on that day\n    \"\"\"\n    if isinstance(timestamp, datetime):\n        return timestamp.replace(hour=0, minute=0, second", "solution": "\n    return datetime.datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for dirpath, dirnames, filenames in os.walk(rootpath):\r\n        for filename in filenames:\r\n            if filename.endswith('.py') and filename not in skip:\r\n                modname = os.path.splitext(filename)[0]\r\n                modpath = os.path.join(dirpath, filename)\r\n                mod = imp.load_source(modname, modpath)\r\n                modules[modname] = mod\r\n    return modules\r\n\r\n\r\ndef find_module_docstrings(modules,_module=None):\r\n    \"\"\"\r\n    Find all docstrings in the given modules and return a", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def institutes(self, institute_ids=None):\n        \"\"\"Fetch all institutes.\n        \n        Args:\n            institute_ids(list(str))\n        \n        Returns:\n            res(pymongo.Cursor)\n        \"\"\"", "output": "\n        if institute_ids:\n            res = self.db.institutes.find({\"_id\": {\"$in\": institute_ids}})\n        else:\n            res = self.db.institutes.find()\n        return res\n\n    def institute(self, institute_id):\n        \"\"\"Fetch an institute by its ID.\n        \n        Args:\n            institute_id(str)\n        \n        Returns:\n            res(dict)\n        \"\"\"\n        res = self.db.institutes.find_one({\"_id\": institute_id})\n        return res\n\n    def create_institute(self,", "solution": "\n        query = {}\n        if institute_ids:\n            query['_id'] = {'$in': institute_ids}\n        LOG.debug(\"Fetching all institutes\")\n        return self.institute_collection.find(query)"}
{"prompt": "def send(self, node, message):\n        \"\"\"\n        Send a message to a node. Returns False if the connection appears to be dead either before or after actually trying to send the message.\n\n        :param node: target node\n        :type node: Node\n        :param message: message\n        :param message: any\n        :returns success\n        :rtype bool\n        \"\"\"", "output": "\n        raise NotImplementedError(\"Subclass must implement send method\")\n\n    def recv(self, timeout=None):\n        \"\"\"\n        Receive a message from a node. Returns None if no message is available within the specified timeout.\n\n        :param timeout: timeout in seconds\n        :type timeout: float\n        :returns message\n        :rtype any\n        \"\"\"\n        raise NotImplementedError(\"Subclass must implement recv method\")\n\n    def close(self):\n        \"\"\"\n        Close the connection to the node.\n        \"\"\"\n        raise NotImplementedError(\"Subclass must implement close method\")\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val", "solution": "\n\n        if node not in self._connections or self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        self._connections[node].send(message)\n        if self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        return True"}
{"prompt": "def create_model(self, project_id, model):\n        \"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"", "output": "\n        return self._client.create_model(project_id, model)\n\n    def update_model(self, model_id, model):\n        \"\"\"\n        Update a Model. Blocks until finished.\n        \"\"\"\n        return self._client.update_model(model_id, model)\n\n    def delete_model(self, model_id):\n        \"\"\"\n        Delete a Model. Blocks until finished.\n        \"\"\"\n        return self._client.delete_model(model_id)\n\n    def get_model(self, model_id):\n        \"\"\"\n        Get a Model. Blocks until finished.\n        \"\"\"\n        return self._client", "solution": "\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()"}
{"prompt": "def read_msg(self):\n        \"\"\"Read one message unit. It's possible however that\n        more than one message will be set in a receive, so we will\n        have to buffer that for the next read.\n        EOFError will be raised on EOF.\n        \"\"\"", "output": "\n        if self._buffer:\n            # We have some buffered data, so return it\n            msg = self._buffer\n            self._buffer = None\n            return msg\n        else:\n            # Read a message unit\n            try:\n                msg = self._sock.recv(self._max_msg_size)\n            except socket.error as e:\n                if e.errno == errno.ECONNRESET:\n                    raise EOFError('Connection reset by peer')\n                else:\n                    raise\n            if not msg:\n                raise EOFError('Connection closed')\n            return msg\n\n    def send_msg(self, msg):\n        \"\"\"Send a message unit", "solution": "\n        if self.state == 'connected':\n            if 0 == len(self.buf):\n                self.buf = self.inout.recv(Mtcpfns.TCP_MAX_PACKET)\n                if 0 == (self.buf):\n                    self.state = 'disconnected'\n                    raise EOFError\n                pass\n            self.buf, data = Mtcpfns.unpack_msg(self.buf)\n            return data.decode('utf-8')\n        else:\n            raise IOError(\"read_msg called in state: %s.\" % self.state)"}
{"prompt": "def _wakeup(self):\n        \"\"\"\n        issue wakeup command to device to take out of standby mode.\n        \"\"\"", "output": "\n        self._send_command(\"AT+WUP\")\n\n    def _send_command(self, command):\n        \"\"\"\n        send command to device and return response.\n        \"\"\"\n        self._device.write(command + \"\\r\\n\")\n        response = self._device.readline()\n        return response.strip()\n\n    def _parse_response(self, response):\n        \"\"\"\n        parse response from device and returnTrue` if response is \"OK\", otherwise `False`.\n        \"\"\"\n        if response == \"OK\":\n            return True\n        else:\n            return False\n\n    def _parse___time(", "solution": "\n        log.info(\"send: WAKEUP\")\n        for i in xrange(3):\n            self.port.write('\\n')  # wakeup device\n            ack = self.port.read(len(self.WAKE_ACK))  # read wakeup string\n            log_raw('read', ack)\n            if ack == self.WAKE_ACK:\n                return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def get_home_dir(require_writable=False):\n    \"\"\"Return the 'home' directory, as a unicode string.\n\n    * First, check for frozen env in case of py2exe\n    * Otherwise, defer to os.path.expanduser('~')\n    \n    See stdlib docs for how this is determined.\n    $HOME is first priority on *ALL* platforms.\n    \n    Parameters\n    ----------\n    \n    require_writable : bool [default: False]\n        if True:\n            guarantees the return value is a writable directory, otherwise\n            raises HomeDirError\n        if False:\n            The path is resolved, but it is not guaranteed to exist or be writable.\n    \"\"\"", "output": " to be a writable\n            directory.\n    \"\"\"\n    if sys.platform == 'win32':\n        import _winreg\n        try:\n            home_dir = _winreg.QueryValue(\n                _winreg.HKEY_CURRENT_USER,\n                r'Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders'\n            )\n            return home_dir\n        except WindowsError:\n            pass\n    elif sys.platform == 'darwin':\n        import os\n        home_dir = os.path.expanduser('~')\n    else:\n        import os\n        home_dir = os.path.expanduser('~')", "solution": "\n\n    # first, check py2exe distribution root directory for _ipython.\n    # This overrides all. Normally does not exist.\n\n    if hasattr(sys, \"frozen\"): #Is frozen by py2exe\n        if '\\\\library.zip\\\\' in IPython.__file__.lower():#libraries compressed to zip-file\n            root, rest = IPython.__file__.lower().split('library.zip')\n        else:\n            root=os.path.join(os.path.split(IPython.__file__)[0],\"../../\")\n        root=os.path.abspath(root).rstrip('\\\\')\n        if _writable_dir(os.path.join(root, '_ipython')):\n            os.environ[\"IPYKITROOT\"] = root\n        return py3compat.cast_unicode(root, fs_encoding)\n    \n    homedir = os.path.expanduser('~')\n    # Next line will make things work even when /home/ is a symlink to\n    # /usr/home as it is on FreeBSD, for example\n    homedir = os.path.realpath(homedir)\n    \n    if not _writable_dir(homedir) and os.name == 'nt':\n        # expanduser failed, use the registry to get the 'My Documents' folder.\n        try:\n            import _winreg as wreg\n            key = wreg.OpenKey(\n                wreg.HKEY_CURRENT_USER,\n                \"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n            )\n            homedir = wreg.QueryValueEx(key,'Personal')[0]\n            key.Close()\n        except:\n            pass\n    \n    if (not require_writable) or _writable_dir(homedir):\n        return py3compat.cast_unicode(homedir, fs_encoding)\n    else:\n        raise HomeDirError('%s is not a writable dir, '\n                'set $HOME environment variable to override' % homedir)"}
{"prompt": "def run_eplus(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None):\n    \"\"\"\n    Parameters\n    ----------\n    epm_or_idf_path:\n    weather_data_or_epw_path\n    simulation_dir_path\n    stdout: default sys.stdout\n    stderr: default sys.stderr\n    beat_freq: if not none, stdout will be used at least every beat_freq (in seconds)\n    \"\"\"", "output": "\n    if beat_freq is not None:\n        stdout = sys.stdout\n       __stdout = sys.stdout\n       _stdout = sys.stdout\n\n    try:\n        ifp_or_idf_path.endswith(\".epm\"):\n            epm_path = epm_or_idf_path\n        else:\n            epm_path = os.path.join(os.path.dirname(epm_or_idf_path), \"epm.epm\")\n\n        if weather_data_or_epw_path.endswith(\".epw\"):\n            epw_path = weather", "solution": "\n    # work with absolute paths\n    simulation_dir_path = os.path.abspath(simulation_dir_path)\n\n    # check dir path\n    if not os.path.isdir(simulation_dir_path):\n        raise NotADirectoryError(\"Simulation directory does not exist: '%s'.\" % simulation_dir_path)\n\n    # epm\n    if not isinstance(epm_or_idf_path, Epm):\n        # we don't copy file directly because we want to manage it's external files\n        # could be optimized (use _copy_without_read_only)\n        epm = Epm.from_idf(epm_or_idf_path)\n    else:\n        epm = epm_or_idf_path\n\n    # create idf\n    simulation_idf_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".idf\")\n    epm.to_idf(simulation_idf_path)\n\n    # weather data\n    simulation_epw_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".epw\")\n    if isinstance(weather_data_or_epw_path, WeatherData):\n        weather_data_or_epw_path.to_epw(simulation_epw_path)\n    else:\n        # no need to load: we copy directly\n        _copy_without_read_only(weather_data_or_epw_path, simulation_epw_path)\n\n    # copy epw if needed (depends on os/eplus version)\n    temp_epw_path = get_simulated_epw_path()\n    if temp_epw_path is not None:\n        _copy_without_read_only(simulation_epw_path, temp_epw_path)\n\n    # prepare command\n    eplus_relative_cmd = get_simulation_base_command()\n    eplus_cmd = os.path.join(CONF.eplus_base_dir_path, eplus_relative_cmd)\n\n    # idf\n    idf_command_style = get_simulation_input_command_style(\"idf\")\n    if idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        idf_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        idf_file_cmd = simulation_idf_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # epw\n    epw_command_style = get_simulation_input_command_style(\"epw\")\n    if epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        epw_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        epw_file_cmd = simulation_epw_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # command list\n    simulation_command_style = get_simulation_command_style()\n    if simulation_command_style == SIMULATION_COMMAND_STYLES.args:\n        cmd_l = [eplus_cmd, idf_file_cmd, epw_file_cmd]\n    elif simulation_command_style == SIMULATION_COMMAND_STYLES.kwargs:\n        cmd_l = [eplus_cmd, \"-w\", epw_file_cmd, \"-r\", idf_file_cmd]\n    else:\n        raise RuntimeError(\"should not be here\")\n\n    # launch calculation\n    run_subprocess(\n        cmd_l,\n        cwd=simulation_dir_path,\n        stdout=stdout,\n        stderr=stderr,\n        beat_freq=beat_freq\n    )\n\n    # if needed, we delete temp weather data (only on Windows, see above)\n    if (temp_epw_path is not None) and os.path.isfile(temp_epw_path):\n        os.remove(os.path.join(temp_epw_path))"}
{"prompt": "def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"", "output": "\n    if isinstance(ctx, ast.Attribute):\n        return _load_attr(name, ctx.value)\n    elif isinstance(ctx, ast.Name):\n        return ast.Attribute(ctx, name, ast.Load())\n    else:\n        raise TypeError(f\"Unsupported context type: {type(ctx)}\")\n\n\ndef _resolve_name(node: ast.AST, name: str) -> Optional[ast.AST]:\n    \"\"\"Recursively resolve a name in a Python AST node.\"\"\"\n    if isinstance(node, ast.Name):\n        if node.id == name:\n            return node\n    elif isinstance", "solution": "\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)"}
{"prompt": "def _prepare_query_values(self):\n        \"\"\"Extra prep on query values by converting\n        dictionaries into :see:HStoreValue expressions.\n\n        This allows putting expressions in a dictionary.\n        The :see:HStoreValue will take care of resolving\n        the expressions inside the dictionary.\"\"\"", "output": "\n        for key, value in self.query_values.items():\n            if isinstance(value, dict):\n                self.query_values[key] = HStoreValue(value)\n\n    def _prepare_query_conditions(self):\n        \"\"\"Extra prep on query conditions by converting\n        dictionaries into :see:HStoreCondition expressions.\n\n        This allows putting expressions in a dictionary.\n        The :see:HStoreCondition will take care of resolving\n        the expressions inside the dictionary.\"\"\"\n        for key, value in self.query_conditions.items():\n            if isinstance(value, dict):\n                self.query_conditions[key", "solution": "\n\n        new_query_values = []\n        for field, model, val in self.query.values:\n            if isinstance(val, dict):\n                val = HStoreValue(val)\n\n            new_query_values.append((\n                field,\n                model,\n                val\n            ))\n\n        self.query.values = new_query_values"}
{"prompt": "def get_handler(level: str, fmt: str) -> logging.Handler:\n    \"\"\"Get the default logging handler for Basilisp.\"\"\"", "output": "\n    if level == \"debug\":\n        return logging.StreamHandler(sys.stdout)\n    elif level == \"info\":\n        return logging.StreamHandler(sys.stderr)\n    else:\n        raise ValueError(f\"Invalid log level: {level}\")\n\n\ndef get_logger(name: str, level: str = \"info\", fmt: str = \"%(asctime)s %(levelname)s %(name)s: %(message)s\") -> logging.Logger:\n    \"\"\"Get a logger for Basilisp.\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    formatter", "solution": "\n    handler: logging.Handler = logging.NullHandler()\n    if os.getenv(\"BASILISP_USE_DEV_LOGGER\") == \"true\":\n        handler = logging.StreamHandler()\n\n    handler.setFormatter(logging.Formatter(fmt))\n    handler.setLevel(level)\n    return handler"}
{"prompt": "def _get_course_content_from_ecommerce(course_id, site_code=None):\n    \"\"\"\n    Get course information using the Ecommerce course api.\n\n    In case of error returns empty response.\n    Arguments:\n        course_id (str): course key of the course\n        site_code (str): site code\n\n    Returns:\n        course information from Ecommerce\n    \"\"\"", "output": "\n    try:\n        response = requests.get(f\"https://api.ecommerce.edx.org/api/v1/courses/{course_id}/\", headers=get_ecommerce_headers())\n        if response.status_code == 200:\n            return response.json()\n        else:\n            return {}\n    except Exception as e:\n        logger.error(f\"Error while fetching course information from Ecommerce: {e}\")\n        return {}\n\n\ndef _get_course_content_from_edx_api(course_id, site_code=None):\n    \"\"\"\n    Get course", "solution": "\n    api = get_ecommerce_client(site_code=site_code)\n    try:\n        api_response = api.courses(course_id).get()\n    except Exception:  # pylint: disable=broad-except\n        logger.exception(\n            'An error occurred while retrieving data for course run [%s] from the Catalog API.',\n            course_id,\n            exc_info=True\n        )\n        return {}\n\n    return {\n        'title': api_response.get('name'),\n        'verification_deadline': api_response.get('verification_deadline')\n    }"}
{"prompt": "def container_services(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-07-01: :class:`ContainerServicesOperations<azure.mgmt.containerservice.v2017_07_01.operations.ContainerServicesOperations>`\n        \"\"\"", "output": "\n        api_version = self._client.api_version\n        if api_version == '2017-07-01':\n            from.v2017_07_01.operations import ContainerServicesOperations\n            return ContainerServicesOperations(self._client)\n        raise NotImplementedError(\"API version {} is not available\".format(api_version))\n", "solution": "\n        api_version = self._get_api_version('container_services')\n        if api_version == '2017-07-01':\n            from .v2017_07_01.operations import ContainerServicesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def doc_from_xml(document_element_name, inner_xml,\n                     xmlns='http://schemas.microsoft.com/windowsazure'):\n        \"\"\"Wraps the specified xml in an xml root element with default azure\n        namespaces\"\"\"", "output": "\n        root_element = ET.Element(document_element_name,\n                                   nsmap={None: xmlns})\n        root_element.text = inner_xml\n        return root_element\n\n\nclass AzureOperationError(Exception):\n    \"\"\"Base class for exceptions raised by the Azure Service Management API\"\"\"\n    pass\n\n\nclass AzureServiceManagementError(AzureOperationError):\n    \"\"\"Base class for exceptions raised by the Azure Service Management API\"\"\"\n    pass\n\n\nclass AzureHttpError(AzureServiceManagementError):\n    \"\"\"Base class for exceptions raised by the Azure Service Management API\"\"\"\n    pass\n\n\nclass AzureInvalidRequestError(Azure", "solution": "\n        xml = ''.join(['<', document_element_name,\n                      ' xmlns=\"{0}\">'.format(xmlns)])\n        xml += inner_xml\n        xml += ''.join(['</', document_element_name, '>'])\n        return xml"}
{"prompt": "def lessThan(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"", "output": "\n        if left < right:\n            return -1\n        elif left > right:\n            return 1\n        else:\n            return 0\n\n    def merge(self, left, right):\n        \"\"\"Merge two sorted lists into a single sorted list.\"\"\"\n        result = []\n        i = j = 0\n        while i < len(left) and j < len(right):\n            if self.lessThan(left[i], right[j]):\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n        result.extend(left[", "solution": "\n        sourceModel = self.sourceModel()\n        if sourceModel:\n            leftItem = sourceModel.item(left)\n            rightItem = sourceModel.item(right)\n\n            if (isinstance(leftItem, Directory)\n                and not isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.AscendingOrder\n\n            elif (not isinstance(leftItem, Directory)\n                and isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.DescendingOrder\n\n        return super(FilesystemSortProxy, self).lessThan(left, right)"}
{"prompt": "def get_entry_properties_from_element(element, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        \"\"\" get properties from element tree element \"\"\"", "output": "\n        properties = {}\n        if include_id:\n            properties['id'] = element.get('id')\n        if use_title_as_id:\n            properties['id'] = element.get('title')\n        if id_prefix_to_skip:\n            if properties['id'].startswith(id_prefix_to_skip):\n                properties['id'] = properties['id'][len(id_prefix_to_skip):]\n        for key, value in element.attrib.items():\n            if key.startswith('data-'):\n                properties[key[5:]] = value\n        return properties\n\n    def get_", "solution": "\n        properties = {}\n\n        etag = element.attrib.get(_make_etree_ns_attr_name(_etree_entity_feed_namespaces['m'], 'etag'), None)\n        if etag is not None:\n            properties['etag'] = etag\n\n        updated = element.findtext('./atom:updated', '', _etree_entity_feed_namespaces)\n        if updated:\n            properties['updated'] = updated\n\n        author_name = element.findtext('./atom:author/atom:name', '', _etree_entity_feed_namespaces)\n        if author_name:\n            properties['author'] = author_name\n\n        if include_id:\n            if use_title_as_id:\n                title = element.findtext('./atom:title', '', _etree_entity_feed_namespaces)\n                if title:\n                    properties['name'] = title\n            else:\n                element_id = element.findtext('./atom:id', '', _etree_entity_feed_namespaces)\n                if element_id:\n                    properties['name'] = _get_readable_id(element_id, id_prefix_to_skip)\n\n        return properties"}
{"prompt": "def source_address(self):\n        \"\"\"Return the authorative source of the link.\"\"\"", "output": "\n        return self._source_address\n\n    @property\n    def_url(self):\n        \"\"\"Return the URL of the.\"\"\"\n        return self._url\n\n    @property\n    def_title(self):\n        \"\"\"Return the title of the post.\"\"\"\n        return self._title\n\n    @property\n    def_date(self):\n        \"\"\"Return the date of the post.\"\"\"\n        return self._date\n\n    @property\n    def post_content(self):\n        \"\"\"Return the content of the post.\"\"\"\n        return self._content\n\n    @property\n    def post_tags(self):\n        \"\"\"Return the tags of the post.\"\"\"\n       ", "solution": "\n        # If link is a sender, source is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_sender:\n            return self._pn_link.source.address\n        else:\n            return self._pn_link.remote_source.address"}
{"prompt": "def _serialize(xp_ast):\n    \"\"\"Generate token strings which, when joined together, form a valid\n    XPath serialization of the AST.\"\"\"", "output": "\n    def _serialize_node(node):\n        if isinstance(node, ast.XPathExpr):\n            return node.xpath\n        elif isinstance(node, ast.XPathPredicate):\n            return f\"[{_serialize_node(node.predicate)}]\"\n        elif isinstance(node, ast.XPathAxis):\n            return f\"@{node.axis}\"\n        elif isinstance(node, ast.XPathStep):\n            return f\"/{_serialize_node(node.node)}\"\n        elif isinstance(node, ast.XPathNameTest):\n            return f\"*{node.name}\"\n        elif isinstance(node, ast.XPathFilterExpr", "solution": "\n\n    if hasattr(xp_ast, '_serialize'):\n        for tok in xp_ast._serialize():\n            yield(tok)\n    elif isinstance(xp_ast, str):\n        yield(repr(xp_ast))"}
{"prompt": "def check_for_prefix(self, bucket_name, prefix, delimiter):\n        \"\"\"\n        Checks that a prefix exists in a bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        \"\"\"", "output": "\n        try:\n            self.s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter=delimiter)\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'NoSuchKey':\n                return False\n            else:\n                raise e\n        return True\n\n    def create_bucket(self, bucket_name):\n        \"\"\"\n        Creates a new bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        self.s3.create_bucket(Bucket=bucket_name)\n\n    def delete_bucket", "solution": "\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist"}
{"prompt": "def group(self, name):\n        \"\"\"GROUP command.\n        \"\"\"", "output": "\n        self.send_command('GROUP', name)\n        return self.read_response()\n\n    def group_list(self):\n        \"\"\"GROUP LIST command.\n        \"\"\"\n        self.send_command('GROUP LIST')\n        return self.read_response()\n\n    def group_info(self, name):\n        \"\"\"GROUP INFO command.\n        \"\"\"\n        self.send_command('GROUP INFO', name)\n        return self.read_response()\n\n    def group_create(self, name):\n        \"\"\"GROUP CREATE command.\n        \"\"\"\n        self.send_command('GROUP CREATE', name)\n        return self.read_response", "solution": "\n        args = name\n\n        code, message = self.command(\"GROUP\", args)\n        if code != 211:\n            raise NNTPReplyError(code, message)\n\n        parts = message.split(None, 4)\n        try:\n            total = int(parts[0])\n            first = int(parts[1])\n            last  = int(parts[2])\n            group = parts[3]\n        except (IndexError, ValueError):\n            raise NNTPDataError(\"Invalid GROUP status '%s'\" % message)\n\n        return total, first, last, group"}
{"prompt": "def get(self, thing_id='0', property_name=None):\n        \"\"\"\n        Handle a GET request.\n\n        thing_id -- ID of the thing this request is for\n        property_name -- the name of the property from the URL path\n        \"\"\"", "output": "\n        if property_name is None:\n            return self.get_thing(thing_id)\n        else:\n            return self.get_property(thing_id, property_name)\n\n    def get_thing(self, thing_id):\n        \"\"\"\n        Handle a GET request for a thing.\n\n        thing_id -- ID of the thing this request is for\n        \"\"\"\n        thing = self.thing_store.get_thing(thing_id)\n        if thing is None:\n            return self.not_found()\n        else:\n            return self.json_response(thing.to_dict())\n\n    def get", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        if thing.has_property(property_name):\n            self.set_header('Content-Type', 'application/json')\n            self.write(json.dumps({\n                property_name: thing.get_property(property_name),\n            }))\n        else:\n            self.set_status(404)"}
{"prompt": "def plexp_inv(P, xmin, alpha, guess=1.):\n    \"\"\"\n    Inverse CDF for a piecewise PDF as defined in eqn. 3.10\n    of Clauset et al.\n\n    (previous version was incorrect and lead to weird discontinuities in the\n    distribution function)\n    \"\"\"", "output": "\n    P: list of tuples (x, p) representing the piecewise PDF\n    xmin: float, the minimum value of the domain\n    alpha: float, the value of the function\n    guess: float, initial guess for the value\n\n    Returns:\n    float, the inverse CDF value\n    \"\"\"\n   _func =_func_(alpha)\n   _func_inv = lambda x: 1. -_func(x)\n\n   _func_inv_ =_func_inv(xmin)\n   _func_inv_ = lambda x: ", "solution": "\n    def equation(x,prob):\n        return plexp_cdf(x, xmin, alpha)-prob\n    # http://stackoverflow.com/questions/19840425/scipy-optimize-faster-root-finding-over-2d-grid\n    def solver(y, x0=guess):\n        return scipy.optimize.fsolve(equation, guess, args=(y,))\n    f = np.vectorize(solver)\n    return f(P)"}
{"prompt": "def start(self, context):\n\t\t\"\"\"Construct the SQLAlchemy engine and session factory.\"\"\"", "output": "\n\t\tself.engine = create_engine(self.config.get('SQLALCHEMY_DATABASE_URI'))\n\t\tself.session_factory = sessionmaker(bind=self.engine)\n\n\tdef get_session(self):\n\t\t\"\"\"Return a new session.\"\"\"\n\t\treturn self.session_factory()\n\n\tdef close_session(self, session):\n\t\t\"\"\"Close the given session.\"\"\"\n\t\tsession.close()\n", "solution": "\n\t\t\n\t\tif __debug__:\n\t\t\tlog.info(\"Connecting SQLAlchemy database layer.\", extra=dict(\n\t\t\t\t\turi = redact_uri(self.uri),\n\t\t\t\t\tconfig = self.config,\n\t\t\t\t\talias = self.alias,\n\t\t\t\t))\n\t\t\n\t\t# Construct the engine.\n\t\tengine = self.engine = create_engine(self.uri, **self.config)\n\t\t\n\t\t# Construct the session factory.\n\t\tself.Session = scoped_session(sessionmaker(bind=engine))\n\t\t\n\t\t# Test the connection.\n\t\tengine.connect().close()\n\t\t\n\t\t# Assign the engine to our database alias.\n\t\tcontext.db[self.alias] = engine"}
{"prompt": "def _matches_filepath_pattern(self, filepath):\n        \"\"\"Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns\"\"\"", "output": "\n        for pattern in self.patterns:\n            if re.match(pattern, filepath):\n                return True\n        return False\n\n    def _get_matching_files(self, directory):\n        \"\"\"Given a directory, this function returns a list of all the files in that directory\n        that match any of the regex patterns in self.patterns\"\"\"\n        matching_files = []\n        for root, dirs, files in os.walk(directory):\n            for filename in files:\n                filepath = os.path.join(root, filename)\n                if self._matches_filepath_pattern(filepath):\n                    matching_files.append(filepath)", "solution": "\n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False"}
{"prompt": "def iter_auth_hashes(user, purpose, minutes_valid):\n    \"\"\"\n    Generate auth tokens tied to user and specified purpose.\n\n    The hash expires at midnight on the minute of now + minutes_valid, such\n    that when minutes_valid=1 you get *at least* 1 minute to use the token.\n    \"\"\"", "output": "\n    now = datetime.datetime.now()\n    expiration = now + datetime.timedelta(minutes=minutes_valid)\n    token = hashlib.sha256(\n        (user.pk + purpose + str(minutes_valid)).encode('utf-8')\n    ).hexdigest()\n    yield token, expiration\n", "solution": "\n    now = timezone.now().replace(microsecond=0, second=0)\n    for minute in range(minutes_valid + 1):\n        yield hashlib.sha1(\n            '%s:%s:%s:%s:%s' % (\n                now - datetime.timedelta(minutes=minute),\n                user.password,\n                purpose,\n                user.pk,\n                settings.SECRET_KEY,\n            ),\n        ).hexdigest()"}
{"prompt": "def _control_key_down(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is down.\n\n        Parameters:\n        -----------\n        include_command : bool, optional (default True)\n            Whether to treat the Command key as a (mutually exclusive) synonym\n            for Control when in Mac OS.\n        \"\"\"", "output": "\n        if_platform() == 'darwin' and include_command:\n            return_key_down('command')\n        else:\n            return_key_down('control')\n\n    def _control_key_up(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is up.\n\n        Parameters:\n        -----------\n        include_command : bool, optional (default True)\n            Whether to treat the Command key as a (mutually exclusive) synonym\n            for Control when in Mac OS.\n        \"\"\"\n        if_platform() == 'darwin'", "solution": "\n        # Note that on Mac OS, ControlModifier corresponds to the Command key\n        # while MetaModifier corresponds to the Control key.\n        if sys.platform == 'darwin':\n            down = include_command and (modifiers & QtCore.Qt.ControlModifier)\n            return bool(down) ^ bool(modifiers & QtCore.Qt.MetaModifier)\n        else:\n            return bool(modifiers & QtCore.Qt.ControlModifier)"}
{"prompt": "def compat_validate_token(cls, *args, **kwargs):\n        \"\"\"Multiple algorithm-compatible token validation.\"\"\"", "output": "\n        return cls.validate_token(*args, **kwargs)\n\n    @classmethod\n    def validate_token(cls, token, algorithm=None, audience=None, issuer=None,\n                       leeway=0, **kwargs):\n        \"\"\"Validate a token using multiple algorithms.\n\n        :param token: The token to validate.\n        :param algorithm: The algorithm to use for validation.\n        :param audience: The expected audience for the token.\n        :param issuer: The expected issuer for the token.\n        :param leeway: The number of seconds to allow for clock skew.\n        :param kwargs: Additional arguments for", "solution": "\n        data = None\n        for algorithm in SUPPORTED_DIGEST_ALGORITHMS:\n            data = cls(algorithm_name=algorithm).validate_token(\n                *args, **kwargs)\n            if not data:  # move to next algorithm\n                continue\n        return data"}
{"prompt": "def timestamp_YmdHMS(value):\n    \"\"\"Convert timestamp string to time in seconds since epoch.\n\n    Timestamps strings like '20130618120000' are able to be converted by this\n    function.\n\n    Args:\n        value: A timestamp string in the format '%Y%m%d%H%M%S'.\n\n    Returns:\n        The time in seconds since epoch as an integer.\n\n    Raises:\n        ValueError: If timestamp is invalid.\n\n    Note: The timezone is assumed to be UTC/GMT.\n    \"\"\"", "output": "\n    try:\n        year, month, day, hour, minute, second = map(int, value.split('%'))\n        return int(datetime(year, month, day, hour, minute, second).timestamp())\n    except ValueError:\n        raise ValueError('Invalid timestamp format: %s' % value)\n\n\ndef timestamp_YmdHMS_to_datetime(value):\n    \"\"\"Convert timestamp string to datetime object.\n\n    Timestamps strings like '20130618120000' are able to be converted by this\n    function.\n\n    Args:\n        value: A timestamp string", "solution": "\n    i = int(value)\n    S = i\n    M = S//100\n    H = M//100\n    d = H//100\n    m = d//100\n    Y = m//100\n    return int(calendar.timegm((\n        Y % 10000, m % 100, d % 100, H % 100, M % 100, S % 100, 0, 0, 0)\n    ))"}
{"prompt": "def report_by_type_stats(sect, stats, _):\n    \"\"\"make a report of\n\n    * percentage of different types documented\n    * percentage of different types with a bad name\n    \"\"\"", "output": "\n   _types =_types_by_sect(sect)\n   _types_with_bad_name =_types_with_bad_name_by_sect(sect)\n\n   _types_with_bad_name_pct = len(item for item in item_types_with_bad_name if item in_types) / len(item_types_with_bad_name) * 100\n   _types_pct = len(item_types_by_sect(sect)) / len(item_types_by_sect(sect,", "solution": "\n    # percentage of different types documented and/or with a bad name\n    nice_stats = {}\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        try:\n            total = stats[node_type]\n        except KeyError:\n            raise exceptions.EmptyReportError()\n        nice_stats[node_type] = {}\n        if total != 0:\n            try:\n                documented = total - stats[\"undocumented_\" + node_type]\n                percent = (documented * 100.0) / total\n                nice_stats[node_type][\"percent_documented\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_documented\"] = \"NC\"\n            try:\n                percent = (stats[\"badname_\" + node_type] * 100.0) / total\n                nice_stats[node_type][\"percent_badname\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_badname\"] = \"NC\"\n    lines = (\"type\", \"number\", \"old number\", \"difference\", \"%documented\", \"%badname\")\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        new = stats[node_type]\n        lines += (\n            node_type,\n            str(new),\n            \"NC\",\n            \"NC\",\n            nice_stats[node_type].get(\"percent_documented\", \"0\"),\n            nice_stats[node_type].get(\"percent_badname\", \"0\"),\n        )\n    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))"}
{"prompt": "def find_safe(ns_qualified_sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.\"\"\"", "output": "\n        ns, sym_name = ns_qualified_sym.split(\"::\")\n        ns_obj =.get_namespace(ns)\n        if not ns_obj:\n            raise ValueError(f\"Namespace {ns} not found\")\n        return ns_obj.get_var(sym_name)\n\n    def find_var(sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the current namespace.\n        If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol", "solution": "\n        v = Var.find(ns_qualified_sym)\n        if v is None:\n            raise RuntimeException(\n                f\"Unable to resolve symbol {ns_qualified_sym} in this context\"\n            )\n        return v"}
{"prompt": "def start(self):\n        \"\"\"Start the logger background synchronization service. This allows you to not need to\n        worry about syncing with ConnectorDB - you just insert into the Logger, and the Logger\n        will by synced every syncperiod.\"\"\"", "output": "\n        self.logger_thread = threading.Thread(target=self.logger_sync_loop)\n        self.logger_thread.start()\n\n    def logger_sync_loop(self):\n        \"\"\"The main loop for the logger background synchronization service. This function\n        periodically checks the Logger for new entries, and if there are any, it sends them to\n        the ConnectorDB API.\"\"\"\n        while True:\n            # Check for new entries in the Logger\n            new_entries = self.logger.get_new_entries()\n\n            # If there are new entries, send them to the ConnectorDB API\n            if new_entries:", "solution": "\n\n        with self.synclock:\n            if self.syncthread is not None:\n                logging.warn(\n                    \"Logger: Start called on a syncer that is already running\")\n                return\n\n        self.sync()  # Attempt a sync right away\n        self.__setsync()"}
{"prompt": "def make_path_relative(path, rel_to):\n    \"\"\"\n    Make a filename relative, where the filename path, and it is\n    relative to rel_to\n\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/usr/share/another-place/src/Directory')\n        '../../../something/a-file.pth'\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/home/user/src/Directory')\n        '../../../usr/share/something/a-file.pth'\n        >>> make_path_relative('/usr/share/a-file.pth', '/usr/share/')\n        'a-file.pth'\n    \"\"\"", "output": " >>> make_path_relative('/usr/share/something/a-file.pth',\n       ...                    '/usr/share/another-place')\n        'a-file.pth'\n\n    \"\"\"\n    if path.startswith(rel_to):\n        return os.path.relpath(path, rel_to)\n    else:\n        return path\n\n\ndef get_file_contents(filename):\n    \"\"\"\n    Get the contents of a file, as a string.\n\n        >>> get_file_contents('README.rst')\n        'This is a README file for the project.\\n'\n\n    \"\"\"\n    with open(", "solution": "\n    path_filename = os.path.basename(path)\n    path = os.path.dirname(path)\n    path = os.path.normpath(os.path.abspath(path))\n    rel_to = os.path.normpath(os.path.abspath(rel_to))\n    path_parts = path.strip(os.path.sep).split(os.path.sep)\n    rel_to_parts = rel_to.strip(os.path.sep).split(os.path.sep)\n    while path_parts and rel_to_parts and path_parts[0] == rel_to_parts[0]:\n        path_parts.pop(0)\n        rel_to_parts.pop(0)\n    full_parts = ['..'] * len(rel_to_parts) + path_parts + [path_filename]\n    if full_parts == ['']:\n        return '.' + os.path.sep\n    return os.path.sep.join(full_parts)"}
{"prompt": "def _rotations_to_disentangle(local_param):\n        \"\"\"\n        Static internal method to work out Ry and Rz rotation angles used\n        to disentangle the LSB qubit.\n        These rotations make up the block diagonal matrix U (i.e. multiplexor)\n        that disentangles the LSB.\n\n        [[Ry(theta_1).Rz(phi_1)  0   .   .   0],\n         [0         Ry(theta_2).Rz(phi_2) .  0],\n                                    .\n                                        .\n          0         0           Ry(theta_2^n).Rz(phi_2^n)]]\n        \"\"\"", "output": "theta_n).Rz(phi_n)]\n\n        where n is the number of-dressed qubits.\n\n        Args:\n            local_param (list): A list of_dressed_qubits,_dressed_angles,_dressed_phases,_dressed_angles,_dressed_phases,_dressed_angles,_dressed_phases,_dressed_angles,_dressed_phases,_dressed_angles,_dressed_phases,_dressed_angles,", "solution": "\n        remaining_vector = []\n        thetas = []\n        phis = []\n\n        param_len = len(local_param)\n\n        for i in range(param_len // 2):\n            # Ry and Rz rotations to move bloch vector from 0 to \"imaginary\"\n            # qubit\n            # (imagine a qubit state signified by the amplitudes at index 2*i\n            # and 2*(i+1), corresponding to the select qubits of the\n            # multiplexor being in state |i>)\n            (remains,\n             add_theta,\n             add_phi) = Initialize._bloch_angles(local_param[2 * i: 2 * (i + 1)])\n\n            remaining_vector.append(remains)\n\n            # rotations for all imaginary qubits of the full vector\n            # to move from where it is to zero, hence the negative sign\n            thetas.append(-add_theta)\n            phis.append(-add_phi)\n\n        return remaining_vector, thetas, phis"}
{"prompt": "def crcMeterRead(self, raw_read, def_buf):\n        \"\"\" Internal read CRC wrapper.\n\n        Args:\n            raw_read (str): Bytes with implicit string cast from serial read\n            def_buf (SerialBlock): Populated read buffer.\n\n        Returns:\n            bool:  True if passed CRC equals calculated CRC.\n        \"\"\"", "output": "\n        crc_calc = self.crc_calc(def_buf.get_read_buffer())\n        crc_read = struct.unpack('>H', raw_read[2:4])[0]\n        return crc_read == crc_calc\n\n    def crc_calc(self, data):\n        \"\"\" Calculate CRC for given data.\n\n        Args:\n            data (bytes): Data to calculate CRC for.\n\n        Returns:\n            int: Calculated CRC.\n        \"\"\"\n        crc = 0xFFFF\n        for byte in data:\n            crc = (crc >> 8) ^ self.crc_table[(crc ^ byte)", "solution": "\n        try:\n            if len(raw_read) == 0:\n                ekm_log(\"(\" + self.m_context + \") Empty return read.\")\n                return False\n            sent_crc = self.calc_crc16(raw_read[1:-2])\n            logstr = \"(\" + self.m_context + \")CRC sent = \" + str(def_buf[\"crc16\"][MeterData.StringValue])\n            logstr += \" CRC calc = \" + sent_crc\n            ekm_log(logstr)\n            if int(def_buf[\"crc16\"][MeterData.StringValue], 16) == int(sent_crc, 16):\n                return True\n\n        # A cross simple test lines on a USB serial adapter, these occur every\n        # 1000 to 2000 reads, and they show up here as a bad unpack or\n        # a bad crc type call.  In either case, we suppress them a log will\n        # become quite large.  ekmcrc errors come through as type errors.\n        # Failures of int type conversion in 16 bit conversion occur as value\n        # errors.\n        except struct.error:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except TypeError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except ValueError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        return False"}
{"prompt": "def is_bare_exception(self, node):\n        \"\"\"\n        Checks if the node is a bare exception name from an except block.\n\n        \"\"\"", "output": "\n        if isinstance(node, ast.Name) and node.id in self.bare_exceptions:\n            return True\n        return False\n\n    def visit_ExceptHandler(self, node):\n        \"\"\"\n        Visits an except handler node and checks if it is a bare exception name.\n\n        \"\"\"\n        if self.is_bare_exception(node.name):\n            self.bare_exceptions.add(node.name.id)\n        if node.type:\n            self.visit(node.type)\n        if node.name:\n            self.visit(node.name)\n        if node.body:\n            self.visit", "solution": "\n        return isinstance(node, Name) and node.id in self.current_except_names"}
{"prompt": "def add_gene_links(gene_obj, build=37):\n    \"\"\"Update a gene object with links\n\n    Args:\n        gene_obj(dict)\n        build(int)\n\n    Returns:\n        gene_obj(dict): gene_obj updated with many links\n    \"\"\"", "output": "\n   _links = get_gene_links(build)\n   _links_dict = {link['name']: link for link in v2_links}\n   _links_dict.update(v3_links)\n\n   _links_dict = {k: {k: v for k, v in link.items() if k in_links_dict[k]} for k, link in_links_dict.items()}\n\n   _links_dict = {k: {k: v for k, v in link.items() if k in_links_dict[k", "solution": "\n    try:\n        build = int(build)\n    except ValueError:\n        build = 37\n    # Add links that use the hgnc_id\n    hgnc_id = gene_obj['hgnc_id']\n\n    gene_obj['hgnc_link'] = genenames(hgnc_id)\n    gene_obj['omim_link'] = omim(hgnc_id)\n    # Add links that use ensembl_id\n    if not 'ensembl_id' in gene_obj:\n        ensembl_id = gene_obj.get('common',{}).get('ensembl_id')\n    else:\n        ensembl_id = gene_obj['ensembl_id']\n    ensembl_37_link = ensembl(ensembl_id, build=37)\n    ensembl_38_link = ensembl(ensembl_id, build=38)\n    gene_obj['ensembl_37_link'] = ensembl_37_link\n    gene_obj['ensembl_38_link'] = ensembl_38_link\n    gene_obj['ensembl_link'] = ensembl_37_link\n    if build == 38:\n        gene_obj['ensembl_link'] = ensembl_38_link\n    gene_obj['hpa_link'] = hpa(ensembl_id)\n    gene_obj['string_link'] = string(ensembl_id)\n    gene_obj['reactome_link'] = reactome(ensembl_id)\n    gene_obj['clingen_link'] = clingen(hgnc_id)\n    gene_obj['expression_atlas_link'] = expression_atlas(ensembl_id)\n    gene_obj['exac_link'] = exac(ensembl_id)\n    # Add links that use entrez_id\n    gene_obj['entrez_link'] = entrez(gene_obj.get('entrez_id'))\n    # Add links that use omim id\n    gene_obj['omim_link'] = omim(gene_obj.get('omim_id'))\n    # Add links that use hgnc_symbol\n    gene_obj['ppaint_link'] = ppaint(gene_obj['hgnc_symbol'])\n    # Add links that use vega_id\n    gene_obj['vega_link'] = vega(gene_obj.get('vega_id'))\n    # Add links that use ucsc link\n    gene_obj['ucsc_link'] = ucsc(gene_obj.get('ucsc_id'))"}
{"prompt": "def cqt_frequencies(n_bins, fmin, bins_per_octave=12, tuning=0.0):\n    \"\"\"Compute the center frequencies of Constant-Q bins.\n\n    Examples\n    --------\n    >>> # Get the CQT frequencies for 24 notes, starting at C2\n    >>> librosa.cqt_frequencies(24, fmin=librosa.note_to_hz('C2'))\n    array([  65.406,   69.296,   73.416,   77.782,   82.407,   87.307,\n             92.499,   97.999,  103.826,  110.   ,  116.541,  123.471,\n            130.813,  138.591,  146.832,  155.563,  164.814,  174.614,\n            184.997,  195.998,  207.652,  220.   ,  233.082,  246.942])\n\n    Parameters\n    ----------\n    n_bins  : int > 0 [scalar]\n        Number of constant-Q bins\n\n    fmin    : float > 0 [scalar]\n        Minimum frequency\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)`\n        Deviation from A440 tuning in fractional bins (cents)\n\n    Returns\n    -------\n    frequencies : np.ndarray [shape=(n_bins,)]\n        Center frequency for each CQT bin\n    \"\"\"", "output": ",   82.407,   87.307,   92.499,\n            97.999,  103.826,  110.000,  116.541,  123.471,  130.813,  138.591,\n           146.832,  155.563,  164.814,  174.614,  184.", "solution": "\n\n    correction = 2.0**(float(tuning) / bins_per_octave)\n    frequencies = 2.0**(np.arange(0, n_bins, dtype=float) / bins_per_octave)\n\n    return correction * fmin * frequencies"}
{"prompt": "def export_keying_material(self, label, olen, context=None):\n        \"\"\"\n        Obtain keying material for application use.\n\n        :param: label - a disambiguating label string as described in RFC 5705\n        :param: olen - the length of the exported key material in bytes\n        :param: context - a per-association context value\n        :return: the exported key material bytes or None\n        \"\"\"", "output": "\n        raise NotImplementedError(\"subclass must implement export_keying_material\")\n\n    def export_keying_material_with_label(self, label,_len, context=None):\n        \"\"\"\n        Obtain keying material for application use with a label.\n\n        :param: label - a disambiguating label string as described in RFC 5705\n        :param:_len - the length of the exported key material in bytes\n        :param: context - a per-association context value\n        :return: a tuple containing the exported key material bytes and the label\n        \"\"\"\n        key_material = self.", "solution": "\n        outp = _no_zero_allocator(\"unsigned char[]\", olen)\n        context_buf = _ffi.NULL\n        context_len = 0\n        use_context = 0\n        if context is not None:\n            context_buf = context\n            context_len = len(context)\n            use_context = 1\n        success = _lib.SSL_export_keying_material(self._ssl, outp, olen,\n                                                  label, len(label),\n                                                  context_buf, context_len,\n                                                  use_context)\n        _openssl_assert(success == 1)\n        return _ffi.buffer(outp, olen)[:]"}
{"prompt": "def register_function(scope=None, as_property=False, name=None):\n    \"\"\"Decorator to register a new function with vaex.\n\n    Example:\n\n    >>> import vaex\n    >>> df = vaex.example()\n    >>> @vaex.register_function()\n    >>> def invert(x):\n    >>>     return 1/x\n    >>> df.x.invert()\n\n\n    >>> import numpy as np\n    >>> df = vaex.from_arrays(departure=np.arange('2015-01-01', '2015-12-05', dtype='datetime64'))\n    >>> @vaex.register_function(as_property=True, scope='dt')\n    >>> def dt_relative_day(x):\n    >>>     return vaex.functions.dt_dayofyear(x)/365.\n    >>> df.departure.dt.relative_day\n    \"\"\"", "output": " dtype='datetime64[D]'),\n   ...                     _id=np.arange(1, 10),\n   ...                     _name=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],\n   ...                     _price=[10, 20, 30, 40, 50, 60, 70, 80, 90],\n   ...                     _quantity=[1, 2, 3, 4, 5, 6, 7,", "solution": "\n    prefix = ''\n    if scope:\n        prefix = scope + \"_\"\n        if scope not in scopes:\n            raise KeyError(\"unknown scope\")\n    def wrapper(f, name=name):\n        name = name or f.__name__\n        # remove possible prefix\n        if name.startswith(prefix):\n            name = name[len(prefix):]\n        full_name = prefix + name\n        if scope:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.expression.ds.func, full_name)\n                    args = (self.expression, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            if as_property:\n                setattr(scopes[scope], name, property(closure()))\n            else:\n                setattr(scopes[scope], name, closure())\n        else:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.ds.func, full_name)\n                    args = (self, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            setattr(vaex.expression.Expression, name, closure())\n        vaex.expression.expression_namespace[prefix + name] = f\n        return f  # we leave the original function as is\n    return wrapper"}
{"prompt": "def iter_hierarchy(self, ontology, size=None, sleep=None):\n        \"\"\"Iterates over parent-child relations\n\n        :param str ontology: The name of the ontology\n        :param int size: The size of each page. Defaults to 500, which is the maximum allowed by the EBI.\n        :param int sleep: The amount of time to sleep between pages. Defaults to 0 seconds.\n        :rtype: iter[tuple[str,str]]\n        \"\"\"", "output": "\n        if size is None:\n            size = 500\n        if sleep is None:\n            sleep = 0\n\n       _url = f\"https://www.ebi.ac.uk/ols/api/ontologies/{ontology}/terms?size={size}\"\n        while True:\n            response = requests.get(url=query_url)\n            response.raise_for_status()\n            data = response.json()\n            for term in data['terms']:\n                yield term['id'], term['label']\n\n            if 'next' not in data:\n                break\n\n            query_url = data['next']", "solution": "\n        for term in self.iter_terms(ontology=ontology, size=size, sleep=sleep):\n            try:\n                hierarchy_children_link = term['_links'][HIERARCHICAL_CHILDREN]['href']\n            except KeyError:  # there's no children for this one\n                continue\n\n            response = requests.get(hierarchy_children_link).json()\n\n            for child_term in response['_embedded']['terms']:\n                yield term['label'], child_term['label']"}
{"prompt": "def closeAllSessions(self, slot):\n        \"\"\"\n        C_CloseAllSessions\n\n        :param slot: slot number\n        :type slot: integer\n        \"\"\"", "output": "\n        =.get_session(slot)\n      .closeAllSessions()\n\n    def closeSession(self, slot, sessionID):\n        \"\"\"\n        C_CloseSession\n\n        :param slot: slot number\n        :type slot: integer\n        :param sessionID: session ID\n        :type sessionID: integer\n        \"\"\"\n        if not isinstance(sessionID, int):\n            raise TypeError(\"sessionID must be an integer\")\n        if sessionID < 0:\n            raise ValueError(\"sessionID must be non-negative\")\n        = self.get_session(slot)\n      .closeSession(sessionID)", "solution": "\n        rv = self.lib.C_CloseAllSessions(slot)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def load_panel_app(adapter, panel_id=None, institute='cust000'):\n    \"\"\"Load PanelApp panels into scout database\n    \n    If no panel_id load all PanelApp panels \n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        panel_id(str): The panel app panel id\n    \"\"\"", "output": "\n    if panel_id:\n        panel_app = adapter.panel_app(panel_id)\n    else:\n        panel_app = adapter.panel_app()\n\n    for panel in panel_app:\n        panel_obj = adapter.panel(panel['_id'])\n        panel_obj['panel_app'] = panel['_id']\n        panel_obj['institute'] = institute\n        adapter.save_panel(panel_obj)\n", "solution": "\n    base_url = 'https://panelapp.genomicsengland.co.uk/WebServices/{0}/'\n    \n    hgnc_map = adapter.genes_by_alias()\n    \n    if panel_id:\n        panel_ids = [panel_id]\n\n    if not panel_id:\n        \n        LOG.info(\"Fetching all panel app panels\")\n        data = get_request(base_url.format('list_panels'))\n    \n        json_lines = json.loads(data)\n        \n        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]\n    \n    for panel_id in panel_ids:\n        panel_data = get_request(base_url.format('get_panel') + panel_id)\n        \n        parsed_panel = parse_panel_app_panel(\n            panel_info = json.loads(panel_data)['result'], \n            hgnc_map=hgnc_map,\n            institute=institute\n        )\n        parsed_panel['panel_id'] = panel_id\n        \n        if len(parsed_panel['genes']) == 0:\n            LOG.warning(\"Panel {} is missing genes. Skipping.\".format(parsed_panel['display_name']))\n            continue\n        \n        try:\n            adapter.load_panel(parsed_panel=parsed_panel)\n        except Exception as err:\n            raise err"}
{"prompt": "def reads_py(s, **kwargs):\n    \"\"\"Read a .py notebook from a string and return the NotebookNode object.\"\"\"", "output": "\n    = get_ipython()\n  .run_cell(s, silent=True)\n  .run_cell(\"import nbformat\", silent=True)\n  .run_cell(\"from nbformat.v4.nbbase import NotebookNode\", silent=True)\n  .run_cell(\"from nbformat.v4.nbformat import v4 as nbformat\", silent=True)\n  .run_cell(\"from nbformat.v4.preprocessors import ExecutePreprocessor\", silent=True)\n  .run_cell(\"from nbformat.v4.preprocessors.", "solution": "\n    nbf, nbm, s = parse_py(s, **kwargs)\n    if nbf == 2:\n        nb = v2.to_notebook_py(s, **kwargs)\n    elif nbf == 3:\n        nb = v3.to_notebook_py(s, **kwargs)\n    else:\n        raise NBFormatError('Unsupported PY nbformat version: %i' % nbf)\n    return nb"}
{"prompt": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"", "output": "\n    try:\n        os.makedirs(path, mode)\n    except OSError as e:\n        if e.errno!= errno.EEXIST:\n            raise\n\n\ndef read_file(path):\n    \"\"\"\n    Reads the contents of a file into a string.\n\n    :param path: The path to the file\n    :type path: str\n    :return: The contents of the file as a string\n    :rtype: str\n    \"\"\"\n    with open(path, 'r') as f:\n        return f.read()\n\n\ndef write_file(path, contents):\n    \"\"\"\n    Writes the contents of a string to", "solution": "\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)"}
{"prompt": "def _resolve_graph(self, distribution_names=None, leaf_name='x'):\n    \"\"\"Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        appropriate `str`.)\n      leaf_name: `str` used when no maker depends on a particular\n        `model` element.\n\n    Returns:\n      graph: `tuple` of `(str tuple)` pairs representing the name of each\n        distribution (maker) and the names of its dependencies.\n\n    #### Example\n\n    ```python\n    d = tfd.JointDistributionSequential([\n                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),\n        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),\n                     tfd.Normal(loc=0, scale=2.),\n        lambda n, g: tfd.Normal(loc=n, scale=g),\n    ])\n    d._resolve_graph()\n    # ==> (\n    #       ('e', ()),\n    #       ('g', ('e',)),\n    #       ('n', ()),\n    #       ('x', ('n', 'g')),\n    #     )\n    ```\n\n    \"\"\"", "output": " distribution.\n\n    Returns:\n      A `tuple` of `tuple`s of dependencies. Each inner `tuple` represents a\n      set of dependencies betweenmber of the model. The outer `tuple`s\n      represent the variables, and the inner `tuple`s represent the\n      variables that depend on them.\n    \"\"\"\n   _variables = []\n   _dependencies = []\n\n    def _resolve_dependencies(node,_variables,_dependencies):\n      if_variables:\n       _variables.append(node)\n      for_variable in_dependencies.get(node, []):", "solution": "\n    # This function additionally depends on:\n    #   self._dist_fn_args\n    #   self._dist_fn_wrapped\n    # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,\n    # ignore args prefixed with `_`.\n    if distribution_names is None or any(self._dist_fn_args):\n      distribution_names = _resolve_distribution_names(\n          self._dist_fn_args, distribution_names, leaf_name)\n    if len(set(distribution_names)) != len(distribution_names):\n      raise ValueError('Distribution names must be unique: {}'.format(\n          distribution_names))\n    if len(distribution_names) != len(self._dist_fn_wrapped):\n      raise ValueError('Distribution names must be 1:1 with `rvs`.')\n    return tuple(zip(distribution_names,\n                     tuple(() if a is None else a for a in self._dist_fn_args)))"}
{"prompt": "def cli_encrypt(context, key):\n    \"\"\"\n    Encrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    This can be useful to encrypt to disk before attempting to\n    upload, allowing uploads retries and segmented encrypted objects.\n\n    See :py:mod:`swiftly.cli.encrypt` for context usage information.\n\n    See :py:class:`CLIEncrypt` for more information.\n    \"\"\"", "output": "\n    encrypt = CLIEncrypt(context, key)\n    encrypt.run()\n\n\nclass CLIEncrypt(object):\n    \"\"\"\n    Encrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    This can be useful to encrypt to disk before attempting to\n    upload, allowing uploads retries and segmented encrypted objects.\n\n    :param context: A :py:class:`swiftly.context.Context` object.\n    :param key: The encryption key to use.\n    \"\"\"\n\n    def __init__(self, context, key):\n        self.context = context\n        self.key = key", "solution": "\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            for chunk in aes_encrypt(key, stdin, preamble=AES256CBC):\n                stdout.write(chunk)\n            stdout.flush()"}
{"prompt": "def _confirm_exit(self):\n        \"\"\"confirm shutdown on ^C\n        \n        A second ^C, or answering 'y' within 5s will cause shutdown,\n        otherwise original SIGINT handler will be restored.\n        \n        This doesn't work on Windows.\n        \"\"\"", "output": "\n        if not_is_windows():\n            signal.signal(signal.SIGINT, self._original_sigint_handler)\n            try:\n                while True:\n                    time.sleep(1)\n            except KeyboardInterrupt:\n                if input(\"Are you sure you want to exit? (y/n) \").lower() == \"y\":\n                    os._exit(0)\n                else:\n                    signal.signal(signal.SIGINT, self._original_sigint_handler)\n        else:\n            print(\"Exiting without confirmation on Windows.\")\n            os._exit(0)\n\n    def _handle_sigint(", "solution": "\n        # FIXME: remove this delay when pyzmq dependency is >= 2.1.11\n        time.sleep(0.1)\n        sys.stdout.write(\"Shutdown Notebook Server (y/[n])? \")\n        sys.stdout.flush()\n        r,w,x = select.select([sys.stdin], [], [], 5)\n        if r:\n            line = sys.stdin.readline()\n            if line.lower().startswith('y'):\n                self.log.critical(\"Shutdown confirmed\")\n                ioloop.IOLoop.instance().stop()\n                return\n        else:\n            print \"No answer for 5s:\",\n        print \"resuming operation...\"\n        # no answer, or answer is no:\n        # set it back to original SIGINT handler\n        # use IOLoop.add_callback because signal.signal must be called\n        # from main thread\n        ioloop.IOLoop.instance().add_callback(self._restore_sigint_handler)"}
{"prompt": "def send_raw(self, stream, msg_list, flags=0, copy=True, ident=None):\n        \"\"\"Send a raw message via ident path.\n\n        This method is used to send a already serialized message.\n\n        Parameters\n        ----------\n        stream : ZMQStream or Socket\n            The ZMQ stream or socket to use for sending the message.\n        msg_list : list\n            The serialized list of messages to send. This only includes the\n            [p_header,p_parent,p_content,buffer1,buffer2,...] portion of\n            the message.\n        ident : ident or list\n            A single ident or a list of idents to use in sending.\n        \"\"\"", "output": " ident or list of idents to use for sending the message.\n            If None, the default ident will be used.\n        flags : int\n            Additional flags to pass to the send operation.\n        copy : bool\n            If True, the message list will be copied before sending.\n        \"\"\"\n        if copy:\n            msg_list = list(msg_list)\n        if ident is None:\n            ident = self.ident\n        if isinstance(ident, list):\n            for i, msg in enumerate(msg_list):\n                stream.send_multipart([ident[i % len(ident)], msg], flags=flags)\n        else:", "solution": "\n        to_send = []\n        if isinstance(ident, bytes):\n            ident = [ident]\n        if ident is not None:\n            to_send.extend(ident)\n\n        to_send.append(DELIM)\n        to_send.append(self.sign(msg_list))\n        to_send.extend(msg_list)\n        stream.send_multipart(msg_list, flags, copy=copy)"}
{"prompt": "def resolve_expression(self, *args, **kwargs) -> HStoreColumn:\n        \"\"\"Resolves the expression into a :see:HStoreColumn expression.\"\"\"", "output": "\n        return HStoreColumn(self.expression, self.model, self.db_table, self.db_column)\n\n    def __str__(self):\n        return str(self.expression)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.expression!r}, {self.model!r}, {self.db_table!r}, {self.db_column!r})\"\n", "solution": "\n\n        original_expression = super().resolve_expression(*args, **kwargs)\n        expression = HStoreColumn(\n            original_expression.alias,\n            original_expression.target,\n            self.key\n        )\n        return expression"}
{"prompt": "def insert_default_options():\n    \"\"\"insert default options to sys.argv\n    \"\"\"", "output": "\n    if len(sys.argv) < 2:\n        sys.argv.append('--help')\n    if '--help' in sys.argv:\n        sys.argv.append('--version')\n    if '--version' in sys.argv:\n        sys.argv.append('--debug')\n    if '--debug' in sys.argv:\n        sys.argv.append('--quiet')\n    if '--quiet' in sys.argv:\n        sys.argv.append('--no-color')\n    if '--no-color' in sys.argv:\n        sys.argv.append('--no-progress')\n    if '--no-progress", "solution": "\n    options = get_default_options()\n    options.reverse()\n    for arg in options:\n        sys.argv.insert(1, arg)"}
{"prompt": "def get_components(self):\n        \"\"\"\n        Returns the components of this name, as a sequence of 2-tuples.\n\n        :return: The components of this name.\n        :rtype: :py:class:`list` of ``name, value`` tuples.\n        \"\"\"", "output": "\n        return self._components\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of this name.\n\n        :return: A string representation of this name.\n        :rtype: :py:class:`str`\n        \"\"\"\n        return self.name\n\n    def __repr__(self):\n        \"\"\"\n        Returns a string representation of this name.\n\n        :return: A string representation of this name.\n        :rtype: :py:class:`str`\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.name!r})\"\n\n    def __eq__(self, other):\n        \"\"\"\n        Compares this name", "solution": "\n        result = []\n        for i in range(_lib.X509_NAME_entry_count(self._name)):\n            ent = _lib.X509_NAME_get_entry(self._name, i)\n\n            fname = _lib.X509_NAME_ENTRY_get_object(ent)\n            fval = _lib.X509_NAME_ENTRY_get_data(ent)\n\n            nid = _lib.OBJ_obj2nid(fname)\n            name = _lib.OBJ_nid2sn(nid)\n\n            # ffi.string does not handle strings containing NULL bytes\n            # (which may have been generated by old, broken software)\n            value = _ffi.buffer(_lib.ASN1_STRING_data(fval),\n                                _lib.ASN1_STRING_length(fval))[:]\n            result.append((_ffi.string(name), value))\n\n        return result"}
{"prompt": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the pages\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"", "output": "\n        raise NotImplementedError()\n\n    def fetch_item(self, category, item_id, **kwargs):\n        \"\"\"Fetch a single item\n\n        :param category: the category of the item to fetch\n        :param item_id: the ID of the item to fetch\n        :param kwargs: backend arguments\n\n        :returns: the fetched item\n        \"\"\"\n        raise NotImplementedError()\n\n    def fetch_items_by_query(self, category, query, **kwargs):\n        \"\"\"Fetch items by a query\n\n        :param category: the category of items to fetch\n        :param query: the query to use for filtering\n        :param kwargs: backend arguments", "solution": "\n        from_date = kwargs['from_date']\n        reviews_api = kwargs['reviews_api']\n\n        mediawiki_version = self.client.get_version()\n        logger.info(\"MediaWiki version: %s\", mediawiki_version)\n\n        if reviews_api:\n            if ((mediawiki_version[0] == 1 and mediawiki_version[1] >= 27) or mediawiki_version[0] > 1):\n                fetcher = self.__fetch_1_27(from_date)\n            else:\n                logger.warning(\"Reviews API only available in MediaWiki >= 1.27\")\n                logger.warning(\"Using the Pages API instead\")\n                fetcher = self.__fetch_pre1_27(from_date)\n        else:\n            fetcher = self.__fetch_pre1_27(from_date)\n\n        for page_reviews in fetcher:\n            yield page_reviews"}
{"prompt": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"", "output": "\n    session = Session()\n    try:\n        yield session\n    finally:\n        session.close()\n\n\ndef create_engine(url):\n    \"\"\"\n    Create a SQLAlchemy engine from a URL.\n    \"\"\"\n    return create_engine(url, pool_pre_ping=True)\n\n\ndef create_database(url):\n    \"\"\"\n    Create a SQLAlchemy database from a URL.\n    \"\"\"\n    engine = create_engine(url)\n    Base.metadata.create_all(engine)\n\n\ndef drop_database(url):\n    \"\"\"\n    Drop a SQLAlchemy database from a URL.\n    \"\"\"\n    engine = create", "solution": "\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()"}
{"prompt": "def disable_report(self, reportid):\n        \"\"\"disable the report of the given id\"\"\"", "output": "\n        self.db.execute(\"UPDATE_reports SET enabled = 0 WHERE id =?\", (reportid,))\n        self.db.commit()\n\n    def enable_report(self, reportid):\n        \"\"\"enable the report of the given id\"\"\"\n        self.db.execute(\"UPDATE it_reports SET enabled = 1 WHERE id =?\", (reportid,))\n        self.db.commit()\n\n    def delete_report(self, reportid):\n        \"\"\"delete the report of the given id\"\"\"\n        self.db.execute(\"DELETE FROM it_reports WHERE id =?\", (reportid,))\n        self.db", "solution": "\n        reportid = reportid.upper()\n        self._reports_state[reportid] = False"}
{"prompt": "def chrono(ctx, app_id, sentence_file,\n           json_flag, sentence, doc_time, request_id):\n    # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None  # NOQA\n    \"\"\"Extract expression expressing date and time and normalize its value \"\"\"", "output": "\n    if not sentence_file:\n        raise ValueError(\"Please provide a sentence file\")\n\n    if not sentence:\n        raise ValueError(\"Please provide a sentence\")\n\n    if not app_id:\n        raise ValueError(\"Please provide an app_id\")\n\n    if not doc_time:\n        raise ValueError(\"Please provide a document time\")\n\n    if not request_id:\n        raise ValueError(\"Please provide a request_id\")\n\n    if not json_flag:\n        raise ValueError(\"Please provide a json_flag\")\n\n    if not os.path.isfile(sentence_file):\n        raise ValueError(\"Sentence file does not exist\")\n\n    # Load", "solution": "\n\n    app_id = clean_app_id(app_id)\n    sentence = clean_sentence(sentence, sentence_file)\n\n    api = GoolabsAPI(app_id)\n    ret = api.chrono(\n        sentence=sentence,\n        doc_time=doc_time,\n        request_id=request_id,\n    )\n\n    if json_flag:\n        click.echo(format_json(api.response.json()))\n        return\n\n    for pair in ret['datetime_list']:\n        click.echo(u'{0}: {1}'.format(text(pair[0]), pair[1]))"}
{"prompt": "def _get_unpacking_extra_info(node, infered):\n    \"\"\"return extra information to add to the message for unpacking-non-sequence\n    and unbalanced-tuple-unpacking errors\n    \"\"\"", "output": "\n    if isinstance(node.parent, ast.Call):\n        # if the parent is a call, we can get the function name and arguments\n        # to include in the error message\n        func_name = node.parent.func.id\n        args = node.parent.args\n    else:\n        # if the parent is not a call, we can get the name of the variable\n        # to include in the error message\n        func_name = node.parent.id\n        args = []\n\n    # if the function is a built-in function, we can get the name of the\n    # function and the arguments to include in", "solution": "\n    more = \"\"\n    infered_module = infered.root().name\n    if node.root().name == infered_module:\n        if node.lineno == infered.lineno:\n            more = \" %s\" % infered.as_string()\n        elif infered.lineno:\n            more = \" defined at line %s\" % infered.lineno\n    elif infered.lineno:\n        more = \" defined at line %s of %s\" % (infered.lineno, infered_module)\n    return more"}
{"prompt": "def godot_options(self, info):\n        \"\"\" Handles display of the options menu. \"\"\"", "output": "\n        if info.type == 'PROPERTY_EDITOR_REQUEST_UPDATE':\n            if info.property_name == 'texture':\n                = self.get_node('/root/Scene')\n                = self.get_node('/root/Scene/Viewport')\n              .texture = info.value\n\nclass MyPlugin(EditorPlugin):\n    \"\"\" Plugin class for the of. \"\"\"\n    bl_idname ='my_plugin.my_plugin'\n    bl_label = 'My Plugin'\n    bl_description = 'A plugin for the.'\n    bl_options = {'REGISTER', '", "solution": "\n\n        if info.initialized:\n            self.edit_traits( parent = info.ui.control,\n                              kind   = \"livemodal\",\n                              view   = \"options_view\" )"}
{"prompt": "def getMappingsOnDeviceType(self, typeId, draft=False):\n        \"\"\"\n        Get all the mappings for a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - draft (boolean) - draft or active\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        return self._get(self._getMappingsUrl(typeId, draft))\n\n    def getMapping(self, mappingId, draft=False):\n        \"\"\"\n        Get a specific mapping.\n        Parameters:\n            - mappingId (string) - the mapping id\n            - draft (boolean) - draft or active\n        Throws APIException on failure.\n        \"\"\"\n        return self._get(self._getMappingUrl(mappingId, draft))\n\n    def createMapping(self, mapping):\n        \"\"\"\n        Create a new mapping.\n        Parameters:\n            - mapping (Mapping) - the mapping to create\n        Throws APIException on failure.\n       ", "solution": "\n        if draft:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"/draft\", typeId)\n        else:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"\", typeId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"All device type mappings retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting all device type mappings\", resp)\n        return resp.json()"}
{"prompt": "def video_list(request, username=None):\n    \"\"\"\n    list of videos of a user\n    if username does not set, shows the currently logged in user\n    \"\"\"", "output": "\n    if not username:\n        username = request.user.username\n\n    user = get_object_or_404(User, username=username)\n    videos = Video.objects.filter(user=user)\n\n    return render(request, 'videos/video_list.html', {'videos': videos})\n\n\ndef video_detail(request, username=None, video_id=None):\n    \"\"\"\n    details of a video of a user\n    if username does not set, shows the currently logged in user\n    \"\"\"\n    if not username:\n        username = request.user.username\n\n    user = get_object", "solution": "\n\n    # If user is not authenticated and username is None, raise an error\n    if username is None and not request.user.is_authenticated():\n        from django.http import Http404\n        raise Http404\n\n    from django.contrib.auth.models import User\n    user = User.objects.get(username=username) if username else request.user\n\n    # loop through the videos of the user\n    videos = Video.objects.filter(user=user).all()\n    video_params = []\n    for video in videos:\n        video_params.append(_video_params(request, video.video_id))\n\n    return render_to_response(\n        \"django_youtube/videos.html\",\n        {\"video_params\": video_params},\n        context_instance=RequestContext(request)\n    )"}
{"prompt": "def weekday(year_or_num, month=None, day=None, full=False):\n    \"\"\"Simple tag - returns the weekday of the given (year, month, day) or of given (weekday_number).\n\n    Usage (in template):\n\n    {% weekday 2014 3 3 %}\n\n    Result: Mon\n\n    Return abbreviation by default. To return full name: pass full=True\n\n    {% weekday 2014 3 3 full=True %}\n\n    Result: Monday\n\n\n    When only number of weekday is given then 0 is considered as \"Monday\"\n\n    {% weekday 0 full=True %}\n\n    Result: Monday\n\n    \"\"\"", "output": " and 1 as \"Tuesday\" and so on.\n    \"\"\"\n    if month is None and day is None:\n        return datetime.date(year_or_num, 1, 1).weekday() + 1\n    elif month is None:\n        return datetime.date(year_or_num, 1, day).weekday() + 1\n    else:\n        return datetime.date(year_or_num, month, day).weekday() + 1\n\n\n@register.filter\ndef weekday_name(weekday_number):\n    \"\"\"Simple tag - returns the full name of the weekday", "solution": "\n    if any([month, day]) and not all([month, day]):\n        raise TemplateSyntaxError(\"weekday accepts 1 or 3 arguments plus optional 'full' argument\")\n\n    try:\n        if all([year_or_num, month, day]):\n            weekday_num = date(*map(int, (year_or_num, month, day))).weekday()\n        else:\n            weekday_num = year_or_num\n        if full:\n            return WEEKDAYS[weekday_num]\n        else:\n            return WEEKDAYS_ABBR[weekday_num]\n    except Exception:\n        return"}
{"prompt": "def clinvar_submissions(store, user_id, institute_id):\n    \"\"\"Get all Clinvar submissions for a user and an institute\"\"\"", "output": "\n    return store.get_clinvar_submissions(user_id, institute_id)\n\n\n@app.route('/clinvar_submissions/<int:submission_id>', methods=['GET'])\ndef clinvar_submission(submission_id):\n    \"\"\"Get a Clinvar submission by its ID\"\"\"\n    return store.get_clinvar_submission(submission_id)\n\n\n@app.route('/clinvar_submissions/<int:submission_id>/submitter', methods=['GET'])\ndef clinvar_submission_submitter(submission_id):\n    \"\"\"Get the submitter of a C", "solution": "\n    submissions = list(store.clinvar_submissions(user_id, institute_id))\n    return submissions"}
{"prompt": "def get_table(self, arch, pattern, colored=False, verbose=False):\n        \"\"\"\n        This function is used in sys command (when user want to find a specific syscall)\n\n        :param Architecture for syscall table;\n        :param Searching pattern;\n        :param Flag for verbose output\n        :return Return a printable table of matched syscalls\n        \"\"\"", "output": "\n       able = []\n       able.append([\"SYSCALL\", \"DESCRIPTION\"])\n        for syscall inable_syscalls[arch]:\n            if pattern in syscall.name:\n                if colored:\n                    syscall_name = colored(syscall.name, 'green')\n                else:\n                    syscall_name = syscall.name\n                syscall_desc = syscall.description\n                if verbose:\n                    syscall_desc = syscall_desc.replace(\"\\n\", \" \")\n                if colored:\n                    syscall_desc = colored(syscall_desc, 'yellow')\n                syscall_desc = syscall_desc.replace(\"\\t\", \" \")\n                syscall_", "solution": "\n\n        rawtable = self.search(arch, pattern)\n        if len(rawtable) == 0:\n            return None\n\n        used_hd = self.__fetch_used_headers(rawtable, verbose)\n        table   = [self.__make_colored_row(used_hd, 'yellow,bold', upper=True) if colored else used_hd]\n\n        for command in rawtable:\n            cur_tb_field = []\n            for hd in used_hd:\n                value = command[hd]\n                cur_tb_field.append(self.__make_colored_field(value, hd, verbose=verbose))\n            table.append(cur_tb_field)\n        return DoubleTable(table)"}
{"prompt": "def str_variant(store, institute_id, case_name, variant_id):\n    \"\"\"Pre-process an STR variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }\n        \"\"\"", "output": "\n            'overlapping_indels': <overlapping_indels>,\n            'consequences': <consequences>,\n            'transcript_consequences': <transcript_consequences>,\n            'gene_consequences': <gene_consequences>,\n            'gene_models': <gene_models>,\n            'gene_models_summary': <gene_models_summary>,\n            'gene_models_summary_html': <gene_models_summary_html>,\n            'gene_models_summary_table': <gene_models_summary_table>,\n            'gene_models_summary_table_html': <gene", "solution": "\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj =  store.variant(variant_id)\n\n    # fill in information for pilup view\n    variant_case(store, case_obj, variant_obj)\n\n    variant_obj['callers'] = callers(variant_obj, category='str')\n\n    # variant_obj['str_ru']\n    # variant_obj['str_repid']\n    # variant_obj['str_ref']\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    return {\n        'institute': institute_obj,\n        'case': case_obj,\n        'variant': variant_obj,\n        'overlapping_snvs': overlapping_snvs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n    }"}
{"prompt": "def index(context, update):\n    \"\"\"Create indexes for the database\"\"\"", "output": "\n    bot = context.bot\n   _db = bot.get_chat_database(update.effective_chat.id)\n   _db.create_indexes()\n    update.effective_message.reply_text(\"Indexes created successfully.\")\n\n\ndef main():\n    \"\"\"Start the bot.\"\"\"\n    bot = telegram.Bot(token=os.environ.get(\"TELEGRAM_BOT_TOKEN\"))\n    dispatcher = Dispatcher(bot, None)\n    dispatcher.add_handler(CommandHandler(\"start\", start))\n    dispatcher.add_handler(CommandHandler(\"help\", help))\n    dispatcher.add_handler(CommandHandler", "solution": "\n    LOG.info(\"Running scout index\")\n    adapter = context.obj['adapter']\n    \n    if update:\n        adapter.update_indexes()\n    else:\n        adapter.load_indexes()"}
{"prompt": "def remove_pid_file(self):\n        \"\"\"Remove the pid file.\n\n        This should be called at shutdown by registering a callback with\n        :func:`reactor.addSystemEventTrigger`. This needs to return\n        ``None``.\n        \"\"\"", "output": "\n        if self.pid_file:\n            os.remove(self.pid_file)\n\n    def start(self):\n        \"\"\"Start thep server.\"\"\"\n        if self.pid_file:\n            with open(self.pid_file, 'w') as pid_file:\n                pid_file.write(str(os.getpid()))\n\n        self.server.start()\n\n    def stop(self):\n        \"\"\"Stop the server.\"\"\"\n        self.server.stop()\n        self.remove_pid_file()\n\n    def restart(self):\n        \"\"\"Restart the server.\"\"\"\n        self.stop()\n        self.", "solution": "\n        pid_file = os.path.join(self.profile_dir.pid_dir, self.name + u'.pid')\n        if os.path.isfile(pid_file):\n            try:\n                self.log.info(\"Removing pid file: %s\" % pid_file)\n                os.remove(pid_file)\n            except:\n                self.log.warn(\"Error removing the pid file: %s\" % pid_file)"}
{"prompt": "def getoutput_pexpect(self, cmd):\n        \"\"\"Run a command and return its stdout/stderr as a string.\n\n        Parameters\n        ----------\n        cmd : str\n          A command to be executed in the system shell.\n\n        Returns\n        -------\n        output : str\n          A string containing the combination of stdout and stderr from the\n        subprocess, in whatever order the subprocess originally wrote to its\n        file descriptors (so the order of the information in this string is the\n        correct order as would be seen if running the command in a terminal).\n        \"\"\"", "output": "\n        child = pexpect.spawn(cmd)\n        child.expect(pexpect.EOF)\n        output = child.before.decode('utf-8') + child.after.decode('utf-8')\n        child.close()\n        return output\n\n    def test____vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_vet_", "solution": "\n        try:\n            return pexpect.run(self.sh, args=['-c', cmd]).replace('\\r\\n', '\\n')\n        except KeyboardInterrupt:\n            print('^C', file=sys.stderr, end='')"}
{"prompt": "def get_signature(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"", "output": "\n        return self.signature_function(value)\n\n    def get_signature_for_value(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"\n        return self.get_signature(value)\n\n    def get_signature_for_values(self, values):\n        \"\"\"Returns the signature for the given values\"\"\"\n        return [self.get_signature(value) for value in values]\n\n    def get_signature_for_values_and_signature_function(self, values, signature_function):\n        \"\"\"Returns the signature for the given values\"\"\"\n        return [signature_function(value) for", "solution": "\n        value = want_bytes(value)\n        key = self.derive_key()\n        sig = self.algorithm.get_signature(key, value)\n        return base64_encode(sig)"}
{"prompt": "def generate_config_file(self):\n        \"\"\"generate default config file from Configurables\"\"\"", "output": "\n        config_file = os.path.join(self.config_dir, 'config.ini')\n        config = ConfigParser()\n        config['DEFAULT'] = {}\n        for configurable in self.config_list:\n            config['DEFAULT'][configurable.name] = configurable.default_value\n        with open(config_file, 'w') as f:\n            config.write(f)\n\n    def load_config(self):\n        \"\"\"load config file and update Configurables\"\"\"\n        config_file = os.path.join(self.config_dir, 'config.ini')\n        if not os.path.", "solution": "\n        lines = [\"# Configuration file for %s.\"%self.name]\n        lines.append('')\n        lines.append('c = get_config()')\n        lines.append('')\n        for cls in self.classes:\n            lines.append(cls.class_config_section())\n        return '\\n'.join(lines)"}
{"prompt": "def _republish_displaypub(self, content, eid):\n        \"\"\"republish individual displaypub content dicts\"\"\"", "output": "\n        = self.get_content_object(content)\n        = self.get_version_object(eid)\n      .content = content\n        v = self.publish_version(eid,)\n        return v\n\n    def _republish_displaypub_list(self, content_list, eid_list):\n        \"\"\"republish individual displaypub content dicts\"\"\"\n       _list = []\n        for content, eid in zip(content_list, eid_list):\n            v = self._republish_displaypub(content, eid)\n            if v:\n                =", "solution": "\n        try:\n            ip = get_ipython()\n        except NameError:\n            # displaypub is meaningless outside IPython\n            return\n        md = content['metadata'] or {}\n        md['engine'] = eid\n        ip.display_pub.publish(content['source'], content['data'], md)"}
{"prompt": "def __make_points_for_label(self, ts, data, label, prefix, gun_stats):\n        \"\"\"x\n        Make a set of points for `this` label\n\n        overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        \"\"\"", "output": "\n       ____ =_u_ =_d_ =_l_ =_r_ =_t_ =_b_ =_c_ =_s_ = 0\n       __ = 0\n       _ down_ = 0\n       _ left_ = 0\n       _ right_ = 0\n       _ top_ = 0\n        query__ = 0\n       _ bottom_ = 0\n       _ center_ = 0\n       _____", "solution": "\n        label_points = list()\n\n        label_points.extend(\n            (\n                # overall quantiles for label\n                self.__make_points(\n                    prefix + \"overall_quantiles\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_quantile_fields(data)\n                ),\n                # overall meta (gun status) for label\n                self.__make_points(\n                    prefix + \"overall_meta\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_overall_meta_fields(data, gun_stats)\n                ),\n                # net codes for label\n                self.__make_points(\n                    prefix + \"net_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_netcodes_fields(data)\n                ),\n                # proto codes for label\n                self.__make_points(\n                    prefix + \"proto_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_protocodes_fields(data)\n                )\n            )\n        )\n        # histograms, one row for each bin\n        if self.histograms:\n            for bin_, count in zip(data[\"interval_real\"][\"hist\"][\"bins\"],\n                                   data[\"interval_real\"][\"hist\"][\"data\"]):\n                label_points.append(\n                    self.__make_points(\n                        prefix + \"histograms\",\n                        {\"label\": label},\n                        ts,\n                        {\"bin\": bin_, \"count\": count}\n                    )\n                )\n        return label_points"}
{"prompt": "def app_url_defaults(self, f):\n        \"\"\"Same as :meth:`url_defaults` but application wide.\n        \"\"\"", "output": "\n        def wrapper(*args, **kwargs):\n            return self.url_defaults(f, *args, **kwargs)\n        return wrapper\n\n    def url_defaults(self, f, *args, **kwargs):\n        \"\"\"Return the default URL for the given view function.\n\n        This method is called by the :meth:`reverse` function to determine\n        the default URL for a view function. It is called with the view\n        function as the first argument and any additional arguments and\n        keyword arguments as specified in the :meth:`url` decorator.\n\n        The default implementation returns the URL as a string.\n\n        Subclasses can override this", "solution": "\n        self.record_once(lambda s: s.app.url_default_functions\n            .setdefault(None, []).append(f))\n        return f"}
{"prompt": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"", "output": "\n        with open(tmp_file, 'w') as f:\n            cursor = self.connection.cursor()\n            cursor.execute(\"SELECT * FROM %s\" % table)\n            headers = [desc[0] for desc in cursor.description]\n            f.write('\\t'.join(headers) + '\\n')\n            for row in cursor.fetchall():\n                f.write('\\t'.join(str(cell) for cell in row) + '\\n')\n        return tmp_file\n\n    def bulk_load(self, table, file_path, delimiter='\\t'):\n        \"\"\"\n        Loads a tab-de", "solution": "\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)"}
{"prompt": "def  make_html_items( self, items ):\n        \"\"\" convert a field's content into some valid HTML \"\"\"", "output": "\n        if not items:\n            return ''\n        html = []\n        for item in items:\n            if isinstance(item, list):\n                html.append(self.make_html_items(item))\n            else:\n                html.append(self.make_html_item(item))\n        return ''.join(html)\n\n    def make_html_item( self, item ):\n        \"\"\" convert a single item into some valid HTML \"\"\"\n        if isinstance(item, str):\n            return item\n        elif isinstance(item, dict):\n            return self.make_html_dict(item)\n        elif isinstance(item, list):", "solution": "\n        lines = []\n        for item in items:\n            if item.lines:\n                lines.append( self.make_html_code( item.lines ) )\n            else:\n                lines.append( self.make_html_para( item.words ) )\n\n        return string.join( lines, '\\n' )"}
{"prompt": "def p_relate_statement_2(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase\"\"\"", "output": "\n        p[0] = RelateStatement(p[2], p[4], p[6], p[1], p[3])\n\n    def p_relate_statement_3(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase AS alias\"\"\"\n        p[0] = RelateStatement(p[2], p[4], p[6], p[1], p[3], p[7])\n\n    def p_relate_statement_4(self, p):\n        \"\"\"statement : RELATE instance_name TO instance", "solution": "\n        p[0] = RelateNode(from_variable_name=p[2],\n                          to_variable_name=p[4],\n                          rel_id=p[6],\n                          phrase=p[8])"}
{"prompt": "def load_exons(self, exons, genes=None, build='37'):\n        \"\"\"Create exon objects and insert them into the database\n        \n        Args:\n            exons(iterable(dict))\n        \"\"\"", "output": "\n        for exon in exons:\n            self.create_exon(exon, genes, build)\n\n    def create_exon(self, exon, genes=None, build='37'):\n        \"\"\"Create an exon object and insert it into the database\n        \n        Args:\n            exon(dict)\n            genes(iterable(dict))\n            build(str)\n        \"\"\"\n        if_exon(exon, build):\n            exon_obj = Exon(exon)\n            exon_obj.insert(genes)\n\n    def update_exon(self, exon, genes=", "solution": "\n        genes = genes or self.ensembl_genes(build)\n        for exon in exons:\n            exon_obj = build_exon(exon, genes)\n            if not exon_obj:\n                continue\n            \n            res = self.exon_collection.insert_one(exon_obj)"}
{"prompt": "def enrich(self, gmt):\n        \"\"\"use local mode\n         \n        p = p-value computed using the Fisher exact test (Hypergeometric test)  \n\n        Not implemented here:\n\n            combine score = log(p)\u00b7z\n\n        see here: http://amp.pharm.mssm.edu/Enrichr/help#background&q=4\n        \n        columns contain:\n            \n            Term Overlap P-value Adjusted_P-value Genes\n\n        \"\"\"", "output": "\n       _genes = set(gmt['Genes'])\n       _genes_list = list(gmt['Genes'])\n       _genes_list.sort()\n       _genes_dict = dict(zip(gmt['Genes'], gmt['Overlap']))\n       _genes_dict_sorted = dict(sorted(gmt['Overlap'].items(), key=lambda x: x[1], reverse=True))\n\n       _genes_dict_sorted_list = []\n        for key, value in sorted(gmt['Overlap'].items(), key=lambda x: x[1], reverse=True):\n           ", "solution": "\n        if isscalar(self.background):\n            if isinstance(self.background, int) or self.background.isdigit():\n                self._bg = int(self.background)\n            elif isinstance(self.background, str):\n                # self.background = set(reduce(lambda x,y: x+y, gmt.values(),[]))\n                self._bg = self.get_background()\n                self._logger.info(\"Background: found %s genes\"%(len(self._bg)))\n            else:\n                raise Exception(\"Unsupported background data type\")\n        else:\n            # handle array object: nd.array, list, tuple, set, Series\n            try:\n                it = iter(self.background)\n                self._bg = set(self.background)\n            except TypeError:\n                self._logger.error(\"Unsupported background data type\")\n        # statistical testing\n        hgtest = list(calc_pvalues(query=self._gls, gene_sets=gmt, \n                                   background=self._bg))\n        if len(hgtest) > 0:\n            terms, pvals, olsz, gsetsz, genes = hgtest\n            fdrs, rej = multiple_testing_correction(ps = pvals, \n                                                    alpha=self.cutoff,\n                                                    method='benjamini-hochberg')\n            # save to a dataframe\n            odict = OrderedDict()\n            odict['Term'] = terms\n            odict['Overlap'] = list(map(lambda h,g: \"%s/%s\"%(h, g), olsz, gsetsz))\n            odict['P-value'] = pvals\n            odict['Adjusted P-value'] = fdrs\n            # odict['Reject (FDR< %s)'%self.cutoff ] = rej\n            odict['Genes'] = [\";\".join(g) for g in genes]\n            res = pd.DataFrame(odict)\n            return res\n        return"}
{"prompt": "def populate_from_sequence(seq: list, r: ref(Edge), sr: state.StateRegister):\n    \"\"\" function that connect each other one sequence of MatchExpr. \"\"\"", "output": "\n    for i in range(len(seq)):\n        if isinstance(seq[i], MatchExpr):\n            r.add_match_expr(seq[i], sr)\n        elif isinstance(seq[i], Sequence):\n            r.add_sequence(seq[i], sr)\n        else:\n            raise ValueError(\"Invalid sequence element\")\n\n\ndef populate_from_match_expr(me: MatchExpr, sr: state.StateRegister):\n    \"\"\" function that connect each other one MatchExpr. \"\"\"\n    if isinstance(me, MatchExpr):\n        me.connect(sr)\n    elif isinstance(me, Sequence", "solution": "\n    base_state = r\n    # we need to detect the last state of the sequence\n    idxlast = len(seq) - 1\n    idx = 0\n    for m in seq:\n        # alternatives are represented by builtin list\n        if isinstance(m, list):\n            # so recursively connect all states of each alternative sequences.\n            for item in m:\n                populate_from_sequence(item, r, sr)\n        elif isinstance(m, MatchExpr):\n            # from the current state, have we a existing edge for this event?\n            eX = r().get_next_edge(m)\n            if eX is None:\n                sX = None\n                if idx != idxlast:\n                    sX = state.State(sr)\n                    sX.matchDefault(base_state().s)\n                else:\n                    # last state of sequence return to the base\n                    sX = base_state().s\n                eX = Edge(sX)\n                r().next_edge[id(sX)] = eX\n                m.attach(r().s, sX, sr)\n            r = ref(eX)\n        idx += 1"}
{"prompt": "def mr_reader(job, input_stream, loads=core.loads):\n    \"\"\" Converts a file object with json serialised pyschema records\n        to a stream of pyschema objects\n\n    Can be used as job.reader in luigi.hadoop.JobTask\n    \"\"\"", "output": "\n    for line in input_stream:\n        try:\n            yield loads(line)\n        except ValueError:\n            job.logger.error(\"Invalid JSON: %s\" % line)\n            raise\n", "solution": "\n    for line in input_stream:\n        yield loads(line),"}
{"prompt": "def add_from_raw_data(self, raw_data, data_type_id, name, description):\n        \"\"\"\n        Upload already serialized raw data as a new dataset.\n\n        Parameters\n        ----------\n        raw_data: bytes\n            Dataset contents to upload.\n        data_type_id : str\n            Serialization format of the raw data.\n            Supported formats are:\n                'PlainText'\n                'GenericCSV'\n                'GenericTSV'\n                'GenericCSVNoHeader'\n                'GenericTSVNoHeader'\n                'ARFF'\n            See the azureml.DataTypeIds class for constants.\n        name : str\n            Name for the new dataset.\n        description : str\n            Description for the new dataset.\n\n        Returns\n        -------\n        SourceDataset\n            Dataset that was just created.\n            Use open(), read_as_binary(), read_as_text() or to_dataframe() on\n            the dataset object to get its contents as a stream, bytes, str or\n            pandas DataFrame.\n        \"\"\"", "output": " Name of the dataset.\n        description : str\n            Description of the dataset.\n\n        Returns\n        -------\n        Dataset\n            The uploaded dataset.\n        \"\"\"\n        if data_type_id not in DataTypeIds.RAW_DATA_TYPE_IDS:\n            raise ValueError(f\"Unsupported data type ID: {data_type_id}\")\n\n        data_type_id = DataTypeIds.RAW_DATA_TYPE_IDS[data_type_id]\n\n        dataset = Dataset.Tabular.from_raw_data(\n            raw_data=raw_data,\n            data_type_id=data_type_id,\n            name", "solution": "\n        _not_none('raw_data', raw_data)\n        _not_none_or_empty('data_type_id', data_type_id)\n        _not_none_or_empty('name', name)\n        _not_none_or_empty('description', description)\n\n        return self._upload(raw_data, data_type_id, name, description)"}
{"prompt": "def write_bit(self, value, bit_num, address):\n        \"\"\"Writes the value given to the bit in the address specified.\n\n        :param value: The value to write.\n        :type value: int\n        :param bit_num: The bit number to write to.\n        :type bit_num: int\n        :param address: The address to write to.\n        :type address: int\n        \"\"\"", "output": "\n       _value = self.read_bit(address)\n       _value = (value << bit_num) | (value & (1 << bit_num) - 1)\n        self.write_byte(address,_value)\n\n    def read_byte(self, address):\n        \"\"\"Reads the byte at the specified address.\n\n        :param address: The address to read from.\n        :type address: int\n        :return: The byte at the specified address.\n        :rtype: int\n        \"\"\"\n       _address = address & 0xFF\n       _data = self.bus.read", "solution": "\n        bit_mask = get_bit_mask(bit_num)\n        old_byte = self.read(address)\n         # generate the new byte\n        if value:\n            new_byte = old_byte | bit_mask\n        else:\n            new_byte = old_byte & ~bit_mask\n        self.write(new_byte, address)"}
{"prompt": "def filter(self, *filters, **kwargs):\n        \"\"\"\n        Returns this Query instance with the query args combined with\n        existing set with AND.\n\n        kwargs are simply passed to a new Filter object and combined to any\n        other filters with AND.\n\n        By default, everything is combined using AND. If you provide\n        multiple filters in a single filter call, those are ANDed\n        together. If you provide multiple filters in multiple filter\n        calls, those are ANDed together.\n\n        If you want something different, use the F class which supports\n        ``&`` (and), ``|`` (or) and ``~`` (not) operators. Then call\n        filter once with the resulting Filter instance.\n        \"\"\"", "output": ") operators.\n        \"\"\"\n        if not filters:\n            filters = []\n        if not kwargs:\n            kwargs = {}\n\n        if not hasattr(self, '_filters'):\n            self._filters = []\n\n        for filter_ in filters:\n            if isinstance(filter_, Filter):\n                self._filters.append(filter_)\n            else:\n                self._filters.append(Filter(filter_))\n\n        for key, value in kwargs.items():\n            if isinstance(value, Filter):\n                self._filters.append(value)\n            else:\n                self._filters.append(Filter(key, value))\n\n        return self\n\n    def exclude(self", "solution": "\n        f = list(filters)\n\n        if kwargs:\n            f += [Filter(**kwargs)]\n\n        return self._clone(filters=f)"}
{"prompt": "def topological_nodes(self):\n        \"\"\"\n        Yield nodes in topological order.\n\n        Returns:\n            generator(DAGNode): node in topological order\n        \"\"\"", "output": "\n        visited = set()\n        stack = []\n        for node in self.nodes:\n            if node not in visited:\n                stack.append(node)\n                while stack:\n                    current = stack.pop()\n                    visited.add(current)\n                    yield current\n                    for child in current.children:\n                        if child not in visited:\n                            stack.append(child)\n\n    def topological_sort(self):\n        \"\"\"\n        Sort nodes in topological order.\n\n        Returns:\n            list(DAGNode): sorted nodes in topological order\n        \"\"\"\n        return list(self.topological_nodes())\n\n    def is_valid(self", "solution": "\n        return nx.lexicographical_topological_sort(self._multi_graph,\n                                                   key=lambda x: str(x.qargs))"}
{"prompt": "def peng_power(snum):\n    r\"\"\"\n    Return engineering suffix and its floating point equivalent of a number.\n\n    :py:func:`peng.peng` lists the correspondence between suffix and floating\n    point exponent.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: named tuple in which the first item is the engineering suffix and\n            the second item is the floating point equivalent of the suffix\n            when the number is represented in engineering notation.\n\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.functions.peng_power\n\n    :raises: RuntimeError (Argument \\`snum\\` is not valid)\n\n    .. [[[end]]]\n\n    For example:\n\n        >>> import peng\n        >>> peng.peng_power(peng.peng(1235.6789E3, 3, False))\n        EngPower(suffix='M', exp=1000000.0)\n    \"\"\"", "output": "oc_type_signature(peng_power,'snum'))]]]\n   .. [[[end]]]\n\n    :Example:\n\n    >>> from peng import peng_power\n    >>> = 12345678901234567890\n    >>> = 12345678901234567890\n    >>> = 12345678901234567890\n    >>> = 123456789012345", "solution": "\n    suffix = \" \" if snum[-1].isdigit() else snum[-1]\n    return EngPower(suffix, _SUFFIX_POWER_DICT[suffix])"}
{"prompt": "def get_next_proto_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by NPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n\n        .. versionadded:: 0.15\n        \"\"\"", "output": "\n        return self._next_proto_negotiated\n\n    def set_next_proto_negotiated(self, proto):\n        \"\"\"\n        Set the protocol that was negotiated by NPN.\n\n        :param proto: A bytestring of the protocol name.\n\n       .. versionadded:: 0.15\n        \"\"\"\n        self._next_proto_negotiated = proto\n\n    def get_alpn_selected(self):\n        \"\"\"\n        Get the application-layer protocol that was selected by ALPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            selected", "solution": "\n        _warn_npn()\n        data = _ffi.new(\"unsigned char **\")\n        data_len = _ffi.new(\"unsigned int *\")\n\n        _lib.SSL_get0_next_proto_negotiated(self._ssl, data, data_len)\n\n        return _ffi.buffer(data[0], data_len[0])[:]"}
{"prompt": "def update_virtual_meta(self):\n        \"\"\"Will read back the virtual column etc, written by :func:`DataFrame.write_virtual_meta`. This will be done when opening a DataFrame.\"\"\"", "output": "\n        self._virtual_meta = self._read_virtual_meta()\n\n    def _read_virtual_meta(self):\n        \"\"\"Read the virtual column metadata from the-disk file.\"\"\"\n        if not self._virtual_meta_file:\n            return {}\n        with open(self._virtual_meta_file, 'rb') as f:\n            return pickle.load(f)\n\n    def _write_virtual_meta(self, virtual_meta):\n        \"\"\"Write the virtual column metadata to the on-disk file.\"\"\"\n        if not self._virtual_meta_file:\n            return\n        with open(self._", "solution": "\n        import astropy.units\n        try:\n            path = os.path.join(self.get_private_dir(create=False), \"virtual_meta.yaml\")\n            if os.path.exists(path):\n                meta_info = vaex.utils.read_json_or_yaml(path)\n                if 'virtual_columns' not in meta_info:\n                    return\n                self.virtual_columns.update(meta_info[\"virtual_columns\"])\n                self.variables.update(meta_info[\"variables\"])\n                self.ucds.update(meta_info[\"ucds\"])\n                self.descriptions.update(meta_info[\"descriptions\"])\n                units = {key: astropy.units.Unit(value) for key, value in meta_info[\"units\"].items()}\n                self.units.update(units)\n        except:\n            logger.exception(\"non fatal error\")"}
{"prompt": "def delete(self, teamId):\n        \"\"\"Delete a team.\n\n        Args:\n            teamId(basestring): The ID of the team to be deleted.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        if not isinstance(teamId, basestring):\n            raise TypeError(\"Parameter 'teamId' must be a string\")\n\n        response = self._delete(\"/teams/{teamId}\".format(teamId=teamId))\n        if response.status_code == 204:\n            return\n        raise ApiError(response)\n\n    def update(self, teamId, name=None, description=None, visibility=None, logo=None):\n        \"\"\"Update a team.\n\n        Args:\n            teamId(basestring): The ID of the team to be updated.\n            name(basestring, optional): The new name", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n\n        # API request\n        self._session.delete(API_ENDPOINT + '/' + teamId)"}
{"prompt": "def per_triangle(script, sidedim=0, textdim=1024, border=2, method=1):\n    \"\"\"Trivial Per-Triangle parameterization\n\n    \"\"\"", "output": "\n    =ap(script, sidedim, textdim, border, method)\n    =(script, sidedim, textdim, border, method)\n    =.reshape(sidedim, sidedim)\n    =.T\n    =.reshape(sidedim, sidedim)\n    =.T\n    =.reshape(sidedim, sidedim)\n    =.T\n    =.reshape(sidedim, sidedim)\n    =.T\n    =.reshape(sidedim, sidedim)\n    =", "solution": "\n    filter_xml = ''.join([\n        '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n',\n        '    <Param name=\"sidedim\"',\n        'value=\"%d\"' % sidedim,\n        'description=\"Quads per line\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"',\n        '/>\\n',\n        '    <Param name=\"textdim\"',\n        'value=\"%d\"' % textdim,\n        'description=\"Texture Dimension (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Gives an indication on how big the texture is\"',\n        '/>\\n',\n        '    <Param name=\"border\"',\n        'value=\"%d\"' % border,\n        'description=\"Inter-Triangle border (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"',\n        '/>\\n',\n        '    <Param name=\"method\"',\n        'value=\"%d\"' % method,\n        'description=\"Method\"',\n        'enum_val0=\"Basic\"',\n        'enum_val1=\"Space-optimizing\"',\n        'enum_cardinality=\"2\"',\n        'type=\"RichEnum\"',\n        'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"'\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None"}
{"prompt": "def add_virtual_columns_cartesian_velocities_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", vx=\"vx\", vy=\"vy\", vz=\"vz\", vr=\"vr\", vlong=\"vlong\", vlat=\"vlat\", distance=None):\n        \"\"\"Concert velocities from a cartesian to a spherical coordinate system\n\n        TODO: errors\n\n        :param x: name of x column (input)\n        :param y:         y\n        :param z:         z\n        :param vx:       vx\n        :param vy:       vy\n        :param vz:       vz\n        :param vr: name of the column for the radial velocity in the r direction (output)\n        :param vlong: name of the column for the velocity component in the longitude direction  (output)\n        :param vlat: name of the column for the velocity component in the latitude direction, positive points to the north pole (output)\n        :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance\n        :return:\n        \"\"\"", "output": " vz:       vz\n        :param vr:       vr\n        :param vlong:    vlong\n        :param vlat:     vlat\n        :param distance: distance (optional)\n        :return:_velocities_velocities_spherical_velocities_cartesian_velocities_cartesian_spherical_velocities_cartesian_spherical_distance_velocities_cartesian_spherical_distance_error\n        \"\"\"\n       ", "solution": "\n        # see http://www.astrosurf.com/jephem/library/li110spherCart_en.htm\n        if distance is None:\n            distance = \"sqrt({x}**2+{y}**2+{z}**2)\".format(**locals())\n        self.add_virtual_column(vr, \"({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}\".format(**locals()))\n        self.add_virtual_column(vlong, \"-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)\".format(**locals()))\n        self.add_virtual_column(vlat, \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( {distance}*sqrt({x}**2+{y}**2) )\".format(**locals()))"}
{"prompt": "def check_token(token):\n    \"\"\" Verify http header token authentification \"\"\"", "output": "\n    try:\n        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=['HS256'])\n        return payload\n    except jwt.InvalidTokenError:\n        return None\n\n\ndef get_token(user):\n    \"\"\" Generate a JWT token for a user \"\"\"\n    payload = {\n       'sub': user.id,\n        'email': user.email,\n        'username': user.username,\n        'exp': datetime.datetime.utcnow() + datetime.timedelta(days=1, seconds=0),\n    }\n    return jwt.encode(payload, settings.SECRET_KEY, algorithm='HS", "solution": "\n    user = models.User.objects(api_key=token).first()\n    return user or None"}
{"prompt": "def validate_value(self, value):\n        \"\"\"\n        Validate new property value before setting it.\n\n        value -- New value\n        \"\"\"", "output": "\n        pass\n\n    def validate_property(self, name, value):\n        \"\"\"\n        Validate new property value before setting it.\n\n        name -- Name of the property\n        value -- New value\n        \"\"\"\n        pass\n\n    def set_property(self, name, value):\n        \"\"\"\n        Set a new property value.\n\n        name -- Name of the property\n        value -- New value\n        \"\"\"\n        self.validate_property(name, value)\n        self.properties[name] = value\n\n    def get_property(self, name):\n        \"\"\"\n        Get a property value.\n\n        name -- Name of the property\n        \"\"\"\n        return self.properties", "solution": "\n        if 'readOnly' in self.metadata and self.metadata['readOnly']:\n            raise PropertyError('Read-only property')\n\n        try:\n            validate(value, self.metadata)\n        except ValidationError:\n            raise PropertyError('Invalid property value')"}
{"prompt": "def pack_metadata_statement(self, metadata, receiver='', iss='', lifetime=0,\n                                sign_alg=''):\n        \"\"\"\n        Given a MetadataStatement instance create a signed JWT.\n\n        :param metadata: Original metadata statement as a MetadataStatement\n            instance\n        :param receiver: Receiver (audience) of the JWT\n        :param iss: Issuer ID if different from default\n        :param lifetime: jWT signature life time\n        :param sign_alg: JWT signature algorithm\n        :return: A JWT\n        \"\"\"", "output": "\n        # Create a copy of the metadata statement to avoid modifying the original\n        metadata_copy = copy.deepcopy(metadata)\n\n        # Set the receiver and issuer if provided\n        if receiver:\n            metadata_copy.receiver = receiver\n        if iss:\n            metadata_copy.issuer = iss\n\n        # Create a JWT with the metadata statement as payload\n        jwt_payload = metadata_copy.to_dict()\n\n        # Create a JWT with the payload and signature\n        jwt = jwt_helper.create_jwt(jwt_payload, sign_alg)\n\n        # Set the signature lifetime if provided\n        if lifetime:\n            jwt_helper.", "solution": "\n\n        return self.self_signer.sign(metadata, receiver=receiver, iss=iss,\n                                     lifetime=lifetime, sign_alg=sign_alg)"}
{"prompt": "def contact(self, id):\n        \"\"\"\n        Retrieve all details for a specific contact, including fields such as birthday and mood.\n\n        Args:\n            id (str): user identifier to lookup\n\n        Returns:\n            SkypeContact: resulting contact object\n        \"\"\"", "output": "\n        response = self._api.get(f\"/users/{id}/contacts\")\n        return SkypeContact(response.json(), self._api)\n\n    def search_contacts(self, query):\n        \"\"\"\n        Search for contacts by name or email.\n\n        Args:\n            query (str): query to search for\n\n        Returns:\n            list[SkypeContact]: list of contact objects\n        \"\"\"\n        response = self._api.get(f\"/users/{self._user_id}/contacts/search\", params={\"q\": query})\n        return [SkypeContact(contact, self._api) for contact in response.json", "solution": "\n        try:\n            json = self.skype.conn(\"POST\", \"{0}/users/batch/profiles\".format(SkypeConnection.API_USER),\n                                   json={\"usernames\": [id]}, auth=SkypeConnection.Auth.SkypeToken).json()\n            contact = SkypeContact.fromRaw(self.skype, json[0])\n            if contact.id not in self.contactIds:\n                self.contactIds.append(contact.id)\n            return self.merge(contact)\n        except SkypeApiException as e:\n            if len(e.args) >= 2 and getattr(e.args[1], \"status_code\", None) == 403:\n                # Not a contact, so no permission to retrieve information.\n                return None\n            raise"}
{"prompt": "def str_repeat(x, repeats):\n    \"\"\"Duplicate each string in a column.\n\n    :param int repeats: number of times each string sample is to be duplicated.\n    :returns: an expression containing the duplicated strings\n\n    Example:\n\n    >>> import vaex\n    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']\n    >>> df = vaex.from_arrays(text=text)\n    >>> df\n      #  text\n      0  Something\n      1  very pretty\n      2  is coming\n      3  our\n      4  way.\n\n    >>> df.text.str.repeat(3)\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: str (expression)\n    ---------------------------------\n    0        SomethingSomethingSomething\n    1  very prettyvery prettyvery pretty\n    2        is comingis comingis coming\n    3                          ourourour\n    4                       way.way.way.\n    \"\"\"", "output": "str_repeat(2)\n      #  text\n      0  SomethingSomething\n      1  very prettyvery pretty\n      2  is comingis coming\n      3  ourour\n      4  way.\n    \"\"\"\n    return.str_repeat(x, repeats)\n\n\n@register_function('str_replace','str_replace_all')\ndef str_replace(x, old, new, regex=False, flags=0):\n    \"\"\"Replace all occurrences of a substring in a string with a new substring.\n\n    :param str x: the string to search in.\n    :param str old: the substring", "solution": "\n    sl = _to_string_sequence(x).repeat(repeats)\n    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)"}
{"prompt": "def deprecated(*args):\n    \"\"\" Deprecation warning decorator. Takes optional deprecation message,\n    otherwise will use a generic warning. \"\"\"", "output": "\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import warnings\n            message = \"This function is deprecated and will be removed in a future release.\"\n            if len(args) > 0:\n                message += \" \" + args[0]\n            warnings.warn(message, DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\ndef deprecated_method(message=None):\n    \"\"\" Decorator for marking a method as deprecated. \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import warnings\n            if message is None:", "solution": "\n    def wrap(func):\n        def wrapped_func(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapped_func\n\n    if len(args) == 1 and callable(args[0]):\n        msg = \"Function '%s' will be deprecated in future versions of \" \\\n            \"Neurosynth.\" % args[0].__name__\n        return wrap(args[0])\n    else:\n        msg = args[0]\n        return wrap"}
{"prompt": "def get_last_activities(self, n):\n        \"\"\"Get all activity data for the last activity\n\n        Keyword arguments:\n        \"\"\"", "output": "\n        activities = self.get_activities()\n        if len(activities) < n:\n            return activities\n        else:\n            return activities[-n:]\n\n    def get_activity(self, activity_id):\n        \"\"\"Get a specific activity data\n\n        Keyword arguments:\n        \"\"\"\n        activities = self.get_activities()\n        for activity in activities:\n            if activity['id'] == activity_id:\n                return activity\n        return None\n\n    def get_activity_data(self, activity_id):\n        \"\"\"Get a specific activity data\n\n        Keyword arguments:\n        \"\"\"\n        activity = self.get_activity(activity_id)\n       ", "solution": "\n        filenames = self.get_activity_list().iloc[-n:].filename.tolist()\n        last_activities = [self.get_activity(f) for f in filenames]\n        return last_activities"}
{"prompt": "def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):\n    \"\"\"\n    Update a user in the database\n    \"\"\"", "output": "\n    user = User.objects.get(id=user_id)\n    if update_role:\n        user.role = update_role\n    if add_institute:\n        user.institutes.add(Institute.objects.get(id=add_institute))\n    if remove_admin:\n        user.is_admin = False\n    if remove_institute:\n        user.institutes.remove(Institute.objects.get(id=remove_institute))\n    user.save()\n    return user\n\n\n@register.inclusion_tag('user_profile/user_profile_form.html", "solution": "\n    adapter = context.obj['adapter']\n\n    user_obj = adapter.user(user_id)\n\n    if not user_obj:\n        LOG.warning(\"User %s could not be found\", user_id)\n        context.abort()\n\n    existing_roles = set(user_obj.get('roles',[]))\n    if update_role:\n        if not update_role in user_obj['roles']:\n            existing_roles = set(user_obj['roles'])\n            existing_roles.add(update_role)\n            LOG.info(\"Adding role %s to user\", update_role)\n        else:\n            LOG.warning(\"User already have role %s\", update_role)\n\n    if remove_admin:\n        try:\n            existing_roles.remove('admin')\n            LOG.info(\"Removing admin rights from user %s\", user_id)\n        except KeyError as err:\n            LOG.info(\"User %s does not have admin rights\", user_id)\n\n    user_obj['roles'] = list(existing_roles)\n\n    existing_institutes = set(user_obj.get('institutes',[]))\n    for institute_id in add_institute:\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.warning(\"Institute %s could not be found\", institute_id)\n        else:\n            existing_institutes.add(institute_id)\n            LOG.info(\"Adding institute %s to user\", institute_id)\n\n    for institute_id in remove_institute:\n        try:\n            existing_institutes.remove(institute_id)\n            LOG.info(\"Removing institute %s from user\", institute_id)\n        except KeyError as err:\n            LOG.info(\"User does not have access to institute %s\", institute_id)\n\n    user_obj['institutes'] = list(existing_institutes)\n\n    updated_user = adapter.update_user(user_obj)"}
{"prompt": "def _update_base(self):\n        \"\"\"update the base, including the URL for GitLab and the API endpoint.\n        \"\"\"", "output": "\n        self.base = self.config.get('gitlab', 'base', '')\n        self.api_endpoint = self.config.get('gitlab', 'api_endpoint', '')\n        self.token = self.config.get('gitlab', 'token', '')\n        self.private_token = self.config.get('gitlab', 'private_token', '')\n        self.project_id = self.config.get('gitlab', 'project_id', '')\n        self.group_id = self.config.get('gitlab', 'group_id', '')\n        self.project_name = self.config", "solution": "\n        self.base = self._get_and_update_setting('SREGISTRY_GITLAB_BASE',\n                                                 \"https://gitlab.com/\")\n        self.api_base = \"%s/api/v4\" % self.base.strip('/')\n        self.artifacts = self._get_and_update_setting('SREGISTRY_GITLAB_FOLDER',\n                                                      'build')\n\n        self.job = self._get_and_update_setting('SREGISTRY_GITLAB_JOB', 'build')\n\n        bot.debug('      Api: %s' % self.api_base)\n        bot.debug('Artifacts: %s' % self.artifacts)\n        bot.debug('      Job: %s' % self.job)"}
{"prompt": "def send(self, message):\n        \"\"\" Send a message object\n\n            :type message: data.OutgoingMessage\n            :param message: The message to send\n            :rtype: data.OutgoingMessage\n            :returns: The sent message with populated fields\n            :raises AssertionError: wrong provider name encountered (returned by the router, or provided to OutgoingMessage)\n            :raises MessageSendError: generic errors\n            :raises AuthError: provider authentication failed\n            :raises LimitsError: sending limits exceeded\n            :raises CreditError: not enough money on the account\n        \"\"\"", "output": "\n        if not message.provider:\n            raise AssertionError(\"No provider name provided in the message\")\n\n        if not self.router.can_send(message.provider):\n            raise AssertionError(\"Provider '{}' is not allowed to send messages\".format(message.provider))\n\n        try:\n            sent_message = self.router.send(message)\n        except Exception as e:\n            raise MessageSendError(\"Error sending message: {}\".format(e))\n\n        return sent_message\n\n    def get_balance(self, provider=None):\n        \"\"\" Get the balance of the account\n\n            :type provider: str\n            :param provider: The provider", "solution": "\n        # Which provider to use?\n        provider_name = self._default_provider  # default\n        if message.provider is not None:\n            assert message.provider in self._providers, \\\n                'Unknown provider specified in OutgoingMessage.provideer: {}'.format(provider_name)\n            provider = self.get_provider(message.provider)\n        else:\n            # Apply routing\n            if message.routing_values is not None: # Use the default provider when no routing values are given\n                # Routing values are present\n                provider_name = self.router(message, *message.routing_values) or self._default_provider\n                assert provider_name in self._providers, \\\n                    'Routing function returned an unknown provider name: {}'.format(provider_name)\n            provider = self.get_provider(provider_name)\n\n        # Set message provider name\n        message.provider = provider.name\n\n        # Send the message using the provider\n        message = provider.send(message)\n\n        # Emit the send event\n        self.onSend(message)\n\n        # Finish\n        return message"}
{"prompt": "def string_arg(ctx, obj):\n    \"\"\"\n    Handles LiteralObjects as well as computable arguments\n    \"\"\"", "output": "\n    if isinstance(obj, LiteralObject):\n        return obj.value\n    elif callable(obj):\n        return obj()\n    else:\n        return obj\n\n\ndef parse_args(args, arg_types):\n    \"\"\"\n    Parses a list of arguments according to their types.\n    \"\"\"\n    parsed_args = []\n    for arg, arg_type in zip(args, arg_types):\n        if arg_type == str:\n            parsed_args.append(string_arg(arg, arg.value))\n        elif arg_type == int:\n            parsed_args.append(int(arg))\n        elif arg_", "solution": "\n    if hasattr(obj, 'compute'):\n        obj = next(obj.compute(ctx), False)\n    return to_string(obj)"}
{"prompt": "def on_kill(self):\n        \"\"\"\n        Cancel the submitted athena query\n        \"\"\"", "output": "\n        if self.athena_query_id:\n            self.athena.cancel_query_execution(self.athena_query_id)\n            self.athena_query_id = None\n\n    def on_error(self, error):\n        \"\"\"\n        Handle errors that occur during the execution of the athena query\n        \"\"\"\n        self.logger.error(f\"Error executing Athena query: {error}\")\n        self.on_kill()\n\n    def on_success(self, result):\n        \"\"\"\n        Handle successful execution of the athena query\n        \"\"\"\n        self.logger.info(f\"", "solution": "\n        if self.query_execution_id:\n            self.log.info('\u26b0\ufe0f\u26b0\ufe0f\u26b0\ufe0f Received a kill Signal. Time to Die')\n            self.log.info(\n                'Stopping Query with executionId - %s', self.query_execution_id\n            )\n            response = self.hook.stop_query(self.query_execution_id)\n            http_status_code = None\n            try:\n                http_status_code = response['ResponseMetadata']['HTTPStatusCode']\n            except Exception as ex:\n                self.log.error('Exception while cancelling query', ex)\n            finally:\n                if http_status_code is None or http_status_code != 200:\n                    self.log.error('Unable to request query cancel on athena. Exiting')\n                else:\n                    self.log.info(\n                        'Polling Athena for query with id %s to reach final state', self.query_execution_id\n                    )\n                    self.hook.poll_query_status(self.query_execution_id)"}
{"prompt": "def get_fernet():\n    \"\"\"\n    Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet object\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n    \"\"\"", "output": "\n    try:\n        from cryptography.fernet import Fernet\n    except ImportError:\n        raise AirflowException(\"Please install cryptography to use Fernet encryption.\")\n\n    try:\n        with open(Fernet_key_file, 'rb') as key_file:\n            key = key_file.read()\n    except IOError:\n        raise AirflowException(\"Invalid Fernet key file.\")\n\n    try:\n        fernet = Fernet(key)\n    except ValueError:\n        raise AirflowException(\"Invalid Fernet key.\")\n\n    return fernet\n", "solution": "\n    global _fernet\n    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet"}
{"prompt": "def update_board(self, query_params=None):\n        \"\"\"\n        Update this board's information. Returns a new board.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n\n        url = self.api_url + '/update'\n        response = self.session.post(url, data=query_params)\n        response.raise_for_status()\n        return Board(response.json(), self.session)\n\n    def delete(self):\n        \"\"\"\n        Delete this board.\n        \"\"\"\n        url = self.api_url + '/delete'\n        response = self.session.post(url)\n        response.raise_for_status()\n\n    def add_label(self, label_name):\n        \"\"\"\n        Add a", "solution": "\n        board_json = self.fetch_json(\n            uri_path=self.base_uri,\n            http_method='PUT',\n            query_params=query_params or {}\n        )\n\n        return self.create_board(board_json)"}
{"prompt": "def compose_u3(theta1, phi1, lambda1, theta2, phi2, lambda2):\n        \"\"\"Return a triple theta, phi, lambda for the product.\n\n        u3(theta, phi, lambda)\n           = u3(theta1, phi1, lambda1).u3(theta2, phi2, lambda2)\n           = Rz(phi1).Ry(theta1).Rz(lambda1+phi2).Ry(theta2).Rz(lambda2)\n           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda2)\n           = u3(theta', phi1 + phi', lambda2 + lambda')\n\n        Return theta, phi, lambda.\n        \"\"\"", "output": "1+phi2).Ry(theta2).Rz(lambda2)\n           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda1+phi2).Ry(theta2).Rz(lambda2)\n           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda1+phi2).Ry(theta2).Rz(lambda2)\n           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda1+phi2).Ry(theta2).Rz(lambda2", "solution": "\n        # Careful with the factor of two in yzy_to_zyz\n        thetap, phip, lambdap = Optimize1qGates.yzy_to_zyz((lambda1 + phi2), theta1, theta2)\n        (theta, phi, lamb) = (thetap, phi1 + phip, lambda2 + lambdap)\n\n        return (theta, phi, lamb)"}
{"prompt": "def filter_trim(self, start=1, end=1, filt=True):\n        \"\"\"\n        Remove points from the start and end of filter regions.\n        \n        Parameters\n        ----------\n        start, end : int\n            The number of points to remove from the start and end of\n            the specified filter.\n        filt : valid filter string or bool\n            Which filter to trim. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n        if filt is True:\n            filt = self.active_filters\n        if isinstance(filt, str):\n            filt = [filt]\n        for f in filt:\n            if f in self.filters:\n                self.filters[f].trim(start, end)\n        self.update_filter_regions()\n\n    def filter_resample(self,_freq,_time,__units,__start,\n                       _time_units,_time_start,_time_end,_time_step,\n                       _freq_units,_freq_", "solution": "\n        params = locals()\n        del(params['self'])\n            \n        f = self.filt.grab_filt(filt)\n        nf = filters.trim(f, start, end)\n        \n        self.filt.add('trimmed_filter',\n                    nf,\n                    'Trimmed Filter ({:.0f} start, {:.0f} end)'.format(start, end),\n                    params, setn=self.filt.maxset + 1)"}
{"prompt": "def verify_reg_list(self, obj, object_type):\n        \"\"\"Verify a list of registers.\"\"\"", "output": "\n        if not isinstance(obj, list):\n            raise ValueError(\"Expected a list of registers, got {}\".format(type(obj)))\n        for reg in obj:\n            if not isinstance(reg, Register):\n                raise ValueError(\"Expected a list of registers, got {}\".format(type(reg)))\n            if reg.object_type!= object_type:\n                raise ValueError(\"Expected registers of type {}, got {}\".format(object_type, reg.object_type))\n\n    def verify_reg_dict(self, obj, object_type):\n        \"\"\"Verify a dictionary of registers.\"\"\"\n        if not isinstance(obj, dict):", "solution": "\n        # We expect the object to be a bitlist or an idlist, we don't care.\n        # We will iterate it and ensure everything in it is declared as a bit,\n        # and throw if not.\n        for children in obj.children:\n            self.verify_reg(children, object_type)"}
{"prompt": "def regression(self, slope=None):\n        \"\"\"regress tip values against branch values\n\n        Parameters\n        ----------\n        slope : None, optional\n            if given, the slope isn't optimized\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"", "output": "\n       _values = self.branch_values\n       _values = np.array(list(map(float, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip, list(map(str.strip", "solution": "\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model"}
{"prompt": "def to_keep(datetimes,\n            years=0, months=0, weeks=0, days=0,\n            hours=0, minutes=0, seconds=0,\n            firstweekday=SATURDAY, now=None):\n    \"\"\"\n    Return a set of datetimes that should be kept, out of ``datetimes``.\n\n    Keeps up to ``years``, ``months``, ``weeks``, ``days``,\n    ``hours``, ``minutes``, and ``seconds`` in the past.\n\n    When keeping weeks, it prefers to keep ``firstweekday``, which\n    defaults to Saturday.\n\n    If ``now`` is None, it will base its calculations on\n    ``datetime.datetime.now()``. Datetimes after this point will always be\n    kept.\n    \"\"\"", "output": "`` is provided, it is used as the current time instead of\n    the current time on the local machine.\n\n    Args:\n        datetimes (list): A list of datetime objects.\n        years (int): The number of years to keep in the past.\n        months (int): The number of months to keep in the past.\n        weeks (int): The number of weeks to keep in the past.\n        days (int): The number of days to keep in the past.\n        hours (int): The number of hours to keep in the past.\n        minutes (int): The number of minutes to keep in the past.\n       ", "solution": "\n    datetimes = set(datetimes)\n    return (filters.Years.filter(datetimes, number=years, now=now) |\n            filters.Months.filter(datetimes, number=months, now=now) |\n            filters.Weeks.filter(datetimes, number=weeks,\n                                 firstweekday=firstweekday, now=now) |\n            filters.Days.filter(datetimes, number=days, now=now) |\n            filters.Hours.filter(datetimes, number=hours, now=now) |\n            filters.Minutes.filter(datetimes, number=minutes, now=now) |\n            filters.Seconds.filter(datetimes, number=seconds, now=now))"}
{"prompt": "def set(self, client_id, code, request, *args, **kwargs):\n        \"\"\"Creates Grant object with the given params\n\n        :param client_id: ID of the client\n        :param code:\n        :param request: OAuthlib request object\n        \"\"\"", "output": "\n        self.client_id = client_id\n        self.code = code\n        self.request = request\n        self.args = args\n        self.kwargs = kwargs\n        self.user = None\n        self.scopes = None\n        self.expires_at = None\n        self.created_at = None\n\n    def __repr__(self):\n        return '<Grant {}>'.format(self.code)\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __eq__(self, other):\n        return self.code == other.code\n\n    def __ne__(self, other):\n        return not self.__eq__(", "solution": "\n        expires = datetime.utcnow() + timedelta(seconds=100)\n        grant = self.model(\n            client_id=request.client.client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scope=' '.join(request.scopes),\n            user=self.current_user(),\n            expires=expires\n        )\n        self.session.add(grant)\n\n        self.session.commit()"}
{"prompt": "def parse_hpo_diseases(hpo_lines):\n    \"\"\"Parse hpo disease phenotypes\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with mim numbers as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        if line.startswith(\"MIM\"):\n            mim_number, disease_name = line.split(\"\\t\")\n            diseases[mim_number] = disease_name\n    return diseases\n\n\ndef parse_hpo_phenotypes(hpo_lines):\n    \"\"\"Parse hpo phenotypes\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            phenotypes(dict): A dictionary with mim numbers as keys\n    \"\"\"\n    phenotypes = {}\n    for line in hpo_lines:\n        if line", "solution": "\n    diseases = {}\n    LOG.info(\"Parsing hpo diseases...\")\n    for index, line in enumerate(hpo_lines):\n        # First line is a header\n        if index == 0:\n            continue\n        # Skip empty lines\n        if not len(line) > 3:\n            continue\n        # Parse the info\n        disease_info = parse_hpo_disease(line)\n        # Skip the line if there where no info\n        if not disease_info:\n            continue\n        disease_nr = disease_info['disease_nr']\n        hgnc_symbol = disease_info['hgnc_symbol']\n        hpo_term = disease_info['hpo_term']\n        source = disease_info['source']\n        disease_id = \"{0}:{1}\".format(source, disease_nr)\n        \n        if disease_id not in diseases:\n            diseases[disease_id] = {\n                'disease_nr': disease_nr,\n                'source': source,\n                'hgnc_symbols': set(),\n                'hpo_terms': set(),\n            }\n\n        if hgnc_symbol:\n            diseases[disease_id]['hgnc_symbols'].add(hgnc_symbol)\n        if hpo_term:\n            diseases[disease_id]['hpo_terms'].add(hpo_term)\n\n    LOG.info(\"Parsing done.\")\n    return diseases"}
{"prompt": "def _check_currency_format(self, format=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            format (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"", "output": "\n        if format is None:\n            format = self.currency_format\n        if format == 'USD':\n            return 'USD'\n        elif format == 'EUR':\n            return 'EUR'\n        elif format == 'JPY':\n            return 'JPY'\n        else:\n            raise ValueError(f'Unsupported currency format: {format}')\n\n    def _check_currency_symbol(self, symbol=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            symbol (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"\n        if symbol is None:\n            symbol = self.currency_symbol\n        if symbol ==", "solution": "\n        defaults = self.settings['currency']['format']\n        if hasattr(format, '__call__'):\n            format = format()\n        if is_str(format) and re.match('%v', format):\n\n            # Create and return positive, negative and zero formats:\n            return {\n                'pos': format,\n                'neg': format.replace(\"-\", \"\").replace(\"%v\", \"-%v\"),\n                'zero': format\n            }\n        elif not format or not format['por'] or not re.match('%v',\n                                                             format['pos']):\n            self.settings['currency']['format'] = {\n                'pos': defaults,\n                'neg': defaults.replace(\"%v\", \"-%v\"),\n                'zero': defaults\n            }\n            return self.settings\n\n        return format"}
{"prompt": "def setMeterPassword(self, new_pwd, pwd=\"00000000\"):\n        \"\"\" Serial Call to set meter password.  USE WITH CAUTION.\n\n        Args:\n            new_pwd (str): 8 digit numeric password to set\n            pwd (str): Old 8 digit numeric password.\n\n        Returns:\n            bool: True on completion with ACK.\n        \"\"\"", "output": "\n       cp_cmd =cp_commands.SetMeterPassword(new_pwd, pwd)\n        return self.send_command(self.meter_id,cp_cmd)\n\n    def getMeterStatus(self):\n        \"\"\" Serial Call to get meter status.\n\n        Returns:\n            MeterStatus: Meter status object.\n        \"\"\"\n        return self.send_command(self.meter_id,cp_commands.GetMeterStatus())\n\n    def getMeterInfo(self):\n        \"\"\" Serial Call to get meter information.\n\n        Returns:\n            MeterInfo: Meter information object.\n        \"\"\"\n        return self.send", "solution": "\n        result = False\n        self.setContext(\"setMeterPassword\")\n        try:\n            if len(new_pwd) != 8 or len(pwd) != 8:\n                self.writeCmdMsg(\"Passwords must be exactly eight characters.\")\n                self.setContext(\"\")\n                return result\n\n            if not self.request(False):\n                self.writeCmdMsg(\"Pre command read failed: check serial line.\")\n            else:\n                if not self.serialCmdPwdAuth(pwd):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_pwd = binascii.hexlify(new_pwd.zfill(8))\n                    req_str = \"015731023030323028\" + req_pwd + \"2903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success(setMeterPassword): 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"", "output": "\n        message = self.getMessage()\n        if self.args:\n            message = message % self.args\n        return message\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of this LogRecord.\n\n        The string representation consists of the message, the level name,\n        the module name, the process ID, the thread ID, and the timestamp.\n        \"\"\"\n        message = self.getMessage()\n        levelname = self.levelname\n        module = self.module\n        process = self.process\n        thread = self.thread\n        timestamp = self.asctime\n\n        return '%s - %s - %s - %s", "solution": "\n        msg = self.msg\n        if self.args:\n            msg = msg.format(*self.args)\n        return maybe_encode(msg)"}
{"prompt": "def add_observer(self, callback, ntype, sender):\n        \"\"\"Add an observer callback to this notification center.\n\n        The given callback will be called upon posting of notifications of\n        the given type/sender and will receive any additional arguments passed\n        to post_notification.\n\n        Parameters\n        ----------\n        callback : callable\n            The callable that will be called by :meth:`post_notification`\n            as ``callback(ntype, sender, *args, **kwargs)\n        ntype : hashable\n            The notification type. If None, all notifications from sender\n            will be posted.\n        sender : hashable\n            The notification sender. If None, all notifications of ntype\n            will be posted.\n        \"\"\"", "output": " If None, all notifications of type ntype\n            will be posted.\n        \"\"\"\n        if ntype is None:\n            ntype = sender\n        if sender is None:\n            sender = ntype\n        if ntype not in self._observers:\n            self._observers[ntype] = {}\n        if sender not in self._observers[ntype]:\n            self._observers[ntype][sender] = []\n        self._observers[ntype][sender].append(callback)\n\n    def remove_observer(self, callback, ntype=None, sender=None):\n        \"\"\"Remove an observer callback from", "solution": "\n        assert(callback != None)\n        self.registered_types.add(ntype)\n        self.registered_senders.add(sender)\n        self.observers.setdefault((ntype,sender), set()).add(callback)"}
{"prompt": "def cli_run():\n    \"\"\"docstring for argparse\"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='Runim')\n    parser.add_argument('-i', '--input', help='Inputif file', required=True)\n    parser.add_argument('-o', '--output', help='Outputif file', required=True)\n    parser.add_argument('-d', '--directory', help='Directory to saveif files', required=True)\n    parser.add_argument('-c', '--channels', help='Number ofi channels to use', type=int, default=1)\n    parser.add_argument('-s', '--sample_rate", "solution": "\n    parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow')\n    parser.add_argument('query', help=\"What's the problem ?\", type=str, nargs='+')\n    parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda')\n    args = parser.parse_args()\n    main(args)"}
{"prompt": "def send_email(self, to, kind, **kwargs):\n        \"\"\" Sends an e-mail to the given address.\n\n        to: The address\n        kind: the ID for an e-mail kind; it should point to a subdirectory of\n            self.template_prefix containing subject.txt and message.html, which\n            are django templates for the subject and HTML message respectively.\n\n        context: a context for rendering the e-mail.\n\n        \"\"\"", "output": "\n        subject_template = self.template_prefix + '/' + kind + '/subject.txt'\n        message_template = self.template_prefix + '/' + kind + '/message.html'\n\n        subject = render_to_string(subject_template, kwargs)\n        message = render_to_string(message_template, kwargs)\n\n        msg = EmailMultiAlternatives(subject, message, self.from_email, [to])\n        msg.attach_alternative(message, \"text/html\")\n        msg.send()\n", "solution": "\n\n        return __send_email__(self.template_prefix, to, kind, **kwargs)"}
{"prompt": "def list_to_str(lst: list, content: str, indent: int=1):\n    \"\"\"recurs into list for string computing \"\"\"", "output": "\n    if isinstance(lst, list):\n        for item in lst:\n            if isinstance(item, list):\n                list_to_str(item, content, indent)\n            else:\n                content += f\"{''* indent}{item}\\n\"\n    else:\n        content += f\"{''* indent}{lst}\\n\"\n    return content\n\ndef str_to_list(s: str):\n    \"\"\"convert string to list\"\"\"\n    lst = []\n    for line in s.split('\\n'):\n        if line.strip():\n            lst.append(line.strip())\n    return lst\n\ndef list", "solution": "\n    for i in lst:\n        if isinstance(i, indentable):\n            content = i.to_str(content, indent)\n        elif isinstance(i, list):\n            content = list_to_str(i, content, indent)\n        elif isinstance(i, str):\n            content = catend(content, i, indent)\n    return content"}
{"prompt": "def add_btn_cancel(self,label_cancel):\n        \"\"\"\n        Adds a cancel button to let the user cancel whatever choice they were given.\n        \n        This widget can be triggered by setting the label ``label_cancel`` to a string.\n        \n        This widget will be positioned slightly below the main label and to the right\n        of the confirm button.\n        \"\"\"", "output": "\n        cancel_btn = QPushButton(label_cancel, self)\n        cancel_btn.move(self.confirm_btn.x() + self.confirm_btn.width() + 5, self.confirm_btn.y())\n        cancel_btn.clicked.connect(self.cancel_clicked)\n        self.cancel_btn = cancel_btn\n    \n    def cancel_clicked(self):\n        \"\"\"\n        Slot for the cancel button.\n        \"\"\"\n        self.close()\n", "solution": "\n        # Cancel Button\n        self.wbtn_cancel = button.Button(\"btn_cancel\",self,self.window,self.peng,\n                        pos=lambda sw,sh, bw,bh: (sw/2+4,sh/2-bh/2-bh*2),\n                        size=[0,0],\n                        label=label_cancel,\n                        borderstyle=self.borderstyle\n                        )\n        self.wbtn_cancel.size = lambda sw,sh: (self.wbtn_cancel._label.font_size*8,self.wbtn_cancel._label.font_size*2)\n        self.addWidget(self.wbtn_cancel)\n        \n        def f():\n            self.doAction(\"cancel\")\n            self.exitDialog()\n        self.wbtn_cancel.addAction(\"click\",f)"}
{"prompt": "def _link_field_to_dict(field):\n        \"\"\" Utility for ripping apart github's Link header field.\n        It's kind of ugly.\n        \"\"\"", "output": "\n        url, name = field.split(';')\n        name = name.strip()\n        return {'url': url, 'name': name}\n\n    def _get_links(self, response):\n        \"\"\" Extracts the Link header field from the response and returns a list of dictionaries.\n        \"\"\"\n        link_header = response.headers.get('Link')\n        if not link_header:\n            return []\n\n        links = []\n        for field in link_header.split(','):\n            link_dict = _link_field_to_dict(field)\n            links.append(link_dict)\n\n        return links\n\n   ", "solution": "\n\n        if not field:\n            return dict()\n\n        return dict([\n            (\n                part.split('; ')[1][5:-1],\n                part.split('; ')[0][1:-1],\n            ) for part in field.split(', ')\n        ])"}
{"prompt": "def _get_protobuf_kind(kind):\n    \"\"\"Converts py_zipkin's Kind to Protobuf's Kind.\n\n    :param kind: py_zipkin's Kind.\n    :type kind: py_zipkin.Kind\n    :return: correcponding protobuf's kind value.\n    :rtype: zipkin_pb2.Span.Kind\n    \"\"\"", "output": "\n    if kind == Kind.SERVER:\n        return zipkin_pb2.Span.SERVER\n    elif kind == Kind.CLIENT:\n        return zipkin_pb2.Span.CLIENT\n    elif kind == Kind.PRODUCER:\n        return zipkin_pb2.Span.PRODUCER\n    elif kind == Kind.CONSUMER:\n        return zipkin_pb2.Span.CONSUMER\n    else:\n        raise ValueError(\"Invalid kind: {}\".format(kind))\n\n\ndef _get_protobuf_annotations(annotations):\n    \"\"\"Converts py_zipkin's Annotations to Protobuf's Annotations.", "solution": "\n    if kind == Kind.CLIENT:\n        return zipkin_pb2.Span.CLIENT\n    elif kind == Kind.SERVER:\n        return zipkin_pb2.Span.SERVER\n    elif kind == Kind.PRODUCER:\n        return zipkin_pb2.Span.PRODUCER\n    elif kind == Kind.CONSUMER:\n        return zipkin_pb2.Span.CONSUMER\n    return None"}
{"prompt": "def _assert_recur_is_tail(node: Node) -> None:  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the tail position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` node.\"\"\"", "output": "\n    if isinstance(node, Do):\n        if node.recur is not None:\n            raise ValueError(f\"Unexpected'recur' form in 'do' node: {node}\")\n    elif isinstance(node, If):\n        if node.recur is not None:\n            raise ValueError(f\"Unexpected'recur' form in 'if' node: {node}\")\n        if node.then is not None and node.then.recur is not None:\n            raise ValueError(f\"Unexpected'recur' form in 'then' expression: {node.then}\")\n        if node.else_ is not None and node.else_.", "solution": "\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)"}
{"prompt": "def remove_namespaces(root):\n    \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\"", "output": "\n    for elem in root.iter():\n        elem.tag = elem.tag.split('}', 1)[-1]\n    return root\n\n\ndef get_text(node):\n    \"\"\"Return the text content of a node, including any nested text nodes\"\"\"\n    text = []\n    for child in node.iter():\n        if child.text:\n            text.append(child.text)\n    return ''.join(text)\n\n\ndef get_xpath(node, path):\n    \"\"\"Return the value of a node at a given xpath\"\"\"\n    return node.xpath(path, namespaces=node.nsmap)[0", "solution": "\n    for elem in root.getiterator():\n        if not hasattr(elem.tag, 'find'):\n            continue\n\n        i = elem.tag.find('}')\n        if i >= 0:\n            elem.tag = elem.tag[i + 1:]\n\n    objectify.deannotate(root, cleanup_namespaces=True)"}
{"prompt": "def _block_tuple(iterator, dtypes, bsize=-1):\n    \"\"\"Pack rdd of tuples as tuples of arrays or scipy.sparse matrices.\"\"\"", "output": "\n    =ajob.Job(\n       ajob.Map(lambda x:ajob.Tuple(x), iterator),\n       ajob.FlatMap(lambda x:ajob.Tuple(x)),\n       ajob.Map(lambda x:.pack_tuple(x, dtypes)),\n       ajob.Map(lambda x:.unpack_tuple(x, dtypes)),\n    )\n  .run(bsize=bsize)\n\n\ndef _block_sparse(iterator,,_dtypes,_shape,_format,", "solution": "\n    i = 0\n    blocked_tuple = None\n    for tuple_i in iterator:\n        if blocked_tuple is None:\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n\n        if (bsize > 0) and (i >= bsize):\n            yield tuple(_pack_accumulated(x, dtype)\n                        for x, dtype in zip(blocked_tuple, dtypes))\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n            i = 0\n\n        for x_j, x in zip(tuple_i, blocked_tuple):\n            x.append(x_j)\n        i += 1\n    if i > 0:\n        yield tuple(_pack_accumulated(x, dtype)\n                    for x, dtype in zip(blocked_tuple, dtypes))"}
{"prompt": "def ddos_custom_policies(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-11-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_11_01.operations.DdosCustomPoliciesOperations>`\n           * 2018-12-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_12_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-02-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`\n        \"\"\"", "output": "01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-04-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_04_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-06-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_06_01.", "solution": "\n        api_version = self._get_api_version('ddos_custom_policies')\n        if api_version == '2018-11-01':\n            from .v2018_11_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2018-12-01':\n            from .v2018_12_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2019-02-01':\n            from .v2019_02_01.operations import DdosCustomPoliciesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def update(self):\n        \"\"\"Update repository from its remote.\n\n        Calling this method, the repository will be synchronized with\n        the remote repository using 'fetch' command for 'heads' refs.\n        Any commit stored in the local copy will be removed; refs\n        will be overwritten.\n\n        :raises RepositoryError: when an error occurs updating the\n            repository\n        \"\"\"", "output": "\n        try:\n            self.repo.fetch()\n            self.repo.reset('--hard', 'origin/master')\n        except GitCommandError as e:\n            raise RepositoryError(str(e)) from e\n\n    def get_commits(self, branch='master'):\n        \"\"\"Return a list of commits on the given branch.\n\n        :param branch: the branch to get commits from\n        :type branch: str\n        :return: a list of commits on the given branch\n        :rtype: list[Commit]\n        \"\"\"\n        try:\n            return [Commit(self.repo, commit) for commit in self.repo.iter", "solution": "\n        cmd_update = ['git', 'fetch', 'origin', '+refs/heads/*:refs/heads/*', '--prune']\n        self._exec(cmd_update, cwd=self.dirpath, env=self.gitenv)\n\n        logger.debug(\"Git %s repository updated into %s\",\n                     self.uri, self.dirpath)"}
{"prompt": "def setZeroResettableKWH(self, password=\"00000000\"):\n        \"\"\" Serial call to zero resettable kWh registers.\n\n        Args:\n            password (str): Optional password.\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"", "output": "\n      .expect([pexpect.TIMEOUT, pexpect.EOF, pexpect.TIMEOUT, pexpect.EOF])\n        self.sendline(\"AT+ZERORSTKWH\")\n        if password:\n            self.expect([pexpect.TIMEOUT, pexpect.EOF, pexpect.TIMEOUT, pexpect.EOF])\n            self.sendline(\"AT+ZERORSTKWH={}\".format(password))\n        self.expect([pexpect.TIMEOUT, pexpect.EOF, pexpect.TIMEOUT, pexpect.EOF])\n        self.sendline(\"AT+ZERORSTKWH", "solution": "\n        result = False\n        self.setContext(\"setZeroResettableKWH\")\n        try:\n            if not self.requestA():\n                self.writeCmdMsg(\"Bad read CRC on setting\")\n            else:\n                if not self.serialCmdPwdAuth(password):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_str = \"0157310230304433282903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success: 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def add_heart_failure_handler(self, handler):\n        \"\"\"add a new handler for heart failure\"\"\"", "output": "\n        self.heart_failure_handlers.append(handler)\n\n    def remove_heart_failure_handler(self, handler):\n        \"\"\"remove a handler for heart failure\"\"\"\n        self.heart_failure_handlers.remove(handler)\n\n    def notify_heart_failure(self,____heart_rate,_____heart_rate):\n        \"\"\"notify all registered handlers about a heart failure\"\"\"\n        for handler in self.heart_failure_handlers:\n            handler(out_of_range=True,_heart_rate=out_of", "solution": "\n        self.log.debug(\"heartbeat::new heart failure handler: %s\", handler)\n        self._failure_handlers.add(handler)"}
{"prompt": "def like_hashtag(self, hashtag, amount=None):\n    \"\"\" Likes last medias from hashtag \"\"\"", "output": "\n    if amount is None:\n        amount = self.config.getint('likes', 'amount')\n    medias = self.get_medias_by_hashtag(hashtag, amount)\n    for media in medias:\n        self.like_media(media)\n\ndef unlike_hashtag(self, hashtag, amount=None):\n    \"\"\" Unlikes last medias from hashtag \"\"\"\n    if amount is None:\n        amount = self.config.getint('likes', 'amount')\n    medias = self.get_medias_by_hashtag(hashtag", "solution": "\n    self.logger.info(\"Going to like media with hashtag #%s.\" % hashtag)\n    medias = self.get_total_hashtag_medias(hashtag, amount)\n    return self.like_medias(medias)"}
{"prompt": "def __coord_fft_hz(n, sr=22050, **_kwargs):\n    \"\"\"Get the frequencies for FFT bins\"\"\"", "output": "\n    =_hz(sr)\n    =_hz(sr,_factor=2)\n    =_hz(sr,_factor=3)\n    =_hz(sr,_factor=4)\n    =_hz(sr,_factor=5)\n    =_hz(sr, v_factor=2)\n    =_hz(sr,_factor=3)\n    =_hz(sr, v_factor=4)\n    =_hz(sr,_factor=5", "solution": "\n    n_fft = 2 * (n - 1)\n    # The following code centers the FFT bins at their frequencies\n    # and clips to the non-negative frequency range [0, nyquist]\n    basis = core.fft_frequencies(sr=sr, n_fft=n_fft)\n    fmax = basis[-1]\n    basis -= 0.5 * (basis[1] - basis[0])\n    basis = np.append(np.maximum(0, basis), [fmax])\n    return basis"}
{"prompt": "def main(ctx, log_level, keeper_hostname, username, password):\n    \"\"\"ltd is a command-line client for LSST the Docs.\n\n    Use ltd to upload new site builds, and to work with the LTD Keeper API.\n    \"\"\"", "output": "\n    logging.basicConfig(level=log_level)\n    logging.getLogger('urllib3').setLevel(logging.WARNING)\n\n    # Set up the Keeper API client\n    keeper_client = KeeperClient(keeper_hostname, username, password)\n\n    # Set up the command-line interface\n    parser = argparse.ArgumentParser(description='ltd is a command-line client for LSST the Docs.')\n    subparsers = parser.add_subparsers(dest='command')\n\n    # Upload a new site build\n    upload_parser = subparsers.add_parser('upload', help='Upload a new site", "solution": "\n    ch = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s %(levelname)8s %(name)s | %(message)s')\n    ch.setFormatter(formatter)\n\n    logger = logging.getLogger('ltdconveyor')\n    logger.addHandler(ch)\n    logger.setLevel(log_level.upper())\n\n    # Subcommands should use the click.pass_obj decorator to get this\n    # ctx.obj object as the first argument.\n    ctx.obj = {\n        'keeper_hostname': keeper_hostname,\n        'username': username,\n        'password': password,\n        'token': None\n    }"}
{"prompt": "def _fetch_from_archive(self, method, args):\n        \"\"\"Fetch data from the archive\n\n        :param method: the name of the command to execute\n        :param args: the arguments required by the command\n        \"\"\"", "output": "\n        if method == 'get_data':\n            return self._get_data(*args)\n        elif method == 'get_metadata':\n            return self._get_metadata(*args)\n        else:\n            raise ValueError(f'Invalid method: {method}')\n\n    def _get_data(self,as,date,ime,lat, urelat,lon, urelon,lev, urelev,time, uretime,res, ureres,res,resu,resv,resw,resx, ureres", "solution": "\n        if not self.archive:\n            raise ArchiveError(cause=\"Archive not provided\")\n\n        data = self.archive.retrieve(method, args, None)\n\n        if isinstance(data, nntplib.NNTPTemporaryError):\n            raise data\n\n        return data"}
{"prompt": "def attach_dynamic_tree_items(tree_alias, src_tree_items):\n        \"\"\"Attaches dynamic sitetrees items registered with `register_dynamic_trees()`\n        to an initial (source) items list.\n\n        :param str|unicode tree_alias:\n        :param list src_tree_items:\n        :rtype: list\n        \"\"\"", "output": "\n        dynamic_trees = get_dynamic_trees()\n        if tree_alias in dynamic_trees:\n            dynamic_tree_items = dynamic_trees[tree_alias]\n            for item in dynamic_tree_items:\n                src_tree_items.append(item)\n        return src_tree_items\n\n\ndef register_dynamic_trees(tree_alias, tree_items):\n    \"\"\"Registers dynamic sitetrees items for later retrieval.\n\n    :param str|unicode tree_alias:\n    :param list tree_items:\n    \"\"\"\n    dynamic_trees = get_dynamic_trees()\n    dynamic_", "solution": "\n        if not _DYNAMIC_TREES:\n            return src_tree_items\n\n        # This guarantees that a dynamic source stays intact,\n        # no matter how dynamic sitetrees are attached.\n        trees = deepcopy(_DYNAMIC_TREES)\n\n        items = []\n        if not src_tree_items:\n            if _IDX_ORPHAN_TREES in trees and tree_alias in trees[_IDX_ORPHAN_TREES]:\n                for tree in trees[_IDX_ORPHAN_TREES][tree_alias]:\n                    items.extend(tree.dynamic_items)\n        else:\n\n            # TODO Seems to be underoptimized %)\n\n            # Tree item attachment by alias.\n            for static_item in list(src_tree_items):\n                items.append(static_item)\n                if not static_item.alias:\n                    continue\n\n                idx = _IDX_TPL % (tree_alias, static_item.alias)\n                if idx not in trees:\n                    continue\n\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    for dyn_item in tree.dynamic_items:\n                        if dyn_item.parent is None:\n                            dyn_item.parent = static_item\n                        # Unique IDs are required for the same trees attached\n                        # to different parents.\n                        dyn_item.id = generate_id_for(dyn_item)\n                        items.append(dyn_item)\n\n            # Tree root attachment.\n            idx = _IDX_TPL % (tree_alias, None)\n            if idx in _DYNAMIC_TREES:\n                trees = deepcopy(_DYNAMIC_TREES)\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    items.extend(tree.dynamic_items)\n\n        return items"}
{"prompt": "def snip_print(str,width = 75,print_full = 0,header = ''):\n    \"\"\"Print a string snipping the midsection to fit in width.\n\n    print_full: mode control:\n      - 0: only snip long strings\n      - 1: send to page() directly.\n      - 2: snip long strings and ask for full length viewing with page()\n    Return 1 if snipping was necessary, 0 otherwise.\"\"\"", "output": "\n    if len(str) <= width:\n        if print_full == 0:\n            print(str)\n        elif print_full == 1:\n            print(str)\n            page()\n        else:\n            print(str)\n            return 1\n    else:\n        mid = width // 2\n        if print_full == 0:\n            print(str[:mid] + '...' + str[mid:])\n        elif print_full == 1:\n            print(str[:mid] + '...' + str[mid:])\n            page()\n        else:\n            print(str[:mid] + '...' + str", "solution": "\n\n    if print_full == 1:\n        page(header+str)\n        return 0\n\n    print header,\n    if len(str) < width:\n        print str\n        snip = 0\n    else:\n        whalf = int((width -5)/2)\n        print str[:whalf] + ' <...> ' + str[-whalf:]\n        snip = 1\n    if snip and print_full == 2:\n        if raw_input(header+' Snipped. View (y/n)? [N]').lower() == 'y':\n            page(str)\n    return snip"}
{"prompt": "def _choi_to_chi(data, input_dim, output_dim):\n    \"\"\"Transform Choi representation to the Chi representation.\"\"\"", "output": "\n    =man.choi_to_choi(data,=input_dim,_prime=output_dim)\n    =man.choi_to_choi(data,=output_dim,_prime=input_dim)\n   _prime =man.choi_to_choi(data,=output_dim,_prime=output_dim)\n   _prime_prime =man.choi_to_choi(data,=output_dim,_prime=output_dim)\n   ", "solution": "\n    num_qubits = int(np.log2(input_dim))\n    return _transform_to_pauli(data, num_qubits)"}
{"prompt": "def rst_msg(self, text, opts={}):\n        \"\"\"Convert ReStructuredText and run through msg()\"\"\"", "output": "\n        rst = docutils.core.publish_string(text, writer_name='html', settings_overrides={'output_encoding': 'unicode'})\n        rst = rst.replace('<html><body>', '').replace('</body></html>', '')\n        self.msg(rst, opts)\n\n    def msg(self, text, opts={}):\n        \"\"\"Print a message to the console with optional formatting\"\"\"\n        if 'color' in opts:\n            color =_colors[opts['color']]\n        else:\n            color = None\n        if 'bold' in opts:\n            text = bold(text)\n        if 'underline'", "solution": "\n        text = Mformat.rst_text(text,\n                                'plain' == self.debugger.settings['highlight'],\n                                self.debugger.settings['width'])\n        return self.msg(text)"}
{"prompt": "def measure_topology(fbasename=None, log=None, ml_version=ml_version):\n    \"\"\"Measures mesh topology\n\n    Args:\n        fbasename (str): input filename.\n        log (str): filename to log output\n\n    Returns:\n        dict: dictionary with the following keys:\n            vert_num (int): number of vertices\n            edge_num (int): number of edges\n            face_num (int): number of faces\n            unref_vert_num (int): number or unreferenced vertices\n            boundry_edge_num (int): number of boundary edges\n            part_num (int): number of parts (components) in the mesh.\n            manifold (bool): True if mesh is two-manifold, otherwise false.\n            non_manifold_edge (int): number of non_manifold edges.\n            non_manifold_vert (int): number of non-manifold verices\n            genus (int or str): genus of the mesh, either a number or\n                'undefined' if the mesh is non-manifold.\n            holes (int or str): number of holes in the mesh, either a number\n                or 'undefined' if the mesh is non-manifold.\n\n    \"\"\"", "output": " of parts\n           _part_num (int): number of parts\n           _edge_num (int): number of edges\n           _face_num (int): number of faces\n           _vert_num (int): number of vertices\n           _boundry_edge_num (int): number of boundary edges\n           _part_edge_num (int): number of part edges\n           _part_face_num (int): number of part faces\n           _part_vert_num (int): number of part vertices\n           _part_boundry_", "solution": "\n    ml_script1_file = 'TEMP3D_measure_topology.mlx'\n    ml_script1 = mlx.FilterScript(file_in=fbasename, ml_version=ml_version)\n    compute.measure_topology(ml_script1)\n    ml_script1.save_to_file(ml_script1_file)\n    ml_script1.run_script(log=log, script_file=ml_script1_file)\n    topology = ml_script1.topology\n    return topology"}
{"prompt": "def import_submodule(mod, subname, fullname):\n    \"\"\"m = import_submodule(mod, subname, fullname)\"\"\"", "output": "\n    if hasattr(mod, subname):\n        return getattr(mod, subname)\n    else:\n        __import__(fullname)\n        return getattr(sys.modules[fullname], subname)\n\n\ndef import_module_from_path(path):\n    \"\"\"\n    Import a module from a path.\n\n    :param path: The path to the module to import.\n    :return: The imported module.\n    \"\"\"\n    if not os.path.exists(path):\n        raise ValueError(\"The path '{}' does not exist.\".format(path))\n\n    if not os.path.isdir(path):\n        raise ValueError(\"The", "solution": "\n    # Require:\n    # if mod == None: subname == fullname\n    # else: mod.__name__ + \".\" + subname == fullname\n\n    global found_now\n    if fullname in found_now and fullname in sys.modules:\n        m = sys.modules[fullname]\n    else:\n        print 'Reloading', fullname\n        found_now[fullname] = 1\n        oldm = sys.modules.get(fullname, None)\n\n        if mod is None:\n            path = None\n        elif hasattr(mod, '__path__'):\n            path = mod.__path__\n        else:\n            return None\n\n        try:\n            # This appears to be necessary on Python 3, because imp.find_module()\n            # tries to import standard libraries (like io) itself, and we don't\n            # want them to be processed by our deep_import_hook.\n            with replace_import_hook(original_import):\n                fp, filename, stuff = imp.find_module(subname, path)\n        except ImportError:\n            return None\n\n        try:\n            m = imp.load_module(fullname, fp, filename, stuff)\n        except:\n            # load_module probably removed name from modules because of\n            # the error.  Put back the original module object.\n            if oldm:\n                sys.modules[fullname] = oldm\n            raise\n        finally:\n            if fp: fp.close()\n\n        add_submodule(mod, m, fullname, subname)\n\n    return m"}
{"prompt": "def velocity_from_bundle(self, bundle):\n        \"\"\"[DEPRECATED] Return velocity, given the `coefficient_bundle()` return value.\"\"\"", "output": "\n        return self.velocity_from_coefficient_bundle(bundle)\n\n    def velocity_from_coefficient_bundle(self, bundle):\n        \"\"\"Return velocity, given the `coefficient_bundle()` return value.\"\"\"\n       _ = bundle['u']\n        v_est = self._from_(u=u_est,=self.get__est(u=u_est))\n        return v_est\n\n    def_from_coefficient_bundle(self, bundle):\n        \"\"\"Return, given the `coefficient_bundle()` return value.\"\"\"\n       _est = self.", "solution": "\n\n        coefficients, days_per_set, T, twot1 = bundle\n        coefficient_count = coefficients.shape[2]\n\n        # Chebyshev derivative:\n\n        dT = np.empty_like(T)\n        dT[0] = 0.0\n        dT[1] = 1.0\n        dT[2] = twot1 + twot1\n        for i in range(3, coefficient_count):\n            dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]\n        dT *= 2.0\n        dT /= days_per_set\n\n        return (dT.T * coefficients).sum(axis=2)"}
{"prompt": "def update(self):\n        \"\"\"Update cameras and motion settings with latest from API.\"\"\"", "output": "\n        self.update_cameras()\n        self.update_motion_settings()\n\n    def update_cameras(self):\n        \"\"\"Update cameras with latest from API.\"\"\"\n        self.cameras = self.api.get_cameras()\n\n    def update_motion_settings(self):\n        \"\"\"Update motion settings with latest from API.\"\"\"\n        self.motion_settings = self.api.get_motion_settings()\n\n    def get_camera_by_id(self, camera_id):\n        \"\"\"Return camera by ID.\"\"\"\n        for camera in self.cameras:\n            if camera['id'] == camera", "solution": "\n        cameras = self._api.camera_list()\n        self._cameras_by_id = {v.camera_id: v for i, v in enumerate(cameras)}\n\n        motion_settings = []\n        for camera_id in self._cameras_by_id.keys():\n            motion_setting = self._api.camera_event_motion_enum(camera_id)\n            motion_settings.append(motion_setting)\n\n        self._motion_settings_by_id = {\n            v.camera_id: v for i, v in enumerate(motion_settings)}"}
{"prompt": "def get_refs(self, location):\n        \"\"\"Return map of named refs (branches or tags) to commit hashes.\"\"\"", "output": "\n        refs = {}\n        for ref in self.repo.iter_references():\n            if ref.name.startswith(location):\n                refs[ref.name[len(location):].strip('/')] = ref.commit.hexsha\n        return refs\n\n    def get_commits(self, location, limit=None):\n        \"\"\"Return list of commit hashes for all commits in the given location.\"\"\"\n        refs = self.get_refs(location)\n        if limit:\n            refs = dict(itertools.islice(refs.items(), limit))\n        return list(refs.values())\n\n    def get_commit(self,", "solution": "\n        output = call_subprocess([self.cmd, 'show-ref'],\n                                 show_stdout=False, cwd=location)\n        rv = {}\n        for line in output.strip().splitlines():\n            commit, ref = line.split(' ', 1)\n            ref = ref.strip()\n            ref_name = None\n            if ref.startswith('refs/remotes/'):\n                ref_name = ref[len('refs/remotes/'):]\n            elif ref.startswith('refs/heads/'):\n                ref_name = ref[len('refs/heads/'):]\n            elif ref.startswith('refs/tags/'):\n                ref_name = ref[len('refs/tags/'):]\n            if ref_name is not None:\n                rv[ref_name] = commit.strip()\n        return rv"}
{"prompt": "def main():\n    \"\"\"The main function. Instantiates a GameState object and then\n    enters a REPL-like main loop, waiting for input, updating the state\n    based on the input, then outputting the new state.\"\"\"", "output": "\n    game_state = GameState()\n    while True:\n        print(game_state)\n        action = input(\"Enter an action (e.g.'move south'): \")\n        if action == 'quit':\n            break\n        game_state.update(action)\n\nif __name__ == '__main__':\n    main()\n", "solution": "\n\n    state = GameState()\n    print(state)\n    while state.running:\n        input = get_single_char()\n\n        state, should_advance = state.handle_input(input)\n        if should_advance:\n            state = state.advance_robots()\n            state = state.check_game_end()\n\n        print(state)\n\n    print(state.message)"}
{"prompt": "def delete_policy_name(self, name):\n        \"\"\"**Description**\n            Delete the policy with the given name.\n\n        **Arguments**\n            - name: the name of the policy to delete\n\n        **Success Return Value**\n            The JSON object representing the now-deleted policy.\n\n        **Example**\n            `examples/delete_policy.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_policy.py>`_\n\n        \"\"\"", "output": "\n        url = self.base_url + '/policies/' + name\n        response = self.session.delete(url)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception('Error deleting policy:'+ response.text)\n", "solution": "\n        res = requests.get(self.url + '/api/policies', headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        # Find the policy with the given name and delete it\n        for policy in res.json()[\"policies\"]:\n            if policy[\"name\"] == name:\n                return self.delete_policy_id(policy[\"id\"])\n\n        return [False, \"No policy with name {}\".format(name)]"}
{"prompt": "def share_vm_image(self, vm_image_name, permission):\n        \"\"\"\n        Share an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to be able to call this.\n\n        vm_image_name:\n            The name of the virtual machine image to share\n        permission:\n            The sharing permission: public, msdn, or private\n        \"\"\"", "output": "\n        if not self.is_publisher:\n            raise ValueError(\"You must be registered as image publisher to share an image.\")\n\n        if permission not in [\"public\", \"msdn\", \"private\"]:\n            raise ValueError(\"Invalid sharing permission: must be public, msdn, or private.\")\n\n        self.client.share_vm_image(vm_image_name, permission)\n\n    def unshare_vm_image(self, vm_image_name):\n        \"\"\"\n        Unshare an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('permission', permission)\n\n        path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n        query = '&permission=' + permission\n        path = path + '?' + query.lstrip('&')\n\n        return self._perform_put(\n            path, None, as_async=True, x_ms_version='2015-04-01'\n        )"}
{"prompt": "def get_stats(self, username='', password='', organization='llnl', force=True):\n        \"\"\"\n        Retrieves the traffic for the users of the given organization.\n        Requires organization admin credentials token to access the data.\n        \"\"\"", "output": "\n        if not self.token or force:\n            self.login(username, password, organization)\n\n        = requests.get(f\"https://api.cloud.com/api/v1/organizations/{organization}/users\", headers=self.headers)\n        if not.ok:\n            raise Exception(f\"Error retrieving traffic for organization {organization}: {self.get_error_message(response=response)}\")\n\n        users = []\n        for user in json.loads(response.text):\n            user_traffic = self.get_user_traffic(user['id'])\n            users.append(", "solution": "\n        date = str(datetime.date.today())\n        referrers_file_path =  ('../github_stats_output/referrers.csv')\n        views_file_path =  ('../github_stats_output/views.csv')\n        clones_file_path =  ('../github_stats_output/clones.csv')\n        if force or not os.path.isfile(file_path):\n            my_github.login(username, password)\n            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1\n            print 'Rate Limit: ' + str(calls_beginning)\n            my_github.get_org(organization)\n            my_github.get_traffic()\n            views_row_count = my_github.check_data_redundancy(file_path=views_file_path,\n                dict_to_check=self.views)\n            clones_row_count = my_github.check_data_redundancy(file_path=clones_file_path,\n                dict_to_check=self.clones)\n            my_github.write_to_file(referrers_file_path=referrers_file_path,\n                views_file_path=views_file_path,\n                clones_file_path=clones_file_path,\n                views_row_count=views_row_count,\n                clones_row_count=clones_row_count)\n            my_github.write_json(dict_to_write=self.referrers_json,\n                path_ending_type='traffic_popular_referrers')\n            my_github.write_json(dict_to_write=self.views_json,\n                path_ending_type='traffic_views')\n            my_github.write_json(dict_to_write=self.clones_json,\n                path_ending_type='traffic_clones')\n            my_github.write_json(dict_to_write=self.releases_json,\n                path_ending_type='releases')\n            calls_remaining = self.logged_in_gh.ratelimit_remaining\n            calls_used = calls_beginning - calls_remaining\n            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\\nUsed '\n                + str(calls_used) + ' API calls.')"}
{"prompt": "def save_setting(self, setting_name, value):\n        \"\"\"Saves the setting value into the database.\"\"\"", "output": "\n        setting = Setting.objects.get(name=setting_name)\n        setting.value = value\n        setting.save()\n\n    def get_setting(self, setting_name):\n        \"\"\"Gets the setting value from the database.\"\"\"\n        setting = Setting.objects.get(name=setting_name)\n        return setting.value\n", "solution": "\n        setting = self.get_setting(setting_name)\n        if setting is None:\n            setting = models.DashboardWidgetSettings.objects.create(\n                widget_name=self.get_name(),\n                setting_name=setting_name,\n                value=value)\n        setting.value = value\n        setting.save()\n        return setting"}
{"prompt": "def get_extended_key_usage_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the extendedKeyUsage\n    extension.\n    \"\"\"", "output": "\n    try:\n        ext = next(ext for ext in certificate.extensions if ext.oid == ExtensionOID.EXTENDED_KEY_USAGE)\n        return ext.value\n    except StopIteration:\n        return None\n\n\ndef get_certificate_subject_alt_names(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the subjectAltName extension.\n    \"\"\"\n    try:\n        ext = next(ext for ext in certificate.extensions if ext.oid == ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n        return ext.value\n    except StopIteration:\n        return None\n\n\ndef get", "solution": "\n    try:\n        return certificate.extensions.get_extension_for_oid(\n            x509.oid.ExtensionOID.EXTENDED_KEY_USAGE\n        ).value\n    except x509.ExtensionNotFound:\n        return None"}
{"prompt": "def join_html_attrs(attrs):\n    \"\"\"Joins the map structure into HTML attributes.\n\n    The return value is a 2-tuple ``(template, ordered_values)``. It should be\n    passed into :class:`markupsafe.Markup` to prevent XSS attacked.\n\n    e.g.::\n\n        >>> join_html_attrs({'href': '/', 'data-active': 'true'})\n        ('data-active=\"{0}\" href=\"{1}\"', ['true', '/'])\n    \"\"\"", "output": "\n    template =''.join(['{}=\"{}\"'.format(key, value) for key, value in sorted(attrs.items())])\n    ordered_values = [attrs[key] for key in sorted(attrs.keys())]\n    return template, ordered_values\n\n\ndef render_template(template_name, context=None, request=None):\n    \"\"\"Renders a Jinja2 template with the given context.\n\n    :param template_name: The name of the template to render.\n    :param context: A dictionary of values to pass to the template.\n    :param request: An optional request", "solution": "\n    attrs = collections.OrderedDict(freeze_dict(attrs or {}))\n    template = ' '.join('%s=\"{%d}\"' % (k, i) for i, k in enumerate(attrs))\n    return template, list(attrs.values())"}
{"prompt": "def restore_sys_module_state(self):\n        \"\"\"Restore the state of the sys module.\"\"\"", "output": "\n        self.sys_module_state = sys.__dict__.copy()\n\n    def restore_sys_path(self):\n        \"\"\"Restore the sys.path.\"\"\"\n        sys.path = self.sys_path_state\n\n    def restore_sys_modules(self):\n        \"\"\"Restore the sys.modules.\"\"\"\n        sys.modules = self.sys_modules_state\n\n    def restore_sys_path_hooks(self):\n        \"\"\"Restore the sys.path_hooks.\"\"\"\n        sys.path_hooks = self.sys_path_hooks_state\n\n    def restore_sys_meta_path(self):\n        \"\"\"Restore", "solution": "\n        try:\n            for k, v in self._orig_sys_module_state.iteritems():\n                setattr(sys, k, v)\n        except AttributeError:\n            pass\n        # Reset what what done in self.init_sys_modules\n        if self._orig_sys_modules_main_mod is not None:\n            sys.modules[self._orig_sys_modules_main_name] = self._orig_sys_modules_main_mod"}
{"prompt": "def dequantize_flow(dx, dy, max_val=0.02, denorm=True):\n    \"\"\"Recover from quantized flow.\n\n    Args:\n        dx (ndarray): Quantized dx.\n        dy (ndarray): Quantized dy.\n        max_val (float): Maximum value used when quantizing.\n        denorm (bool): Whether to multiply flow values with width/height.\n\n    Returns:\n        ndarray: Dequantized flow.\n    \"\"\"", "output": "\n    if denorm:\n        dx = dx *_width /_max_val\n        dy = dy *_height /_max_val\n\n    dx = np.clip(dx, -max_val, max_val)\n    dy = np.clip(dy, -max_val, max_val)\n\n    return dx, dy\n\n\ndef_flow(x,_width,_height,_max_val,__val,_denorm):\n    \"\"\"Quantize flow.\n\n    Args:\n        x (ndarray): Flow values.", "solution": "\n    assert dx.shape == dy.shape\n    assert dx.ndim == 2 or (dx.ndim == 3 and dx.shape[-1] == 1)\n\n    dx, dy = [dequantize(d, -max_val, max_val, 255) for d in [dx, dy]]\n\n    if denorm:\n        dx *= dx.shape[1]\n        dy *= dx.shape[0]\n    flow = np.dstack((dx, dy))\n    return flow"}
{"prompt": "def addCases(self, tupesValStmnts):\n        \"\"\"\n        Add multiple case statements from iterable of tuleles\n        (caseVal, statements)\n        \"\"\"", "output": "\n        for caseVal, statements in tupesValStmnts:\n            self.addCase(caseVal, statements)\n\n    def addCase(self, caseVal, statements):\n        \"\"\"\n        Add a case statement with a value and statements\n        \"\"\"\n        if caseVal in self.cases:\n            raise ValueError(f\"Case value {caseVal} already exists in {self.name}\")\n        self.cases[caseVal] = statements\n\n    def getCaseStatements(self, caseVal):\n        \"\"\"\n        Get the statements for a case statement with a given value\n        \"\"\"\n        if caseVal not in self.cases:\n           ", "solution": "\n        s = self\n        for val, statements in tupesValStmnts:\n            s = s.Case(val, statements)\n        return s"}
{"prompt": "def create_endpoint(port=None, service_name=None, host=None, use_defaults=True):\n    \"\"\"Creates a new Endpoint object.\n\n    :param port: TCP/UDP port. Defaults to 0.\n    :type port: int\n    :param service_name: service name as a str. Defaults to 'unknown'.\n    :type service_name: str\n    :param host: ipv4 or ipv6 address of the host. Defaults to the\n    current host ip.\n    :type host: str\n    :param use_defaults: whether to use defaults.\n    :type use_defaults: bool\n    :returns: zipkin Endpoint object\n    \"\"\"", "output": " bool\n    :return: Endpoint object.\n    :rtype: Endpoint\n    \"\"\"\n    if use_defaults:\n        port = 0\n        service_name = 'unknown'\n        host = socket.gethostbyname(socket.gethostname())\n\n    return Endpoint(port=port, service_name=service_name, host=host)\n\n\nclass Endpoint:\n    \"\"\"Endpoint object.\n\n    :param port: TCP/UDP port.\n    :type port: int\n    :param service_name: service name as a str.\n    :type service_name: str\n    :param host: ipv4 or ipv6 address", "solution": "\n    if use_defaults:\n        if port is None:\n            port = 0\n        if service_name is None:\n            service_name = 'unknown'\n        if host is None:\n            try:\n                host = socket.gethostbyname(socket.gethostname())\n            except socket.gaierror:\n                host = '127.0.0.1'\n\n    ipv4 = None\n    ipv6 = None\n\n    if host:\n        # Check ipv4 or ipv6.\n        try:\n            socket.inet_pton(socket.AF_INET, host)\n            ipv4 = host\n        except socket.error:\n            # If it's not an ipv4 address, maybe it's ipv6.\n            try:\n                socket.inet_pton(socket.AF_INET6, host)\n                ipv6 = host\n            except socket.error:\n                # If it's neither ipv4 or ipv6, leave both ip addresses unset.\n                pass\n\n    return Endpoint(\n        ipv4=ipv4,\n        ipv6=ipv6,\n        port=port,\n        service_name=service_name,\n    )"}
{"prompt": "def main():\n    \"\"\"\n    Parse argv for options and arguments, and start schema generation.\n    \"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='GenerateNOS schema')\n    parser.add_argument('--input', '-i', help='Input file', required=True)\n    parser.add_argument('--output', '-o', help='Output file', required=True)\n    parser.add_argument('--namespace', '-n', help='Namespace', required=True)\n    parser.add_argument('--version', '-v', help='Version', required=True)\n    parser.add_argument('--description', '-d', help='Description', required=True)\n    parser.add_argument('--author', '-a", "solution": "\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path...]\",\n                                   version=xtuml.version.complete_string,\n                                   formatter=optparse.TitledHelpFormatter())\n                                   \n    parser.set_description(__doc__.strip())\n    \n    parser.add_option(\"-c\", \"--component\", dest=\"component\", metavar=\"NAME\",\n                      help=\"export sql schema for the component named NAME\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-d\", \"--derived-attributes\", dest=\"derived\",\n                      help=\"include derived attributes in the schema\",\n                      action=\"store_true\", default=False)\n    \n    parser.add_option(\"-o\", \"--output\", dest='output', metavar=\"PATH\",\n                      help=\"save sql schema to PATH (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity', action=\"count\", \n                      help=\"increase debug logging level\", default=2)\n\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or opts.output is None:\n        parser.print_help()\n        sys.exit(1)\n\n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n\n    loader = ooaofooa.Loader()\n    for filename in args:\n        loader.filename_input(filename)\n\n    c = loader.build_component(opts.component, opts.derived)\n    xtuml.persist_database(c, opts.output)"}
{"prompt": "def disconnect(self, mol):\n        \"\"\"Break covalent bonds between metals and organic atoms under certain conditions.\n\n        The algorithm works as follows:\n\n        - Disconnect N, O, F from any metal.\n        - Disconnect other non-metals from transition metals + Al (but not Hg, Ga, Ge, In, Sn, As, Tl, Pb, Bi, Po).\n        - For every bond broken, adjust the charges of the begin and end atoms accordingly.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: The molecule with metals disconnected.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"", "output": "\n        :return: The modified molecule.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"\n       _metals = ['Li', 'Na', 'K', 'Rb', 'Cs', 'Fr', 'Be', 'Mg', 'Ca', 'Sr', 'Ba', 'Ra', 'Sc', 'Y', 'Zr', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po']\n       _metals = ['Li', 'Na', 'K',", "solution": "\n        log.debug('Running MetalDisconnector')\n        # Remove bonds that match SMARTS\n        for smarts in [self._metal_nof, self._metal_non]:\n            pairs = mol.GetSubstructMatches(smarts)\n            rwmol = Chem.RWMol(mol)\n            orders = []\n            for i, j in pairs:\n                # TODO: Could get the valence contributions of the bond instead of GetBondTypeAsDouble?\n                orders.append(int(mol.GetBondBetweenAtoms(i, j).GetBondTypeAsDouble()))\n                rwmol.RemoveBond(i, j)\n            # Adjust neighbouring charges accordingly\n            mol = rwmol.GetMol()\n            for n, (i, j) in enumerate(pairs):\n                chg = orders[n]\n                atom1 = mol.GetAtomWithIdx(i)\n                atom1.SetFormalCharge(atom1.GetFormalCharge() + chg)\n                atom2 = mol.GetAtomWithIdx(j)\n                atom2.SetFormalCharge(atom2.GetFormalCharge() - chg)\n                log.info('Removed covalent bond between %s and %s', atom1.GetSymbol(), atom2.GetSymbol())\n        Chem.SanitizeMol(mol)\n        return mol"}
{"prompt": "def accept_alert(self, text=None, wait=None):\n        \"\"\"\n        Execute the wrapped code, accepting an alert.\n\n        Args:\n            text (str | RegexObject, optional): Text to match against the text in the modal.\n            wait (int | float, optional): Maximum time to wait for the modal to appear after\n                executing the wrapped code.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been found.\n        \"\"\"", "output": "\n        if text is None:\n            text = self.default_text\n        if isinstance(text, str):\n            text = re.compile(text)\n\n        if wait is None:\n            wait = self.default_wait\n\n        self.execute_code()\n\n        try:\n            WebDriverWait(self.driver, wait).until(\n                EC.presence_of_element_located((By.XPATH, \"//div[@class='modal-dialog']\"))\n            )\n        except TimeoutException:\n            raise ModalNotFound(\"Modal dialog not found after waiting for {} seconds.\".format(wait))\n\n        modal_dialog = self.driver.", "solution": "\n\n        wait = wait or capybara.default_max_wait_time\n        with self.driver.accept_modal(\"alert\", text=text, wait=wait):\n            yield"}
{"prompt": "def build_type(s_dt):\n    \"\"\"\n    Build a partial xsd tree out of a S_DT and its sub types S_CDT, S_EDT, S_SDT and S_UDT.\n    \"\"\"", "output": "\n   _dt = s_dt.get_type()\n    if isinstance(s_dt, S_CDT):\n       _dt = \"CDT\"\n    elif isinstance(s_dt, S_EDT):\n        if s_dt.get_type() == \"EDT_DATE\":\n           _dt = \"EDT_DATE\"\n        else:\n            raise ValueError(\"EDT type not supported.\")\n    elif isinstance(s_dt, S_SDT):\n        if s_dt.get_type() == \"SDT_DATE\":\n            if s_dt.get_precision", "solution": "\n    s_cdt = nav_one(s_dt).S_CDT[17]()\n    if s_cdt:\n        return build_core_type(s_cdt)\n    \n    s_edt = nav_one(s_dt).S_EDT[17]()\n    if s_edt:\n        return build_enum_type(s_edt)\n    \n    s_udt = nav_one(s_dt).S_UDT[17]()\n    if s_udt:\n        return build_user_type(s_udt)"}
{"prompt": "def _get_obj_count_difference(objs1, objs2):\n    \"\"\"Returns count difference in two collections of Python objects.\"\"\"", "output": "\n    count_diff = len(objs1) - len(objs2)\n    return count_diff\n\n\ndef _get_obj_count_difference_percentage(objs1, objs2):\n    \"\"\"Returns percentage difference in two collections of Python objects.\"\"\"\n    count_diff = _get_obj_count_difference(objs1, objs2)\n    total_count = len(objs1) + len(objs2)\n    percentage_diff = count_diff / total_count * 100\n    return percentage_diff\n\n\ndef _get_obj_count_difference_summary(objs1,", "solution": "\n    clean_obj_list1 = _process_in_memory_objects(objs1)\n    clean_obj_list2 = _process_in_memory_objects(objs2)\n    obj_count_1 = _get_object_count_by_type(clean_obj_list1)\n    obj_count_2 = _get_object_count_by_type(clean_obj_list2)\n    return obj_count_1 - obj_count_2"}
{"prompt": "def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"", "output": "\n        = self.get_spelling_settings()\n        = self.get_spelling_settings()\n        = self.get_spelling_settings()\n        = self.get_spelling_settings()\n        return\n\n    def get_spelling_settings(self):\n        \"\"\"\n        get current spelling settings\n        :return: spelling settings as number\n        \"\"\"\n        = self.get_spelling_settings()\n        = self.get_spelling_settings()\n        v = self.get_spelling_settings()\n        = self.get_spelling_settings", "solution": "\n        options = 0\n        if self._ignore_uppercase:\n            options |= 1\n        if self._ignore_digits:\n            options |= 2\n        if self._ignore_urls:\n            options |= 4\n        if self._find_repeat_words:\n            options |= 8\n        if self._ignore_latin:\n            options |= 16\n        if self._flag_latin:\n            options |= 128\n        if self._by_words:\n            options |= 256\n        if self._ignore_capitalization:\n            options |= 512\n        if self._ignore_roman_numerals:\n            options |= 2048\n        return options"}
{"prompt": "def get_all_intervals(self):\n        \"\"\"Returns the true list of intervals including the empty intervals.\"\"\"", "output": "\n        = self.intervals\n      .sort(key=lambda x: x[0])\n        = [x for x in viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_merge(viterbi_", "solution": "\n        ints = sorted(self.get_intervals(True))\n        if self.tier_type == 'IntervalTier':\n            if not ints:\n                ints.append((self.xmin, self.xmax, ''))\n            else:\n                if ints[0][0] > self.xmin:\n                    ints.insert(0, (self.xmin, ints[0][0], ''))\n                if ints[-1][1] < self.xmax:\n                    ints.append((ints[-1][1], self.xmax, ''))\n                p = ints[-1]\n                for index, i in reversed(list(enumerate(ints[:-1], 1))):\n                    if p[0] - i[1] != 0:\n                        ints.insert(index, (i[1], p[0], ''))\n                    p = i\n        return ints"}
{"prompt": "def info_update(self, obj_id, data):\n\t\t\"\"\"Update metadata with of a specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type.\"\"\"", "output": "1.\n\t\t\tNote: This method is not implemented for items.\n\t\t\"\"\"\n\t\traise NotImplementedError(\"This method is not implemented for items.\")\n\n\tdef delete_object(self, obj_id):\n\t\t\"\"\"Delete a specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type1.\n\t\t\tNote: This method is not implemented for items.\n\t\t\"\"\"\n\t\traise NotImplementedError(\"This method is not implemented for items.\")\n\n\tdef", "solution": "\n\t\treturn self(obj_id, method='put', data=data, auth_header=True)"}
{"prompt": "def hide_error_message(self):\n        \"\"\"\n        This function hides the error message when all values are correct.\n        \"\"\"", "output": "\n        self.error_message.hide()\n\n    def validate_input(self):\n        \"\"\"\n        This function validates the input fields and returns True if all values are correct, False otherwise.\n        \"\"\"\n        if self.name_entry.get() == \"\" or self.email_entry.get() == \"\" or self.phone_entry.get() == \"\" or self.address_entry.get() == \"\":\n            self.error_message.show()\n            return False\n        else:\n            self.hide_error_message()\n            return True\n\n    def submit_form(self):\n        \"\"\"\n        This function submits the", "solution": "\n        self.ui.error_label.setScaledContents(False)  # Warning image hiden.\n        self.ui.error_text_label.hide()"}
{"prompt": "def conditional_entropy(X, Y, base=2):\n    \"\"\"Calculates the conditional entropy, H(X|Y), in the given base\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    Y: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    base: integer (default: 2)\n        The base in which to calculate conditional entropy\n\n    Returns\n    ----------\n    conditional_entropy: float\n        The conditional entropy calculated according to the equation H(X|Y) = H(X,Y) - H(Y)\n\n    \"\"\"", "output": " - H(X)\n    \"\"\"\n    # Calculate the joint entropy\n    joint_entropy = joint_entropy_base(X, Y, base)\n\n    # Calculate the marginal entropy for each value of X\n    marginal_entropies = np.zeros(len(X))\n    for i, x in enumerate(X):\n        marginal_entropies[i] = marginal_entropy_base(x, base)\n\n    # Calculate the conditional entropy\n    conditional_entropy = joint_entropy - np.sum(marginal_entropies)\n\n    return conditional_entropy\n\n\ndef joint_entropy_", "solution": "\n    return joint_entropy(X, Y, base=base) - entropy(Y, base=base)"}
{"prompt": "def update_policy(self, id, policy):\n        \"\"\" Create policy.\n\n            https://www.nomadproject.io/api/acl-policies.html\n\n            arguments:\n                - name\n                - policy\n            returns: request.Response\n\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.URLNotFoundNomadException\n        \"\"\"", "output": "\n        url = f\"{self.url}/v1/acl/policies/{id}\"\n        response = self._put(url, json=policy)\n        return response\n\n    def delete_policy(self, id):\n        \"\"\" Delete policy.\n\n            https://www.nomadproject.io/api/acl-policies.html\n\n            arguments:\n                - id\n            returns: request.Response\n\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.URLNotFoundNomadException\n        \"\"\"\n        url = f\"{self.url}/v1/acl/policies/{id", "solution": "\n        return self.request(\"policy\", id, json=policy, method=\"post\")"}
{"prompt": "def mark(self, value=1):\n        \"\"\"Record an event with the derive.\n\n        :param value: counter value to record\n        \"\"\"", "output": "\n        self.counter += value\n\n    def reset(self):\n        \"\"\"Reset the counter to zero.\"\"\"\n        self.counter = 0\n\n    def get_counter(self):\n        \"\"\"Return the current counter value.\"\"\"\n        return self.counter\n\n\nclass Derive(object):\n    \"\"\"A class to represent a derive.\"\"\"\n\n    def __init__(self, name, counter=None):\n        \"\"\"Initialize a derive with a name and optional counter.\n\n        :param name: name of the derive\n        :param counter: optional counter object to use for tracking events\n        \"\"\"\n        self.name = name\n        self.counter = counter if counter", "solution": "\n        last = self.last.get_and_set(value)\n        if last <= value:\n            value = value - last\n        super(Derive, self).mark(value)"}
{"prompt": "def __fetch_pull_requests(self, from_date, to_date):\n        \"\"\"Fetch the pull requests\"\"\"", "output": "\n        pull_requests = []\n        for page in range(1, 1000):\n            url = f\"https://api.github.com/search/issues?q=is:pr+created:{from_date}..{to_date}&page={page}&per_page=100\"\n            response = requests.get(url)\n            response.raise_for_status()\n            data = response.json()\n            for item in data[\"items\"]:\n                pull_requests.append(item)\n        return pull_requests\n\n    def __fetch_commits(self, pull_requests):\n       ", "solution": "\n\n        raw_pulls = self.client.pulls(from_date=from_date)\n        for raw_pull in raw_pulls:\n            pull = json.loads(raw_pull)\n\n            if str_to_datetime(pull['updated_at']) > to_date:\n                return\n\n            self.__init_extra_pull_fields(pull)\n            for field in TARGET_PULL_FIELDS:\n\n                if not pull[field]:\n                    continue\n\n                if field == 'user':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'merged_by':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'review_comments':\n                    pull[field + '_data'] = self.__get_pull_review_comments(pull['number'])\n                elif field == 'requested_reviewers':\n                    pull[field + '_data'] = self.__get_pull_requested_reviewers(pull['number'])\n                elif field == 'commits':\n                    pull[field + '_data'] = self.__get_pull_commits(pull['number'])\n\n            yield pull"}
{"prompt": "def connect(src, *destinations, exclude: set=None, fit=False):\n    \"\"\"\n    Connect src (signals/interfaces/values) to all destinations\n\n    :param exclude: interfaces on any level on src or destinations\n        which should be excluded from connection process\n    :param fit: auto fit source width to destination width\n    \"\"\"", "output": "\n    if exclude is None:\n        exclude = set()\n\n    # find paths\n   _paths = []\n    for dest in destinations:\n        if isinstance(dest, (list, tuple)):\n            dest = dest[0]\n        if dest in exclude:\n            continue\n        path =_path(src, dest)\n        if path:\n            path = [p for p in path if p not in exclude]\n            if path:\n                if fit:\n                   _paths.append(fit_path(src, dest, path))\n                else:\n                    for p in path:\n                        if p not in exclude:\n                           ", "solution": "\n    assignemnts = []\n\n    if isinstance(src, HObjList):\n        for dst in destinations:\n            assert len(src) == len(dst), (src, dst)\n        _destinations = [iter(d) for d in destinations]\n        for _src in src:\n            dsts = [next(d) for d in _destinations]\n            assignemnts.append(connect(_src, *dsts, exclude=exclude, fit=fit))\n    else:\n        for dst in destinations:\n            assignemnts.append(_connect(src, dst, exclude, fit))\n\n    return assignemnts"}
{"prompt": "def addLogicalInterfaceToDeviceType(self, typeId, logicalInterfaceId):\n        \"\"\"\n        Adds a logical interface to a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - logicalInterfaceId (string) - the id returned by the platform on creation of the logical interface\n            - description (string) - optional (not used)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        self._api.addLogicalInterfaceToDeviceType(typeId, logicalInterfaceId)\n\n    def deleteLogicalInterfaceFromDeviceType(self, typeId, logicalInterfaceId):\n        \"\"\"\n        Deletes a logical interface from a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - logicalInterfaceId (string) - the id returned by the platform on creation of the logical interface\n        Throws APIException on failure.\n        \"\"\"\n        self._api.deleteLogicalInterfaceFromDeviceType(typeId, logicalInterfaceId)\n\n    def getLogicalInterfacesForDeviceType(self, typeId):\n        \"\"\"\n        Gets the logical", "solution": "\n        req = ApiClient.allDeviceTypeLogicalInterfacesUrl % (self.host, \"/draft\", typeId)\n        body = {\"id\" : logicalInterfaceId}\n#       body = {\"name\" : \"required but not used!!!\", \"id\" : logicalInterfaceId, \"schemaId\" : schemaId}\n#       if description:\n#           body[\"description\"] = description\n        resp = requests.post(req, auth=self.credentials, headers={\"Content-Type\":\"application/json\"}, data=json.dumps(body),\n                        verify=self.verify)\n        if resp.status_code == 201:\n            self.logger.debug(\"Logical interface added to a device type\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error adding logical interface to a device type\", resp)\n        return resp.json()"}
{"prompt": "def repository(self, owner, repository):\n        \"\"\"Fetch information about a repository.\"\"\"", "output": "\n        return self.get(f\"/repos/{owner}/{repository}\")\n\n    def create_repository(self, owner, repository, **kwargs):\n        \"\"\"Create a new repository.\"\"\"\n        return self.post(f\"/user/repos\", data={\"name\": repository, **kwargs})\n\n    def edit_repository(self, owner, repository, **kwargs):\n        \"\"\"Edit a repository.\"\"\"\n        return self.patch(f\"/repos/{owner}/{repository}\", data=kwargs)\n\n    def delete_repository(self, owner, repository):\n        \"\"\"Delete a repository.\"\"\"\n        return self.delete(f\"/repos/{owner}/{repository}\")", "solution": "\n\n        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)\n\n        logger.debug(\"DockerHub client requests: %s\", url)\n\n        response = self.fetch(url)\n\n        return response.text"}
{"prompt": "def target_address(self):\n        \"\"\"Return the authorative target of the link.\"\"\"", "output": "\n        return self.link.target_address\n\n    def get_link_text(self):\n        \"\"\"Return the text of the link.\"\"\"\n        return self.link.text\n\n    def get_link_url(self):\n        \"\"\"Return the URL of the link.\"\"\"\n        return self.link.url\n\n    def get_link_description(self):\n        \"\"\"Return the description of the link.\"\"\"\n        return self.link.description\n\n    def get_link_tags(self):\n        \"\"\"Return the tags of the link.\"\"\"\n        return self.link.tags\n\n    def get_link_created_at(self):\n        \"\"\"Return", "solution": "\n        # If link is a receiver, target is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_receiver:\n            return self._pn_link.target.address\n        else:\n            return self._pn_link.remote_target.address"}
{"prompt": "def recv_unsub(self, id_=None):\n        \"\"\"DDP unsub handler.\"\"\"", "output": "\n        if id_ is None:\n            id_ = self.id\n        if id_ in self.unsubscribed:\n            self.unsubscribed.remove(id_)\n            if not self.unsubscribed:\n                self.stop()\n\n    def send_unsub(self, id_=None):\n        \"\"\"DDP unsub handler.\"\"\"\n        if id_ is None:\n            id_ = self.id\n        if id_ not in self.unsubscribed:\n            self.unsubscribed.append(id_)\n            self.send_packet(dict(msg='unsub', id=id_))\n\n    def send_packet(self", "solution": "\n        if id_:\n            self.api.unsub(id_)\n        else:\n            self.reply('nosub')"}
{"prompt": "def score(infile, outfile, classifier, xgb_autotune, apply_weights, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test):\n    \"\"\"\n    Conduct semi-supervised learning and error-rate estimation for MS1, MS2 and transition-level data. \n    \"\"\"", "output": "_____method,__adj,_ neg_neg_neg_method, y_neg_neg_neg_method,_ neg_neg_pos_method, y_neg_neg_pos_method, y_neg_pos_pos_method, y_neg_pos_neg_method, y_pos_pos_pos_method, y_pos_pos_neg_method, y_pos_neg_pos_method, y_pos_neg_neg_method,_pos_pos_pos_method,", "solution": "\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    # Prepare XGBoost-specific parameters\n    xgb_hyperparams = {'autotune': xgb_autotune, 'autotune_num_rounds': 10, 'num_boost_round': 100, 'early_stopping_rounds': 10, 'test_size': 0.33}\n\n    xgb_params = {'eta': 0.3, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': 1, 'alpha': 0, 'scale_pos_weight': 1, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    xgb_params_space = {'eta': hp.uniform('eta', 0.0, 0.3), 'gamma': hp.uniform('gamma', 0.0, 0.5), 'max_depth': hp.quniform('max_depth', 2, 8, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': hp.uniform('lambda', 0.0, 1.0), 'alpha': hp.uniform('alpha', 0.0, 1.0), 'scale_pos_weight': 1.0, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    if not apply_weights:\n        PyProphetLearner(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test).run()\n    else:\n        PyProphetWeightApplier(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test, apply_weights).run()"}
{"prompt": "def cleanup(controller, engines):\n    \"\"\"Cleanup routine to shut down all subprocesses we opened.\"\"\"", "output": "\n    for engine in engines:\n        engine.terminate()\n\n\ndef main():\n    \"\"\"Main function to runuring the.\"\"\"\n   uring_engine = TuringEngine()\n   uring_engine.run()\n    cleanup(turing_engine, [turing_engine])\n\n\nif __name__ == \"__main__\":\n    main()\n", "solution": "\n    import signal, time\n    \n    print('Starting cleanup')\n    print('Stopping engines...')\n    for e in engines:\n        e.send_signal(signal.SIGINT)\n    print('Stopping controller...')\n    # so it can shut down its queues\n    controller.send_signal(signal.SIGINT)\n    time.sleep(0.1)\n    print('Killing controller...')\n    controller.kill()\n    print('Cleanup done')"}
{"prompt": "def reset(self):\n        \"\"\"Reset the input buffer and associated state.\"\"\"", "output": "\n        self.input_buffer = []\n        self.state = self.initial_state\n\n    def step(self, action):\n        \"\"\"Perform a step with the given action and return the next state, reward, and done flag.\"\"\"\n        # Add the action to the input buffer\n        self.input_buffer.append(action)\n\n        # If the input buffer is full, update the state\n        if len(self.input_buffer) == self.buffer_size:\n            self.update_state()\n\n        # Calculate the reward\n        reward = self.calculate_reward()\n\n        # Check if the episode is done\n        done", "solution": "\n        super(IPythonInputSplitter, self).reset()\n        self._buffer_raw[:] = []\n        self.source_raw = ''\n        self.cell_magic_parts = []\n        self.processing_cell_magic = False"}
{"prompt": "def for_type_by_name(type_module, type_name, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the module and name of a type\n    rather than the type object itself.\n    \"\"\"", "output": "\n    type_obj = getattr(type_module, type_name, None)\n    if type_obj is not None:\n      .pretty_printers.append(lambda obj, ctx: func(obj, ctx))\n\n\ndef for_type(type_obj, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the type object itself.\n    \"\"\"\n    if type_obj is not None:\n        v =.get_value(type_obj)\n        if v is not None:\n            if.is_builtin_type(type_obj):\n                v = str(v", "solution": "\n    key = (type_module, type_name)\n    oldfunc = _deferred_type_pprinters.get(key, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _deferred_type_pprinters[key] = func\n    return oldfunc"}
{"prompt": "def domain(self, domain=None, last_domain=None):\n        \"\"\"\n        Manage the case that we want to test only a domain.\n\n        :param domain: The domain or IP to test.\n        :type domain: str\n\n        :param last_domain:\n            The last domain to test if we are testing a file.\n        :type last_domain: str\n\n        :param return_status: Tell us if we need to return the status.\n        :type return_status: bool\n        \"\"\"", "output": "\n        if domain is None:\n            raise ValueError(\"You need to specify a domain or IP to test.\")\n\n        if last_domain is None:\n            last_domain = domain\n\n        self.domain = domain\n        self.last_domain = last_domain\n        self.return_status = return_status\n\n    def test_domain(self, domain=None, last_domain=None, return_status=False):\n        \"\"\"\n        Test a domain or IP.\n\n        :param domain: The domain or IP to test.\n        :type domain: str\n\n        :param last_domain:\n            The last domain to test if we are", "solution": "\n\n        # We print the header.\n        self._print_header()\n\n        if domain:\n            # A domain is given.\n\n            # We format and set the domain we are testing and treating.\n            PyFunceble.INTERN[\"to_test\"] = self._format_domain(domain)\n        else:\n            # A domain is not given.\n\n            # We set the domain we are testing and treating to None.\n            PyFunceble.INTERN[\"to_test\"] = None\n\n        if PyFunceble.INTERN[\"to_test\"]:\n            # The domain is given (Not None).\n\n            if PyFunceble.CONFIGURATION[\"syntax\"]:\n                # The syntax mode is activated.\n\n                # We get the status from Syntax.\n                status = self.syntax_status.get()\n            else:\n                # We test and get the status of the domain.\n                status, _ = self.status.get()\n\n            # We run the file decision logic.\n            self._file_decision(PyFunceble.INTERN[\"to_test\"], last_domain, status)\n\n            if PyFunceble.CONFIGURATION[\"simple\"]:\n                # The simple mode is activated.\n\n                # We print the domain and the status.\n                print(PyFunceble.INTERN[\"to_test\"], status)\n\n            # We return the tested domain and its status.\n            return PyFunceble.INTERN[\"to_test\"], status\n\n        # We return None, there is nothing to test.\n        return None"}
{"prompt": "def varimp(self, use_pandas=False):\n        \"\"\"\n        Pretty print the variable importances, or return them in a list/pandas DataFrame.\n\n        :param bool use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n\n        :returns: A dictionary of lists or Pandas DataFrame instances.\n        \"\"\"", "output": "\n        if use_pandas:\n            import pandas as pd\n           _importances = pd.DataFrame(self.importances_, columns=['importance']).sort_values('importance', ascending=False)\n            return pd.concat([pd.DataFrame(self.feature_names_), pd.DataFrame(self.importances_)], axis=1)\n        else:\n           _importances = dict(zip(self.feature_names_, self.importances_))\n            return {k: sorted(v, reverse=True) for k, v in sorted(list(zip(self.feature_names_, self.", "solution": "\n        return {model.model_id: model.varimp(use_pandas) for model in self.models}"}
{"prompt": "def statementsAreSame(statements: List[HdlStatement]) -> bool:\n    \"\"\"\n    :return: True if all statements are same\n    \"\"\"", "output": "\n    if len(statements) == 0:\n        return True\n\n    first_statement = statements[0]\n    for statement in statements[1:]:\n        if not statement.isSame(first_statement):\n            return False\n\n    return True\n\n\ndef statementsAreDifferent(statements: List[HdlStatement]) -> bool:\n    \"\"\"\n    :return: True if all statements are different\n    \"\"\"\n    return not statementsAreSame(statements)\n\n\ndef statementsAreEquivalent(statements1: List[HdlStatement], statements2: List[HdlStatement]) -> bool:\n    \"\"\"\n    :return: True if all statements", "solution": "\n    iterator = iter(statements)\n    try:\n        first = next(iterator)\n    except StopIteration:\n        return True\n\n    return all(first.isSame(rest) for rest in iterator)"}
{"prompt": "def get_meta_netnode():\n    \"\"\"\n    Get the netnode used to store settings metadata in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"", "output": "\n    return.idb.get_meta_netnode()\n\ndef set_meta_netnode(netnode):\n    \"\"\"\n    Set the netnode used to store settings metadata in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"\n    return me.idb.set_meta_netnode(netnode)\n\ndef get_meta_key(key):\n    \"\"\"\n    Get the value of a metadata key in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"\n    return me", "solution": "\n    node_name = \"$ {org:s}.{application:s}\".format(\n        org=IDA_SETTINGS_ORGANIZATION,\n        application=IDA_SETTINGS_APPLICATION)\n    return netnode.Netnode(node_name)"}
{"prompt": "def setPin(self, old_pin, new_pin):\n        \"\"\"\n        C_SetPIN\n\n        :param old_pin: old PIN\n        :param new_pin: new PIN\n        \"\"\"", "output": "\n       _pin = self.get_pin()\n        if old_pin == self.get_pin():\n            self.set_pin(new_pin)\n        else:\n            raise ValueError(\"Incorrect old PIN\")\n\n    def get_pin(self):\n        \"\"\"\n        C_GetPin\n\n        :return: PIN\n        \"\"\"\n        return self._pin\n\n    def set_pin(self, pin):\n        \"\"\"\n        C_SetPin\n\n        :param pin: PIN\n        \"\"\"\n        self._pin = pin\n\n    def get__info(self):\n        \"\"\"\n        C_GetVutInfo\n\n        :return: VUT info", "solution": "\n        old_pin1 = ckbytelist(old_pin)\n        new_pin1 = ckbytelist(new_pin)\n        rv = self.lib.C_SetPIN(self.session, old_pin1, new_pin1)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def divide(df, new_column, column_1, column_2):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"", "output": "\n    df[new_column] = df[column_1] / df[column_2]\n    return df\n\n\ndef formula(df, formula, new_column=None):\n    \"\"\"\n    Calculate a new column using a formula.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to calculate the new column in.\n    formula : str\n        The formula to use for calculating the new column.\n    new_column : str, optional\n        The name of the new column to create. If not provided, the formula will be used as the name.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The DataFrame with", "solution": "\n    return _basic_math_operation(df, new_column, column_1, column_2, op='truediv')"}
{"prompt": "def _setup_template_file(self, template_file_path):\n        \"\"\" Setup self.template\n\n        Parameters\n        ----------\n        template_file_path: str\n            Document template file path.\n        \"\"\"", "output": "\n        self.template = Template(filename=template_file_path)\n\n    def _setup_template_variables(self,_data):\n        \"\"\" Setup template variables\n\n        Parameters\n        ----------\n        o_data: dict\n            Output data.\n        \"\"\"\n        self.template.variables.update(o_data)\n\n    def _render_template(self):\n        \"\"\" Render template and return rendered string. \"\"\"\n        return self.template.render()\n\n    def render(self, o_data):\n        \"\"\" Render template with output data.\n\n        Parameters\n        ----------\n        o_data: dict\n            Output data.\n\n        Returns\n        -------\n       ", "solution": "\n        try:\n            template_file = template_file_path\n            template_env = get_environment_for(template_file_path)\n            template = template_env.get_template(os.path.basename(template_file))\n        except:\n            raise\n        else:\n            self._template_file = template_file\n            self._template_env = template_env\n            self.template = template"}
{"prompt": "def pxconfig(self, line):\n        \"\"\"configure default targets/blocking for %px magics\"\"\"", "output": "\n        if not line:\n            raise ValueError(\"No arguments provided\")\n        if line.startswith(\"set\"):\n            key, value = line.split(\"=\", 1)\n            if key.lower() == \"default_targets\":\n                self.default_targets = value.split(\",\")\n            elif key.lower() == \"default_blocking\":\n                self.default_blocking = value.split(\",\")\n            else:\n                raise ValueError(\"Invalid key: %s\" % key)\n        else:\n            raise ValueError(\"Invalid line: %s\" % line)\n\n    def px(self, line):\n        \"\"\"executecode incode", "solution": "\n        args = magic_arguments.parse_argstring(self.pxconfig, line)\n        if args.targets:\n            self.view.targets = self._eval_target_str(args.targets)\n        if args.block is not None:\n            self.view.block = args.block\n        if args.set_verbose is not None:\n            self.verbose = args.set_verbose"}
{"prompt": "def l(*members, meta=None) -> List:\n    \"\"\"Creates a new list from members.\"\"\"", "output": "\n    return List(members, meta)\n\n\nclass List(BaseObject):\n    \"\"\"Represents a list object in thebot.\"\"\"\n\n    def __init__(self, members: List[Union[str, Member]], meta: Optional[dict] = None):\n        \"\"\"\n        Initializes a new list object.\n\n        :param members: The members of the list.\n        :param meta: The metadata of the list.\n        \"\"\"\n        super().__init__(meta)\n        self.members = members\n\n    def __str__(self) -> str:\n        \"\"\"\n        Returns a string representation of the list.\n\n        :return: The string representation", "solution": "\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )"}
{"prompt": "def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n    \"\"\"wrapper\"\"\"", "output": "\n    if re.match(r'https?://www\\.fc2video\\.com/watch/(\\d+)', url):\n        video_id = re.match(r'https?://www\\.fc2video\\.com/watch/(\\d+)', url).group(1)\n        title = 'FC2 Video -'+ video_id\n    else:\n        video_id = url\n        title = 'FC2 Video -'+ video_id\n\n    try:\n        info = get_video_info(video_id, ['title', 'description', 'duration', 'view_count', 'like", "solution": "\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/tw/content/20151021bTVKnbEw'\n    hostname = urlparse(url).hostname\n    if not ('fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname):\n        return False\n    upid = match1(url, r'.+/content/(\\w+)')\n\n    fc2video_download_by_upid(upid, output_dir, merge, info_only)"}
{"prompt": "def tobin(deci_num, len=32):\n    \"\"\"\n    Given a decimal number, returns a string bitfield of length = len\n    Example: given deci_num = 1 and len = 10, it return 0000000001\n    \"\"\"", "output": "\n    bin_num = bin(deci_num)[2:].zfill(len)\n    return bin_num\n\n\ndefcd(a, b):\n    \"\"\"\n    Returns the greatest common divisor of a and b\n    \"\"\"\n    while b:\n        a, b = b, a % b\n    return a\n\n\ndef gcd(a, b):\n    \"\"\"\n    Returns the greatest common divisor of a and b\n    \"\"\"\n    while b:\n        a, b = b, a % b\n    return a\n\n\ndef lcm(a, b):\n    \"\"\"\n    Returns the least common multiple of a and b", "solution": "\n    bitstr = \"\".join(map(lambda y: str((deci_num >> y) & 1), range(len - 1, -1, -1)))\n    return bitstr"}
{"prompt": "def export_mt_variants(variants, sample_id):\n    \"\"\"Export mitochondrial variants for a case to create a MT excel report\n\n    Args:\n        variants(list): all MT variants for a case, sorted by position\n        sample_id(str) : the id of a sample within the case\n\n    Returns:\n        document_lines(list): list of lines to include in the document\n    \"\"\"", "output": "\n    document_lines = []\n   _line = []\n   _line.append(\"Sample ID: {}\".format(sample_id))\n    document_lines.append(u\"\\n\".join(u\"{}\".format(line) for line in_line))\n    document_lines.append(u\"\\n\")\n    document_lines.append(u\"Mitochondrial variants:\")\n    document_lines.append(u\"\\n\")\n    for variant in variants:\n        document_lines.append(u\"{}\\n\".format(variant))\n    document_lines.append(u\"\\n\")\n   ", "solution": "\n    document_lines = []\n    for variant in variants:\n        line = []\n        position = variant.get('position')\n        change = '>'.join([variant.get('reference'),variant.get('alternative')])\n        line.append(position)\n        line.append(change)\n        line.append(str(position)+change)\n        genes = []\n        prot_effect = []\n        for gene in variant.get('genes'):\n            genes.append(gene.get('hgnc_symbol',''))\n            for transcript in gene.get('transcripts'):\n                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n        line.append(','.join(prot_effect))\n        line.append(','.join(genes))\n        ref_ad = ''\n        alt_ad = ''\n        for sample in variant['samples']:\n            if sample.get('sample_id') == sample_id:\n                ref_ad = sample['allele_depths'][0]\n                alt_ad = sample['allele_depths'][1]\n        line.append(ref_ad)\n        line.append(alt_ad)\n        document_lines.append(line)\n    return document_lines"}
{"prompt": "def write_sky_params_to_file(self):\n        \"\"\"Writes the params to file that skytool_Free needs to generate the sky radiance distribution.\"\"\"", "output": "\n        with open(self.sky_params_file, 'w') as f:\n            f.write('{}\\n'.format(self.sky_params_file))\n            f.write('{}\\n'.format(self.sky_params_version))\n            f.write('{}\\n'.format(self.sky_params_type))\n            f.write('{}\\n'.format(self.sky_params_resolution))\n            f.write('{}\\n'.format(self.sky_params_zenith_angle))\n            f.write('{}\\n'.format(self.sky_params_az", "solution": "\n\n        inp_file = self.sky_file + '_params.txt'\n        lg.info('Writing Inputs to file : ' + inp_file)\n\n        f = open(inp_file, 'w')\n\n        f.write('verbose= ' + str(self.verbose) + '\\n')\n        f.write('band_count= ' + str(self.num_bands) + '\\n')\n        f.write('band_centres_data= ')\n        f.write(\",\".join([str(wave) for wave in self.wavelengths]) + '\\n')\n        f.write('partition= ' + self.partition + '\\n')\n        f.write('vn= ' + str(self.vn) + '\\n')\n        f.write('hn= ' + str(self.hn) + '\\n')\n        f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.write('theta_points= ')\n        f.write(\",\".join([str(theta) for theta in self.theta_points]) + '\\n')\n        f.write('type= ' + self.sky_type + '\\n')\n        f.write('azimuth= ' + str(self.sky_azimuth) + '\\n')\n        f.write('zenith= ' + str(self.sky_zenith) + '\\n')\n        f.write('sky_save_fp= ' + inp_file.strip('_params.txt') + '\\n')\n        f.write('sky_image_save_fp= ' + self.sky_file + '.ppm' + '\\n')\n        f.write('sky_image_size= 256' + '\\n')\n        if self.sky_type == 'hlideal':\n            f.write('C= ' + str(self.sky_c) + '\\n')\n            f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.flush()\n        f.close()"}
{"prompt": "def defrag(filt, threshold=3, mode='include'):\n    \"\"\"\n    'Defragment' a filter.\n\n    Parameters\n    ----------\n    filt : boolean array\n        A filter\n    threshold : int\n        Consecutive values equal to or below this threshold\n        length are considered fragments, and will be removed.\n    mode : str\n        Wheter to change False fragments to True ('include')\n        or True fragments to False ('exclude')\n\n    Returns\n    -------\n    defragmented filter : boolean array\n    \"\"\"", "output": "\n   _filt = filt.copy()\n   _filt[filt ==_filt] = True\n   _filt[filt!=_filt] = False\n\n   _filt =_filt.astype(int)\n   _filt[filt == 0] = 1\n   _filt[filt == 1] = 0\n\n   _filt =_filt.astype(bool)\n\n   _filt =_filt.astype(int)\n   _filt[filt == 0] = 1\n   _filt[filt == 1] = 0\n\n   ", "solution": "\n    if bool_2_indices(filt) is None:\n        return filt\n\n    if mode == 'include':\n        inds = bool_2_indices(~filt) + 1\n        rep = True\n    if mode == 'exclude':\n        inds = bool_2_indices(filt) + 1\n        rep = False\n\n    rem = (np.diff(inds) <= threshold)[:, 0]\n\n    cfilt = filt.copy()\n    if any(rem):\n        for lo, hi in inds[rem]:\n            cfilt[lo:hi] = rep\n\n    return cfilt"}
{"prompt": "def _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku):\n    \"\"\"Build and return Sailthru purchase item object\"\"\"", "output": "\n    purchase_item = {\n        \"item_id\": course_id,\n        \"item_name\": course_data.get(\"title\"),\n        \"item_description\": course_data.get(\"description\"),\n        \"item_url\": course_url,\n        \"item_image_url\": course_data.get(\"image_url\"),\n        \"item_price\": cost_in_cents,\n        \"item_currency\": \"USD\",\n        \"item_quantity\": 1,\n        \"item_mode\": mode,\n        \"item_sku\": sku,\n    }\n    return purchase_item\n\n\n", "solution": "\n\n    # build item description\n    item = {\n        'id': \"{}-{}\".format(course_id, mode),\n        'url': course_url,\n        'price': cost_in_cents,\n        'qty': 1,\n    }\n\n    # get title from course info if we don't already have it from Sailthru\n    if 'title' in course_data:\n        item['title'] = course_data['title']\n    else:\n        # can't find, just invent title\n        item['title'] = 'Course {} mode: {}'.format(course_id, mode)\n\n    if 'tags' in course_data:\n        item['tags'] = course_data['tags']\n\n    # add vars to item\n    item['vars'] = dict(course_data.get('vars', {}), mode=mode, course_run_id=course_id)\n\n    item['vars']['purchase_sku'] = sku\n\n    return item"}
{"prompt": "def _kl_normal_normal(n_a, n_b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n\n  Args:\n    n_a: instance of a Normal distribution object.\n    n_b: instance of a Normal distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_normal_normal\".\n\n  Returns:\n    Batchwise KL(n_a || n_b)\n  \"\"\"", "output": "\n  with tf.name_scope(name, \"kl_normal_normal\", [n_a, n_b]):\n   _a = n_a.mean()\n   _b = n_b.mean()\n   _a2 = n_a.variance()\n   _b2 = n_b.variance()\n   _ab = n_a.stddev() * n_b.stddev()\n   _ab2 = n_a.stddev() * n_b.stddev()\n   _ab3 = n_a.stddev() * n", "solution": "\n  with tf.name_scope(name or \"kl_normal_normal\"):\n    one = tf.constant(1, dtype=n_a.dtype)\n    two = tf.constant(2, dtype=n_a.dtype)\n    half = tf.constant(0.5, dtype=n_a.dtype)\n    s_a_squared = tf.square(n_a.scale)\n    s_b_squared = tf.square(n_b.scale)\n    ratio = s_a_squared / s_b_squared\n    return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *\n            (ratio - one - tf.math.log(ratio)))"}
{"prompt": "def scan(self, *key_ranges, **kwargs):\n        \"\"\"Scan for FCs in the given id ranges.\n\n        :param key_ranges:\n          ``key_ranges`` should be a list of pairs of ranges. The first\n          value is the lower bound id and the second value is the\n          upper bound id. Use ``()`` in either position to leave it\n          unbounded. If no ``key_ranges`` are given, then all FCs in\n          the store are returned.\n        :param [str] feature_names:\n          A list of feature names to retrieve. When ``None``, all\n          features are retrieved. Wildcards are allowed.\n        :rtype: Iterable of ``(content_id, FC)``\n        \"\"\"", "output": " returned.\n        :param [str]_features:\n          A list of feature names to retrieve from the store.\n          When ``None``, all features are returned.\n        :param [str]_feature_names:\n          A list of feature names to retrieve from the store.\n          When ``None``, all features are returned.\n        :param [str]_key_ranges:\n          ``key_ranges`` for the store.\n        :param [str]_feature_names:\n          A list of feature names to retrieve from the store.\n          When ``None``, all features are returned.", "solution": "\n        for hit in self._scan(*key_ranges, **kwargs):\n            yield did(hit['_id']), self.fc_from_dict(hit['_source']['fc'])"}
{"prompt": "def encode_span(self, span):\n        \"\"\"Encodes a single span to protobuf.\"\"\"", "output": "\n        encoded_span = self._encode_span(span)\n        self._write_encoded_span(encoded_span)\n\n    def _encode_span(self, span):\n        \"\"\"Encodes a single span to protobuf.\"\"\"\n        if self._use_proto_3:\n            return self._encode_span_proto_3(span)\n        else:\n            return self._encode_span_proto_2(span)\n\n    def _encode_span_proto_3(self, span):\n        \"\"\"Encodes a single span to protobuf using proto3.\"\"\"\n        trace_id = span.trace_id\n        span", "solution": "\n        if not protobuf.installed():\n            raise ZipkinError(\n                'protobuf encoding requires installing the protobuf\\'s extra '\n                'requirements. Use py-zipkin[protobuf] in your requirements.txt.'\n            )\n\n        pb_span = protobuf.create_protobuf_span(span)\n        return protobuf.encode_pb_list([pb_span])"}
{"prompt": "def verilogTypeOfSig(signalItem):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"", "output": "\n    if signalItem.startswith(\"reg\"):\n        return \"register\"\n    elif signalItem.startswith(\"wire\"):\n        return \"wire\"\n    else:\n        return \"unknown\"\n\ndef verilogTypeOfNet(netItem):\n    \"\"\"\n    Check if is or input\n    \"\"\"\n    if netItem.startswith(\"input\"):\n        return \"input\"\n    elif netItem.startswith(\"output\"):\n        return \"output\"\n    else:\n        return \"unknown\"\n\ndef verilogTypeOfNetList(netList):\n    \"\"\"\n    Check if all nets are input or output\n    \"\"\"\n    for netItem in netList:", "solution": "\n    driver_cnt = len(signalItem.drivers)\n    if signalItem._const or driver_cnt > 1 or\\\n       arr_any(signalItem.drivers, _isEventDependentDriver):\n        return SIGNAL_TYPE.REG\n    else:\n        if driver_cnt == 1:\n            d = signalItem.drivers[0]\n            if not isinstance(d, (Assignment, PortItem)):\n                return SIGNAL_TYPE.REG\n\n        return SIGNAL_TYPE.WIRE"}
{"prompt": "def check_for_bucket(self, bucket_name):\n        \"\"\"\n        Check if bucket_name exists.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"", "output": "\n        try:\n            self.s3.head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                return False\n            else:\n                raise e\n\n    def create_bucket(self, bucket_name):\n        \"\"\"\n        Create a new bucket.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        self.s3.create_bucket(Bucket=bucket_name)\n\n    def delete_bucket(self, bucket_name):\n        \"\"\"\n        Delete", "solution": "\n        try:\n            self.get_conn().head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False"}
{"prompt": "def get_event_hub(self, hub_name):\n        \"\"\"\n        Retrieves an existing event hub.\n\n        hub_name:\n            Name of the event hub.\n        \"\"\"", "output": "\n        return self._client.get_event_hub(hub_name)\n\n    def create_event_hub(self, event_hub_name, event_hub_partition_count, event_hub_message_retention_in_days):\n        \"\"\"\n        Creates a new event hub.\n\n        event_hub_name:\n            Name of the event hub.\n        event_hub_partition_count:\n            Number of partitions to create for the event hub.\n        event_hub_message_retention_in_days:\n            Number of days to retain messages for the event hub.\n        \"\"\"\n        return self._client.", "solution": "\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + _str(hub_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)"}
{"prompt": "def get_plot_data(self):\n        \"\"\" Generates the JSON report to plot the gene boxes\n\n        Following the convention of the reports platform, this method returns\n        a list of JSON/dict objects with the information about each entry in\n        the abricate file. The information contained in this JSON is::\n\n            {contig_id: <str>,\n             seqRange: [<int>, <int>],\n             gene: <str>,\n             accession: <str>,\n             coverage: <float>,\n             identity: <float>\n             }\n\n        Note that the `seqRange` entry contains the position in the\n        corresponding contig, not the absolute position in the whole assembly.\n\n        Returns\n        -------\n        json_dic : list\n            List of JSON/dict objects with the report data.\n        \"\"\"", "output": " and the range is inclusive.\n\n        Returns:\n            list: A list of JSON objects with the gene box information.\n        \"\"\"\n       _boxes = []\n       _boxes_dict = {}\n       _boxes_list = []\n\n       _boxes_dict[\"contig_id\"] = self.contig_id\n       _boxes_dict[\"seqRange\"] = [self.start, self.end]\n       _boxes_dict[\"gene\"] = self.gene\n        if self.accession:\n            accession = self.accession\n        else:\n            accession = \"NA\"\n        if self.coverage", "solution": "\n\n        json_dic = {\"plotData\": []}\n        sample_dic = {}\n        sample_assembly_map = {}\n\n        for entry in self.storage.values():\n\n            sample_id = re.match(\"(.*)_abr\", entry[\"log_file\"]).groups()[0]\n            if sample_id not in sample_dic:\n                sample_dic[sample_id] = {}\n\n            # Get contig ID using the same regex as in `assembly_report.py`\n            # template\n            contig_id = self._get_contig_id(entry[\"reference\"])\n            # Get database\n            database = entry[\"database\"]\n            if database not in sample_dic[sample_id]:\n                sample_dic[sample_id][database] = []\n\n            # Update the sample-assembly correspondence dict\n            if sample_id not in sample_assembly_map:\n                sample_assembly_map[sample_id] = entry[\"infile\"]\n\n            sample_dic[sample_id][database].append(\n                {\"contig\": contig_id,\n                 \"seqRange\": entry[\"seq_range\"],\n                 \"gene\": entry[\"gene\"].replace(\"'\", \"\"),\n                 \"accession\": entry[\"accession\"],\n                 \"coverage\": entry[\"coverage\"],\n                 \"identity\": entry[\"identity\"],\n                 },\n            )\n\n        for sample, data in sample_dic.items():\n            json_dic[\"plotData\"].append(\n                {\n                    \"sample\": sample,\n                    \"data\": {\"abricateXrange\": data},\n                    \"assemblyFile\": sample_assembly_map[sample]\n                }\n            )\n\n        return json_dic"}
{"prompt": "def usi(self):\n        \"\"\"\n        Gets an USI string for the move.\n        For example a move from 7A to 8A would be `7a8a` or `7a8a+` if it is\n        a promotion.\n        \"\"\"", "output": "\n        if self.is_capture:\n            return self.from_square + self.to_square + self.piece_type\n        else:\n            return self.from_square + self.to_square\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of the move.\n        \"\"\"\n        if self.is_capture:\n            return f\"{self.from_square}{self.to_square}{self.piece_type}\"\n        else:\n            return f\"{self.from_square}{self.to_square}\"\n\n    def __repr__(self):\n        \"\"\"\n        Returns a string representation of the", "solution": "\n        if self:\n            if self.drop_piece_type:\n                return '{0}*{1}'.format(PIECE_SYMBOLS[self.drop_piece_type].upper(), SQUARE_NAMES[self.to_square])\n            else:\n                return SQUARE_NAMES[self.from_square] + SQUARE_NAMES[self.to_square] + \\\n                       ('+' if self.promotion else '')\n        else:\n            return '0000'"}
{"prompt": "def load_institute(adapter, internal_id, display_name, sanger_recipients=None):\n    \"\"\"Load a institute into the database\n\n        Args:\n            adapter(MongoAdapter)\n            internal_id(str)\n            display_name(str)\n            sanger_recipients(list(email))\n    \"\"\"", "output": "\n    institute = {\n        \"internal_id\": internal_id,\n        \"display_name\": display_name,\n        \"sanger_recipients\": sanger_recipients\n    }\n    adapter.insert_institute(institute)\n\n\ndef load_samples(adapter, institute_id, sample_data):\n    \"\"\"Load sample data into the database\n\n        Args:\n            adapter(MongoAdapter)\n            institute_id(str)\n            sample_data(list(dict))\n    \"\"\"\n    for sample in sample_data:\n        sample[\"institute_id\"] = institute_id\n        adapter.insert_sample(", "solution": "\n\n    institute_obj = build_institute(\n        internal_id=internal_id,\n        display_name=display_name,\n        sanger_recipients=sanger_recipients\n    )\n    log.info(\"Loading institute {0} with display name {1}\" \\\n             \" into database\".format(internal_id, display_name))\n\n    adapter.add_institute(institute_obj)"}
{"prompt": "def compute_nc(X, G):\n    \"\"\"Computes the novelty curve from the self-similarity matrix X and\n        the gaussian kernel G.\"\"\"", "output": "\n    =_matrix(X, G)\n   _norm = / np.sum(np.sum(np.abs(X), axis=1), axis=1)[:, np.newaxis]\n   _norm =_norm / np.sum(np.sum(np.abs(X), axis=1), axis=1)[:, np.newaxis]\n   _norm = np.nan_to_num(np.abs(np.dot(np.dot(X, np.linalg.inv(G)), X.T)))\n   _norm = np", "solution": "\n    N = X.shape[0]\n    M = G.shape[0]\n    nc = np.zeros(N)\n\n    for i in range(M // 2, N - M // 2 + 1):\n        nc[i] = np.sum(X[i - M // 2:i + M // 2, i - M // 2:i + M // 2] * G)\n\n    # Normalize\n    nc += nc.min()\n    nc /= nc.max()\n    return nc"}
{"prompt": "def _get_boot_time():\n    \"\"\"Return system boot time (epoch in seconds)\"\"\"", "output": "\n    with open('/proc/uptime', 'r') as f:\n        uptime_seconds = float(f.readline().split()[0])\n    return time.time() - uptime_seconds\n\n\ndef _get_cpu_usage():\n    \"\"\"Return CPU usage as a percentage\"\"\"\n    with open('/proc/stat', 'r') as f:\n        cpu_stats = f.readline().split()\n    cpu_user = int(cpu_stats[1])\n    cpu_nice = int(cpu_stats[2])\n    cpu_system = int(cpu_stats[3])\n    cpu_idle", "solution": "\n    f = open('/proc/stat', 'r')\n    try:\n        for line in f:\n            if line.startswith('btime'):\n                return float(line.strip().split()[1])\n        raise RuntimeError(\"line not found\")\n    finally:\n        f.close()"}
{"prompt": "def get_plugin_of_type(self, plugin_class):\n        \"\"\"\n        Retrieve a plugin of desired class, KeyError raised otherwise\n        \"\"\"", "output": "\n        for plugin in self.plugins:\n            if isinstance(plugin, plugin_class):\n                return plugin\n        raise KeyError(\"No plugin of type {} found\".format(plugin_class.__name__))\n\n    def get_plugin_by_name(self, name):\n        \"\"\"\n        Retrieve a plugin by name, KeyError raised otherwise\n        \"\"\"\n        for plugin in self.plugins:\n            if plugin.name == name:\n                return plugin\n        raise KeyError(\"No plugin named {} found\".format(name))\n\n    def get_plugin_by_id(self, plugin_id):\n        \"\"\"\n        Retrieve a plugin by id, KeyError raised", "solution": "\n        logger.debug(\"Searching for plugin: %s\", plugin_class)\n        matches = [plugin for plugin in self.plugins.values() if isinstance(plugin, plugin_class)]\n        if matches:\n            if len(matches) > 1:\n                logger.debug(\n                    \"More then one plugin of type %s found. Using first one.\",\n                    plugin_class)\n            return matches[-1]\n        else:\n            raise KeyError(\"Requested plugin type not found: %s\" % plugin_class)"}
{"prompt": "def calculate_linear_predictor(model_matrix, model_coefficients, offset=None,\n                               name=None):\n  \"\"\"Computes `model_matrix @ model_coefficients + offset`.\"\"\"", "output": "\n  if offset is None:\n    offset = np.zeros(model_matrix.shape[0])\n  return np.dot(model_matrix, model_coefficients) + offset\n\n\ndef calculate_linear_predictor_with_(model_matrix,_matrix,_vector,\n                                        __vector,_prime_vector,\n                                        _prime_prime_vector,_prime_prime_prime_vector,\n                                        _prime_prime_prime_prime_vector,\n                                        _prime_prime_prime_prime_prime_vector,\n                                        _prime_", "solution": "\n  with tf.compat.v1.name_scope(name, 'calculate_linear_predictor',\n                               [model_matrix, model_coefficients, offset]):\n    predicted_linear_response = tf.linalg.matvec(model_matrix,\n                                                 model_coefficients)\n    if offset is not None:\n      predicted_linear_response += offset\n    return predicted_linear_response"}
{"prompt": "def render_hero_slider(context):\n    \"\"\"\n    Renders the hero slider.\n\n    \"\"\"", "output": "\n    hero_slider = context.get('hero_slider', None)\n    if not hero_slider:\n        return ''\n\n    return render_to_string('wagtail_hero_slider/hero_slider.html', {\n        'hero_slider': hero_slider,\n    })\n\n\n@register.simple_tag(takes_context=True)\ndef render_hero_slider_item(context, item):\n    \"\"\"\n    Renders a single hero slider item.\n\n    \"\"\"\n    return render_to_string('wagtail_hero_slider/hero_slider_item.html', {\n        '", "solution": "\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    return {\n        'slider_items': qs,\n    }"}
{"prompt": "def get_color(self, color, intensity=0):\n        \"\"\" Returns a QColor for a given color code, or None if one cannot be\n            constructed.\n        \"\"\"", "output": "\n        if color.startswith('#'):\n            color = color[1:]\n        if len(color) == 3:\n            color = color[0] + color[0] + color[1] + color[1] + color[2] + color[2]\n        if len(color) == 6:\n            r, g, b = int(color[0:2], 16), int(color[2:4], 16), int(color[4:6], 16)\n            if intensity:\n                r, g, b = (r * intensity, g * intensity", "solution": "\n        if color is None:\n            return None\n\n        # Adjust for intensity, if possible.\n        if color < 8 and intensity > 0:\n            color += 8\n\n        constructor = self.color_map.get(color, None)\n        if isinstance(constructor, basestring):\n            # If this is an X11 color name, we just hope there is a close SVG\n            # color name. We could use QColor's static method\n            # 'setAllowX11ColorNames()', but this is global and only available\n            # on X11. It seems cleaner to aim for uniformity of behavior.\n            return QtGui.QColor(constructor)\n\n        elif isinstance(constructor, (tuple, list)):\n            return QtGui.QColor(*constructor)\n\n        return None"}
{"prompt": "def update_z(self, z, indices=None):\n        \"\"\"\n        Update partial or entire z.\n\n        Args:\n            z (numpy.ndarray or list): to-be-updated z\n            indices (numpy.ndarray or list or optional): to-be-updated qubit indices\n\n        Returns:\n            Pauli: self\n\n        Raises:\n            QiskitError: when updating whole z, the number of qubits must be the same.\n        \"\"\"", "output": "\n        if.is_list(z):\n            z =.to_numpy(z)\n        if.is_list(indices):\n            indices =.to_numpy(indices)\n\n        if.is_numpy(z) and.is_numpy(indices):\n            if.shape(z)!=.shape(indices):\n                raise QiskitError(\"The shape of z and indices must be the same.\")\n            if.shape(z)[0]!= self.num_qubits:\n                raise QiskitError(\"The number of qubits must be the same", "solution": "\n        z = _make_np_bool(z)\n        if indices is None:\n            if len(self._z) != len(z):\n                raise QiskitError(\"During updating whole z, you can not \"\n                                  \"change the number of qubits.\")\n            self._z = z\n        else:\n            if not isinstance(indices, list) and not isinstance(indices, np.ndarray):\n                indices = [indices]\n            for p, idx in enumerate(indices):\n                self._z[idx] = z[p]\n\n        return self"}
{"prompt": "def is_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if completely-positive trace-preserving.\"\"\"", "output": "\n        =(self., atol=atol, rtol=rtol)\n        =(self. vt, atol=atol, rtol=rtol)\n        =(self. vt.T, atol=atol, rtol=rtol)\n        =(self. vt @ self. vt.T, atol=atol, rtol=rtol)\n        =(self. vt @ self. vt, atol=atol, rtol=rtol)\n        =(self. vt @ self. vt @ self. vt", "solution": "\n        if self._data[1] is not None:\n            return False\n        if atol is None:\n            atol = self._atol\n        if rtol is None:\n            rtol = self._rtol\n        accum = 0j\n        for op in self._data[0]:\n            accum += np.dot(np.transpose(np.conj(op)), op)\n        return is_identity_matrix(accum, rtol=rtol, atol=atol)"}
{"prompt": "def delete_clinvar_object(self, object_id, object_type, submission_id):\n        \"\"\"Remove a variant object from clinvar database and update the relative submission object\n\n            Args:\n                object_id(str) : the id of an object to remove from clinvar_collection database collection (a variant of a case)\n                object_type(str) : either 'variant_data' or 'case_data'. It's a key in the clinvar_submission object.\n                submission_id(str): the _id key of a clinvar submission\n\n            Returns:\n                updated_submission(obj): an updated clinvar submission\n        \"\"\"", "output": " the updated clinvar_submission object with the removed object\n        \"\"\"\n        clinvar_collection = self.db.clinvar_collection\n        clinvar_submission = clinvar_collection.find_one({'_id': ObjectId(submission_id)})\n        if clinvar_submission is None:\n            raise ValueError(f'No clinvar_submission object found with _id={submission_id}')\n\n        if object_type not in clinvar_submission:\n            raise ValueError(f'No {object_type} object found in clinvar_submission object with _id={submission_id}')\n\n        if object_id not in", "solution": "\n\n        LOG.info(\"Deleting clinvar object %s (%s)\", object_id, object_type)\n\n        # If it's a variant object to be removed:\n        #   remove reference to it in the submission object 'variant_data' list field\n        #   remove the variant object from clinvar collection\n        #   remove casedata object from clinvar collection\n        #   remove reference to it in the submission object 'caset_data' list field\n\n        # if it's a casedata object to be removed:\n        #   remove reference to it in the submission object 'caset_data' list field\n        #   remove casedata object from clinvar collection\n\n        result = ''\n\n        if object_type == 'variant_data':\n            # pull out a variant from submission object\n            self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'variant_data': object_id} })\n\n            variant_object = self.clinvar_collection.find_one( {'_id': object_id} )\n            linking_id = variant_object.get(\"linking_id\") #it's the original ID of the variant in scout, it's linking clinvar variants and casedata objects together\n\n            # remove any object with that linking_id from clinvar_collection. This removes variant and casedata\n            result = self.clinvar_collection.delete_many( {'linking_id': linking_id } )\n\n        else: # remove case_data but keep variant in submission\n            # delete the object itself from clinvar_collection\n             result = self.clinvar_collection.delete_one( {'_id': object_id } )\n\n        # in any case remove reference to it in the submission object 'caset_data' list field\n        self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'case_data': object_id} })\n\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id':submission_id}, { '$set' : {'updated_at': datetime.now()} }, return_document=pymongo.ReturnDocument.AFTER )\n\n        return updated_submission"}
{"prompt": "def _loadFromArray(self, dtype: HdlType, bitAddr: int) -> int:\n        \"\"\"\n        Parse HArray type to this transaction template instance\n\n        :return: address of it's end\n        \"\"\"", "output": "\n        = self.template.get(bitAddr, dtype)\n        if not isinstance(dtype, HArray):\n            raise TypeError(f\"Invalid type for: {dtype}\")\n        = self.template.get(bitAddr +.bitAddr,.dtype)\n       _end = bitAddr +.bitAddr +.bitWidth\n       _val =.loadFromArray(self,_end)\n        self.template.set(bitAddr,)\n        self.template.set(bitAddr +.bitAddr,_val)\n        return_", "solution": "\n        self.itemCnt = evalParam(dtype.size).val\n        self.children = TransTmpl(\n            dtype.elmType, 0, parent=self, origin=self.origin)\n        return bitAddr + self.itemCnt * self.children.bitAddrEnd"}
{"prompt": "def select(selector, obj):\n    \"\"\"Appy selector to obj and return matching nodes.\n\n    If only one node is found, return it, otherwise return a list of matches.\n    Returns False on syntax error. None if no results found.\n    \"\"\"", "output": "\n    try:\n        return selector.select(obj)\n    except Exception as e:\n        return False\n\ndef find(selector, obj):\n    \"\"\"Appy selector to obj and return first matching node.\n\n    Returns False on syntax error. None if no results found.\n    \"\"\"\n    try:\n        return selector.find(obj)\n    except Exception as e:\n        return False\n\ndef find_all(selector, obj):\n    \"\"\"Appy selector to obj and return all matching nodes.\n\n    Returns False on syntax error. None if no results found.\n    \"\"\"\n    try:\n        return selector.find_all(obj", "solution": "\n\n    parser = Parser(obj)\n    try:\n        return parser.parse(selector)\n    except SelectorSyntaxError as e:\n        log.exception(e)\n        return False"}
{"prompt": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"", "output": "\n    if ':' in host:\n        host, port = host.split(':', 1)\n        port = int(port)\n    else:\n        port = 80\n    return host, port\n\n\ndef get_ip_address(host):\n    \"\"\"Returns the IP address of a host.\n    \"\"\"\n    try:\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        ip_address = None\n    return ip_address\n\n\ndef get_local_ip_address():\n    \"\"\"Returns the IP address of the local host.\n    \"\"\"\n    return get_ip", "solution": "\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)"}
{"prompt": "def receive_nak_rebinding(self, pkt):\n        \"\"\"Receive NAK in REBINDING state.\"\"\"", "output": "\n        self.logger.info(\"Received NAK in REBINDING state\")\n        self.state =cp.State.IDLE\n        self.send_nak_ack(pkt)\n\n    def receive_nak_ack(self, pkt):\n        \"\"\"Receive NAK in ACK state.\"\"\"\n        self.logger.info(\"Received NAK in ACK state\")\n        self.state =cp.State.IDLE\n        self.send_nak_ack(pkt)\n\n    def receive_nak_fin(self, pkt):\n        \"\"\"Receive NAK in FIN state.\"\"\"\n        self.logger.info(\"Received NAK in FIN state", "solution": "\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()"}
{"prompt": "def diseases(context):\n    \"\"\"Show all diseases in the database\"\"\"", "output": "\n    = context.bot.get_user(context.author.id)\n  .send(\"Here are all the diseases in the database:\\n\\n\" + \"\\n\".join(d.name for d inc.Disease.objects.all()))\n\n@context.command()\ndefc(context, disease_name: str):\n    \"\"\"Showc for a disease\"\"\"\n    v = context.bot.get_user(context.author.id)\n    disease = next((d for d inc.Disease.objects.all() if d.name.", "solution": "\n    LOG.info(\"Running scout view diseases\")\n    adapter = context.obj['adapter']\n\n    disease_objs = adapter.disease_terms()\n\n    nr_diseases = disease_objs.count()\n    if nr_diseases == 0:\n        click.echo(\"No diseases found\")\n    else:\n        click.echo(\"Disease\")\n        for disease_obj in adapter.disease_terms():\n            click.echo(\"{0}\".format(disease_obj['_id']))\n        LOG.info(\"{0} diseases found\".format(nr_diseases))"}
{"prompt": "def infix_to_postfix(nodes, *, recurse_types=None):\n  \"\"\"Convert a list of nodes in infix order to a list of nodes in postfix order.\n\n  E.G. with normal algebraic precedence, 3 + 4 * 5 -> 3 4 5 * +\n  \"\"\"", "output": "\n  if recurse_types is None:\n    recurse_types = []\n\n  def visit(node):\n    if isinstance(node, list):\n      for child in node:\n        visit(child)\n    elif isinstance(node, Operator):\n      if node.precedence > 0 and node.precedence <= len(recurse_types):\n        for child in node.children:\n          visit(child)\n      else:\n        postfix.append(node)\n    else:\n      postfix.append(node)\n\n  postfix = []\n  for node in nodes:\n    visit(node)\n  return postfix\n\n\ndef postfix_to_infix", "solution": "\n  output = []\n  operators = []\n\n  for node in nodes:\n    if isinstance(node, OperatorNode):\n      # Drain out all operators whose precedence is gte the node's...\n      cmp_operator = node.operator\n      while operators:\n        current_operator = operators[-1].operator\n        if current_operator.precedence > cmp_operator.precedence or \\\n           current_operator.precedence == cmp_operator.precedence and current_operator.association == Association.left:\n          output.append(operators.pop())\n        else:\n          break\n      operators.append(node)\n    else:\n      if recurse_types is not None and node.node_type in recurse_types:\n        output.extend(infix_to_postfix(node.children, recurse_types=recurse_types))\n      else:\n        output.append(node)\n\n  return output + list(reversed(operators))"}
{"prompt": "def get(cls, key, default=None):\n        \"\"\"\n        Resembles the :meth:`dict.get` method.\n\n        :returns:\n            A configuration dictionary for specified provider.\n\n        \"\"\"", "output": "\n        return cls._config.get(key, default)\n\n    @classmethod\n    def set(cls, key, value):\n        \"\"\"\n        Resembles the :meth:`dict.set` method.\n\n        :param key:\n            The key to set.\n\n        :param value:\n            The value to set.\n\n        \"\"\"\n        cls._config[key] = value\n\n    @classmethod\n    def clear(cls):\n        \"\"\"\n        Resembles the :meth:`dict.clear` method.\n\n        \"\"\"\n        cls._config.clear()\n\n    @classmethod\n    def items(cls):\n        \"\"\"\n        Resembles the :meth:`dict", "solution": "\n\n        # Query datastore.\n        result = cls.query(cls.provider_name == key).get()\n\n        if result:\n            result_dict = result.to_dict()\n\n            # Use NDBOpenIDStore by default\n            result_dict['store'] = NDBOpenIDStore\n\n            # Convert coma-separated values to list. Currently only scope is\n            # csv.\n            for i in ('scope', ):\n                prop = result_dict.get(i)\n                if prop:\n                    result_dict[i] = [s.strip() for s in prop.split(',')]\n                else:\n                    result_dict[i] = None\n\n            return result_dict\n        else:\n            return default"}
{"prompt": "def dump(obj, file, reducers=None, protocol=None):\n    \"\"\"Replacement for pickle.dump() using _LokyPickler.\"\"\"", "output": "\n    if reducers is None:\n        reducers = {}\n    if protocol is None:\n        protocol = pickle.DEFAULT_PROTOCOL\n    with open(file, 'wb') as f:\n        _LokyPickler(f, reducers).dump(obj)\n\n\ndef load(file, reducers=None, encoding='utf-8'):\n    \"\"\"Replacement for pickle.load() using _LokyUnpickler.\"\"\"\n    if reducers is None:\n        reducers = {}\n    with open(file, 'rb') as f:\n        return _LokyUnpickler(f, reducers", "solution": "\n    global _LokyPickler\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)"}
{"prompt": "def rule(cls, rulename=None, erase=False):\n    \"\"\"Attach a method to a parsing class and register it as a parser rule.\n\n       The method is registered with its name unless rulename is provided.\n    \"\"\"", "output": "\n    def decorator(func):\n        if rulename is None:\n            rulename = func.__name__\n        if erase:\n            cls.erase_rule(rulename)\n        cls.register_rule(rulename, func)\n        return func\n    return decorator\n\n\nclass Parser(object):\n    \"\"\"A parser that uses a set of registered rules to parse a stream of tokens.\n\n       The parser can be configured to ignore certain tokens or to raise an\n       exception when a rule is not found.\n    \"\"\"\n    def __init__(self, ignore_tokens=None, raise_on_missing=False):", "solution": "\n    if not hasattr(cls, '_rules'):\n        raise TypeError(\n            \"%s didn't seems to be a BasicParser subsclasse\" % cls.__name__)\n    class_hook_list = cls._hooks\n    class_rule_list = cls._rules\n\n    def wrapper(f):\n        nonlocal rulename\n        add_method(cls)(f)\n        if rulename is None:\n            rulename = f.__name__\n        if not erase and (rulename in class_hook_list or rulename in class_rule_list):\n            raise TypeError(\"%s is already define has rule or hook\" % rulename)\n        if '.' not in rulename:\n            rulename = cls.__module__ + '.' + cls.__name__ + '.' + rulename\n        set_one(class_rule_list, rulename, f)\n        return f\n    return wrapper"}
{"prompt": "def download_url(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"", "output": "\n    from urllib.request import urlretrieve\n    from hashlib import md5\n\n    if filename is None:\n        filename = url.split('/')[-1]\n\n    if md5 is not None:\n        expected_md5 = md5(b'').hexdigest()\n        if md5!= expected_md5:\n            raise ValueError(f\"MD5 checksum mismatch for {url}. Expected {expected_md5}, got {md5}\")\n\n    local_filename = os.path.join(root, filename)\n    print(f\"Downloading {url} to {local_filename}\")\n    urlretrieve(url, local_filename", "solution": "\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == 'https':\n                url = url.replace('https:', 'http:')\n                print('Failed download. Trying https -> http instead.'\n                      ' Downloading ' + url + ' to ' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )"}
{"prompt": "def _in_buffer(self, position=None):\n        \"\"\" Returns whether the current cursor (or, if specified, a position) is\n            inside the editing region.\n        \"\"\"", "output": "\n        if position is None:\n            position = self.get_cursor_position()\n        return position >= 0 and position < len(self.buffer)\n\n    def _get_line_number(self, position):\n        \"\"\" Returns the line number of the line containing the given position. \"\"\"\n        return self.buffer.rfind('\\n', 0, position) + 1\n\n    def _get_line_position(self, line_number):\n        \"\"\" Returns the position of the first character of the given line number. \"\"\"\n        return self.buffer.rfind('\\n', 0, line_number *", "solution": "\n        cursor = self._control.textCursor()\n        if position is None:\n            position = cursor.position()\n        else:\n            cursor.setPosition(position)\n        line = cursor.blockNumber()\n        prompt_line = self._get_prompt_cursor().blockNumber()\n        if line == prompt_line:\n            return position >= self._prompt_pos\n        elif line > prompt_line:\n            cursor.movePosition(QtGui.QTextCursor.StartOfBlock)\n            prompt_pos = cursor.position() + len(self._continuation_prompt)\n            return position >= prompt_pos\n        return False"}
{"prompt": "def split_type(self, typename):\n        \"\"\"\n        Given a potentially complex type, split it into its base type and specializers\n        \"\"\"", "output": "\n        if typename.startswith('std::'):\n            typename = typename[5:]\n        if typename.startswith('const '):\n            typename = typename[6:]\n        if typename.startswith('volatile '):\n            typename = typename[9:]\n        if typename.startswith('const volatile '):\n            typename = typename[14:]\n        if typename.startswith('typename '):\n            typename = typename[10:]\n        if typename.startswith('class '):\n            typename = typename[6:]\n        if typename.startswith('struct '):\n            typename = typename[7:]\n        if typename.startswith('union '):\n            typename = typename", "solution": "\n\n        name = self._canonicalize_type(typename)\n        if '(' not in name:\n            return name, False, []\n\n        base, sub = name.split('(')\n        if len(sub) == 0 or sub[-1] != ')':\n            raise ArgumentError(\"syntax error in complex type, no matching ) found\", passed_type=typename, basetype=base, subtype_string=sub)\n\n        sub = sub[:-1]\n\n        subs = sub.split(',')\n        return base, True, subs"}
{"prompt": "def get_response_code(url, timeout=10):\n    \"\"\"\n    Visit the URL and return the HTTP response code in 'int'\n    \"\"\"", "output": "\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response.status_code\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n\ndef get_response_time(url, timeout=10):\n    \"\"\"\n    Visit the URL and return the response time in 'float'\n    \"\"\"\n    start_time = time.time()\n    response = requests.get(url, timeout=timeout)\n    end_time = time.time()\n    return end_time - start_time\n\n\ndef get_response_size(", "solution": "\n    try:    \n        req = urllib2.urlopen(url, timeout=timeout)\n    except HTTPError, e:\n        return e.getcode()\n    except Exception, _:\n        fail(\"Couldn't reach the URL '%s'\" % url)\n    else:\n        return req.getcode()"}
{"prompt": "def asAMP(cls):\n        \"\"\"\n        Returns the exception's name in an AMP Command friendly format.\n\n        For example, given a class named ``ExampleExceptionClass``, returns\n        ``\"EXAMPLE_EXCEPTION_CLASS\"``.\n        \"\"\"", "output": "\n        return cls.__name__.upper().replace(\"_\", \"_AND_\")\n\n\nclass CommandError(Exception, AMPException):\n    \"\"\"\n    Base class for all Command-related exceptions.\n    \"\"\"\n    pass\n\n\nclass CommandNotFound(CommandError):\n    \"\"\"\n    Raised when a command is not found in the command registry.\n    \"\"\"\n    pass\n\n\nclass CommandUsageError(CommandError):\n    \"\"\"\n    Raised when a command is used incorrectly or has invalid arguments.\n    \"\"\"\n    pass\n\n\nclass CommandArgumentError(CommandUsageError):\n    \"\"\"\n    Raised when a command argument is invalid.\n    \"\"\"\n    pass\n\n", "solution": "\n        parts = groupByUpperCase(cls.__name__)\n        return cls, \"_\".join(part.upper() for part in parts)"}
{"prompt": "def to_grayscale(img):\n    \"\"\"Convert PIL image to numpy grayscale array and numpy alpha array.\n\n    Args:\n      img (PIL.Image): PIL Image object.\n\n    Returns:\n      (gray, alpha): both numpy arrays.\n    \"\"\"", "output": "\n    gray = np.array(img.convert('L'))\n    alpha = np.array(img.convert('RGBA'))[:, :, 3]\n    return gray, alpha\n\n\ndef resize_image(img, size):\n    \"\"\"Resize PIL image to given size.\n\n    Args:\n      img (PIL.Image): PIL Image object.\n      size (tuple): (width, height) tuple.\n\n    Returns:\n      PIL.Image: resized image.\n    \"\"\"\n    return img.resize(size, resample=Image.BICUBIC)\n\n\ndef pad_image(img, size,=0", "solution": "\n    gray = numpy.asarray(ImageOps.grayscale(img)).astype(numpy.float)\n\n    imbands = img.getbands()\n    alpha = None\n    if 'A' in imbands:\n        alpha = numpy.asarray(img.split()[-1]).astype(numpy.float)\n\n    return gray, alpha"}
{"prompt": "def _handle_options(self, options):\n        \"\"\"\n        Handle the data from the options.\n\n        :param options: The list of options from the rule.\n        :type options: list\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"", "output": "\n        domains = []\n        for option in options:\n            if option.startswith('domain:'):\n                domain = option.split('domain:')[1].strip()\n                domains.append(domain)\n        return domains\n\n    def _handle_domains(self, domains):\n        \"\"\"\n        Handle the data from the domains.\n\n        :param domains: The list of domains from the rule.\n        :type domains: list\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"\n        return domains\n\n    def _handle_ip_ranges(self, ip_ranges):\n        \"\"\"\n        Handle the data from the", "solution": "\n\n        # We initiate a variable which will save our result\n        result = []\n\n        # We initiate the regex which will be used to extract the domain listed\n        # under the option domain=\n        regex_domain_option = r\"domain=(.*)\"\n\n        for option in options:\n            # We loop through the list of option.\n            try:\n                # We try to extract the list of domains from the currently read\n                # option.\n                domains = Regex(\n                    option, regex_domain_option, return_data=True, rematch=True, group=0\n                ).match()[-1]\n\n                if domains:\n                    # We could extract something.\n\n                    if self.aggressive:  # pragma: no cover\n                        result.extend(\n                            [\n                                x\n                                for x in domains.split(\"|\")\n                                if x and not x.startswith(\"~\")\n                            ]\n                        )\n                    else:\n                        # We return True.\n                        return True\n            except TypeError:\n                pass\n\n        # We return the result.\n        return result"}
{"prompt": "def to_fmt(self) -> fmt.indentable:\n    \"\"\"\n    Return an Fmt representation for pretty-printing\n    \"\"\"", "output": "\n    return fmt.indentable(\n        fmt.text(f\"{self.name} = {self.value}\"),\n        fmt.text(f\"type: {self.type}\"),\n        fmt.text(f\"description: {self.description}\"),\n    )\n\n\nclass Function(Node):\n    \"\"\"\n    Represents a function definition\n    \"\"\"\n\n    def __init__(self, name: str, args: List[Argument], body: List[Node]):\n        \"\"\"\n        Create a new Function node\n\n        :param name: The name of the function\n        :param args: The arguments of the function\n        :param body", "solution": "\n    qual = \"scope\"\n    txt = fmt.sep(\" \", [qual])\n    name = self.show_name()\n    if name != \"\":\n        txt.lsdata.append(name)\n    if len(self._hsig) > 0 or len(self.mapTypeTranslate) > 0:\n        lsb = []\n        if len(self.mapTypeTranslate) > 0:\n            lsb.append(\"translate:\\n\")\n            lsb.append(fmt.end(\"\\n\", self.mapTypeTranslate.to_fmt()))\n        for k in sorted(self._hsig.keys()):\n            s = self._hsig[k]\n            lsb.append(fmt.end(\"\\n\", [s.to_fmt()]))\n        block = fmt.block(\":\\n\", \"\", fmt.tab(lsb))\n        txt.lsdata.append(block)\n    return txt"}
{"prompt": "def when_children_replaced ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for children being replaced on a\n            specified object.\n        \"\"\"", "output": "\n        if remove:\n            if object in self._child_replaced_listeners:\n                self._child_replaced_listeners[object].remove(listener)\n        else:\n            if object not in self._child_replaced_listeners:\n                self._child_replaced_listeners[object] = set()\n            self._child_replaced_listeners[object].add(listener)\n\n    def when_children_removed ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for children being removed from a\n            specified object.\n        \"\"\"\n        if remove:\n            if object in self._child", "solution": "\n        object.on_trait_change( listener, \"subgraphs\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"clusters\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"nodes\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"edges\", remove = remove,\n                                dispatch = \"fast_ui\" )"}
{"prompt": "def tokenize_line(line):\n    \"\"\"\n    Tokenize a line:\n    * split tokens on whitespace\n    * treat quoted strings as a single token\n    * drop comments\n    * handle escaped spaces and comment delimiters\n    \"\"\"", "output": "\n    tokens = []\n    in_string = False\n    in_comment = False\n    comment_delimiter = None\n    escape_next = False\n    for i, c in enumerate(line):\n        if escape_next:\n            escape_next = False\n            continue\n        if in_string:\n            if c == comment_delimiter:\n                in_comment = not in_comment\n            elif c == '\\\\':\n                escape_next = True\n            elif c == '\"' and not in_comment:\n                in_string = False\n        elif in_comment:\n            if c == '\\n':\n                in_comment = False\n        else:\n            if", "solution": "\n    ret = []\n    escape = False\n    quote = False\n    tokbuf = \"\"\n    ll = list(line)\n    while len(ll) > 0:\n        c = ll.pop(0)\n        if c.isspace():\n            if not quote and not escape:\n                # end of token\n                if len(tokbuf) > 0:\n                    ret.append(tokbuf)\n\n                tokbuf = \"\"\n            elif quote:\n                # in quotes\n                tokbuf += c\n            elif escape:\n                # escaped space\n                tokbuf += c\n                escape = False\n            else:\n                tokbuf = \"\"\n\n            continue\n\n        if c == '\\\\':\n            escape = True\n            continue\n        elif c == '\"':\n            if not escape:\n                if quote:\n                    # end of quote\n                    ret.append(tokbuf)\n                    tokbuf = \"\"\n                    quote = False\n                    continue\n                else:\n                    # beginning of quote\n                    quote = True\n                    continue\n        elif c == ';':\n            if not escape:\n                # comment \n                ret.append(tokbuf)\n                tokbuf = \"\"\n                break\n            \n        # normal character\n        tokbuf += c\n        escape = False\n\n    if len(tokbuf.strip(\" \").strip(\"\\n\")) > 0:\n        ret.append(tokbuf)\n\n    return ret"}
{"prompt": "def tasks(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-09-01: :class:`TasksOperations<azure.mgmt.containerregistry.v2018_09_01.operations.TasksOperations>`\n        \"\"\"", "output": "\n        api_version = self._client.api_version\n        if api_version == '2018-09-01':\n            from.v2018_09_01.operations import TasksOperations\n            return TasksOperations(self._client)\n        raise NotImplementedError(\"API version {} is not available\".format(api_version))\n", "solution": "\n        api_version = self._get_api_version('tasks')\n        if api_version == '2018-09-01':\n            from .v2018_09_01.operations import TasksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def f_supports(self, data):\n        \"\"\"Checks if input data is supported by the parameter.\"\"\"", "output": "\n        return isinstance(data, str)\n\n    def f_validate(self, data):\n        \"\"\"Validates input data for the parameter.\"\"\"\n        if not self.f_supports(data):\n            raise ValueError(f\"Input data '{data}' is not supported by the parameter.\")\n        return data\n\n\nclass StringParameter(Parameter):\n    \"\"\"Parameter that expects a string as input.\"\"\"\n\n    def __init__(self, name, description, default=None, required=False):\n        super().__init__(name, description, default, required)\n\n    def f_supports(self, data):\n        \"\"\"Checks if input data is supported", "solution": "\n        dtype = type(data)\n        if dtype is tuple or dtype is list and len(data) == 0:\n            return True  #  ArrayParameter does support empty tuples\n        elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:\n                return True  #  ArrayParameter supports empty numpy arrays\n        else:\n            return super(ArrayParameter, self).f_supports(data)"}
{"prompt": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"", "output": "\n  # Compute the mean and variance of the masked time series.\n  masked_time_series = tf.boolean_mask(time_series_tensor, broadcast_mask)\n  mean = tf.reduce_mean(masked_time_series, axis=0)\n  variance = tf.reduce_mean(tf.square(masked_time_series - mean), axis=0)\n\n  return mean, variance\n\n\ndef_distribution__time_series(time_series_tensor,_distribution,_distribution_params):\n  \"\"\"Compute-distribution of a time series.\n\n  Args:", "solution": "\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance"}
{"prompt": "def flush(self, full=False, all=False, force=False):\n        \"\"\"\n        Call ``.save()`` on all modified entities in the session. Use when you\n        want to flush changes to Redis, but don't want to lose your local\n        session cache.\n\n        See the ``.commit()`` method for arguments and their meanings.\n        \"\"\"", "output": "\n        self.commit(full=full, all=all, force=force)\n\n    def commit(self, full=False, all=False, force=False):\n        \"\"\"\n        Commit all changes to the session.\n\n        :param full: If ``True``, commit all changes, including those that have not been saved to the database yet.\n        :param all: If ``True``, commit all changes, including those that have not been saved to the database yet.\n        :param force: If ``True``, commit all changes, including those that have not been saved to the database yet.\n        \"\"\"\n        if full:\n            self", "solution": "\n        self._init()\n\n        return self.save(*self.known.values(), full=full, all=all, force=force)"}
{"prompt": "def format_price_commas(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) with commas\n    \"\"\"", "output": "\n    return '{:,}'.format(int(price * 100))\n\ndef format_price_currency(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) with currency symbol\n    \"\"\"\n    return f'{format_price_commas(price)} pounds'\n\ndef format_price_(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) with symbol\n    \"\"\"\n    return f'{format_price_commas(price)} \u00a3'\n", "solution": "\n    if price is None:\n        return None\n    if price >= 0:\n        return jinja2.Markup('&pound;{:,.2f}'.format(price))\n    else:\n        return jinja2.Markup('-&pound;{:,.2f}'.format(-price))"}
{"prompt": "def _distribute_jobs(self):\n        \"\"\"\n        Distributes jobs from the active job set to any waiting get_job\n        callbacks.\n        \"\"\"", "output": "\n        for callback in self.waiting_get_job_callbacks:\n            callback()\n        self.waiting_get_job_callbacks = []\n\n    def _get_job(self):\n        \"\"\"\n        Returns the next job from the active job set, or None if there are no\n        jobs.\n        \"\"\"\n        if not self.active_jobs:\n            return None\n        job = self.active_jobs.pop(0)\n        self.waiting_get_job_callbacks.append(job.get_job)\n        return job\n\n    def _add_job(self, job):\n        \"\"\"\n        Adds a job to the", "solution": "\n\n        while (self._active_js.job_available()\n                and len(self._ready_callbacks) > 0):\n            job = self._active_js.get_job()\n            self._job_sources[job] = self._active_js\n            callback = self._ready_callbacks.popleft()\n            callback(job)"}
{"prompt": "def finalize(self, success_or_fail, message=''):\n        \"\"\"\n        :param success_or_fail: string that is 'success' or 'fail'\n        :param message:\n        \"\"\"", "output": "\n        if success_or_fail =='success':\n            self.success_count += 1\n            self.success_messages.append(message)\n        else:\n            self.fail_count += 1\n            self.fail_messages.append(message)\n\n    def get_success_count(self):\n        return self.success_count\n\n    def get_fail_count(self):\n        return self.fail_count\n\n    def get_success_messages(self):\n        return self.success_messages\n\n    def get_fail_messages(self):\n        return self.fail_messages\n\n    def get_total_", "solution": "\n        self.logit.debug('String OutputPorts: %s' % self.__string_output_ports)\n        if self.__string_output_ports:\n            with open(os.path.join(self.output_path, 'ports.json'), 'w') as opf:\n                json.dump(self.__string_output_ports, opf, indent=4)\n            self.logit.debug('Ports.json written to %s' % os.path.join(self.output_path, 'ports.json'))\n\n        with open(os.path.join(self.base_path, 'status.json'), 'w') as sf:\n            json.dump({'status': success_or_fail, 'reason': message}, sf, indent=4)"}
{"prompt": "def get_users_for_sis_course_id(self, sis_course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given sis course id.\n        \"\"\"", "output": "\n        url = f\"{self.base_url}/users?course_id={sis_course_id}\"\n        response = self.session.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n\n    def get_user_by_sis_user_id(self, sis_user_id):\n        \"\"\"\n        Returns a single user for the given sis user id.\n        \"\"\"\n        url = f\"{self.base_url}/users/{sis_user_id}\"\n        response = self.session.get(url)\n        response.raise_", "solution": "\n        return self.get_users_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)"}
{"prompt": "def sync_folder(self, path, bucket):\n        \"\"\"Syncs a local directory with an S3 bucket.\n     \n        Currently does not delete files from S3 that are not in the local directory.\n\n        path: The path to the directory to sync to S3\n        bucket: The name of the bucket on S3\n        \"\"\"", "output": "\n        s3 = boto3.resource('s3')\n        bucket = s3.Bucket(bucket)\n\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                local_path = os.path.join(root, file)\n                s3_path = os.path.join(root[len(path):], file)\n                bucket.upload_file(local_path, bucket.name, s3_path)\n\n    def delete_files_from_s3(self, bucket, prefix):\n        \"\"\"Deletes files from an S3 bucket that match a prefix", "solution": "\n        bucket = self.conn.get_bucket(bucket)\n        local_files = self._get_local_files(path)\n        s3_files = self._get_s3_files(bucket)\n        for filename, hash in local_files.iteritems():\n            s3_key = s3_files[filename]\n            if s3_key is None:\n                s3_key = Key(bucket)\n                s3_key.key = filename\n                s3_key.etag = '\"!\"'\n            \n            if s3_key.etag[1:-1] != hash[0]:\n                s3_key.set_contents_from_filename(join(path, filename), md5=hash)"}
{"prompt": "def new(self, *args, **kwargs):\n        \"\"\"\n        Create and return a new instance.\n        \"\"\"", "output": "\n        return self.model(*args, **kwargs)\n\n    def get_queryset(self):\n        \"\"\"\n        Return the filtered queryset based on the request.\n        \"\"\"\n        queryset = self.model.objects.all()\n        if self.request.user.is_authenticated:\n            queryset = queryset.filter(user=self.request.user)\n        return queryset\n\n    def get_context_data(self, **kwargs):\n        \"\"\"\n        Add the current user to the context data.\n        \"\"\"\n        context = super(ListView, self).get_context_data(**kwargs)\n        context['user'] = self.request.", "solution": "\n        inst = self.clazz()\n        self.storage.append(inst)\n        \n        # set all attributes with an initial default value\n        referential_attributes = dict()\n        for name, ty in self.attributes:\n            if name not in self.referential_attributes:\n                value = self.default_value(ty)\n                setattr(inst, name, value)\n            \n        # set all positional arguments\n        for attr, value in zip(self.attributes, args):\n            name, ty = attr\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n            \n        # set all named arguments\n        for name, value in kwargs.items():\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n        \n        if not referential_attributes:\n            return inst\n        \n        # batch relate referential attributes \n        for link in self.links.values():\n            if set(link.key_map.values()) - set(referential_attributes.keys()):\n                continue\n             \n            kwargs = dict()\n            for key, value in link.key_map.items():\n                kwargs[key] = referential_attributes[value]\n            \n            if not kwargs:\n                continue\n            \n            for other_inst in link.to_metaclass.query(kwargs):\n                relate(other_inst, inst, link.rel_id, link.phrase)\n        \n        for name, value in referential_attributes.items():\n            if getattr(inst, name) != value:\n                logger.warning('unable to assign %s to %s', name, inst)\n                \n        return inst"}
{"prompt": "def plotBrightLimitInV(gBright, pdf=False, png=False):\n  \"\"\"\n  Plot the bright limit of Gaia in V as a function of (V-I).\n\n  Parameters\n  ----------\n\n  gBright - The bright limit of Gaia in G\n  \"\"\"", "output": "\n\n  =_gaia_bright_limit_in_v(gBright)\n  =_gaia_bright_limit_in_v(gBright,=True)\n\n  =_gaia_bright_limit_in_v(gBright,=True,=True)\n\n  =_gaia_bright_limit_in_v(gBright,=True,=True,=True)\n\n  =_gaia_bright_limit_in_v(gBright,=True,", "solution": "\n  vmini=np.linspace(0.0,6.0,1001)\n  gminv=gminvFromVmini(vmini)\n  vBright=gBright-gminv\n\n  fig=plt.figure(figsize=(10,6.5))\n  plt.plot(vmini,vBright,'b-')\n  plt.xlabel('$(V-I)$')\n  plt.ylabel('Bright limit of Gaia in $V$')\n  plt.xlim(0,6)\n  plt.ylim(5,11)\n  plt.grid(which='both')\n  plt.title(\"Bright limit in $G$: {0}\".format(gBright))\n\n  if (pdf):\n    plt.savefig('VBandBrightLimit.pdf')\n  elif (png):\n    plt.savefig('VBandBrightLimit.png')\n  else:\n    plt.show()"}
{"prompt": "def _all_get_or_create_table(self, where, tablename, description, expectedrows=None):\n        \"\"\"Creates a new table, or if the table already exists, returns it.\"\"\"", "output": "\n        if expectedrows is None:\n            expectedrows = 0\n        try:\n            self.cursor.execute(\"SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name=?\", (tablename,))\n            if self.cursor.fetchone()[0] == 0:\n                self.cursor.execute(\"CREATE TABLE {} ({})\".format(tablename, description))\n                self.conn.commit()\n                self.cursor.execute(\"SELECT COUNT(*) FROM {} WHERE {}\".format(tablename, where))\n                if self.cursor.fetchone()[0]!= expectedrows:\n                    raise Exception(\"Table {} not created or not", "solution": "\n        where_node = self._hdf5file.get_node(where)\n\n        if not tablename in where_node:\n            if not expectedrows is None:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              expectedrows=expectedrows,\n                                              filters=self._all_get_filters())\n            else:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              filters=self._all_get_filters())\n        else:\n            table = where_node._f_get_child(tablename)\n\n        return table"}
{"prompt": "def _align(self, axes, key_shape=None):\n        \"\"\"\n        Align local bolt array so that axes for iteration are in the keys.\n\n        This operation is applied before most functional operators.\n        It ensures that the specified axes are valid, and might transpose/reshape\n        the underlying array so that the functional operators can be applied\n        over the correct records.\n\n        Parameters\n        ----------\n        axes: tuple[int]\n            One or more axes that will be iterated over by a functional operator\n\n        Returns\n        -------\n        BoltArrayLocal\n        \"\"\"", "output": "\n       _axes = self.axes\n       _shape = self.shape\n\n        # Check if the specified axes are valid\n        for axis in axes:\n            if axis not in self.axes:\n                raise ValueError(f\"Axis {axis} is not valid for_axes {self.axes}\")\n\n        # Check if the_axes are already aligned\n        if_axes == axes:\n            return self\n\n        # Transpose the_axes to match the_shape\n       _axes_transposed = tuple(axes)\n       _shape_transposed = tuple(self.shape[i] for", "solution": "\n\n        # ensure that the key axes are valid for an ndarray of this shape\n        inshape(self.shape, axes)\n\n        # compute the set of dimensions/axes that will be used to reshape\n        remaining = [dim for dim in range(len(self.shape)) if dim not in axes]\n        key_shape = key_shape if key_shape else [self.shape[axis] for axis in axes]\n        remaining_shape = [self.shape[axis] for axis in remaining]\n        linearized_shape = [prod(key_shape)] + remaining_shape\n\n        # compute the transpose permutation\n        transpose_order = axes + remaining\n\n        # transpose the array so that the keys being mapped over come first, then linearize keys\n        reshaped = self.transpose(*transpose_order).reshape(*linearized_shape)\n\n        return reshaped"}
{"prompt": "def info(self):\n        \"\"\" Returns a description of the trait.\"\"\"", "output": "\n        return self.description\n\n    def validate(self, obj, name, value):\n        \"\"\" Validates the trait value. \"\"\"\n        if not self.allow_none and value is None:\n            raise TraitError(f\"{name} cannot be None.\")\n        if self.regex and not re.match(self.regex, str(value)):\n            raise TraitError(f\"{name} must match the regular expression {self.regex}.\")\n        if self.min_value is not None and value < self.min_value:\n            raise TraitError(f\"{name} must be greater than or equal to {self", "solution": "\n        result = 'any of ' + repr(self.values)\n        if self._allow_none:\n            return result + ' or None'\n        return result"}
{"prompt": "def update_affinity_group(self, affinity_group_name, label,\n                              description=None):\n        \"\"\"\n        Updates the label and/or the description for an affinity group for the\n        specified subscription.\n\n        affinity_group_name:\n            The name of the affinity group.\n        label:\n            A name for the affinity group. The name can be up to 100 characters\n            in length.\n        description:\n            A description for the affinity group. The description can be up to\n            1024 characters in length.\n        \"\"\"", "output": "\n        if not isinstance(affinity_group_name, six.string_types):\n            raise TypeError(\"affinity_group_name must be a string\")\n        if not isinstance(label, six.string_types):\n            raise TypeError(\"label must be a string\")\n        if description is not None and not isinstance(description, six.string_types):\n            raise TypeError(\"description must be a string or None\")\n\n        data = {\n            \"label\": label,\n            \"description\": description\n        }\n\n        url = self._build_url(\n            \"subscriptions/{subscription_id}/affinity_groups/{affinity_group_name}\",", "solution": "\n        _validate_not_none('affinity_group_name', affinity_group_name)\n        _validate_not_none('label', label)\n        return self._perform_put(\n            '/' + self.subscription_id + '/affinitygroups/' +\n            _str(affinity_group_name),\n            _XmlSerializer.update_affinity_group_to_xml(label, description))"}
{"prompt": "def HWProcess(cls, proc, ctx):\n        \"\"\"\n        Serialize HWProcess instance\n\n        :param scope: name scope to prevent name collisions\n        \"\"\"", "output": "\n       _dict = {}\n       _dict['name'] = proc.name\n        if hasattr(proc, 'clock_domain'):\n            if proc.clock_domain is not None:\n                if isinstance(proc.clock_domain, str):\n                    clock_domain = proc.clock_domain\n                else:\n                    clock_domain = proc.clock_domain.name\n                if clock_domain is not None:\n                    if clock_domain not in ctx.clock_domains:\n                        raise ValueError(f\"Clock domain '{clock_domain}' not found in context\")\n                    clock_domain_id = ctx.clock_domains", "solution": "\n        body = proc.statements\n        childCtx = ctx.withIndent()\n        statemets = [cls.asHdl(s, childCtx) for s in body]\n        proc.name = ctx.scope.checkedName(proc.name, proc)\n\n        return cls.methodTmpl.render(\n            indent=getIndent(ctx.indent),\n            name=proc.name,\n            statements=statemets\n        )"}
{"prompt": "def _get_tol(tol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.\"\"\"", "output": "\n  if tol is None:\n    return tf.constant(0, dtype=dtype)\n  if validate_args:\n    tf.debugging.assert_non_negative(tol, message=\"tol must be non-negative\")\n  return tf.convert_to_tensor(tol, dtype=dtype)\n\n\ndef _get___tol(tol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 1 if `tol` is None, validation optional.\"\"\"\n  if tol is None:\n    return tf.constant(1, dtype=dtype)\n  if validate_args:", "solution": "\n  if tol is None:\n    return tf.convert_to_tensor(value=0, dtype=dtype)\n\n  tol = tf.convert_to_tensor(value=tol, dtype=dtype)\n  if validate_args:\n    tol = distribution_util.with_dependencies([\n        assert_util.assert_non_negative(\n            tol, message=\"Argument 'tol' must be non-negative\")\n    ], tol)\n  return tol"}
{"prompt": "def get_item_objects(self, query_params=None):\n        \"\"\"\n        Get the items for this checklist. Returns a list of ChecklistItem objects.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n\n        items = self.client.get(f\"{self.api_url}/items\", query_params=query_params)\n        return [ChecklistItem(item, self.client) for item in items]\n\n    def get_item(self, item_id):\n        \"\"\"\n        Get a specific item for this checklist. Returns a ChecklistItem object.\n        \"\"\"\n        item = self.client.get(f\"{self.api_url}/items/{item_id}\")\n        return ChecklistItem(item, self.client)\n\n    def create_item", "solution": "\n        card = self.get_card()\n        checklistitems_list = []\n        for checklistitem_json in self.get_items(query_params):\n            checklistitems_list.append(self.create_checklist_item(card.id, self.id, checklistitem_json))\n\n        return checklistitems_list"}
{"prompt": "def onTWriteCallback__init(self, sim):\n        \"\"\"\n        Process for injecting of this callback loop into simulator\n        \"\"\"", "output": "\n        sim.register_callback(self.onTWriteCallback)\n\n    def onTWriteCallback(self, sim,,):\n        \"\"\"\n        Callback function for messages.\n        \"\"\"\n       ___ = sim.get__by_id(o.get_id(sim, o.get__by_id(sim, o.get_id(sim, o.get_id(sim, o.get_id(sim, o.get_id(sim, o.get_id(sim, o.get_id(sim, o", "solution": "\n        yield from self.onTWriteCallback(sim)\n        self.intf.t._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)\n        self.intf.o._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)"}
{"prompt": "def GetParam(tag, param, default=__SENTINEL):\r\n    \"\"\" Convenience function for accessing tag parameters\"\"\"", "output": "\r\n    if tag.get(param, default) == __SENTINEL:\r\n        raise ValueError(\"Tag does not have parameter %s\" % param)\r\n    return tag.get(param)\r\n\r\n\r\ndef GetParamInt(tag, param, default=__SENTINEL):\r\n    \"\"\" Convenience function for accessing tag parameters as integers\"\"\"\r\n    value = GetParam(tag, param, default)\r\n    try:\r\n        return int(value)\r\n    except ValueError:\r\n        raise ValueError(\"Tag parameter %s is not an integer: %s\" % (param, value))\r\n\r\n\r\ndef GetParamFloat(tag, param, default=", "solution": "\r\n    if tag.HasParam(param):\r\n        return tag.GetParam(param)\r\n    else:\r\n        if default == __SENTINEL:\r\n            raise KeyError\r\n        else:\r\n            return default"}
{"prompt": "def _connection_failed(self, error=\"Error not specified!\"):\n        \"\"\"Clean up after connection failure detected.\"\"\"", "output": "\n        self.logger.error(error)\n        self.disconnect()\n\n    def _connection_succeeded(self):\n        \"\"\"Clean up after successful connection.\"\"\"\n        self.logger.info(\"Connected toCenter Server.\")\n        self.disconnect()\n\n    def _disconnect_failed(self, error=\"Error not specified!\"):\n        \"\"\"Clean up after disconnection failure detected.\"\"\"\n        self.logger.error(error)\n        self.disconnect()\n\n    def _disconnect_succeeded(self):\n        \"\"\"Clean up after successful disconnection.\"\"\"\n        self.logger.info(\"Disconnected from vCenter Server.\")\n\n    def _get_vcenter_", "solution": "\n        if not self._error:\n            LOG.error(\"Connection failed: %s\", str(error))\n            self._error = error"}
{"prompt": "def sequence_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use out-of-sequence message detection for messages\n        protected by :meth:`get_mic` and :meth:`wrap`. False if OOS detection cannot be used.\n        \"\"\"", "output": "\n        return self._sequence_detection_negotiated\n\n    def get_mic(self, message):\n        \"\"\"\n        Get the message integrity code (MIC) for a message.\n\n        :param message: The message to get the MIC for.\n        :type message: bytes\n        :return: The MIC for the message.\n        :rtype: bytes\n        \"\"\"\n        raise NotImplementedError()\n\n    def wrap(self, message, mic):\n        \"\"\"\n        Wrap a message with the MIC.\n\n        :param message: The message to wrap.\n        :type message: bytes\n        :param mic: The MIC for the message.\n        :type mic:", "solution": "\n        return (\n            self.flags & C.GSS_C_SEQUENCE_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def domain_to_fqdn(domain, proto=None):\n    \"\"\" returns a fully qualified app domain name \"\"\"", "output": "\n    if proto is None:\n        proto = 'https'\n    return f\"{proto}://{domain}\"\n\n\ndef get_app_domain(request):\n    \"\"\" returns the fully qualified app domain name \"\"\"\n    return domain_to_fqdn(request.get_host())\n\n\ndef get_app_url(request):\n    \"\"\" returns the app url without the protocol \"\"\"\n    return request.get_host()\n\n\ndef get_app_name(request):\n    \"\"\" returns the app name \"\"\"\n    return request.resolver_match.app_name\n\n\ndef get_app_version(request):\n    \"\"\"", "solution": "\n    from .generic import get_site_proto\n    proto = proto or get_site_proto()\n    fdqn = '{proto}://{domain}'.format(proto=proto, domain=domain)\n    return fdqn"}
{"prompt": "def default(self, o):\n        \"\"\"Implement this method in a subclass such that it returns a\n        serializable object for ``o``, or calls the base implementation (to\n        raise a ``TypeError``).\n\n        For example, to support arbitrary iterators, you could implement\n        default like this::\n\n            def default(self, o):\n                try:\n                    iterable = iter(o)\n                except TypeError:\n                    pass\n                else:\n                    return list(iterable)\n                return JSONEncoder.default(self, o)\n        \"\"\"", "output": "\n        raise TypeError(\"Object of type '{}' is not JSON serializable\".format(type(o).__name__))\n\nclass CustomJSONEncoder(JSONEncoder):\n    def default(self, o):\n        if isinstance(o, datetime):\n            return o.isoformat()\n        elif isinstance(o, Decimal):\n            return str(o)\n        elif isinstance(o, uuid.UUID):\n            return str(o)\n        else:\n            return JSONEncoder.default(self, o)\n\ndef custom_json_dumps(obj, **kwargs):\n    return json.dumps(obj, cls=CustomJSONEncoder, **", "solution": "\n        if isinstance(o, datetime):\n            return http_date(o)\n        if isinstance(o, uuid.UUID):\n            return str(o)\n        if hasattr(o, '__html__'):\n            return text_type(o.__html__())\n        return _json.JSONEncoder.default(self, o)"}
{"prompt": "def merge(\n            self, reservation_order_id, sources=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Merges two `Reservation`s.\n\n        Merge the specified `Reservation`s into a new `Reservation`. The two\n        `Reservation`s being merged must have same properties.\n\n        :param reservation_order_id: Order Id of the reservation\n        :type reservation_order_id: str\n        :param sources: Format of the resource id should be\n         /providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}\n        :type sources: list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns list or\n         ClientRawResponse<list> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`\n        \"\"\"", "output": "}\n        :type sources: list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param polling: True for asynchronous, False for synchronous operation\n        :return: An instance of LROPoller that returns either Reservation or\n         ClientRawResponse if raw=true\n        :rtype: ~msrestazure.azure_exceptions.LROPoller[~azure.mgmt.reservations.models.Reservation]\n         or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`", "solution": "\n        raw_result = self._merge_initial(\n            reservation_order_id=reservation_order_id,\n            sources=sources,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[ReservationResponse]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _get_concatenation(extractors, text, *, ignore_whitespace=True):\n  \"\"\"Returns a concatenation ParseNode whose children are the nodes returned by each of the\n  methods in the extractors enumerable.\n\n  If ignore_whitespace is True, whitespace will be ignored and then attached to the child it\n  preceeded.\n  \"\"\"", "output": "\n  result = ParseNode(None, None)\n  for extractor in extractors:\n    child = extractor(text)\n    if child:\n      if ignore_whitespace and child.is_whitespace:\n        if result.children:\n          result.children[-1].value += child.value\n        else:\n          result.children.append(child)\n      else:\n        result.children.append(child)\n  return result\n\n\ndef _get_text(node):\n  \"\"\"Returns the text content of a node, including any whitespace.\"\"\"\n  return ''.join(child.value for child in node.children if not child.is", "solution": "\n  ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n\n  extractor, *remaining = extractors\n\n  child = _call_extractor(extractor, use_text)\n  child.add_ignored(ignored_ws)\n\n  # TODO: Should I set node.position = -len(text) for the case that ignored whitespace will cause\n  #       the first child's position to not be the whitespace, and therefore the concatenation's\n  #       position will be the first non-whitespace? I think not, but I'm adding this note in\n  #       case that causes an issue I'm not seeing at the moment.\n  node = ParseNode(ParseNodeType.concatenation, children=[child])\n\n  if remaining:\n    # child.consumed will include ignored whitespace, so we base the text we pass on on text rather\n    # than use_text.\n    return node.merged(_get_concatenation(remaining,\n                                          text[child.consumed:],\n                                          ignore_whitespace=ignore_whitespace))\n  else:\n    return node"}
{"prompt": "def add_virtual_columns_rotation(self, x, y, xnew, ynew, angle_degrees, propagate_uncertainties=False):\n        \"\"\"Rotation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:\n        \"\"\"", "output": "\n        if not isinstance(angle_degrees, (int, float)):\n            raise ValueError(\"angle_degrees must be a number\")\n        if angle_degrees < 0:\n            raise ValueError(\"angle_degrees must be non-negative\")\n\n        if not isinstance(propagate_uncertainties, bool):\n            raise ValueError(\"propagate_uncertainties must be a boolean\")\n\n        if x not in self.data_columns:\n            raise ValueError(f\"x column {x} not found in data\")\n        if y not in self.data_columns:\n            raise ValueError(f\"y column {y} not found", "solution": "\n        x = _ensure_string_from_expression(x)\n        y = _ensure_string_from_expression(y)\n        theta = np.radians(angle_degrees)\n        matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n        m = matrix_name = x + \"_\" + y + \"_rot\"\n        for i in range(2):\n            for j in range(2):\n                self.set_variable(matrix_name + \"_%d%d\" % (i, j), matrix[i, j].item())\n        self[xnew] = self._expr(\"{m}_00 * {x} + {m}_01 * {y}\".format(**locals()))\n        self[ynew] = self._expr(\"{m}_10 * {x} + {m}_11 * {y}\".format(**locals()))\n        if propagate_uncertainties:\n            self.propagate_uncertainties([self[xnew], self[ynew]])"}
{"prompt": "def _call(self, resource, params):\n        \"\"\"Call to get a resource.\n\n        :param method: resource to get\n        :param params: dict with the HTTP parameters needed to get\n            the given resource\n        \"\"\"", "output": "\n        url = self.api_url + resource\n        headers = self._headers()\n        response = requests.get(url, headers=headers, params=params)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception(f\"Error getting {resource}: {response.text}\")\n\n    def _headers(self):\n        \"\"\"Return the headers needed for the API requests.\"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        return headers\n\n    def _post", "solution": "\n        url = self.URL % {'base': self.base_url, 'resource': resource}\n\n        if self.api_token:\n            params[self.PKEY] = self.api_token\n\n        logger.debug(\"Redmine client requests: %s params: %s\",\n                     resource, str(params))\n\n        r = self.fetch(url, payload=params, verify=False)\n\n        return r.text"}
{"prompt": "def maintained_selection():\n    \"\"\"Maintain selection during context\n\n    Example:\n        >>> with maintained_selection():\n        ...     # Modify selection\n        ...     cmds.select('node', replace=True)\n        >>> # Selection restored\n\n    \"\"\"", "output": "\n    selection = cmds.ls(sl=True)\n    try:\n        yield\n    finally:\n        cmds.select(selection, replace=True)\n\n\ndef select_all_objects():\n    \"\"\"Select all objects in the scene\"\"\"\n    cmds.select(cmds.ls(type='object', sl=True))\n\n\ndef select_all_meshes():\n    \"\"\"Select all meshes in the scene\"\"\"\n    cmds.select(cmds.ls(type='mesh', sl=True))\n\n\ndef select_all_nurbs_curves():\n    \"\"\"Select all nurbs curves in the scene\"\"\"\n    cmds.select", "solution": "\n\n    previous_selection = cmds.ls(selection=True)\n    try:\n        yield\n    finally:\n        if previous_selection:\n            cmds.select(previous_selection,\n                        replace=True,\n                        noExpand=True)\n        else:\n            cmds.select(deselect=True,\n                        noExpand=True)"}
{"prompt": "def get_marginal_distribution(self, index_points=None):\n    \"\"\"Compute the marginal of this GP over function values at `index_points`.\n\n    Args:\n      index_points: `float` `Tensor` representing finite (batch of) vector(s) of\n        points in the index set over which the GP is defined. Shape has the form\n        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e` is the number\n        (size) of index points in each batch. Ultimately this distribution\n        corresponds to a `e`-dimensional multivariate normal. The batch shape\n        must be broadcastable with `kernel.batch_shape` and any batch dims\n        yielded by `mean_fn`.\n\n    Returns:\n      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,\n        according to whether `index_points` consists of one or many index\n        points, respectively.\n    \"\"\"", "output": " index points. If `None`, the GP is assumed to be defined\n        on the entire index set.\n\n    Returns:\n      `float` `Tensor` representing the marginal distribution over function\n      values at `index_points`. Shape has the form `[b1,..., bB, e]` where `e`\n      is the number of index points.\n    \"\"\"\n   _index_points =_index_points_to_tensor(index_points,_index_points=True)\n   _index_points =_index_points.expand(-1,_index_points.", "solution": "\n    with self._name_scope('get_marginal_distribution'):\n      # TODO(cgs): consider caching the result here, keyed on `index_points`.\n      index_points = self._get_index_points(index_points)\n      covariance = self._compute_covariance(index_points)\n      loc = self._mean_fn(index_points)\n      # If we're sure the number of index points is 1, we can just construct a\n      # scalar Normal. This has computational benefits and supports things like\n      # CDF that aren't otherwise straightforward to provide.\n      if self._is_univariate_marginal(index_points):\n        scale = tf.sqrt(covariance)\n        # `loc` has a trailing 1 in the shape; squeeze it.\n        loc = tf.squeeze(loc, axis=-1)\n        return normal.Normal(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')\n      else:\n        scale = tf.linalg.LinearOperatorLowerTriangular(\n            tf.linalg.cholesky(_add_diagonal_shift(covariance, self.jitter)),\n            is_non_singular=True,\n            name='GaussianProcessScaleLinearOperator')\n        return mvn_linear_operator.MultivariateNormalLinearOperator(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')"}
{"prompt": "def _reset(self, index, total, percentage_step, length):\n        \"\"\"Resets to the progressbar to start a new one\"\"\"", "output": "\n        self.index = index\n        self.total = total\n        self.percentage_step = percentage_step\n        self.length = length\n        self.percentage = 0\n        self.start_time = time.time()\n        self.last_update_time = self.start_time\n        self.last_update_percentage = 0\n        self.last_update_length = 0\n        self.last_update_message = \"\"\n        self.last_update_message_length = 0\n        self.last_update_message_color = \"\"\n        self.last_update_message_color_", "solution": "\n        self._start_time = datetime.datetime.now()\n        self._start_index = index\n        self._current_index = index\n        self._percentage_step = percentage_step\n        self._total = float(total)\n        self._total_minus_one = total - 1\n        self._length = length\n        self._norm_factor = total * percentage_step / 100.0\n        self._current_interval = int((index + 1.0) / self._norm_factor)"}
{"prompt": "def _swap_ops_from_edge(edge, layout):\n    \"\"\"Generate list of ops to implement a SWAP gate along a coupling edge.\"\"\"", "output": "\n   _qubit = edge.control\n    target_qubit = edge.target\n   _qubit_index = layout.index(edge.control)\n    target_qubit_index = layout.index(edge.target)\n\n   _qubit_op =_qubits[layout.index(edge.control)]\n    target_qubit_op = target_qubits[layout.index(edge.target)]\n\n   _qubit_op_dag =_qubits[layout.index(edge.target)]\n    target_qubit_op_dag = target_q", "solution": "\n\n    device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q')\n    qreg_edge = [(device_qreg, i) for i in edge]\n\n    # TODO shouldn't be making other nodes not by the DAG!!\n    return [\n        DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'})\n    ]"}
{"prompt": "def frames(self, key=None, timeoutSecs=60, **kwargs):\n    if not (key is None or isinstance(key, (basestring, Key))):\n        raise Exception(\"frames: key should be string or Key type %s %s\" % (type(key), key))\n\n    params_dict = {\n        'find_compatible_models': 0,\n        'row_offset': 0, # is offset working yet?\n        'row_count': 5,\n    }\n    \"\"\"\n    Return a single Frame or all of the Frames in the h2o cluster.  The\n    frames are contained in a list called \"frames\" at the top level of the\n    result.  Currently the list is unordered.\n    TODO:\n    When find_compatible_models is implemented then the top level \n    dict will also contain a \"models\" list.\n    \"\"\"", "output": " Frames are returned as a list.\n\n    :param key: If specified, return only the Frame with the specified key.\n    :param timeoutSecs: Maximum time to wait for the Frame to be available.\n    :param kwargs: Additional parameters to pass to the h2o.get_frame() function.\n    :return: A single Frame or a list of Frames.\n    \"\"\"\n    if key is not None:\n        params_dict['key'] = key\n\n    params_dict.update(kwargs)\n\n    # TODO: Add timeoutSecs parameter\n    frames = h2o.get_frame(**params_dict)\n   ", "solution": "\n    h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'frames', False)\n    \n    # key can be type Key? (from h2o_xl) str(key) should return\n    if key:\n        if isinstance(key, Key):\n            keyStr = key.frame\n        else:\n            keyStr = key\n        result = self.do_json_request('3/Frames.json/' + keyStr, timeout=timeoutSecs, params=params_dict)\n    else:\n        result = self.do_json_request('3/Frames.json', timeout=timeoutSecs, params=params_dict)\n    return result"}
{"prompt": "def queryGitHubFromFile(self, filePath, gitvars={}, verbosity=0, **kwargs):\n        \"\"\"Submit a GitHub GraphQL query from a file.\n\n        Can only be used with GraphQL queries.\n        For REST queries, see the 'queryGitHub' method.\n\n        Args:\n            filePath (str): A relative or absolute path to a file containing\n                a GraphQL query.\n                File may use comments and multi-line formatting.\n                .. _GitHub GraphQL Explorer:\n                   https://developer.github.com/v4/explorer/\n            gitvars (Optional[Dict]): All query variables.\n                Defaults to empty.\n                GraphQL Only.\n            verbosity (Optional[int]): Changes output verbosity levels.\n                If < 0, all extra printouts are suppressed.\n                If == 0, normal print statements are displayed.\n                If > 0, additional status print statements are displayed.\n                Defaults to 0.\n            **kwargs: Keyword arguments for the 'queryGitHub' method.\n\n        Returns:\n            Dict: A JSON style dictionary.\n\n        \"\"\"", "output": " verbosity (int): The level of output to display.\n                Defaults to 0.\n            **kwargs: Additional keyword arguments for the 'queryGitHub' method.\n\n        Returns:\n            Dict: The parsed JSON response from the GitHub GraphQL API.\n\n        Raises:\n            ValueError: If the file path is not valid.\n            ValueError: If the file does not contain a valid GraphQL query.\n        \"\"\"\n        if not os.path.isfile(filePath):\n            raise ValueError(f\"File '{filePath}' does not exist.\")\n\n        with open(filePath, 'r') as file:\n            query = file.read()\n\n        if not query.strip", "solution": "\n        gitquery = self._readGQL(filePath, verbose=(verbosity >= 0))\n        return self.queryGitHub(gitquery, gitvars=gitvars, verbosity=verbosity, **kwargs)"}
{"prompt": "def get_revision(self, location):\n        \"\"\"\n        Return the maximum revision for all files under a given location\n        \"\"\"", "output": "\n        revisions = {}\n        for root, dirs, files in os.walk(location):\n            for file in files:\n                if file.endswith('.py'):\n                    revisions[file] = max(revisions.get(file, 0), int(file.split('.')[0]))\n        return revisions\n\n    def update_files(self, location, revisions):\n        \"\"\"\n        Update all files under a given location with the given revisions\n        \"\"\"\n        for root, dirs, files in os.walk(location):\n            for file in files:\n                if file.endswith('.py'):\n                    file_path = os.path.join(", "solution": "\n        # Note: taken from setuptools.command.egg_info\n        revision = 0\n\n        for base, dirs, files in os.walk(location):\n            if self.dirname not in dirs:\n                dirs[:] = []\n                continue    # no sense walking uncontrolled subdirs\n            dirs.remove(self.dirname)\n            entries_fn = os.path.join(base, self.dirname, 'entries')\n            if not os.path.exists(entries_fn):\n                # FIXME: should we warn?\n                continue\n\n            dirurl, localrev = self._get_svn_url_rev(base)\n\n            if base == location:\n                base_url = dirurl + '/'   # save the root url\n            elif not dirurl or not dirurl.startswith(base_url):\n                dirs[:] = []\n                continue    # not part of the same svn tree, skip it\n            revision = max(revision, localrev)\n        return revision"}
{"prompt": "def scientific_notation_elements(self, value, locale):\n        \"\"\" Returns normalized scientific notation components of a value.\"\"\"", "output": "\n        components = []\n        if value.is_integer():\n            components.append(str(value))\n        else:\n            components.append(locale.format(\"%.1e\", value))\n        return components\n\n    def scientific_notation_format(self, value, locale):\n        \"\"\" Returns a formatted scientific notation string of a value.\"\"\"\n        components = self.scientific_notation_elements(value, locale)\n        return \" \".join(components)\n\n    def scientific_notation__format(self, value, locale):\n        \"\"\" Returns a formatted scientific notation string of a value.\"\"\"\n        components = self.scientific_notation_", "solution": "\n        # Normalize value to only have one lead digit.\n        exp = value.adjusted()\n        value = value * get_decimal_quantum(exp)\n        assert value.adjusted() == 0\n\n        # Shift exponent and value by the minimum number of leading digits\n        # imposed by the rendering pattern. And always make that number\n        # greater or equal to 1.\n        lead_shift = max([1, min(self.int_prec)]) - 1\n        exp = exp - lead_shift\n        value = value * get_decimal_quantum(-lead_shift)\n\n        # Get exponent sign symbol.\n        exp_sign = ''\n        if exp < 0:\n            exp_sign = babel.numbers.get_minus_sign_symbol(locale)\n        elif self.exp_plus:\n            exp_sign = babel.numbers.get_plus_sign_symbol(locale)\n\n        # Normalize exponent value now that we have the sign.\n        exp = abs(exp)\n\n        return value, exp, exp_sign"}
{"prompt": "def difflag1(self):\n        \"\"\"\n        Conduct a diff-1 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the previous-row element in the same frame.\n        \"\"\"", "output": "\n       f = self._get_frame()\n       f =f.drop(columns=self._get_column_name())\n       f =f.reset_index(drop=True)\n       f =f.assign(diff1=lambda x: x[self._get_column_name()] - x.shift(1)[self._get_column_name()])\n       f =f.drop(columns=self._get_column_name())\n        return H2OFrame(data=list(zip(self._get_column_name(), list(map(", "solution": "\n        if self.ncols > 1:\n            raise H2OValueError(\"Only single-column frames supported\")\n        if self.types[self.columns[0]] not in {\"real\", \"int\", \"bool\"}:\n            raise H2OValueError(\"Numeric column expected\")\n        fr = H2OFrame._expr(expr=ExprNode(\"difflag1\", self), cache=self._ex._cache)\n        return fr"}
{"prompt": "async def _run_spider_hook(self, hook_func):\n        \"\"\"\n        Run hook before/after spider start crawling\n        :param hook_func: aws function\n        :return:\n        \"\"\"", "output": "\n        if not self.spider_config.get('hooks'):\n            return\n        for hook in self.spider_config['hooks']:\n            if hook['type'] == 'before_spider':\n                await hook_func(hook['function_name'], self.spider_config)\n            elif hook['type'] == 'after_spider':\n                await hook_func(hook['function_name'], self.spider_config)\n            else:\n                raise ValueError(f\"Unsupported hook type: {hook['type']}\")\n\n    async def start_spider(self):\n        \"\"\"\n        Start crawling process\n        :return:\n       ", "solution": "\n        if callable(hook_func):\n            try:\n                aws_hook_func = hook_func(weakref.proxy(self))\n                if isawaitable(aws_hook_func):\n                    await aws_hook_func\n            except Exception as e:\n                self.logger.error(f'<Hook {hook_func.__name__}: {e}')"}
{"prompt": "def save(self, name=None, output='png', dirc=None):\n        \"\"\"Saves Bloch sphere to file of type ``format`` in directory ``dirc``.\n        Args:\n            name (str):\n                Name of saved image. Must include path and format as well.\n                i.e. '/Users/Paul/Desktop/bloch.png'\n                This overrides the 'format' and 'dirc' arguments.\n            output (str):\n                Format of output image.\n            dirc (str):\n                Directory for output images. Defaults to current working directory.\n        \"\"\"", "output": "\n       _dir = os.getcwd()\n        if dirc is not None:\n            os.chdir(dirc)\n       _image =_dir + '/' + name\n       _image =_image.replace(' ', '_')\n       _image =_image.replace('/', '_')\n       _image =_image.replace('\\\\', '_')\n       _image =_image.replace(':', '_')\n       _image =_image.replace('|', '_')\n       _image =_image.replace('?', '_')\n       _image =_image", "solution": "\n\n        self.render()\n        if dirc:\n            if not os.path.isdir(os.getcwd() + \"/\" + str(dirc)):\n                os.makedirs(os.getcwd() + \"/\" + str(dirc))\n        if name is None:\n            if dirc:\n                self.fig.savefig(os.getcwd() + \"/\" + str(dirc) + '/bloch_' +\n                                 str(self.savenum) + '.' + output)\n            else:\n                self.fig.savefig(os.getcwd() + '/bloch_' + str(self.savenum) +\n                                 '.' + output)\n        else:\n            self.fig.savefig(name)\n        self.savenum += 1\n        if self.fig:\n            plt.close(self.fig)"}
{"prompt": "def rejester_run(work_unit):\n     \"\"\"get a rejester.WorkUnit with KBA s3 path, fetch it, and save\n     some counts about it.\n     \"\"\"", "output": "\n     # get the KBA s3 path\n     kba_s3_path = work_unit.kba_s3_path\n\n     # fetch the KBA s3 path\n     kba_s3_path_contents = fetch_s3_path(kba_s3_path)\n\n     # parse the KBA s3 path contents\n    _counts = parse_kba_s3_path_contents(kba_s3_path_contents)\n\n     # save the counts\n     work_unit.save_counts(vegetable_counts=vegetable_counts,", "solution": "\n     #fname = 'verify-chunks-%d-%d' % (os.getpid(), time.time())\n     fname = work_unit.key.strip().split('/')[-1]\n     \n     output_dir_path = work_unit.data.get('output_dir_path', '/mnt')\n     u = uuid.uuid3(uuid.UUID(int=0), work_unit.key.strip())\n     path1 = u.hex[0]\n     path2 = u.hex[1]\n     fpath = os.path.join(output_dir_path, path1, path2, fname)\n     if not os.path.exists(os.path.dirname(fpath)):\n          os.makedirs(os.path.dirname(fpath))\n\n     output = gzip.open(fpath + '-out.gz', 'wb')\n\n     expected_si_count = int(fname.split('-')[1])\n\n     max_tries = 20\n     tries = 0\n     while tries < max_tries:\n          try:\n               exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, stream_ids = \\\n                   attempt_fetch(work_unit, fpath)\n               if si_count != expected_si_count:\n                    print 'retrying because si_count = %d != %d expected_si_count' % (si_count, expected_si_count)\n                    sys.stdout.flush()\n                    tries += 1\n                    continue\n               else:\n                    print 'succeeded in reading si_count = %d' % (si_count,)\n                    sys.stdout.flush()\n               output.write( '%s\\t%d\\t%d\\t%d\\t%d\\t%s\\t%s\\n' % (\n                         exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, \n                         work_unit.key.strip(), ','.join(['%s|%s' % tup for tup in stream_ids])) )\n               break\n          except Exception, exc:\n               print 'broken?'\n               print traceback.format_exc(exc)\n               sys.stdout.flush()\n               tries += 1\n               output.write(traceback.format_exc(exc))\n\n     output.close()"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        parser = argparse.ArgumentParser(description=\"This is a CLI for a tool\")\n        parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n        parser.add_argument(\"-f\", \"--file\", type=str, help=\"Specify a file to process\")\n        return parser.parse_args()\n\n    def process_file(self, file_path):\n        \"\"\"\n        Processes a file using the tool\n        \"\"\"\n        pass\n\n    def run(self):\n        \"\"\"\n        Runs the CLI\n        \"\"\"\n        args = self.get_arguments()\n        if args.", "solution": "\n\n        AlarmModify.get_arguments(self)\n        self._alarm_id = self.args.alarm_id if self.args.alarm_id is not None else None\n        self.get_api_parameters()"}
{"prompt": "def html_tags_for(self, asset_type, *args, **kwargs):\n        \"\"\"Return html tags for urls of asset_type\n        \"\"\"", "output": "\n        raise NotImplementedError()\n\n    def get_absolute_url(self):\n        return self.url\n\n    def get_absolute_path(self):\n        return self.path\n\n    def get_file_size(self):\n        return self.size\n\n    def get_file_type(self):\n        return self.file_type\n\n    def get_file_extension(self):\n        return self.extension\n\n    def get_file_name(self):\n        return self.name\n\n    def get_file_name_without_extension(self):\n        return self.name.split('.')[0]\n\n    def get_file_name", "solution": "\n        html = []\n        for ref in self.depends:\n            html.append(self._ref(ref).html_tags_for(asset_type, *args, **kwargs))\n        if asset_type in self.typed_bundles:\n            html.append(render_asset_html_tags(asset_type, self.urls_for_self(asset_type, *args, **kwargs)))\n        return \"\\n\".join(html)"}
{"prompt": "def read_key(self, key, bucket_name=None):\n        \"\"\"\n        Reads a key from S3\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"", "output": "\n        if bucket_name is None:\n            bucket_name = self.bucket_name\n\n        s3 = boto3.resource('s3')\n        bucket = s3.Bucket(bucket_name)\n\n        try:\n            obj = bucket.Object(key)\n            return obj.get()['Body'].read().decode('utf-8')\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                return None\n            else:\n                raise e\n\n    def write_key(self, key, content, bucket_name=None):\n        \"\"\"\n       ", "solution": "\n\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')"}
{"prompt": "def _map_type_to_dict(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding dictionary in root.\n\n        \"\"\"", "output": "\n        if type_name == 'RESULT':\n            return self.root['result']\n        elif type_name == 'ERROR':\n            return self.root['error']\n        elif type_name == 'WARNING':\n            return self.root['warning']\n        else:\n            raise ValueError('Invalid type name: %s' % type_name)\n\n    def _map_dict_to_type(self, type_dict, type_name):\n        \"\"\" Maps a dictionary in the root to an instance type representation\n        string (e.g. 'RESULT').\n\n        \"\"\"\n        if type_name == 'RESULT':\n            return '", "solution": "\n        root = self._root_instance\n\n        if type_name == RESULT:\n            return root._results\n        elif type_name == PARAMETER:\n            return root._parameters\n        elif type_name == DERIVED_PARAMETER:\n            return root._derived_parameters\n        elif type_name == CONFIG:\n            return root._config\n        elif type_name == LEAF:\n            return root._other_leaves\n        else:\n            raise RuntimeError('You shall not pass!')"}
{"prompt": "def create(self, public=False, **kwargs):\n        \"\"\"Creates the device. Attempts to create private devices by default,\n        but if public is set to true, creates public devices.\n\n        You can also set other default properties by passing in the relevant information.\n        For example, setting a device with the given nickname and description::\n\n            dev.create(nickname=\"mydevice\", description=\"This is an example\")\n\n        Furthermore, ConnectorDB supports creation of a device's streams immediately,\n        which can considerably speed up device setup::\n\n            dev.create(streams={\n                \"stream1\": {\"schema\": '{\\\"type\\\":\\\"number\\\"}'}\n            })\n\n        Note that the schema must be encoded as a string when creating in this format.\n        \"\"\"", "output": "\\\":\\\"int\\\"}'},\n                \"stream2\": {\"schema\": '{\\\"type\\\":\\\"float\\\"}'}\n            })\n\n        This will create two streams, \"stream1\" and \"stream2\", with the given schema.\n        \"\"\"\n        if public:\n            = \"public\"\n        else:\n            if not self.private:\n                raise ValueError(\"Private devices cannot be created without a private key.\")\n            else:\n                = \"private\"\n\n        if == \"private\":\n           _key = self.private_key\n        else:\n           _key = self.public_key\n\n        if_key is None:\n           ", "solution": "\n        kwargs[\"public\"] = public\n        self.metadata = self.db.create(self.path, kwargs).json()"}
{"prompt": "def _check_init(self, node):\n        \"\"\"check that the __init__ method call super or ancestors'__init__\n        method\n        \"\"\"", "output": "\n        if not_ast.hasattr(node, 'body'):\n            return\n        for stmt in node.body:\n            if isinstance(stmt, ast.Call) and isinstance(stmt.func, ast.Name) and stmt.func.id == '__init__':\n                if not_ast.hasattr(stmt, 'args'):\n                    return\n                if not_ast.hasattr(stmt.args, 'args'):\n                    return\n                if not_ast.hasattr(stmt.args.args, '__len__'):\n                    return\n                if stmt.args.args.__len__() == 0:", "solution": "\n        if not self.linter.is_message_enabled(\n            \"super-init-not-called\"\n        ) and not self.linter.is_message_enabled(\"non-parent-init-called\"):\n            return\n        klass_node = node.parent.frame()\n        to_call = _ancestors_to_call(klass_node)\n        not_called_yet = dict(to_call)\n        for stmt in node.nodes_of_class(astroid.Call):\n            expr = stmt.func\n            if not isinstance(expr, astroid.Attribute) or expr.attrname != \"__init__\":\n                continue\n            # skip the test if using super\n            if (\n                isinstance(expr.expr, astroid.Call)\n                and isinstance(expr.expr.func, astroid.Name)\n                and expr.expr.func.name == \"super\"\n            ):\n                return\n            try:\n                for klass in expr.expr.infer():\n                    if klass is astroid.Uninferable:\n                        continue\n                    # The infered klass can be super(), which was\n                    # assigned to a variable and the `__init__`\n                    # was called later.\n                    #\n                    # base = super()\n                    # base.__init__(...)\n\n                    if (\n                        isinstance(klass, astroid.Instance)\n                        and isinstance(klass._proxied, astroid.ClassDef)\n                        and is_builtin_object(klass._proxied)\n                        and klass._proxied.name == \"super\"\n                    ):\n                        return\n                    if isinstance(klass, objects.Super):\n                        return\n                    try:\n                        del not_called_yet[klass]\n                    except KeyError:\n                        if klass not in to_call:\n                            self.add_message(\n                                \"non-parent-init-called\", node=expr, args=klass.name\n                            )\n            except astroid.InferenceError:\n                continue\n        for klass, method in not_called_yet.items():\n            cls = node_frame_class(method)\n            if klass.name == \"object\" or (cls and cls.name == \"object\"):\n                continue\n            self.add_message(\"super-init-not-called\", args=klass.name, node=node)"}
{"prompt": "def analyse(self, traj, network, current_subrun, subrun_list, network_dict):\n        \"\"\"Calculates average Fano Factor of a network.\n\n        :param traj:\n\n            Trajectory container\n\n            Expects:\n\n            `results.monitors.spikes_e`: Data from SpikeMonitor for excitatory neurons\n\n            Adds:\n\n            `results.statistics.mean_fano_factor`: Average Fano Factor\n\n        :param network:\n\n            The BRIAN network\n\n        :param current_subrun:\n\n            BrianParameter\n\n        :param subrun_list:\n\n            Upcoming subruns, analysis is only performed if subruns is empty,\n            aka the final subrun has finished.\n\n        :param network_dict:\n\n            Dictionary of items shared among componetns\n\n        \"\"\"", "output": "runs\n\n        :param network_dict:\n\n            Dictionary of_network objects\n\n        :return:\n\n        \"\"\"\n       _network = network_dict[current_subrun.network]\n       _network.set_spikes(traj.results.monitors.spikes_e)\n       _network.run_network(current_subrun.start_time, current_subrun.end_time,_network.dt)\n       _network.calculate_fano_factor()\n       _network.calculate_mean_fano_factor()\n       _network.calculate_mean_f", "solution": "\n        #Check if we finished all subruns\n        if len(subrun_list)==0:\n            spikes_e = traj.results.monitors.spikes_e\n\n            time_window = traj.parameters.analysis.statistics.time_window\n            start_time = traj.parameters.simulation.durations.initial_run\n            end_time = start_time+traj.parameters.simulation.durations.measurement_run\n            neuron_ids = traj.parameters.analysis.statistics.neuron_ids\n\n            mean_ff = self._compute_mean_fano_factor(\n                neuron_ids, spikes_e, time_window, start_time, end_time)\n\n            traj.f_add_result('statistics.mean_fano_factor', mean_ff, comment='Average Fano '\n                                                                      'Factor over all '\n                                                                      'exc neurons')\n\n            print('R_ee: %f, Mean FF: %f' % (traj.R_ee, mean_ff))"}
{"prompt": "def _get_properties(config):\n    \"\"\"Returns a tuple of property classes and names.\n\n    Property classes are fully qualified, such as 'abc.abstractproperty' and\n    property names are the actual names, such as 'abstract_property'.\n    \"\"\"", "output": "\n    property_classes = []\n    property_names = []\n    for prop_name, prop_config in config.items():\n        if 'class' not in prop_config:\n            raise ValueError(f\"Property '{prop_name}' is missing 'class' key.\")\n        property_classes.append(prop_config['class'])\n        property_names.append(prop_name)\n    return property_classes, property_names\n\n\ndef _get_property_instances(property_classes, property_names, instance_config):\n    \"\"\"Returns a list of property instances.\"\"\"\n    property_instances = []\n    for", "solution": "\n    property_classes = {BUILTIN_PROPERTY}\n    property_names = set()  # Not returning 'property', it has its own check.\n    if config is not None:\n        property_classes.update(config.property_classes)\n        property_names.update(\n            (prop.rsplit(\".\", 1)[-1] for prop in config.property_classes)\n        )\n    return property_classes, property_names"}
{"prompt": "def _call(self, utterances_batch: list, utterances_ids: Optional[list]=None) -> list:\n        \"\"\"\n        Processes batch of utterances and returns corresponding responses batch.\n\n        Each call of Agent passes incoming utterances batch through skills filter,\n        agent skills, skills processor. Batch of dialog IDs can be provided, in\n        other case utterances indexes in incoming batch are used as dialog IDs.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        \"\"\"", "output": " corresponding to incoming utterances.\n        \"\"\"\n        if utterances_ids is None:\n            utterances_ids = [i for i in range(len(utterances_batch))]\n\n       _utterances = [utterances_batch[i] for i in utterances_ids]\n       _ids = utterances_ids\n\n       _responses = []\n        for i,_utterance in enumerate(self.filter_utterances(self.utterances,_utterances)):\n           _id = utterances_ids[i]\n           _responses.append(self.process_utterance(", "solution": "\n        batch_size = len(utterances_batch)\n        ids = utterances_ids or list(range(batch_size))\n        batch_history = [self.history[utt_id] for utt_id in ids]\n        responses = []\n\n        filtered = self.skills_filter(utterances_batch, batch_history)\n\n        for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)):\n            skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter]\n\n            if skill_i_utt_indexes:\n                skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes]\n                skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes]\n                res = [(None, 0.)] * batch_size\n                predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids)\n\n                for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence):\n                    res[i] = (predicted, confidence)\n\n                responses.append(res)\n\n        responses = self.skills_processor(utterances_batch, batch_history, *responses)\n\n        return responses"}
{"prompt": "def _add_label_from_dict(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from a dictionary.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        label_name = query_params.get('label_name', None)\n        if label_name is not None:\n            label = self.client.labels.get_by_name(label_name)\n            self.add_label(label)\n\n    def _add_label_from_id(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from an ID.\n        \"\"\"\n        if query_params is None:\n            query_params = {}\n        label_id = query_params.get('", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri + '/labels',\n            http_method='POST',\n            query_params=query_params or {}\n        )"}
{"prompt": "def diff_lorenz(value_array, sigma, beta, rho):\n    \"\"\"The Lorenz attractor differential equation\n\n    :param value_array: 3d array containing the x,y, and z component values.\n    :param sigma: Constant attractor parameter\n    :param beta: FConstant attractor parameter\n    :param rho: Constant attractor parameter\n\n    :return: 3d array of the Lorenz system evaluated at `value_array`\n\n    \"\"\"", "output": "\n    x, y, z = value_array\n    dx_dt = sigma * (y - x)\n    dy_dt = x * (rho - z) - y\n    dz_dt = x * y - beta * z\n    return np.array([dx_dt, dy_dt, dz_dt])\n\n\ndefk_lorenz(value_array,_array,_sigma,_beta,_rho):\n    \"\"\"The-Kolmogorov attractor differential equation\n\n    :param value_array: 3d array containing the x,y,", "solution": "\n    diff_array = np.zeros(3)\n    diff_array[0] = sigma * (value_array[1]-value_array[0])\n    diff_array[1] = value_array[0] * (rho - value_array[2]) - value_array[1]\n    diff_array[2] = value_array[0] * value_array[1] - beta * value_array[2]\n\n    return diff_array"}
{"prompt": "def _onDeviceCommand(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for gateway command messages, parses source device from topic string and\n        passes the information on to the registered device command callback\n        \"\"\"", "output": "\n        topic = pahoMessage.topic\n        sourceDevice = topic.split(\"/\")[1]\n        if sourceDevice in self.deviceCommandCallbacks:\n            self.deviceCommandCallbacks[sourceDevice](pahoMessage.payload.decode(\"utf-8\"))\n        else:\n            self.logger.warning(\"Received command for unknown device: %s\", sourceDevice)\n\n    def registerDeviceCommandCallback(self, deviceName, callback):\n        \"\"\"\n        Registers a callback function for a specific device command.\n        :param deviceName: The name of the device to register the callback for\n        :param callback: The callback function to register\n       ", "solution": "\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received gateway command '%s'\" % (command.command))\n            if self.deviceCommandCallback:\n                self.deviceCommandCallback(command)"}
{"prompt": "def wait_next_block_factory(app, timeout=None):\n    \"\"\"Creates a `wait_next_block` function, that\n    will wait `timeout` seconds (`None` = indefinitely)\n    for a new block to appear.\n\n    :param app: the app-instance the function should work for\n    :param timeout: timeout in seconds\n    \"\"\"", "output": "\n    def wait_next_block():\n        \"\"\"Waits for a new block to appear.\"\"\"\n        if timeout is not None:\n            app.wait_for_block(timeout=timeout)\n        else:\n            app.wait_for_block()\n\n    return wait_next_block\n\n\ndef wait_for_block_factory(app, timeout=None):\n    \"\"\"Creates a `wait_for_block` function, that\n    will wait `timeout` seconds (`None` = indefinitely)\n    for a block to appear.\n\n    :param app: the app-instance the function should work for\n    :param", "solution": "\n\n    chain = app.services.chain\n\n    # setup new block callbacks and events\n    new_block_evt = gevent.event.Event()\n\n    def _on_new_block(app):\n        log.DEV('new block mined')\n        new_block_evt.set()\n    chain.on_new_head_cbs.append(_on_new_block)\n\n    def wait_next_block():\n        bn = chain.chain.head.number\n        chain.consensus_manager.log('waiting for new block', block=bn)\n        new_block_evt.wait(timeout)\n        new_block_evt.clear()\n        if chain.chain.head.number > bn:\n            chain.consensus_manager.log('new block event', block=chain.chain.head.number)\n        elif chain.chain.head.number == bn:\n            chain.consensus_manager.log('wait_next_block timed out', block=bn)\n\n    return wait_next_block"}
{"prompt": "def _set_affiliation(self, v, load=False):\n        \"\"\"\n    Setter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_affiliation is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_affiliation() directly.\n    \"\"\"", "output": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=RestrictedClassType(base_type=six.text_type,                                     restriction_type=\"dict_key\",                                     restriction_arg={u'university': {'value': 1}, u'company': {'value': 2}, u'other': {'value': 3}},), is_leaf=True, yang_name=\"affiliation\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods,", "solution": "\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=RestrictedClassType(\n                    base_type=unicode,\n                    restriction_type=\"dict_key\",\n                    restriction_arg={\n                        u\"napalm-star-wars:EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"napalm-star-wars:REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                    },\n                ),\n                is_leaf=True,\n                yang_name=\"affiliation\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                namespace=\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                defining_module=\"napalm-star-wars\",\n                yang_type=\"identityref\",\n                is_config=True,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": "}
{"prompt": "def run(self, cmd, start_opts=None, globals_=None, locals_=None):\n        \"\"\" Run debugger on string `cmd' using builtin function eval\n        and if that builtin exec.  Arguments `globals_' and `locals_'\n        are the dictionaries to use for local and global variables. By\n        default, the value of globals is globals(), the current global\n        variables. If `locals_' is not given, it becomes a copy of\n        `globals_'.\n\n        Debugger.core.start settings are passed via optional\n        dictionary `start_opts'. Overall debugger settings are in\n        Debugger.settings which changed after an instance is created\n        . Also see `run_eval' if what you want to run is an\n        run_eval'able expression have that result returned and\n        `run_call' if you want to debug function run_call.\n        \"\"\"", "output": " initial call to\n        Debugger.start(). \"\"\"\n\n        if start_opts is None:\n            start_opts = {}\n\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_.copy()\n\n        # Save current settings\n        old_settings = self.settings.copy()\n\n        # Set new settings\n        self.settings.update(start_opts)\n\n        # Set globals and locals\n        self.globals = globals_\n        self.locals = locals_\n\n        # Run command\n        try:\n            exec(cmd, self.globals, self.locals)\n        except Exception as e", "solution": "\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_\n        if not isinstance(cmd, types.CodeType):\n            self.eval_string = cmd\n            cmd = cmd+'\\n'\n            pass\n        retval = None\n        self.core.start(start_opts)\n        try:\n            retval = eval(cmd, globals_, locals_)\n        except SyntaxError:\n            try:\n                exec(cmd, globals_, locals_)\n            except DebuggerQuit:\n                pass\n            except DebuggerQuit:\n                pass\n            pass\n        except DebuggerQuit:\n            pass\n        finally:\n            self.core.stop()\n        return retval"}
{"prompt": "def make_links_absolute(self, base_url=None, resolve_base_href=True,\n                            handle_failures=None):\n        \"\"\"\n        Make all links in the document absolute, given the\n        ``base_url`` for the document (the full URL where the document\n        came from), or if no ``base_url`` is given, then the ``.base_url``\n        of the document.\n\n        If ``resolve_base_href`` is true, then any ``<base href>``\n        tags in the document are used *and* removed from the document.\n        If it is false then any such tag is ignored.\n\n        If ``handle_failures`` is None (default), a failure to process\n        a URL will abort the processing.  If set to 'ignore', errors\n        are ignored.  If set to 'discard', failing URLs will be removed.\n        \"\"\"", "output": " the ``<base href>`` tags are left in the\n        document and the ``base_url`` is used instead.\n\n        If ``handle_failures`` is true, then any links that cannot be\n        made absolute will be left in the document with a warning. If\n        it is false, then any links that cannot be made absolute will\n        raise a ``ValueError``.\n\n        Returns the modified document.\n        \"\"\"\n        if not base_url:\n            base_url = self.base_url\n\n        if resolve_base_href:\n            self._resolve_base_href(base_url)\n\n        links = self.find_", "solution": "\n        if base_url is None:\n            base_url = self.base_url\n            if base_url is None:\n                raise TypeError(\n                    \"No base_url given, and the document has no base_url\")\n        if resolve_base_href:\n            self.resolve_base_href()\n\n        if handle_failures == 'ignore':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return href\n        elif handle_failures == 'discard':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return None\n        elif handle_failures is None:\n            def link_repl(href):\n                return urljoin(base_url, href)\n        else:\n            raise ValueError(\n                \"unexpected value for handle_failures: %r\" % handle_failures)\n\n        self.rewrite_links(link_repl)"}
{"prompt": "def signin_card(card: SigninCard) -> Attachment:\n        \"\"\"\n        Returns an attachment for a signin card. For channels that don't natively support signin cards an alternative\n        message will be rendered. Will raise a TypeError if 'card' argument is not a SigninCard.\n        :param card:\n        :return:\n        \"\"\"", "output": "\n        if not isinstance(card, SigninCard):\n            raise TypeError(\"Argument 'card' must be a SigninCard\")\n\n        if card.title:\n            title = card.title\n        else:\n            title = \"Sign in to your account\"\n\n        if card.subtitle:\n            subtitle = card.subtitle\n        else:\n            subtitle = \"Please enter your email and password to continue.\"\n\n        if card.buttons:\n            buttons = card.buttons\n        else:\n            buttons = [\n                {\n                    \"type\": \"postback\",\n                    \"title\": \"Sign in\",\n                    \"payload\": \"SIGNIN\"\n                }\n            ]", "solution": "\n        if not isinstance(card, SigninCard):\n            raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of an SigninCard, '\n                            'unable to prepare attachment.')\n\n        return Attachment(content_type=CardFactory.content_types.signin_card,\n                          content=card)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Clear GDoc Spreadsheet by sending empty csv file.\n        \"\"\"", "output": "\n        self.spreadsheet.clear()\n        self.spreadsheet.add_worksheet(title='Sheet1')\n        self.spreadsheet.add_worksheet(title='Sheet2')\n        self.spreadsheet.add_worksheet(title='Sheet3')\n        self.spreadsheet.add_worksheet(title='Sheet4')\n        self.spreadsheet.add_worksheet(title='Sheet5')\n        self.spreadsheet.add_worksheet(title='Sheet6')\n        self.spreadsheet.add_worksheet(title='Sheet7')\n        self.spreadsheet.add_worksheet(title='Sheet8')\n        self.spreadsheet.add_worksheet(", "solution": "\n        empty_file_path = os.path.join(self.temp_path, 'empty.csv')\n        try:\n            empty_file = open(empty_file_path, 'w')\n            empty_file.write(',')\n            empty_file.close()\n        except IOError as e:\n            raise PODocsError(e)\n\n        self._upload_file_to_gdoc(empty_file_path, content_type='text/csv')\n\n        os.remove(empty_file_path)"}
{"prompt": "def add_item_to_basket(self, item, variant=VARIANT.MEDIUM, quantity=1):\n        \"\"\"\n        Add an item to the current basket.\n\n        :param Item item: Item from menu.\n        :param int variant: Item SKU id. Ignored if the item is a side.\n        :param int quantity: The quantity of item to be added.\n        :return: A response having added an item to the current basket.\n        :rtype: requests.Response\n        \"\"\"", "output": "\n        data = {\n            \"variant\": variant,\n            \"quantity\": quantity,\n            \"item\": item.id\n        }\n        response = self.session.post(self.url, data=data)\n        return response\n\n    def remove_item_from_basket(self, item, variant=VARIANT.MEDIUM):\n        \"\"\"\n        Remove an item from the current basket.\n\n        :param Item item: Item from menu.\n        :param int variant: Item SKU id. Ignored if the item is a side.\n        :return: A response having removed an item from the current basket.\n        :rtype: requests.Response", "solution": "\n        item_type = item.type\n\n        if item_type == 'Pizza':\n            return self.add_pizza_to_basket(item, variant, quantity)\n        elif item_type == 'Side':\n            return self.add_side_to_basket(item, quantity)\n        return None"}
{"prompt": "def validate_expression(self, expression):\n        \"\"\"Validate an expression (may throw Exceptions)\"\"\"", "output": "\n        try:\n            ast.parse(expression, mode='eval')\n            return True\n        except SyntaxError:\n            return False\n\n    def validate_expression_string(self, expression_string):\n        \"\"\"Validate an expression string (may throw Exceptions)\"\"\"\n        try:\n            ast.parse(expression_string, mode='eval')\n            return True\n        except SyntaxError:\n            return False\n\n    def validate_expression_file(self, expression_file):\n        \"\"\"Validate an expression file (may throw Exceptions)\"\"\"\n        try:\n            with open(expression_file, 'r') as f:\n                expression_string = f.", "solution": "\n        # return self.evaluate(expression, 0, 2)\n        vars = set(self.get_column_names()) | set(self.variables.keys())\n        funcs = set(expression_namespace.keys())\n        return vaex.expresso.validate_expression(expression, vars, funcs)"}
{"prompt": "def plot(self, fig=None):\n        \"\"\"Plot visualization of model prediction.\"\"\"", "output": "\n        if fig is None:\n            fig = plt.figure()\n       _pred = self.model.predict(self.X_test)\n       _true = self.y_test\n       _diff = np.abs(o_pred - o_true)\n       _diff = np.mean(o_diff, axis=1)\n       _diff = np.sort(o_diff)\n       _diff = np.mean(o_diff)\n       _diff_std = np.std(o_diff)\n       _diff_min = np.min(o_", "solution": "\n        if not fig:\n            fig = plt.gcf()\n\n        fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)\n        ax = fig.add_subplot(1, 1, 1)\n\n        sorted_overlapping_ports = sorted(\n            [(p, self.results['port cycles'][p]) for p in self.machine['overlapping ports']],\n            key=lambda x: x[1])\n\n        yticks_labels = []\n        yticks = []\n        xticks_labels = []\n        xticks = []\n\n        # Plot configuration\n        height = 0.9\n\n        i = 0\n        # T_OL\n        colors = ([(254. / 255, 177. / 255., 178. / 255.)] +\n                  [(255. / 255., 255. / 255., 255. / 255.)] * (len(sorted_overlapping_ports) - 1))\n        for p, c in sorted_overlapping_ports:\n            ax.barh(i, c, height, align='center', color=colors.pop(),\n                    edgecolor=(0.5, 0.5, 0.5), linestyle='dashed')\n            if i == len(sorted_overlapping_ports) - 1:\n                ax.text(c / 2.0, i, '$T_\\mathrm{OL}$', ha='center', va='center')\n            yticks_labels.append(p)\n            yticks.append(i)\n            i += 1\n        xticks.append(sorted_overlapping_ports[-1][1])\n        xticks_labels.append('{:.1f}'.format(sorted_overlapping_ports[-1][1]))\n\n        # T_nOL + memory transfers\n        y = 0\n        colors = [(187. / 255., 255 / 255., 188. / 255.)] * (len(self.results['cycles'])) + \\\n                 [(119. / 255, 194. / 255., 255. / 255.)]\n        for k, v in [('nOL', self.results['T_nOL'])] + self.results['cycles']:\n            ax.barh(i, v, height, y, align='center', color=colors.pop())\n            ax.text(y + v / 2.0, i, '$T_\\mathrm{' + k + '}$', ha='center', va='center')\n            xticks.append(y + v)\n            xticks_labels.append('{:.1f}'.format(y + v))\n            y += v\n        yticks_labels.append('LD')\n        yticks.append(i)\n\n        ax.tick_params(axis='y', which='both', left='off', right='off')\n        ax.tick_params(axis='x', which='both', top='off')\n        ax.set_xlabel('t [cy]')\n        ax.set_ylabel('execution port')\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticks_labels)\n        ax.set_xticks(xticks)\n        ax.set_xticklabels(xticks_labels, rotation='vertical')\n        ax.xaxis.grid(alpha=0.7, linestyle='--')\n        fig.savefig(self._args.ecm_plot)"}
{"prompt": "def reduceCnf(cnf):\n    \"\"\"\n    I just found a remarkably large bug in my SAT solver and found an\n    interesting solution.\n    Remove all b | -b\n    (-b | b) & (b | -a) & (-b | a) & (a | -a)\n    becomes\n    (b | -a) & (-b | a)\n\n    Remove all (-e) & (-e)\n    (-e | a) & (-e | a) & (-e | a) & (-e | a)\n    becomes\n    (-e | a)\n    (-b | b | c) becomes nothing, not (c)\n    \"\"\"", "output": " & (-b | b | d) & (-b | c | d) & (-b | a | b) & (-b | a | c) & (-b | a | d)\n    becomes\n    (-b | b | c) & (-b | b | d) & (-b | c | d) & (-b | a | b) & (-b | a | c) & (-b | a | d)\n\n    Remove all (-e) & (e)\n    (-e | a) & (-e | a) & (-e | a) & (-e | a)\n    becomes\n    (-", "solution": "\n    output = Cnf()\n    for x in cnf.dis:\n        dont_add = False\n        for y in x:\n            for z in x:\n                if z == -y:\n                    dont_add = True\n                    break\n            if dont_add: break\n        if dont_add: continue\n        # TODO: Is this necessary anymore? Probably not. Do statistical analysis.\n        if x not in output.dis:\n            output.dis |= frozenset([x])\n    return output"}
{"prompt": "def edit(self,index=None):\n        \"\"\"Edit a block.\n\n        If no number is given, use the last block executed.\n\n        This edits the in-memory copy of the demo, it does NOT modify the\n        original source file.  If you want to do that, simply open the file in\n        an editor and use reload() when you make changes to the file.  This\n        method is meant to let you change a block during a demonstration for\n        explanatory purposes, without damaging your original script.\"\"\"", "output": "\n        if index is None:\n            index = self.last_index\n        if index is None:\n            raise ValueError(\"No block to edit\")\n        block = self.blocks[index]\n        block.edit()\n\n    def execute(self,index=None):\n        \"\"\"Execute a block.\n\n        If no number is given, use the last block executed.\n\n        This executes the in-memory copy of the demo, it does NOT modify the\n        original source file.  If you want to do that, simply open the file in\n        an editor and use reload() when you make changes to the file.  This\n        method is meant", "solution": "\n\n        index = self._get_index(index)\n        if index is None:\n            return\n        # decrease the index by one (unless we're at the very beginning), so\n        # that the default demo.edit() call opens up the sblock we've last run\n        if index>0:\n            index -= 1\n\n        filename = self.shell.mktempfile(self.src_blocks[index])\n        self.shell.hooks.editor(filename,1)\n        new_block = file_read(filename)\n        # update the source and colored block\n        self.src_blocks[index] = new_block\n        self.src_blocks_colored[index] = self.ip_colorize(new_block)\n        self.block_index = index\n        # call to run with the newly edited index\n        self()"}
{"prompt": "def enum(*sequential, **named):\n    \"\"\"\n    Build an enum statement\n    \"\"\"", "output": "\n    enums = []\n    for value, name in zip(sequential, named.values()):\n        enums.append(f\"{name} = {value}\")\n    return f\"enum {', '.join(enums)}:\"\n\n\ndef_input(prompt: str, default: str = \"\") -> str:\n    \"\"\"\n    Prompt the user for input and return the result.\n    \"\"\"\n    if default:\n        prompt += f\" (default: {default}) \"\n    return input(prompt).strip() or default\n\n\ndef main():\n   _name = \"John\"\n   _age = 25\n   _", "solution": "\n    #: build enums from parameter\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    enums['map'] = copy.copy(enums)\n    #: build reverse mapping\n    enums['rmap'] = {}\n    for key, value in enums.items():\n        if type(value) is int:\n            enums['rmap'][value] = key\n    return type('Enum', (), enums)"}
{"prompt": "def check_part(state, name, part_msg, missing_msg=None, expand_msg=None):\n    \"\"\"Return child state with name part as its ast tree\"\"\"", "output": "\n    if name in state:\n        return state[name]\n    elif missing_msg:\n        raise ValueError(missing_msg)\n    else:\n        return None\n\n\ndef check_expand(state, name, expand_msg):\n    \"\"\"Return child state with name expanded as its ast tree\"\"\"\n    if name in state:\n        return state[name]\n    else:\n        raise ValueError(expand_msg)\n\n\ndef check_expand_part(state, name, part_msg, expand_msg):\n    \"\"\"Return child state with name expanded as its ast tree\"\"\"\n    if name in state:\n        return state[name", "solution": "\n\n    if missing_msg is None:\n        missing_msg = \"Are you sure you defined the {{part}}? \"\n    if expand_msg is None:\n        expand_msg = \"Did you correctly specify the {{part}}? \"\n\n    if not part_msg:\n        part_msg = name\n    append_message = {\"msg\": expand_msg, \"kwargs\": {\"part\": part_msg}}\n\n    has_part(state, name, missing_msg, append_message[\"kwargs\"])\n\n    stu_part = state.student_parts[name]\n    sol_part = state.solution_parts[name]\n\n    assert_ast(state, sol_part, append_message[\"kwargs\"])\n\n    return part_to_child(stu_part, sol_part, append_message, state)"}
{"prompt": "def list_datasets(self, get_global_public):\n        \"\"\"\n        Lists datasets in resources. Setting 'get_global_public' to 'True'\n        will retrieve all public datasets in cloud. 'False' will get user's\n        public datasets.\n\n        Arguments:\n            get_global_public (bool): True if user wants all public datasets in\n                                      cloud. False if user wants only their\n                                      public datasets.\n\n        Returns:\n            dict: Returns datasets in JSON format\n\n        \"\"\"", "output": "\n        url = f\"{self.base_url}/datasets\"\n        headers = self.headers\n        if get_global_public:\n            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n        else:\n            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        return response.json()\n\n    def get_dataset(self, dataset_id):\n        \"\"\"\n        Retrieves a dataset by its ID.\n\n        Arguments:\n            dataset_id (str): ID of the dataset to retrieve", "solution": "\n        appending = \"\"\n        if get_global_public:\n            appending = \"public\"\n        url = self.url() + \"/resource/{}dataset/\".format(appending)\n        req = self.remote_utils.get_url(url)\n\n        if req.status_code is not 200:\n            raise RemoteDataNotFoundError('Could not find {}'.format(req.text))\n        else:\n            return req.json()"}
{"prompt": "def is_known_type(self, type_name):\n        \"\"\"Check if type is known to the type system.\n\n        Returns:\n            bool: True if the type is a known instantiated simple type, False otherwise\n        \"\"\"", "output": "\n        return type_name in self.simple_types\n\n    def is_known_subtype(self, type_name, subtype_name):\n        \"\"\"Check if a subtype is known to the type system.\n\n        Args:\n            type_name (str): The name of the type for which to check the subtype\n            subtype_name (str): The name of the subtype to check\n\n        Returns:\n            bool: True if the subtype is a known subtype of the given type, False otherwise\n        \"\"\"\n        if type_name not in self.simple_types:\n            return False\n\n        if subtype_name in self.simple_types[", "solution": "\n\n        type_name = str(type_name)\n        if type_name in self.known_types:\n            return True\n\n        return False"}
{"prompt": "async def fetch_nearby(lat: float, long: float, limit: int = 10) -> Optional[List[Dict]]:\n    \"\"\"\n    Gets wikipedia articles near a given set of coordinates.\n    :raise ApiError: When there was an error connecting to the API.\n\n    todo cache\n    \"\"\"", "output": ".\n    \"\"\"\n    url = f\"https://en.wikipedia.org/w/api.php?action=query&list=geosearch&gsradius=500&gscoord={lat},{long}&gslimit={limit}&format=json\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            if response.status!= 200:\n                raise ApiError(f\"Failed to fetch nearby articles: {response.status}\")\n            data = await response.json()\n            if \"query\" not in data or \"geosearch\"", "solution": "\n    request_url = f\"https://en.wikipedia.org/w/api.php?action=query\" \\\n                  f\"&list=geosearch\" \\\n                  f\"&gscoord={lat}%7C{long}\" \\\n                  f\"&gsradius=10000\" \\\n                  f\"&gslimit={limit}\" \\\n                  f\"&format=json\"\n\n    async with ClientSession() as session:\n        try:\n            async with session.get(request_url) as request:\n                if request.status == 404:\n                    return None\n                data = (await request.json())[\"query\"][\"geosearch\"]\n\n        except ClientConnectionError as con_err:\n            logger.debug(f\"Could not connect to {con_err.host}\")\n            raise ApiError(f\"Could not connect to {con_err.host}\")\n        except JSONDecodeError as dec_err:\n            logger.error(f\"Could not decode data: {dec_err}\")\n            raise ApiError(f\"Could not decode data: {dec_err}\")\n        except KeyError:\n            return None\n        else:\n            for location in data:\n                location.pop(\"ns\")\n                location.pop(\"primary\")\n            return data"}
{"prompt": "def create_symmetric_key(self, algorithm, length):\n        \"\"\"\n        Create a symmetric key.\n\n        Args:\n            algorithm(CryptographicAlgorithm): An enumeration specifying the\n                algorithm for which the created key will be compliant.\n            length(int): The length of the key to be created. This value must\n                be compliant with the constraints of the provided algorithm.\n\n        Returns:\n            dict: A dictionary containing the key data, with the following\n                key/value fields:\n                * value - the bytes of the key\n                * format - a KeyFormatType enumeration for the bytes format\n\n        Raises:\n            InvalidField: Raised when the algorithm is unsupported or the\n                length is incompatible with the algorithm.\n            CryptographicFailure: Raised when the key generation process\n                fails.\n\n        Example:\n            >>> engine = CryptographyEngine()\n            >>> key = engine.create_symmetric_key(\n            ...     CryptographicAlgorithm.AES, 256)\n        \"\"\"", "output": " the provided algorithm or length are not\n                valid for the selected algorithm.\n        \"\"\"\n        if not isinstance(algorithm, CryptographicAlgorithm):\n            raise InvalidField(\"algorithm must be a CryptographicAlgorithm\")\n        if not isinstance(length, int):\n            raise InvalidField(\"length must be an integer\")\n\n        if algorithm == CryptographicAlgorithm.AES:\n            if length not in [128, 192, 256]:\n                raise InvalidField(\"AES key length must be 128, 192, or 256\")\n            key = self._generate_aes_key(length", "solution": "\n        if algorithm not in self._symmetric_key_algorithms.keys():\n            raise exceptions.InvalidField(\n                \"The cryptographic algorithm {0} is not a supported symmetric \"\n                \"key algorithm.\".format(algorithm)\n            )\n\n        cryptography_algorithm = self._symmetric_key_algorithms.get(algorithm)\n\n        if length not in cryptography_algorithm.key_sizes:\n            raise exceptions.InvalidField(\n                \"The cryptographic length ({0}) is not valid for \"\n                \"the cryptographic algorithm ({1}).\".format(\n                    length, algorithm.name\n                )\n            )\n\n        self.logger.info(\n            \"Generating a {0} symmetric key with length: {1}\".format(\n                algorithm.name, length\n            )\n        )\n\n        key_bytes = os.urandom(length // 8)\n        try:\n            cryptography_algorithm(key_bytes)\n        except Exception as e:\n            self.logger.exception(e)\n            raise exceptions.CryptographicFailure(\n                \"Invalid bytes for the provided cryptographic algorithm.\")\n\n        return {'value': key_bytes, 'format': enums.KeyFormatType.RAW}"}
{"prompt": "def _get_organisations(self):\n        \"\"\"\n        :returns: list of organisations, sorted alphabetically\n        :rtype: list(list(str))\n        \"\"\"", "output": "\n        organisations = []\n        for org in self.data:\n            if org[0] not in organisations:\n                organisations.append(org[0])\n        organisations.sort()\n        return organisations\n\n    def _get_organisations_with_data(self):\n        \"\"\"\n        :returns: list of organisations with data, sorted alphabetically\n        :rtype: list(list(str))\n        \"\"\"\n        organisations_with_data = []\n        for org in self.data:\n            if org[1] is not None:\n                organisations_with_data.append(org)\n        organ", "solution": "\n        organisations = []\n        for child in self.vcard.getChildren():\n            if child.name == \"ORG\":\n                organisations.append(child.value)\n        return sorted(organisations)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Completely resets the database. This means that all information in\n        the local cache and on disc will be erased.\n        \"\"\"", "output": "\n        self.cache.clear()\n        self.db.clear()\n\n    def __len__(self):\n        return len(self.cache)\n\n    def __getitem__(self, key):\n        if key in self.cache:\n            return self.cache[key]\n        else:\n            value = self.db[key]\n            self.cache[key] = value\n            return value\n\n    def __setitem__(self, key, value):\n        self.cache[key] = value\n        self.db[key] = value\n\n    def __delitem__(self, key):\n        del self.cache[key]\n        del", "solution": "\n        if not os.path.isdir(self.fdir):\n            os.makedirs(self.fdir, exist_ok=True)\n            return\n\n        for f in os.listdir(self.fdir):\n            del self[f]"}
{"prompt": "def reset_selective(self, regex=None):\n        \"\"\"Clear selective variables from internal namespaces based on a\n        specified regular expression.\n\n        Parameters\n        ----------\n        regex : string or compiled pattern, optional\n            A regular expression pattern that will be used in searching\n            variable names in the users namespaces.\n        \"\"\"", "output": "\n        if regex is None:\n            regex = r'__\\w+'\n        elif not isinstance(regex, re.Pattern):\n            regex = re.compile(regex)\n\n        for name in self.namespaces:\n            for var in self.namespaces[name].keys():\n                if regex.match(var):\n                    del self.namespaces[name][var]\n\n    def reset_all(self):\n        \"\"\"Clear all variables from internal namespaces.\"\"\"\n        for name in self.namespaces:\n            self.namespaces[name].clear()\n\n    def __getitem__(self, name):\n        \"\"\"Get a variable from the current namespace.\"\"\"\n        for", "solution": "\n        if regex is not None:\n            try:\n                m = re.compile(regex)\n            except TypeError:\n                raise TypeError('regex must be a string or compiled pattern')\n            # Search for keys in each namespace that match the given regex\n            # If a match is found, delete the key/value pair.\n            for ns in self.all_ns_refs:\n                for var in ns:\n                    if m.search(var):\n                        del ns[var]"}
{"prompt": "def load_from_path(path):\n    \"\"\"\n    Load a spec from a given path, discovering specs if a directory is given.\n\n    \"\"\"", "output": "\n    if os.path.isdir(path):\n        specs = []\n        for filename in os.listdir(path):\n            if filename.endswith('.yaml') or filename.endswith('.yml'):\n                specs.append(os.path.join(path, filename))\n        return specs\n    else:\n        return [path]\n\n\ndef load_specs(paths):\n    \"\"\"\n    Load specs from a list of paths, discovering specs if a directory is given.\n\n    \"\"\"\n    specs = []\n    for path in paths:\n        specs.extend(load_from_path(path))\n    return specs\n\n\ndef load_spec", "solution": "\n\n    if os.path.isdir(path):\n        paths = discover(path)\n    else:\n        paths = [path]\n\n    for path in paths:\n        name = os.path.basename(os.path.splitext(path)[0])\n        imp.load_source(name, path)"}
{"prompt": "def _has_role(self, role_name_or_list):\n        \"\"\"\n        Whether the user has this role name\n        \"\"\"", "output": "\n        if isinstance(role_name_or_list, list):\n            return any(role in self.roles for role in role_name_or_list)\n        else:\n            return role_name_or_list in self.roles\n\n    def _has_any_role(self, role_name_or_list):\n        \"\"\"\n        Whether the user has any of these role names\n        \"\"\"\n        if isinstance(role_name_or_list, list):\n            return any(role in self.roles for role in role_name_or_list)\n        else:\n            return role_name_or_", "solution": "\n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])"}
{"prompt": "def submit(self, executor, task, tag=None):\n        \"\"\"Submits a task to a provided executor\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to submit to the executor\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: A tag to associate to the submitted task\n\n        :rtype: concurrent.futures.Future\n        :returns: A future representing the submitted task\n        \"\"\"", "output": "\n        return executor.submit(task, tag)\n\n    def submit_file(self, executor, source_path, target_path, tag=None):\n        \"\"\"Submits a task to a provided executor to copy a file from a source to a target\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type source_path: str\n        :param source_path: The path of the file to copy from\n\n        :type target_path: str\n        :param target_path: The path of the file to copy to\n\n        :type tag:", "solution": "\n        logger.debug(\n            \"Submitting task %s to executor %s for transfer request: %s.\" % (\n                task, executor, self.transfer_id)\n        )\n        future = executor.submit(task, tag=tag)\n        # Add this created future to the list of associated future just\n        # in case it is needed during cleanups.\n        self.add_associated_future(future)\n        future.add_done_callback(\n            FunctionContainer(self.remove_associated_future, future))\n        return future"}
{"prompt": "def _get_streams(self):\n        \"\"\"\n        Find the streams for OlympicChannel\n        :return:\n        \"\"\"", "output": "\n        = self.client.get_streams(self.channel_id)\n        streams = []\n        for stream in videostreams:\n            if stream.get('type') == 'video':\n                streams.append(stream)\n        return streams\n\n    def get_stream(self, stream_id):\n        \"\"\"\n        Get the stream with the given id\n        :param stream_id:\n        :return:\n        \"\"\"\n        streams = self._get_streams()\n        for stream in streams:\n            if stream.get('id') == stream_id:\n                return stream\n        return None\n\n    def get_stream_url", "solution": "\n        match = self._url_re.match(self.url)\n        type_of_stream = match.group('type')\n        lang = re.search(r\"/../\", self.url).group(0)\n\n        if type_of_stream == 'tv':\n            path = re.search(r\"tv/.*-\\d/$\", self.url).group(0)\n\n            return self._get_live_streams(lang, path)\n        elif type_of_stream == 'playback':\n            path = re.search(r\"/playback/.*/$\", self.url).group(0)\n            return self._get_vod_streams()"}
{"prompt": "def compute_ffmc2d(X):\n    \"\"\"Computes the 2D-Fourier Magnitude Coefficients.\"\"\"", "output": "\n    =_2d(X)\n    =_2d(X)\n   [np.isnan(X)] = 0\n    =.astype(np.float64)\n   _fft = np.fft.fft2(X)\n   _fft[np.isnan(X)] = 0\n   _fft = np.fft.fftshift(np.fft.fft2(X))\n   _fft = np.abs(np.fft.fftshift(np.fft.fft2(X)))\n   _fft[np.isnan(", "solution": "\n    # 2d-fft\n    fft2 = scipy.fftpack.fft2(X)\n\n    # Magnitude\n    fft2m = magnitude(fft2)\n\n    # FFTshift and flatten\n    fftshift = scipy.fftpack.fftshift(fft2m).flatten()\n\n    #cmap = plt.cm.get_cmap('hot')\n    #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation=\"nearest\",\n    #    aspect=\"auto\", cmap=cmap)\n    #plt.show()\n\n    # Take out redundant components\n    return fftshift[:fftshift.shape[0] // 2 + 1]"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        for line in lines:\n            self.prefilter_line(line, continue_prompt=continue_prompt)\n\n    def prefilter_line(self, line, continue_prompt=False):\n        \"\"\"Prefilter a single input line of text.\n\n        This is the main entry point for prefiltering a single line of input.\n        It is called by :meth:`prefilter_lines` for each line of input.\n\n        This covers cases where there is only one line of input, which is the\n        case when the user enters a single line of input.\n        \"\"\"\n        # If the line is empty, do", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def get_service_certificate(self, service_name, thumbalgorithm, thumbprint):\n        \"\"\"\n        Returns the public data for the specified X.509 certificate associated\n        with a hosted service.\n\n        service_name:\n            Name of the hosted service.\n        thumbalgorithm:\n            The algorithm for the certificate's thumbprint.\n        thumbprint:\n            The hexadecimal representation of the thumbprint.\n        \"\"\"", "output": "\n        return self._get_service_certificate(service_name, thumbalgorithm, thumbprint)\n\n    def _get_service_certificate(self, service_name, thumbalgorithm, thumbprint):\n        \"\"\"\n        Returns the public data for the specified X.509 certificate associated\n        with a hosted service.\n\n        service_name:\n            Name of the hosted service.\n        thumbalgorithm:\n            The algorithm for the certificate's thumbprint.\n        thumbprint:\n            The hexadecimal representation of the thumbprint.\n        \"\"\"\n        return self.client.get_service_certificate(service_name, thumbalgorithm, thumbprint)\n\n   ", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('thumbalgorithm', thumbalgorithm)\n        _validate_not_none('thumbprint', thumbprint)\n        return self._perform_get(\n            '/' + self.subscription_id + '/services/hostedservices/' +\n            _str(service_name) + '/certificates/' +\n            _str(thumbalgorithm) + '-' + _str(thumbprint) + '',\n            Certificate)"}
{"prompt": "def get_default_args(func):\n    \"\"\"\n    returns a dictionary of arg_name:default_values for the input function\n    \"\"\"", "output": "\n    signature = inspect.signature(func)\n    default_args = {}\n    for name, param in signature.parameters.items():\n        if param.default is not inspect.Parameter.empty:\n            default_args[name] = param.default\n    return default_args\n\n\ndef get_args_from_kwargs(func, **kwargs):\n    \"\"\"\n    returns a dictionary of arg_name:arg_value for the input function\n    \"\"\"\n    signature = inspect.signature(func)\n    args = {}\n    for name, param in signature.parameters.items():\n        if name in kwargs:\n            args[name]", "solution": "\n    args, _, _, defaults, *rest = inspect.getfullargspec(func)\n    return dict(zip(reversed(args), reversed(defaults)))"}
{"prompt": "def index():\n    \"\"\"Display a list of all user institutes.\"\"\"", "output": "\n    institutes = Institute.query.all()\n    return render_template('index.html', institutes=institutes)\n\n\n@app.route('/institutes/<int:institute_id>/')\ndef institute(institute_id):\n    \"\"\"Display information about a specific institute.\"\"\"\n    institute = Institute.query.get_or_404(institute_id)\n    return render_template('institute.html', institute=institute)\n\n\n@app.route('/institutes/new/', methods=['GET', 'POST'])\ndef new_institute():\n   ", "solution": "\n    institute_objs = user_institutes(store, current_user)\n    institutes_count = ((institute_obj, store.cases(collaborator=institute_obj['_id']).count())\n                        for institute_obj in institute_objs if institute_obj)\n    return dict(institutes=institutes_count)"}
{"prompt": "def _plotting(self, rank_metric, results, graph_num, outdir, \n                  format, figsize, pheno_pos='', pheno_neg=''):\n        \"\"\" Plotting API.\n            :param rank_metric: sorted pd.Series with rankings values.\n            :param results: self.results\n            :param data: preprocessed expression table\n\n        \"\"\"", "output": "\n       _ =__ =___pos_ = 0\n       _est_ =_est_pos_ =_est_neg_ = 0\n       __neg_ =_est_neg_neg_ =_est_pos_neg_ = 0\n       _est_pos_pos_ = 0\n       _est_neg_pos_ = 0\n       _est_pos_neg_neg_ = 0\n       _est_neg_neg_ = 0\n       _est_pos_", "solution": "\n        \n        # no values need to be returned\n        if self._outdir is None: return\n        #Plotting\n        top_term = self.res2d.index[:graph_num]\n        # multi-threading\n        pool = Pool(self._processes)\n        for gs in top_term:\n            hit = results.get(gs)['hits_indices']\n            NES = 'nes' if self.module != 'ssgsea' else 'es'\n            term = gs.replace('/','_').replace(\":\",\"_\")\n            outfile = '{0}/{1}.{2}.{3}'.format(self.outdir, term, self.module, self.format)\n            # gseaplot(rank_metric=rank_metric, term=term, hits_indices=hit,\n            #           nes=results.get(gs)[NES], pval=results.get(gs)['pval'], \n            #           fdr=results.get(gs)['fdr'], RES=results.get(gs)['RES'],\n            #           pheno_pos=pheno_pos, pheno_neg=pheno_neg, figsize=figsize,\n            #           ofname=outfile)\n            pool.apply_async(gseaplot, args=(rank_metric, term, hit, results.get(gs)[NES],\n                                              results.get(gs)['pval'],results.get(gs)['fdr'],\n                                              results.get(gs)['RES'],\n                                              pheno_pos, pheno_neg, \n                                              figsize, 'seismic', outfile))\n            if self.module == 'gsea':\n                outfile2 = \"{0}/{1}.heatmap.{2}\".format(self.outdir, term, self.format)\n                # heatmap(df=self.heatmat.iloc[hit, :], title=term, ofname=outfile2, \n                #         z_score=0, figsize=(self._width, len(hit)/2))\n                pool.apply_async(heatmap, args=(self.heatmat.iloc[hit, :], 0, term, \n                                               (self._width, len(hit)/2+2), 'RdBu_r',\n                                                True, True, outfile2))\n        pool.close()\n        pool.join()"}
{"prompt": "def get(self, requestId):\n        \"\"\"\n        Gets details of a device management request.\n        It accepts requestId (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"", "output": "\n        try:\n            response = self._client.get(self._endpoint + \"/\" + requestId)\n            if response.status_code == 200:\n                return response.json()\n            else:\n                raise APIException(response.status_code, response.text)\n        except Exception as e:\n            raise APIException(500, str(e))\n\n    def approve(self, requestId, approval):\n        \"\"\"\n        Approves or rejects a device management request.\n        It accepts requestId (string), approval (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"\n        try:\n            data", "solution": "\n        url = MgmtRequests.mgmtSingleRequest % (requestId)\n        r = self._apiClient.get(url)\n\n        if r.status_code == 200:\n            return r.json()\n        else:\n            raise ApiException(r)"}
{"prompt": "def load(self, reload=False):\n        \"\"\"\n        Load the config and defaults from files.\n        \"\"\"", "output": "\n        if reload or not self.config:\n            self.config = self.load_config()\n        if reload or not self.defaults:\n            self.defaults = self.load_defaults()\n\n    def load_config(self):\n        \"\"\"\n        Load thescrape config file.\n        \"\"\"\n        config_file = os.path.join(self.config_dir, 'config.yaml')\n        if not os.path.exists(config_file):\n            raise ValueError('Config file not found: {}'.format(config_file))\n        with open(config_file, 'r') as f:\n            return yaml", "solution": "\n        if reload or not self._loaded:\n            # load defaults\n            if self._defaults_file and type(self._defaults_file) == str:\n                self._defaults_file = File(self._defaults_file, parent=self._parent)\n            defaults = {}\n            if self._defaults_file:\n                defaults = yaml.safe_load(self._defaults_file.read().replace('\\t', '    '))\n\n            # load data\n            data = {}\n            if self.exists:\n                data = yaml.safe_load(self.read().replace('\\t', '    '))\n\n            # initialise with the loaded data\n            self._defaults = defaults\n            self._data = copy.deepcopy(self._defaults)\n            self.update(data=data)\n\n            # if specified, apply environment variables\n            if self._apply_env:\n                self.update(ConfigEnv(self._env_prefix))\n\n            self._loaded = True\n\n        return self"}
{"prompt": "def walk_files(mgr):\n    \"\"\"\n    Iterate over all files visible to ``mgr``.\n    \"\"\"", "output": "\n    for file_info in mgr.list_files():\n        yield file_info.path\n\ndef walk_directories(mgr):\n    \"\"\"\n    Iterate over all directories visible to ``mgr``.\n    \"\"\"\n    for dir_info in mgr.list_directories():\n        yield dir_info.path\n\ndef walk_files_and_directories(mgr):\n    \"\"\"\n    Iterate over all files and directories visible to ``mgr``.\n    \"\"\"\n    for file_info in mgr.list_files():\n        yield file_info.path\n    for dir_info in mgr.list_directories():\n        yield dir_info.path", "solution": "\n    for dir_, subdirs, files in walk_files(mgr):\n        for file_ in files:\n            yield file_"}
{"prompt": "def data_types(self):\n        \"\"\"Return a list of data types.\"\"\"", "output": "\n        return ['float', 'int','str', 'bool']\n\n    def get_data_type(self, value):\n        \"\"\"Return the data type of a value.\"\"\"\n        if isinstance(value, (int, float)):\n            return 'float'\n        elif isinstance(value, str):\n            return'str'\n        elif isinstance(value, bool):\n            return 'bool'\n        else:\n            return 'unknown'\n\n    def get_data_type_from_value(self, value):\n        \"\"\"Return the data type of a value.\"\"\"\n        if isinstance(value, (int, float)):\n            return 'float'", "solution": "\n        data = self.gencloud.project_data(self.id)\n        return sorted(set(d.type for d in data))"}
{"prompt": "def ProductsForm(category, products):\n    \"\"\" Produces an appropriate _ProductsForm subclass for the given render\n    type. \"\"\"", "output": "\n    if category == 'list':\n        return ListProductsForm(products)\n    elif category == 'grid':\n        return GridProductsForm(products)\n    else:\n        raise ValueError('Unknown category: %s' % category)\n\n\nclass ListProductsForm(Form):\n    \"\"\" A form for displaying a list of products. \"\"\"\n    def __init__(self, products, *args, **kwargs):\n        super(ListProductsForm, self).__init__(*args, **kwargs)\n        self.fields['products'] = MultipleChoiceField(\n            label=_('Products'),\n            choices=[(p.pk, p.name)", "solution": "\n\n    # Each Category.RENDER_TYPE value has a subclass here.\n    cat = inventory.Category\n    RENDER_TYPES = {\n        cat.RENDER_TYPE_QUANTITY: _QuantityBoxProductsForm,\n        cat.RENDER_TYPE_RADIO: _RadioButtonProductsForm,\n        cat.RENDER_TYPE_ITEM_QUANTITY: _ItemQuantityProductsForm,\n        cat.RENDER_TYPE_CHECKBOX: _CheckboxProductsForm,\n    }\n\n    # Produce a subclass of _ProductsForm which we can alter the base_fields on\n    class ProductsForm(RENDER_TYPES[category.render_type]):\n        pass\n\n    products = list(products)\n    products.sort(key=lambda prod: prod.order)\n\n    ProductsForm.set_fields(category, products)\n\n    if category.render_type == inventory.Category.RENDER_TYPE_ITEM_QUANTITY:\n        ProductsForm = forms.formset_factory(\n            ProductsForm,\n            formset=_ItemQuantityProductsFormSet,\n        )\n\n    return ProductsForm"}
{"prompt": "def chop(array, epsilon=1e-10):\n    \"\"\"\n    Truncate small values of a complex array.\n\n    Args:\n        array (array_like): array to truncte small values.\n        epsilon (float): threshold.\n\n    Returns:\n        np.array: A new operator with small values set to zero.\n    \"\"\"", "output": "\n    return np.abs(array) > epsilon\n\ndef_chop(array,_array,_threshold=1e-10,_value=0):\n    \"\"\"\n    Truncate small values of a complex array and apply a.\n\n    Args:\n        array (array_like): array to truncte small values.\n        epsilon (float): threshold.\n       _value (float): value.\n\n    Returns:\n        np.array: A new operator with small values set to zero.\n   ", "solution": "\n    ret = np.array(array)\n\n    if np.isrealobj(ret):\n        ret[abs(ret) < epsilon] = 0.0\n    else:\n        ret.real[abs(ret.real) < epsilon] = 0.0\n        ret.imag[abs(ret.imag) < epsilon] = 0.0\n    return ret"}
{"prompt": "def make_clean_html(raw, stream_item=None, encoding=None):\n    \"\"\"Get a clean text representation of presumed HTML.\n\n    Treat `raw` as though it is HTML, even if we have no idea what it\n    really is, and attempt to get a properly formatted HTML document\n    with all HTML-escaped characters converted to their unicode.\n\n    This is called below by the `clean_html` transform stage, which\n    interprets MIME-type.  If `character_encoding` is not provided,\n    and `stream_item` is provided, then this falles back to\n    :attr:`streamcorpus.StreamItem.body.encoding`.\n\n    :param str raw: raw text to clean up\n    :param stream_item: optional stream item with encoding metadata\n    :type stream_item: :class:`streamcorpus.StreamItem`\n    :returns: UTF-8-encoded byte string of cleaned HTML text\n    :returntype: :class:`str`\n\n    \"\"\"", "output": "func:`guess_encoding` to determine the encoding of the document.\n\n    \"\"\"\n    if encoding is None:\n        if stream_item is not None:\n            encoding = guess_encoding(stream_item)\n        else:\n            encoding = 'utf-8'\n\n    try:\n        soup = BeautifulSoup(raw, 'html.parser')\n    except Exception as e:\n        raise ValueError('Invalid HTML: {}'.format(e))\n\n    # Convert all HTML-escaped characters to their unicode\n    for tag in soup.find_all():\n        if tag.name is not None and tag.name.startswith('script'):\n            tag.string", "solution": "\n    # Fix emails by protecting the <,> from HTML\n    raw = fix_emails(raw)\n    raw_decoded = nice_decode(raw, stream_item=stream_item, encoding=encoding)\n    if raw_decoded is None:\n        # give up on decoding it... maybe this should use force_unicode\n        raw_decoded = raw\n\n    # default attempt uses vanilla lxml.html\n    try:\n        root = lxml.html.document_fromstring(raw_decoded)\n    except ValueError, exc:\n        if 'with encoding declaration' in str(exc):\n            root = lxml.html.document_fromstring(raw)\n        else:\n            raise\n\n    # While we have the document parsed as a DOM, let's strip attributes.\n    # (The HTML cleaner seems to only support whitelisting attributes.\n    # As of now, we just want to blacklist a few.)\n    lxml.etree.strip_attributes(root, 'class', 'id')\n\n    # if that worked, then we will be able to generate a\n    # valid HTML string\n    fixed_html = lxml.html.tostring(root, encoding=unicode)\n\n    # remove any ^M characters\n    fixed_html = string.replace(fixed_html, '\\r', ' ')\n\n    # We drop utf8 characters that are above 0xFFFF as\n    # Lingpipe seems to be doing the wrong thing with them.\n    fixed_html = drop_invalid_and_upper_utf8_chars(fixed_html)\n\n    # construct a Cleaner that removes any ``<script>`` tags,\n    # Javascript, like an ``onclick`` attribute, comments, style\n    # tags or attributes, ``<link>`` tags\n    cleaner = lxml.html.clean.Cleaner(\n        scripts=True, javascript=True,\n        comments=True,\n        # do not remove <html> <head> <title> etc\n        page_structure=False,\n        remove_tags=['base'],\n        style=True, links=True)\n\n    # now get the really sanitized HTML\n    _clean_html = cleaner.clean_html(fixed_html)\n\n    # generate pretty HTML in utf-8\n    _clean_html = lxml.html.tostring(\n        lxml.html.document_fromstring(_clean_html),\n        method='html', encoding='utf-8',\n        pretty_print=True,\n        # include_meta_content_type=True\n        )\n\n    return uniform_html(_clean_html)"}
{"prompt": "def func(self):\n        \"\"\"Alternative naming, you can use `node.func.name` instead of `node.f_func`\"\"\"", "output": "\n        return self.f_func.name\n\n    def lineno(self):\n        \"\"\"Returns the line number of the function definition.\"\"\"\n        return self.f_code.co_firstlineno\n\n    def is_generator(self):\n        \"\"\"Returns True if the function is a generator function.\"\"\"\n        return self.f_code.co_flags & 0x0008\n\n    def is_async(self):\n        \"\"\"Returns True if the function is an asynchronous function.\"\"\"\n        return self.f_code.co_flags & 0x0010\n\n    def is_class(self):\n        \"\"\"Returns True", "solution": "\n        if self._func is None:\n            self._func = NNTreeNodeFunc(self)\n        return self._func"}
{"prompt": "def load_java_messages_to_ignore():\n    \"\"\"\n    Load in pickle file that contains dict structure with bad java messages to ignore per unit test\n    or for all cases.  The ignored bad java info is stored in g_ok_java_messages dict.\n\n    :return:\n    \"\"\"", "output": "\n    global g_ok_java_messages\n    if not g_ok_java_messages:\n        with open(os.path.join(os.path.dirname(__file__), 'java_messages_to_ignore.pickle'), 'rb') as f:\n            g_ok_java_messages = pickle.load(f)\n\n\ndef is_java_message_to_ignore(message):\n    \"\"\"\n    Check if the given message is in the list of bad java messages to ignore per unit test.\n\n    :param message: the message to check\n    :return: True if the message is in the list", "solution": "\n    global g_ok_java_messages\n    global g_java_message_pickle_filename\n\n    if os.path.isfile(g_java_message_pickle_filename):\n        with open(g_java_message_pickle_filename,'rb') as tfile:\n            g_ok_java_messages = pickle.load(tfile)\n    else:\n        g_ok_java_messages[\"general\"] = []"}
{"prompt": "def rowCount(self, parent):\n        \"\"\"Return number of children *parent* index has.\"\"\"", "output": "\n        return len(self.children[parent])\n\n    def parent(self, child):\n        \"\"\"Return index of parent of *child* index.\"\"\"\n        return self.parents[child]\n\n    def child(self, parent, index):\n        \"\"\"Return index of child of *parent* index at *index* position.\"\"\"\n        return self.children[parent][index]\n\n    def insertChild(self, parent, index, child):\n        \"\"\"Insert *child* index at *index* position in *parent* index.\"\"\"\n        self.children[parent].insert(index, child)\n        self.parents[child] = parent", "solution": "\n        if parent.column() > 0:\n            return 0\n\n        if parent.isValid():\n            item = parent.internalPointer()\n        else:\n            item = self.root\n\n        return len(item.children)"}
{"prompt": "def sh(cmd, escape=True):\n    \"\"\" Executes the given command.\n    returns a 2-tuple with returncode (integer) and OUTPUT (string)\n    \"\"\"", "output": "\n    if escape:\n        cmd = shlex.split(cmd)\n    else:\n        cmd = cmd.split()\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    output, error = process.communicate()\n    return process.returncode, output.decode('utf-8'), error.decode('utf-8')\n\n\ndef get_git_branch():\n    \"\"\" Returns the current git branch \"\"\"\n    return sh('git rev-parse --abbrev-ref HEAD', escape=False)[0]\n\n\ndef get_git_commit():\n    \"\"\" Returns the", "solution": "\n\n    if escape:\n        cmd = quote(cmd)\n\n    process = Popen(cmd, stdout=PIPE, stderr=STDOUT, shell=True)\n    output, unused_err = process.communicate()\n    retcode = process.poll()\n\n    return (retcode, output)"}
{"prompt": "def abort(self, jobs=None, targets=None, block=None):\n        \"\"\"Abort specific jobs from the execution queues of target(s).\n\n        This is a mechanism to prevent jobs that have already been submitted\n        from executing.\n\n        Parameters\n        ----------\n\n        jobs : msg_id, list of msg_ids, or AsyncResult\n            The jobs to be aborted\n            \n            If unspecified/None: abort all outstanding jobs.\n\n        \"\"\"", "output": "\n        if jobs is None:\n            jobs = self.outstanding_jobs\n        elif isinstance(jobs, list):\n            jobs = [self.get_job(job_id) for job_id in jobs]\n        else:\n            jobs = [jobs]\n\n        for job in jobs:\n            if job.status == 'RUNNING':\n                job.abort(block=block)\n            elif job.status == 'PENDING':\n                job.cancel(block=block)\n            else:\n                raise ValueError(f\"Job {job.msg_id} is not in a valid state to abort.\")\n\n    def get_job(self", "solution": "\n        block = self.block if block is None else block\n        jobs = jobs if jobs is not None else list(self.outstanding)\n        targets = self._build_targets(targets)[0]\n        \n        msg_ids = []\n        if isinstance(jobs, (basestring,AsyncResult)):\n            jobs = [jobs]\n        bad_ids = filter(lambda obj: not isinstance(obj, (basestring, AsyncResult)), jobs)\n        if bad_ids:\n            raise TypeError(\"Invalid msg_id type %r, expected str or AsyncResult\"%bad_ids[0])\n        for j in jobs:\n            if isinstance(j, AsyncResult):\n                msg_ids.extend(j.msg_ids)\n            else:\n                msg_ids.append(j)\n        content = dict(msg_ids=msg_ids)\n        for t in targets:\n            self.session.send(self._control_socket, 'abort_request',\n                    content=content, ident=t)\n        error = False\n        if block:\n            self._flush_ignored_control()\n            for i in range(len(targets)):\n                idents,msg = self.session.recv(self._control_socket,0)\n                if self.debug:\n                    pprint(msg)\n                if msg['content']['status'] != 'ok':\n                    error = self._unwrap_exception(msg['content'])\n        else:\n            self._ignored_control_replies += len(targets)\n        if error:\n            raise error"}
{"prompt": "def format_execution_time(self, start=None, end=None):\n        \"\"\"\n        Format the calculated time into a human readable format.\n\n        :param start: A starting time.\n        :type start: int|str\n\n        :param stop: A ending time.\n        :type stop: int|str\n\n        :return: A human readable date.\n        :rtype: str\n        \"\"\"", "output": "\n        if start is None:\n            start = time.time()\n        if end is None:\n            end = time.time()\n\n        elapsed_time = end - start\n        hours, remainder = divmod(elapsed_time, 3600)\n        minutes, seconds = divmod(remainder, 60)\n\n        if hours > 0:\n            return f\"{hours}h {minutes}m {seconds}s\"\n        elif minutes > 0:\n            return f\"{minutes}m {seconds}s\"\n        else:\n            return f\"{seconds}s\"\n", "solution": "\n\n        # We return the formatted execution time.\n        return \":\".join(list(self._calculate(start, end).values()))"}
{"prompt": "def _all_load_skeleton(self, traj_node, hdf5_group):\n        \"\"\"Reloads skeleton data of a tree node\"\"\"", "output": "\n       __ =_utils.load_skeleton(hdf5_group)\n       _utils.update_skeleton(traj_node,_)\n\n    def _load__(self,_ut_group,_ut___node):\n        \"\"\"Reloads-ut data of a tree node\"\"\"\n       _ut_ =_ut_utils.load_ut_data(hdf5_group)\n       _ut_utils.update_ut_data(ut_node,_ut_)\n\n    def _load__(self", "solution": "\n        if traj_node.v_annotations.f_is_empty():\n            self._ann_load_annotations(traj_node, hdf5_group)\n        if traj_node.v_comment == '':\n            comment = self._all_get_from_attrs(hdf5_group, HDF5StorageService.COMMENT)\n            if comment is None:\n                comment = ''\n            traj_node.v_comment = comment"}
{"prompt": "def overlapping(self, variant_obj):\n        \"\"\"Return overlapping variants.\n\n        Look at the genes that a variant overlaps to.\n        Then return all variants that overlap these genes.\n\n        If variant_obj is sv it will return the overlapping snvs and oposite\n        There is a problem when SVs are huge since there are to many overlapping variants.\n\n        Args:\n            variant_obj(dict)\n\n        Returns:\n            variants(iterable(dict))\n        \"\"\"", "output": "\n       _variant = variant_obj['variant']\n       _gene = variant_obj['gene']\n       _transcript = variant_obj['transcript']\n\n       _gene_transcripts = self.gene_transcripts[self.gene_transcripts['gene_id'] == int(self.gene_id_to_gene_name[self.gene_id_to_gene_name['gene_id'] == int(self.gene_id_to_gene_name['gene_id'] == int(self.gene_id_to_gene_name['gene_id']", "solution": "\n        #This is the category of the variants that we want to collect\n        category = 'snv' if variant_obj['category'] == 'sv' else 'sv'\n\n        query = {\n            '$and': [\n                {'case_id': variant_obj['case_id']},\n                {'category': category},\n                {'hgnc_ids' : { '$in' : variant_obj['hgnc_ids']}}\n            ]\n        }\n\n        sort_key = [('rank_score', pymongo.DESCENDING)]\n        # We collect the 30 most severe overlapping variants\n        variants = self.variant_collection.find(query).sort(sort_key).limit(30)\n\n        return variants"}
{"prompt": "def validate(self):\n        \"\"\"\n        Verify that the contents of the SecretData object are valid.\n\n        Raises:\n            TypeError: if the types of any SecretData attributes are invalid.\n        \"\"\"", "output": "\n        if not isinstance(self.name, str):\n            raise TypeError(\"SecretData.name must be a string\")\n        if not isinstance(self.type, str):\n            raise TypeError(\"SecretData.type must be a string\")\n        if not isinstance(self.data, bytes):\n            raise TypeError(\"SecretData.data must be bytes\")\n        if not isinstance(self.version, int):\n            raise TypeError(\"SecretData.version must be an integer\")\n        if not isinstance(self.last_modified, datetime):\n            raise TypeError(\"SecretData.last_modified must be a datetime\")\n        if not isinstance(", "solution": "\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"secret value must be bytes\")\n        elif not isinstance(self.data_type, enums.SecretDataType):\n            raise TypeError(\"secret data type must be a SecretDataType \"\n                            \"enumeration\")\n\n        mask_count = len(self.cryptographic_usage_masks)\n        for i in range(mask_count):\n            mask = self.cryptographic_usage_masks[i]\n            if not isinstance(mask, enums.CryptographicUsageMask):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\n                    \"secret data mask {0} must be a CryptographicUsageMask \"\n                    \"enumeration\".format(position))\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"secret data name {0} must be a string\".format(\n                    position))"}
{"prompt": "def to_matrix(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"", "output": "\n        = self.params[0]\n        phi = self.params[1]\n        lam = self.params[2]\n        return np.array([[np.cos(phi/2), -1j*np.sin(phi/2)*np.exp(-1j*lam)],\n                         [1j*np.sin(phi/2)*np.exp(1j*lam), np.cos(phi/2)]])\n\n    def to_matrix_(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"\n        = self.params[0]\n        phim", "solution": "\n        theta, phi, lam = self.params\n        return numpy.array(\n            [[\n                numpy.cos(theta / 2),\n                -numpy.exp(1j * lam) * numpy.sin(theta / 2)\n            ],\n             [\n                 numpy.exp(1j * phi) * numpy.sin(theta / 2),\n                 numpy.exp(1j * (phi + lam)) * numpy.cos(theta / 2)\n             ]],\n            dtype=complex)"}
{"prompt": "def _merge_with_other_stm(self, other: \"IfContainer\") -> None:\n        \"\"\"\n        Merge other statement to this statement\n        \"\"\"", "output": "\n        if isinstance(other, IfContainer):\n            self.if_stmts.extend(other.if_stmts)\n            self.else_stmts.extend(other.else_stmts)\n        else:\n            raise ValueError(\"Can only merge IfContainer with other statement\")\n\n    def __str__(self) -> str:\n        \"\"\"\n        Return the string representation of the statement\n        \"\"\"\n        if self.if_stmts:\n            if self.else_stmts:\n                return f\"if {self.if_stmts} else {self.else_stmts}\"\n            else:\n                return f\"", "solution": "\n        merge = self._merge_statement_lists\n        newCases = []\n        for (c, caseA), (_, caseB) in zip(self.cases, other.cases):\n            newCases.append((c, merge(caseA, caseB)))\n\n        self.cases = newCases\n\n        if self.default is not None:\n            self.default = merge(self.default, other.default)\n\n        self._on_merge(other)"}
{"prompt": "def  main( argv ):\n    \"\"\"main program loop\"\"\"", "output": "\n   _file = argv[1]\n   _file = argv[2]\n   _file_out = argv[3]\n   _file_ = argv[4]\n   _file_out_ = argv[5]\n   _file_out_ = argv[6]\n   _file_out_ = argv[7]\n   _file_out_ = argv[8]\n   _file_out_ = argv[9]\n   _file_out_ = argv[10]\n   _file_out_ = argv[11]\n   ", "solution": "\n\n    global output_dir\n\n    try:\n        opts, args = getopt.getopt( sys.argv[1:], \\\n                                    \"hb\",         \\\n                                    [\"help\", \"backup\"] )\n    except getopt.GetoptError:\n        usage()\n        sys.exit( 2 )\n\n    if args == []:\n        usage()\n        sys.exit( 1 )\n\n    # process options\n    #\n    output_dir = None\n    do_backup  = None\n\n    for opt in opts:\n        if opt[0] in ( \"-h\", \"--help\" ):\n            usage()\n            sys.exit( 0 )\n\n        if opt[0] in ( \"-b\", \"--backup\" ):\n            do_backup = 1\n\n    # create context and processor\n    source_processor = SourceProcessor()\n\n    # retrieve the list of files to process\n    file_list = make_file_list( args )\n    for filename in file_list:\n        source_processor.parse_file( filename )\n\n        for block in source_processor.blocks:\n            beautify_block( block )\n\n        new_name = filename + \".new\"\n        ok       = None\n\n        try:\n            file = open( new_name, \"wt\" )\n            for block in source_processor.blocks:\n                for line in block.lines:\n                    file.write( line )\n                    file.write( \"\\n\" )\n            file.close()\n        except:\n            ok = 0"}
{"prompt": "def checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"", "output": "\n    if b.funcname == \"main\":\n        return True\n    return False\n\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    # Set breakpoints\n    breakpoint()\n    breakpoint(checkfunc=checkfuncname)\n\n    # Run the program\n    while True:\n        try:\n            input()\n        except KeyboardInterrupt:\n            break\n\n\nif __name__ == \"__main__\":\n    main()\n", "solution": "\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True"}
{"prompt": "def gene_variants(self, query=None,\n                   category='snv', variant_type=['clinical'],\n                   nr_of_variants=50, skip=0):\n        \"\"\"Return all variants seen in a given gene.\n\n        If skip not equal to 0 skip the first n variants.\n\n        Arguments:\n            query(dict): A dictionary with querys for the database, including\n            variant_type: 'clinical', 'research'\n            category(str): 'sv', 'str', 'snv' or 'cancer'\n            nr_of_variants(int): if -1 return all variants\n            skip(int): How many variants to skip\n        \"\"\"", "output": ", otherwise return\n            at most this many variants.\n            skip(int): if not 0, skip the first n variants.\n\n        Returns:\n            A list of_variant objects.\n        \"\"\"\n        if query is None:\n            query = {}\n\n        if 'gene' not in query:\n            raise ValueError('Query must include gene')\n\n        gene = query['gene']\n        query.pop('gene')\n\n        if category not in ['sv','str','snv', 'cancer']:\n            raise ValueError('Category must be sv, str, snv or cancer')\n\n        if variant_type not in ['clinical", "solution": "\n\n        mongo_variant_query = self.build_variant_query(query=query,\n                                   category=category, variant_type=variant_type)\n\n        sorting = [('rank_score', pymongo.DESCENDING)]\n\n        if nr_of_variants == -1:\n            nr_of_variants = 0 # This will return all variants\n        else:\n            nr_of_variants = skip + nr_of_variants\n\n        result = self.variant_collection.find(\n            mongo_variant_query\n            ).sort(sorting).skip(skip).limit(nr_of_variants)\n\n        return result"}
{"prompt": "def filter_exclude_downhole(self, threshold, filt=True, samples=None, subset=None):\n        \"\"\"\n        Exclude all points down-hole (after) the first excluded data.\n\n        Parameters\n        ----------\n        threhold : int\n            The minimum number of contiguous excluded data points\n            that must exist before downhole exclusion occurs.\n        file : valid filter string or bool\n            Which filter to consider. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n       _filter = self.get_filter(file)\n       _filter.exclude_downhole(threshold,_filter.data,_filter.downhole_count,_filter.downhole_index,_filter.downhole__index,_filter.downhole_count,_filter.downhole_index,_filter.downhole_count,_filter.downhole_index,_filter.downhole_count,_filter.downhole_index,_filter.downhole_count,_filter.down", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            self.data[s].filter_exclude_downhole(threshold, filt)"}
{"prompt": "def match(self, path):\n        \"\"\"Return route handler with arguments if path matches this route.\n\n        Arguments:\n          path (str): Request path\n\n        Returns:\n          tuple or None: A tuple of three items:\n\n            1. Route handler (callable)\n            2. Positional arguments (list)\n            3. Keyword arguments (dict)\n\n          ``None`` if the route does not match the path.\n        \"\"\"", "output": "\n        if not self._compiled:\n            self._compile()\n        match = self._match_path(path)\n        if match:\n            handler, args, kwargs = match\n            return handler, args, kwargs\n        return None\n\n    def _match_path(self, path):\n        \"\"\"Match a path against this route's compiled pattern.\n\n        Arguments:\n          path (str): Request path\n\n        Returns:\n          tuple or None: A tuple of three items:\n\n            1. Route handler (callable)\n            2. Positional arguments (list)\n            3. Keyword arguments (dict)\n\n          ``None`` if the route does not match the", "solution": "\n        match = self._re.search(path)\n        if match is None:\n            return None\n        kwargs_indexes = match.re.groupindex.values()\n        args_indexes = [i for i in range(1, match.re.groups + 1)\n                          if i not in kwargs_indexes]\n        args = [match.group(i) for i in args_indexes]\n        kwargs = {}\n        for name, index in match.re.groupindex.items():\n            kwargs[name] = match.group(index)\n        return self._callback, args, kwargs"}
{"prompt": "def get_int(errmsg, arg, default=1, cmdname=None):\n    \"\"\"If arg is an int, use that otherwise take default.\"\"\"", "output": "\n    if isinstance(arg, int):\n        return arg\n    else:\n        if cmdname is None:\n            cmdname = \"argument\"\n        raise argparse.ArgumentTypeError(f\"{errmsg}: {arg} is not an integer for {cmdname}\")\n\ndef get_float(errmsg, arg, default=1.0, cmdname=None):\n    \"\"\"If arg is a float, use that otherwise take default.\"\"\"\n    if isinstance(arg, float):\n        return arg\n    else:\n        if cmdname is None:\n            cmdname = \"argument\"\n        raise argparse.ArgumentTypeError(f\"{errmsg", "solution": "\n    if arg:\n        try:\n            # eval() is used so we will allow arithmetic expressions,\n            # variables etc.\n            default = int(eval(arg))\n        except (SyntaxError, NameError, ValueError):\n            if cmdname:\n                errmsg(\"Command '%s' expects an integer; got: %s.\" %\n                       (cmdname, str(arg)))\n            else:\n                errmsg('Expecting an integer, got: %s.' % str(arg))\n                pass\n            raise ValueError\n    return default"}
{"prompt": "def get(*args, **kwargs):\n    \"\"\"Get users.\"\"\"", "output": "\n    return User.objects.all()\n\n@api_view(['GET'])\ndef me(request):\n    \"\"\"Get the authenticated user.\"\"\"\n    user = request.user\n    if user.is_anonymous:\n        return Response(status=status.HTTP_401_UNAUTHORIZED)\n    return Response(user.to_dict())\n\n@api_view(['GET'])\ndef user(request, pk):\n    \"\"\"Get a user by ID.\"\"\"\n    try:\n        user = User.objects.get(pk=pk)\n    except User.DoesNotExist:\n        return Response(status=status.HTTP", "solution": "\n    from invenio.modules.oauth2server.models import Client\n    q = Client.query\n    return q.count(), q.all()"}
{"prompt": "def verify(\n            cls,\n            timestamp: int,\n            message_hash: SHA512Hash,\n            signature: bytes,\n    ) -> bool:\n        \"\"\"\n        Verify a given NIST message hash and signature for a beacon value.\n\n        :param timestamp: The timestamp of the record being verified.\n        :param message_hash:\n            The hash that was carried out over the message.\n            This is an object belonging to the `Crypto.Hash` module.\n        :param signature: The signature that needs to be validated.\n        :return: True if verification is correct. False otherwise.\n        \"\"\"", "output": "\n        # Convert the timestamp to a byte string\n        timestamp_bytes = timestamp.to_bytes(8, byteorder='big')\n\n        # Combine the timestamp and message hash into a single byte string\n        combined_bytes = timestamp_bytes + message_hash.digest()\n\n        # Verify the signature using the combined bytes and the public key\n        return_verify(combined_bytes, signature)\n\n\ndef verify_____ hash_ signature(\n        timestamp: int,\n        message_hash: SHA512Hash,\n        signature: bytes,\n) -> bool:\n    \"\"\"\n    Verify a", "solution": "\n\n        # Determine verifier type to use based on timestamp.\n        if timestamp < 1496176860:\n            verifier = cls._VERIFIER_20130905\n        elif timestamp < 1502202360:\n            verifier = None\n        else:\n            verifier = cls._VERIFIER_20170808\n\n        # If a verifier exists to handle this problem, use it directly.\n        # Else, we cannot verify the record and must mark it invalid.\n        if verifier:\n            result = verifier.verify(\n                message_hash,\n                signature,\n            )\n        else:\n            result = False\n\n        # Convert 1 to 'True', 'False' otherwise\n        if isinstance(result, int):\n            result = True if result == 1 else False\n\n        return result"}
{"prompt": "def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from function bodies.\"\"\"", "output": "\n        if node.name.startswith('__'):\n            return node\n\n        if node.body:\n            # Remove dead code from the function body\n            dead_code = self.find_dead_code(node.body)\n            if dead_code:\n                node.body = [stmt for stmt in node.body if stmt not in dead_code]\n\n        return node\n\n    def find_dead_code(self, body: List[ast.stmt]) -> List[ast.stmt]:\n        \"\"\"Find all statements that are not executed by the function.\"\"\"\n        dead_code = []\n        for stmt in body:\n            if isinstance", "solution": "\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.FunctionDef)\n        return ast.copy_location(\n            ast.FunctionDef(\n                name=new_node.name,\n                args=new_node.args,\n                body=_filter_dead_code(new_node.body),\n                decorator_list=new_node.decorator_list,\n                returns=new_node.returns,\n            ),\n            new_node,\n        )"}
{"prompt": "def register_success(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        succeeded\n        \"\"\"", "output": "\n        self.bot.join(channel)\n        self.bot.say(cmd_channel, nick + \" has registered successfully!\")\n\n    def register_failed(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration failure from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        failed\n        \"\"\"\n        self.bot.join(channel)\n        self.bot.say(cmd_channel, nick + \" has failed to register!\")\n\ndef main():\n    = IRCBot()\n    irc_server = \"irc.", "solution": "\n        # the boss will tell what channel to join\n        self.channel = cmd_channel\n        self.conn.join(self.channel)\n        \n        # indicate that registered so we'll stop trying\n        self.registered.set()"}
{"prompt": "def csv_to_po(trans_csv_path, meta_csv_path, locale_root,\n              po_files_path, header=None):\n    \"\"\"\n    Converts GDocs spreadsheet generated csv file into po file.\n    :param trans_csv_path: path to temporary file with translations\n    :param meta_csv_path: path to temporary file with meta information\n    :param locale_root: path to locale root folder containing directories\n                        with languages\n    :param po_files_path: path from lang directory to po file\n    \"\"\"", "output": "\n    # read translation csv file\n    with open(trans_csv_path, 'r', encoding='utf-8') as f:\n        trans_csv = csv.reader(f)\n        if header:\n            trans_csv.next()\n       _translations = {}\n        for row in trans_csv:\n            if row[0] not in_translations:\n               _translations[row[0]] = {}\n           _translations[row[0]][row[1]] = row[2]\n\n    # read meta csv file\n    with open(meta_csv_path, 'r', encoding='utf", "solution": "\n    pattern = \"^\\w+.*po$\"\n    for root, dirs, files in os.walk(locale_root):\n        for f in filter(lambda x: re.match(pattern, x), files):\n            os.remove(os.path.join(root, f))\n\n    # read title row and prepare descriptors for po files in each lang\n    trans_reader = UnicodeReader(trans_csv_path)\n    meta_reader = UnicodeReader(meta_csv_path)\n    try:\n        title_row = trans_reader.next()\n    except StopIteration:\n        # empty file\n        return\n\n    trans_languages = _prepare_locale_dirs(title_row[3:], locale_root)\n\n    po_files = {}\n\n    meta_reader.next()\n    # go through every row in downloaded csv file\n    for trans_row, meta_row in izip_longest(trans_reader, meta_reader):\n        filename = trans_row[0].rstrip()\n        metadata = meta_row[0].rstrip() if meta_row else METADATA_EMPTY\n        comment = trans_row[1]\n        msgid = trans_row[2]\n\n        if filename not in po_files:\n            _prepare_polib_files(po_files, filename, trans_languages,\n                                 locale_root, po_files_path, header)\n\n        _write_entries(po_files[filename], trans_languages, msgid,\n                       trans_row[3:], metadata, comment)\n    for filename in po_files:\n        for lang in po_files[filename]:\n            po_files[filename][lang].save()\n\n    trans_reader.close()\n    meta_reader.close()"}
{"prompt": "def _add_header(self):\n        \"\"\"Add email header info.\"\"\"", "output": "\n        self.msg['From'] = self.sender\n        self.msg['To'] = self.recipient\n        self.msg['Subject'] = self.subject\n        self.msg['Date'] = formatdate(localtime=True)\n\n    def _add_body(self):\n        \"\"\"Add email body info.\"\"\"\n        self.msg.set_content(self.body)\n\n    def send(self):\n        \"\"\"Send email.\"\"\"\n        self._add_header()\n        self._add_body()\n        self.smtp_server.sendmail(self.sender, self.recipient, self.msg.as_string", "solution": "\n        self.message[\"From\"] = self.from_\n        self.message[\"Subject\"] = self.subject\n        if self.to:\n            self.message[\"To\"] = self.list_to_string(self.to)\n        if self.cc:\n            self.message[\"Cc\"] = self.list_to_string(self.cc)\n        if self.bcc:\n            self.message[\"Bcc\"] = self.list_to_string(self.bcc)"}
{"prompt": "def url_dequery(url):\n    \"\"\"Return a URL with the query component removed.\n\n    :param url: URL to dequery.\n    :type url: str\n    :rtype: str\n    \"\"\"", "output": "\n    return urlparse(url).path\n", "solution": "\n    url = urlparse.urlparse(url)\n    return urlparse.urlunparse((url.scheme,\n                                url.netloc,\n                                url.path,\n                                url.params,\n                                '',\n                                url.fragment))"}
{"prompt": "def rendered_content(self):\n        \"\"\"Returns a 'minified' version of the javascript content\"\"\"", "output": "\n        return self.content.replace('\\n', '')\n\n    def __str__(self):\n        return self.rendered_content\n", "solution": "\n        template = self.resolve_template(self.template_name)\n        if django.VERSION[1] < 8:\n            if template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        else:\n            if template.template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        # if no minified template exists, minify the response\n        content = super(MinifiedJsTemplateResponse, self).rendered_content\n        content = jsmin.jsmin(content)\n        return content"}
{"prompt": "def get_except_handler_name(self, node):\n        \"\"\"\n        Helper to get the exception name from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"", "output": "\n        if isinstance(node, ast.ExceptHandler):\n            if hasattr(node, 'name'):\n                return node.name\n            elif hasattr(node, 'type'):\n                return node.type.id\n            else:\n                return None\n        else:\n            return None\n\n    def get_except_handler_type(self, node):\n        \"\"\"\n        Helper to get the exception type from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"\n        if isinstance(node, ast.ExceptHandler):\n            if hasattr(node, 'type'):\n                return node.type\n            else:\n                return None\n        else:\n            return None", "solution": "\n        name = node.name\n        if not name:\n            return None\n\n        if version_info < (3,):\n            return name.id\n        return name"}
{"prompt": "def _get_live_streams(self, lang, path):\n        \"\"\"\n        Get the live stream in a particular language\n        :param lang:\n        :param path:\n        :return:\n        \"\"\"", "output": "\n        url = f\"https://www.youtube.com/feeds/videos.xml?channel_id={self.channel_id}&hl={lang}&gl={lang}\"\n        response = requests.get(url)\n        xml_data = response.text\n        soup = BeautifulSoup(xml_data, \"xml\")\n        items = soup.find_all(\"item\")\n        live_streams = []\n        for item in items:\n            title = item.find(\"title\").text\n            link = item.find(\"link\").text\n            duration = item.find(\"yt:duration\").text\n            live_streams.append({\"", "solution": "\n        res = self.session.http.get(self._live_api_url.format(lang, path))\n        live_res = self.session.http.json(res)['default']['uid']\n        post_data = '{\"channel_url\":\"/api/channels/%s/\"}' % live_res\n        try:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['stream_url']\n        except BaseException:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['channel_url']\n        return HLSStream.parse_variant_playlist(self.session, stream_data)"}
{"prompt": "def rotatePoint(self, pointX, pointY):\n        \"\"\"\n        Rotates a point relative to the mesh origin by the angle specified in the angle property.\n        Uses the angle formed between the segment linking the point of interest to the origin and\n        the parallel intersecting the origin. This angle is called beta in the code.\n        \"\"\"", "output": "\n        # Calculate the angle formed between the segment linking the point of interest to the origin and\n        # the parallel intersecting the origin.\n        beta = math.atan2(pointY, pointX)\n\n        # Rotate the point by the angle formed between the segment linking the point of interest to the origin and\n        # the parallel intersecting the origin.\n        rotatedX = pointX * math.cos(beta) - pointY * math.sin(beta)\n        rotatedY = pointX * math.sin(beta) + pointY * math.cos(beta)\n\n        return rotatedX, rotatedY\n\n    defable", "solution": "    \n        if(self.angle == 0 or self.angle == None):\n            return(pointX, pointY)\n              \n        # 1. Compute the segment length\n        length = math.sqrt((pointX - self.xll) ** 2 + (pointY - self.yll) ** 2)\n        \n        # 2. Compute beta\n        beta = math.acos((pointX - self.xll) / length) \n        if(pointY < self.yll):\n            beta = math.pi * 2 - beta\n           \n        # 3. Compute offsets\n        offsetX = math.cos(beta) * length - math.cos(self._angle_rd + beta) * length\n        offsetY = math.sin(self._angle_rd + beta) * length - math.sin(beta) * length \n        return (pointX - offsetX, pointY + offsetY)"}
{"prompt": "def get_term_by_sis_id(self, sis_term_id):\n        \"\"\"\n        Return a term resource for the passed SIS ID.\n        \"\"\"", "output": "\n        return self.get_resource(f\"/terms/{sis_term_id}\")\n\n    def get_term_by_name(self, name):\n        \"\"\"\n        Return a term resource for the passed name.\n        \"\"\"\n        return self.get_resource(f\"/terms?name={name}\")\n\n    def get_term_by_course_id(self, course_id):\n        \"\"\"\n        Return a term resource for the passed course ID.\n        \"\"\"\n        return self.get_resource(f\"/terms?course_id={course_id}\")\n\n    def get_term_by_course_name(", "solution": "\n        for term in self.get_all_terms():\n            if term.sis_term_id == sis_term_id:\n                return term"}
{"prompt": "def set_hvac_mode(self, index, hvac_mode):\n        \"\"\" possible hvac modes are auto, auxHeatOnly, cool, heat, off \"\"\"", "output": "\n        if hvac_mode not in [\"auto\", \"auxHeatOnly\", \"cool\", \"heat\", \"off\"]:\n            raise ValueError(\"Invalid hvac mode. Possible modes are auto, auxHeatOnly, cool, heat, off\")\n        self.hvac_mode[index] = hvac_mode\n\n    def get_hvac_mode(self, index):\n        \"\"\" returns the hvac mode of the index \"\"\"\n        return self.hvac_mode[index]\n\n    def set_fan_mode(self, index, fan_mode):\n        \"\"\" possible fan modes are auto, on, autoLow", "solution": "\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"hvacMode\": hvac_mode\n                                  }\n                              }}\n        log_msg_action = \"set HVAC mode\"\n        return self.make_request(body, log_msg_action)"}
{"prompt": "def process_non_raw_string_token(self, prefix, string_body, start_row):\n        \"\"\"check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.\n        \"\"\"", "output": "\n        if not string_body:\n            return\n\n        # check for bad escapes in the string\n        for i, c in enumerate(string_body):\n            if c == prefix:\n                # check for escaped prefix\n                if i > 0 and string_body[i - 1] == '\\\\':\n                    continue\n\n                # check for escaped prefix\n                if i < len(string_body) - 1 and string_body[i + 1] == '\\\\':\n                    continue\n\n                # check for escaped prefix\n                if i < len(string_body) - 2 and string_body[i + 2] ==", "solution": "\n        # Walk through the string; if we see a backslash then escape the next\n        # character, and skip over it.  If we see a non-escaped character,\n        # alert, and continue.\n        #\n        # Accept a backslash when it escapes a backslash, or a quote, or\n        # end-of-line, or one of the letters that introduce a special escape\n        # sequence <http://docs.python.org/reference/lexical_analysis.html>\n        #\n        # TODO(mbp): Maybe give a separate warning about the rarely-used\n        # \\a \\b \\v \\f?\n        #\n        # TODO(mbp): We could give the column of the problem character, but\n        # add_message doesn't seem to have a way to pass it through at present.\n        i = 0\n        while True:\n            i = string_body.find(\"\\\\\", i)\n            if i == -1:\n                break\n            # There must be a next character; having a backslash at the end\n            # of the string would be a SyntaxError.\n            next_char = string_body[i + 1]\n            match = string_body[i : i + 2]\n            if next_char in self.UNICODE_ESCAPE_CHARACTERS:\n                if \"u\" in prefix:\n                    pass\n                elif (_PY3K or self._unicode_literals) and \"b\" not in prefix:\n                    pass  # unicode by default\n                else:\n                    self.add_message(\n                        \"anomalous-unicode-escape-in-string\",\n                        line=start_row,\n                        args=(match,),\n                    )\n            elif next_char not in self.ESCAPE_CHARACTERS:\n                self.add_message(\n                    \"anomalous-backslash-in-string\", line=start_row, args=(match,)\n                )\n            # Whether it was a valid escape or not, backslash followed by\n            # another character can always be consumed whole: the second\n            # character can never be the start of a new backslash escape.\n            i += 2"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.for flat segmentation.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated times for the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n       _ = self.data['_']\n       __ =_.reshape(-1, 1)\n       _idxs = np.arange(len(self.data['_']))\n       _labels = np.zeros(len(self.data[' est_']))\n\n       _labels[0] = self.data[' est_'][0]\n       _idxs[0] = 0\n\n       _labels[1:] =_labels[:-1] + 1\n\n       _idxs[1:] =_idxs[:-1] +_labels[:-1]", "solution": "\n        self.config[\"hier\"] = False\n        est_idxs, est_labels, F = self.process()\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1\n        return self._postprocess(est_idxs, est_labels)"}
{"prompt": "def finish_registration(self, heart):\n        \"\"\"Second half of engine registration, called after our HeartMonitor\n        has received a beat from the Engine's Heart.\"\"\"", "output": "\n       _ = self.heart_monitor.get__()\n       _ = self.heart_monitor.get__()\n       _ = self.heart_monitor.get_ user_id_()\n       _ = self.heart_monitor.get_ user_name_()\n       _ = self.heart_monitor.get_ user_email_()\n       _ = self.heart_monitor.get_ user_phone_()\n       _ = self.heart_monitor.get_ user_address_()\n       _ = self.heart_monitor.", "solution": "\n        try:\n            (eid,queue,reg,purge) = self.incoming_registrations.pop(heart)\n        except KeyError:\n            self.log.error(\"registration::tried to finish nonexistant registration\", exc_info=True)\n            return\n        self.log.info(\"registration::finished registering engine %i:%r\", eid, queue)\n        if purge is not None:\n            purge.stop()\n        control = queue\n        self.ids.add(eid)\n        self.keytable[eid] = queue\n        self.engines[eid] = EngineConnector(id=eid, queue=queue, registration=reg,\n                                    control=control, heartbeat=heart)\n        self.by_ident[queue] = eid\n        self.queues[eid] = list()\n        self.tasks[eid] = list()\n        self.completed[eid] = list()\n        self.hearts[heart] = eid\n        content = dict(id=eid, queue=self.engines[eid].queue.decode('ascii'))\n        if self.notifier:\n            self.session.send(self.notifier, \"registration_notification\", content=content)\n        self.log.info(\"engine::Engine Connected: %i\", eid)"}
{"prompt": "def get_ticker_price(self, ticker,\n                         startDate=None, endDate=None,\n                         fmt='json', frequency='daily'):\n        \"\"\"By default, return latest EOD Composite Price for a stock ticker.\n           On average, each feed contains 3 data sources.\n\n            Supported tickers + Available Day Ranges are here:\n            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip\n\n            Args:\n                ticker (string): Unique identifier for stock ticker\n                startDate (string): Start of ticker range in YYYY-MM-DD format\n                endDate (string): End of ticker range in YYYY-MM-DD format\n                fmt (string): 'csv' or 'json'\n                frequency (string): Resample frequency\n        \"\"\"", "output": " of ticker range in YYYY-MM-DD format\n                fmt (string): Output format (json or csv)\n                frequency (string): Frequency of data (daily, weekly, monthly)\n\n            Returns:\n                DataFrame or JSON: DataFrame with ticker data or JSON with ticker data\n        \"\"\"\n        # Check if valid ticker\n        if not self.is_valid_ticker(ticker):\n            raise ValueError(f\"Invalid ticker: {ticker}\")\n\n        # Check if valid frequency\n        if frequency not in ['daily', 'weekly','monthly']:\n            raise ValueError(f\"Invalid frequency: {frequency}\")\n\n        # Check if valid date range\n        if", "solution": "\n        url = self._get_url(ticker, frequency)\n        params = {\n            'format': fmt if fmt != \"object\" else 'json',  # conversion local\n            'resampleFreq': frequency\n        }\n\n        if startDate:\n            params['startDate'] = startDate\n        if endDate:\n            params['endDate'] = endDate\n\n        # TODO: evaluate whether to stream CSV to cache on disk, or\n        # load as array in memory, or just pass plain text\n        response = self._request('GET', url, params=params)\n        if fmt == \"json\":\n            return response.json()\n        elif fmt == \"object\":\n            data = response.json()\n            return [dict_to_object(item, \"TickerPrice\") for item in data]\n        else:\n            return response.content.decode(\"utf-8\")"}
{"prompt": "def _plant_trie(strings: _List[str]) -> dict:\n    \"\"\"Create a Trie out of a list of words and return an atomic regex pattern.\n\n    The corresponding Regex should match much faster than a simple Regex union.\n    \"\"\"", "output": "\n    root = TrieNode()\n    for string in strings:\n        node = root\n        for char in string:\n            if char not in node.children:\n                node.children[char] = TrieNode()\n            node = node.children[char]\n        node.is_end_of_word = True\n    return root.to_regex()\n\n\ndef _regex_to_pattern(regex: str) -> str:\n    \"\"\"Convert a Regex pattern to a regular expression pattern.\"\"\"\n    pattern = \"\"\n    for char in regex:\n        if char == \".\":\n            pattern += \".\"\n        elif char == \"*\":", "solution": "\n    # plant the trie\n    trie = {}\n    for string in strings:\n        d = trie\n        for char in string:\n            d[char] = char in d and d[char] or {}\n            d = d[char]\n        d[''] = None  # EOS\n    return trie"}
{"prompt": "def normalize_profile(in_profile, log=False, return_offset = True):\n    \"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n    return_offset : bool, optional\n        return the log of the scale factor for each row\n\n    Returns\n    -------\n    tuple\n        normalized profile (fresh np object) and offset (if return_offset==True)\n    \"\"\"", "output": "\n    if log:\n        in_profile = np.log(in_profile)\n   _profile = in_profile.sum(axis=1)\n   _profile = np.divide(in_profile, np.tile(np.expand_dims(np.maximum(1e-10, np.abs(in_profile)), axis=1), (1, len(in_profile))))\n   _profile = np.multiply(np.tile(np.expand_dims(np.maximum(1e-10, np.abs(in_profile)), axis=1), (", "solution": "\n    if log:\n        tmp_prefactor = in_profile.max(axis=1)\n        tmp_prof = np.exp(in_profile.T - tmp_prefactor).T\n    else:\n        tmp_prefactor = 0.0\n        tmp_prof = in_profile\n\n    norm_vector = tmp_prof.sum(axis=1)\n    return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)),\n            (np.log(norm_vector) + tmp_prefactor) if return_offset else None)"}
{"prompt": "def delete_dashboard(self, dashboard):\n        \"\"\"**Description**\n            Deletes a dashboard.\n\n        **Arguments**\n            - **dashboard**: the dashboard object as returned by :func:`~SdcClient.get_dashboards`.\n\n        **Success Return Value**\n            `None`.\n\n        **Example**\n            `examples/delete_dashboard.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_dashboard.py>`_\n        \"\"\"", "output": "\n        self.sdc_client.delete_dashboard(dashboard)\n\n    def get_dashboard_by_id(self, dashboard_id):\n        \"\"\"**Description**\n            Retrieves a dashboard by its ID.\n\n        **Arguments**\n            - **dashboard_id**: the ID of the dashboard to retrieve.\n\n        **Success Return Value**\n            The dashboard object as returned by :func:`~SdcClient.get_dashboards`.\n\n        **Example**\n            `examples/get_dashboard_by_id.py <https://github.com/draios/python-sdc-client/blob/master/examples/get", "solution": "\n        if 'id' not in dashboard:\n            return [False, \"Invalid dashboard format\"]\n\n        res = requests.delete(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        return [True, None]"}
{"prompt": "def _make_masked_initializer(mask, initializer):\n  \"\"\"Returns a masked version of the given initializer.\"\"\"", "output": "\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    if.is_tensor(mask):\n      mask = mask.numpy()\n    mask = tf.constant(mask, dtype=tf.float32)\n    masked_shape = [s if s is not None else -1 for s in shape]\n    masked_shape = [s if s!= -1 elseask.shape[i] for i, s in enumerate(masked_shape)]\n    masked_shape = tuple(masked_shape)\n    masked_dtype =.dtype_util.common_dtype", "solution": "\n  initializer = tf.keras.initializers.get(initializer)\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    # If no `partition_info` is given, then don't pass it to `initializer`, as\n    # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which\n    # don't accept a `partition_info` argument).\n    if partition_info is None:\n      x = initializer(shape, dtype)\n    else:\n      x = initializer(shape, dtype, partition_info)\n    return tf.cast(mask, x.dtype) * x\n  return masked_initializer"}
{"prompt": "def wipe(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"", "output": "\n    try:\n        client = MongoClient()\n        db = client[ctx.guild.id]\n        db.drop_database()\n        await ctx.send(\"Database wiped.\")\n    except Exception as e:\n        await ctx.send(f\"An error occurred: {e}\")\n\n@bot.command()\n@commands.has_permissions(manage_messages=True)\nasync def purge(ctx, amount: int):\n    \"\"\"Delete a certain amount of messages from the channel.\"\"\"\n    try:\n        await ctx.channel.purge(limit=amount+1)\n        await ctx.send(f\"", "solution": "\n    LOG.info(\"Running scout wipe\")\n    db_name = ctx.obj['mongodb']\n    LOG.info(\"Dropping database %s\", db_name)\n    try:\n        ctx.obj['client'].drop_database(db_name)\n    except Exception as err:\n        LOG.warning(err)\n        ctx.abort()\n    LOG.info(\"Dropped whole database\")"}
{"prompt": "def new_contact(cls, address_book, supported_private_objects, version,\n            localize_dates):\n        \"\"\"Use this to create a new and empty contact.\"\"\"", "output": "\n        return cls(address_book, supported_private_objects, version,\n                   localize_dates, None)\n\n    def __init__(self, address_book, supported_private_objects, version,\n                 localize_dates,fn):\n        \"\"\"Create a new contact.\n\n        Args:\n            address_book (AddressBook): The address book to which this contact\n                belongs.\n            supported_private_objects (list): A list of supported private\n                objects.\n            version (int): The version of the contact format.\n            localize_dates (bool): Whether to localize dates in the contact.\n            out", "solution": "\n        return cls(address_book, None, supported_private_objects, version,\n                localize_dates)"}
{"prompt": "def sample_variants(self, variants, sample_name, category = 'snv'):\n        \"\"\"Given a list of variants get variant objects found in a specific patient\n\n        Args:\n            variants(list): a list of variant ids\n            sample_name(str): a sample display name\n            category(str): 'snv', 'sv' ..\n\n        Returns:\n            result(iterable(Variant))\n        \"\"\"", "output": "\n       _variants = []\n        for variant_id in variants:\n            variant = self.get_variant(variant_id)\n            if variant.sample_name == sample_name and variant.category == category:\n               _variants.append(variant)\n        return iter(sorted(set(self.filter_variants(variants=list(set(variants))),_variants)))\n\n    def get_variant(self, variant_id):\n        \"\"\"Get a variant object by its id\n\n        Args:\n            variant_id(str): the variant id\n\n        Returns:\n            variant(Variant)\n        \"\"\"\n        return next((", "solution": "\n        LOG.info('Retrieving variants for subject : {0}'.format(sample_name))\n        has_allele = re.compile('1|2') # a non wild-type allele is called at least once in this sample\n\n        query = {\n            '$and': [\n                {'_id' : { '$in' : variants}},\n                {'category' : category},\n                {'samples': {\n                    '$elemMatch': { 'display_name' : sample_name, 'genotype_call': { '$regex' : has_allele } }\n                }}\n            ]\n        }\n\n        result = self.variant_collection.find(query)\n        return result"}
{"prompt": "def _register_by_id_managed_msg(self, msgid, line, is_disabled=True):\n        \"\"\"If the msgid is a numeric one, then register it to inform the user\n        it could furnish instead a symbolic msgid.\"\"\"", "output": "\n        if msgid.isdigit():\n            self.register_message(msgid, line, is_disabled)\n\n    def register_message(self, msgid, line, is_disabled=True):\n        \"\"\"Register a message to be displayed to the user.\n\n        :param msgid: The message id, either a symbolic or a numeric one.\n        :param line: The message line to display.\n        :param is_disabled: Whether the message should be disabled or not.\n        \"\"\"\n        if isinstance(msgid, str):\n            msgid = int(msgid)\n        self.messages[msgid] = (line, is", "solution": "\n        try:\n            message_definitions = self.msgs_store.get_message_definitions(msgid)\n            for message_definition in message_definitions:\n                if msgid == message_definition.msgid:\n                    MessagesHandlerMixIn.__by_id_managed_msgs.append(\n                        (\n                            self.current_name,\n                            message_definition.msgid,\n                            message_definition.symbol,\n                            line,\n                            is_disabled,\n                        )\n                    )\n        except UnknownMessageError:\n            pass"}
{"prompt": "def pretty(obj, verbose=False, max_width=79, newline='\\n'):\n    \"\"\"\n    Pretty print the object's representation.\n    \"\"\"", "output": "\n    def _pretty_print(obj, level, max_width, newline):\n        if isinstance(obj, (list, tuple)):\n            if len(obj) == 0:\n                return '()'\n            elif len(obj) == 1:\n                return f'({_pretty_print(obj[0], level, max_width, newline)})'\n            else:\n                return f'({newline.join([_pretty_print(x, level, max_width, newline) for x in obj])})'\n        elif isinstance(obj, dict):\n            if len(obj) == 0:\n               ", "solution": "\n    stream = StringIO()\n    printer = RepresentationPrinter(stream, verbose, max_width, newline)\n    printer.pretty(obj)\n    printer.flush()\n    return stream.getvalue()"}
{"prompt": "def __match_intervals(intervals_from, intervals_to, strict=True):  # pragma: no cover\n    \"\"\"Numba-accelerated interval matching algorithm.\n\n    \"\"\"", "output": "\n   _intervals = []\n   _intervals_to = []\n   _intervals_from = []\n\n    for i, interval_to in enumerate(intervals_to):\n       _intervals_to.append(interval_to)\n       _intervals_from.append(intervals_from[i])\n\n   _intervals_to.sort(key=lambda x: x[0])\n   _intervals_from.sort(key=lambda x: x[0])\n\n   _intervals_from_index = 0\n    for i, interval_to in enumerate(intervals_to):\n        while (", "solution": "\n    # sort index of the interval starts\n    start_index = np.argsort(intervals_to[:, 0])\n\n    # sort index of the interval ends\n    end_index = np.argsort(intervals_to[:, 1])\n\n    # and sorted values of starts\n    start_sorted = intervals_to[start_index, 0]\n    # and ends\n    end_sorted = intervals_to[end_index, 1]\n\n    search_ends = np.searchsorted(start_sorted, intervals_from[:, 1], side='right')\n    search_starts = np.searchsorted(end_sorted, intervals_from[:, 0], side='left')\n\n    output = np.empty(len(intervals_from), dtype=numba.uint32)\n    for i in range(len(intervals_from)):\n        query = intervals_from[i]\n\n        # Find the intervals that start after our query ends\n        after_query = search_ends[i]\n        # And the intervals that end after our query begins\n        before_query = search_starts[i]\n\n        # Candidates for overlapping have to (end after we start) and (begin before we end)\n        candidates = set(start_index[:after_query]) & set(end_index[before_query:])\n\n        # Proceed as before\n        if len(candidates) > 0:\n            output[i] = __match_interval_overlaps(query, intervals_to, candidates)\n        elif strict:\n            # Numba only lets us use compile-time constants in exception messages\n            raise ParameterError\n        else:\n            # Find the closest interval\n            # (start_index[after_query] - query[1]) is the distance to the next interval\n            # (query[0] - end_index[before_query])\n            dist_before = np.inf\n            dist_after = np.inf\n            if search_starts[i] > 0:\n                dist_before = query[0] - end_sorted[search_starts[i]-1]\n            if search_ends[i] + 1 < len(intervals_to):\n                dist_after = start_sorted[search_ends[i]+1] - query[1]\n            if dist_before < dist_after:\n                output[i] = end_index[search_starts[i]-1]\n            else:\n                output[i] = start_index[search_ends[i]+1]\n    return output"}
{"prompt": "def on_key_press(self,symbol,modifiers):\n        \"\"\"\n        Keyboard event handler handling only the escape key.\n        \n        If an escape key press is detected, mouse exclusivity is toggled via :py:meth:`PengWindow.toggle_exclusivity()`\\ .\n        \"\"\"", "output": "\n        if symbol == pygame.K_ESCAPE:\n            self.toggle_exclusivity()\n    \n    def toggle_exclusivity(self):\n        \"\"\"\n        Toggles mouse exclusivity for the window.\n        \n        If mouse exclusivity is currently enabled, it is disabled. If it is currently disabled, it is enabled.\n        \"\"\"\n        if self.mouse_exclusivity:\n            self.mouse_exclusivity = False\n            pygame.mouse.set_visible(True)\n        else:\n            self.mouse_exclusivity = True\n            pygame.mouse.set_visible(False)", "solution": "\n        if symbol == key.ESCAPE:\n            self.world.peng.window.toggle_exclusivity()\n            return pyglet.event.EVENT_HANDLED"}
{"prompt": "def pprint_simple_array(val, displaywidth, msg_nocr, msg, lineprefix=''):\n    \"\"\"Try to pretty print a simple case where a list is not nested.\n    Return True if we can do it and False if not. \"\"\"", "output": "\n    if not isinstance(val, list):\n        return False\n    if len(val) == 0:\n        return False\n    = val[0]\n    if isinstance(me, list):\n        return False\n    if isinstance(me, dict):\n        return False\n    if isinstance(me, str):\n        return False\n    if isinstance(me, int):\n        return False\n    if isinstance(me, float):\n        return False\n    if isinstance(me, bool):\n        return False\n    if isinstance(me, type):\n        return False\n    if isinstance(me, complex):\n        return False\n    if isinstance(me, bytes):", "solution": "\n\n    if type(val) != list:\n        return False\n\n    numeric = True\n    for i in range(len(val)):\n        if not (type(val[i]) in [bool, float, int]):\n            numeric = False\n            if not (type(val[i]) in [bool, float, int, bytes]):\n                return False\n            pass\n        pass\n    mess = columnize([repr(v) for v in val],\n                     opts={\"arrange_array\": True,\n                           \"lineprefix\": lineprefix,\n                           \"displaywidth\": int(displaywidth)-3,\n                           'ljust': not numeric})\n    msg_nocr(mess)\n    return True"}
{"prompt": "def size(self, source):\n    \"\"\"Get the size component of the given s3url. If it is a\n       directory, combine the sizes of all the files under\n       that directory. Subdirectories will not be counted unless\n       --recursive option is set.\n    \"\"\"", "output": "\n    if not self.is_s3url(source):\n        raise ValueError(\"Invalid s3url: {}\".format(source))\n\n    bucket, key = self.parse_s3url(source)\n    if self.is_dir(bucket, key):\n        if self.recursive:\n            files = self.list_files(bucket, key)\n            sizes = [self.size(f) for f in files]\n            return sum(sizes)\n        else:\n            return 0\n    else:\n        return self.get_size(bucket, key)\n\n\ndef main():\n    parser = argparse.ArgumentParser(", "solution": "\n    result = []\n    for src in self.source_expand(source):\n      size = 0\n      for f in self.s3walk(src):\n        size += f['size']\n      result.append((src, size))\n\n    return result"}
{"prompt": "def parse_compounds(compound_info, case_id, variant_type):\n    \"\"\"Get a list with compounds objects for this variant.\n\n        Arguments:\n            compound_info(str): A Variant dictionary\n            case_id (str): unique family id\n            variant_type(str): 'research' or 'clinical'\n\n        Returns:\n            compounds(list(dict)): A list of compounds\n    \"\"\"", "output": "\n    compounds = []\n    for compound in compound_info:\n        compound_dict = {}\n        compound_dict['case_id'] = case_id\n        compound_dict['variant_type'] = variant_type\n        compound_dict['compound_name'] = compound['name']\n        compound_dict['compound_id'] = compound['id']\n        compound_dict['compound_type'] = compound['type']\n        compound_dict['compound_description'] = compound['description']\n        compound_dict['compound_source'] = compound['source']\n        compound_dict['compound_url'] = compound['url", "solution": "\n    # We need the case to construct the correct id\n    compounds = []\n    if compound_info:\n        for family_info in compound_info.split(','):\n            splitted_entry = family_info.split(':')\n            # This is the family id\n            if splitted_entry[0] == case_id:\n                for compound in splitted_entry[1].split('|'):\n                    splitted_compound = compound.split('>')\n                    compound_obj = {}\n                    compound_name = splitted_compound[0]\n                    compound_obj['variant'] = generate_md5_key(compound_name.split('_') +\n                                                               [variant_type, case_id])\n\n                    try:\n                        compound_score = float(splitted_compound[1])\n                    except (TypeError, IndexError):\n                        compound_score = 0.0\n\n                    compound_obj['score'] = compound_score\n                    compound_obj['display_name'] = compound_name\n\n                    compounds.append(compound_obj)\n\n    return compounds"}
{"prompt": "def any_slug_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for SlugField\r\n    >>> result = any_field(models.SlugField())\r\n    >>> type(result)\r\n    <type 'str'>\r\n    >>> from django.core.validators import slug_re\r\n    >>> re.match(slug_re, result) is not None\r\n    True\r\n    \"\"\"", "output": "\r\n    from django.core.validators import slug_re\r\n    import random\r\n    import string\r\n\r\n    def random_slug(length=10):\r\n        return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\r\n\r\n    while True:\r\n        result = random_slug(**kwargs)\r\n        if re.match(slug_re, result) is not None:\r\n            return result\r\n\r\n\r\ndef any_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for any field\r\n    >>> result = any_field(models.CharField())\r\n    >>> type(result)\r\n    <type'str'>", "solution": "\r\n    letters = ascii_letters + digits + '_-'\r\n    return xunit.any_string(letters = letters, max_length = field.max_length)"}
{"prompt": "def factorize(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n            .C : updated values for C.\n            .U : updated values for U.\n            .R : updated values for R.\n        \"\"\"", "output": "\n      , n = self.data.shape\n       _prime = self.data.T @ self.data\n       _prime = self.data.T @ self.data @ self.data\n       _prime = self.data.T @ self.data @ self.data @ self.data\n\n       _prime_inv = np.linalg.inv(np.eye(n) -_prime)\n       _prime_inv = np.linalg.inv(np.eye(n) - 2 *_prime +_prime.T)\n       _prime_inv =", "solution": "\n\n        [prow, pcol] = self.sample_probability()\n\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._cmdinit()\n\n        self.computeUCR()"}
{"prompt": "def _deduplicate_indexed_slices(values, indices):\n    \"\"\"Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank >= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n      dimension of `values` (as in an IndexedSlices object).\n    Returns:\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n      de-duplicated version of `indices` and `summed_values` contains the sum of\n      `values` slices associated with each unique index.\n    \"\"\"", "output": " with each unique index.\n    \"\"\"\n    unique_indices = tf.unique(indices)[0]\n    summed_values = tf.reduce_sum(tf.gather(values, unique_indices), axis=0)\n    return summed_values, unique_indices\n\n\ndef_____sum(values, indices):\n    \"\"\"Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank >= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n      dimension of `values`", "solution": "\n    unique_indices, new_index_positions = tf.unique(indices)\n    summed_values = tf.unsorted_segment_sum(values,\n                                            new_index_positions,\n                                            tf.shape(unique_indices)[0])\n    return (summed_values, unique_indices)"}
{"prompt": "def token_validate_with_login(self, **kwargs):\n        \"\"\"\n        Authenticate a user with a TMDb username and password.  The user\n        must have a verified email address and be registered on TMDb.\n\n        Args:\n            request_token: The token you generated for the user to approve.\n            username: The user's username on TMDb.\n            password: The user's password on TMDb.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "\n        data = {\n           'request_token': kwargs['request_token'],\n            'username': kwargs['username'],\n            'password': kwargs['password'],\n        }\n        response = self.session.post(self.token_validate_url, data=data)\n        response.raise_for_status()\n        return response.json()\n\n    def token_validate_with_refresh_token(self, **kwargs):\n        \"\"\"\n        Authenticate a user with a refresh token.  The user must have a verified\n        email address and be registered on TMDb.\n\n        Args:\n            refresh_token: The refresh token", "solution": "\n        path = self._get_path('token_validate_with_login')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def setLang(self,lang):\n        \"\"\"\n        Sets the default language for all domains.\n        \n        For recommendations regarding the format of the language code, see\n        :py:class:`TranslationManager`\\ .\n        \n        Note that the ``lang`` parameter of both :py:meth:`translate()` and\n        :py:meth:`translate_lazy()` will override this setting.\n        \n        Also note that the code won't be checked for existence or plausibility.\n        This may cause the fallback strings to be displayed instead if the language\n        does not exist.\n        \n        Calling this method will cause the ``setlang`` action and the\n        :peng3d:event`peng3d:i18n.set_lang` event to be triggered. Note that both\n        action and event will be triggered even if the language did not actually change.\n        \n        This method also automatically updates the :confval:`i18n.lang` config value.\n        \"\"\"", "output": ":action:`setlang`lay to be sent to the server.\n        \"\"\"\n        self._lang = lang\n        self._send_action('setlang', {'lang': lang})\n\n    def set_lang(self, lang):\n        \"\"\"\n        Sets the default language for all domains.\n        \n        For recommendations regarding the format of the language code, see\n        :py:class:`TranslationManager`\\.\n        \n        Note that the ``lang`` parameter of both :py:meth:`translate()` and\n        :py:meth:`translate_lazy()` will override this setting.\n        \n        Also note that the code won't be checked for existence or", "solution": "\n        self.lang = lang\n        self.peng.cfg[\"i18n.lang\"] = lang\n        \n        if lang not in self.cache:\n            self.cache[lang]={}\n        \n        self.doAction(\"setlang\")\n        self.peng.sendEvent(\"peng3d:i18n.set_lang\",{\"lang\":self.lang,\"i18n\":self})"}
{"prompt": "def end_group(self, dedent=0, close=''):\n        \"\"\"End a group. See `begin_group` for more details.\"\"\"", "output": "\n        self.indent -= dedent\n        self.write('{}{}'.format(self.indent * self.indent_char, close))\n\n    def write(self, text):\n        \"\"\"Write text to the output stream.\"\"\"\n        self.output.write(text)\n\n    def writeln(self, text):\n        \"\"\"Write text to the output stream with a newline character.\"\"\"\n        self.output.write(text + '\\n')\n\n    def indent_level(self):\n        \"\"\"Return the current indentation level.\"\"\"\n        return self.indent\n\n    def set_indent_level(self, level):\n        \"\"\"Set the current indentation level", "solution": "\n        self.indentation -= dedent\n        group = self.group_stack.pop()\n        if not group.breakables:\n            self.group_queue.remove(group)\n        if close:\n            self.text(close)"}
{"prompt": "def revoke_token(self, token, token_type_hint, request, *args, **kwargs):\n        \"\"\"Revoke an access or refresh token.\n        \"\"\"", "output": "\n        if token_type_hint == 'access':\n            self.access_token_model.objects.filter(token=token).delete()\n        elif token_type_hint =='refresh':\n            self.refresh_token_model.objects.filter(token=token).delete()\n        else:\n            raise ValueError('Invalid token type hint.')\n\n        return Response(status=status.HTTP_200_OK)\n", "solution": "\n        if token_type_hint:\n            tok = self._tokengetter(**{token_type_hint: token})\n        else:\n            tok = self._tokengetter(access_token=token)\n            if not tok:\n                tok = self._tokengetter(refresh_token=token)\n\n        if tok:\n            request.client_id = tok.client_id\n            request.user = tok.user\n            tok.delete()\n            return True\n\n        msg = 'Invalid token supplied.'\n        log.debug(msg)\n        request.error_message = msg\n        return False"}
{"prompt": "def is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"", "output": "\n    return getattr(node, \"__postponed_evaluation__\", False)\n\n\ndef set_postponed_evaluation(node: astroid.node_classes.NodeNG, value: bool) -> None:\n    \"\"\"Set the postponed evaluation of annotations\"\"\"\n    node.__postponed_evaluation__ = value\n\n\ndef is_postponed_evaluation_enabled_for_node(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled for a node\"\"\"\n    return getattr(node, \"__postponed_evaluation", "solution": "\n    name = \"annotations\"\n    module = node.root()\n    stmt = module.locals.get(name)\n    return (\n        stmt\n        and isinstance(stmt[0], astroid.ImportFrom)\n        and stmt[0].modname == \"__future__\"\n    )"}
{"prompt": "def _log(cls, level, msg, **kwargs):\n        \"\"\"\n        Logs a message with pre-formatted prefix.\n\n        :param int level:\n            Logging level as specified in the\n            `login module <http://docs.python.org/2/library/logging.html>`_ of\n            Python standard library.\n\n        :param str msg:\n            The actual message.\n\n        \"\"\"", "output": "\n        if level >= cls.level:\n            prefix = cls.prefix.format(level=level, name=cls.name)\n            if cls.format:\n                msg = cls.format.format(msg, **kwargs)\n            if cls.stream:\n                cls.stream.write(prefix + msg + '\\n')\n            if cls.file:\n                with open(cls.file, 'a') as f:\n                    f.write(prefix + msg + '\\n')\n\n\nclass Debug(_Log):\n    level = logging.DEBUG\n    prefix = '[DEBUG] {level} {name}: '\n\n\nclass", "solution": "\n\n        logger = getattr(cls, '_logger', None) or authomatic.core._logger\n        logger.log(\n            level, ': '.join(\n                ('authomatic', cls.__name__, msg)), **kwargs)"}
{"prompt": "def print_list_lines(self, filename, first, last):\n        \"\"\"The printing (as opposed to the parsing part of a 'list'\n        command.\"\"\"", "output": "\n        with open(filename, 'r') as f:\n            lines = f.readlines()\n        for i, line in enumerate(lines):\n            if i >= first and i <= last:\n                print(line.strip())\n\n\nclass ListCommand(Command):\n    \"\"\"The 'list' command.\"\"\"\n\n    def __init__(self,):\n        super().__init__(name='list', aliases=['ls'], usage='list [filename] [first line] [last line]',\n                         description='Prints the lines of a file, starting from the specified line(s).')\n        self.usage_example = 'list", "solution": "\n        try:\n            Colors = self.color_scheme_table.active_colors\n            ColorsNormal = Colors.Normal\n            tpl_line = '%%s%s%%s %s%%s' % (Colors.lineno, ColorsNormal)\n            tpl_line_em = '%%s%s%%s %s%%s%s' % (Colors.linenoEm, Colors.line, ColorsNormal)\n            src = []\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno)\n                if not line:\n                    break\n\n                if lineno == self.curframe.f_lineno:\n                    line = self.__format_line(tpl_line_em, filename, lineno, line, arrow = True)\n                else:\n                    line = self.__format_line(tpl_line, filename, lineno, line, arrow = False)\n\n                src.append(line)\n                self.lineno = lineno\n\n            print >>io.stdout, ''.join(src)\n\n        except KeyboardInterrupt:\n            pass"}
{"prompt": "def _propagate_mean(mean, linop, dist):\n  \"\"\"Propagate a mean through linear Gaussian transformation.\"\"\"", "output": "\n  =_from_mean(mean, dist)\n  =_from_mean(mean,)\n  =_from_mean(mean,)\n  =_from_mean(mean,)\n  =_from_mean(mean,)\n  =_from_mean(mean,)\n  =_from_mean(mean,)\n  =_from_mean(mean,)\n  =_from_mean(mean,)\n  =_from_mean(mean,", "solution": "\n  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]"}
{"prompt": "def get_cdata(self, *args):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Returns-->cdata (if a SINGLE argument was provided)\n                  LIST of cdata (if a args was a tuple or list)\n        \"\"\"", "output": "\n        if len(args) == 1:\n            return self._cffi_backend.buffer(args[0])\n        else:\n            return [self._cffi_backend.buffer(arg) for arg in args]\n\n    def get_cdata_array(self, *args):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Returns-->cdata_array (if a SINGLE argument was provided)\n                  LIST of cdata_array (if a args was a tuple or list)\n        \"\"\"\n        if len(args) == 1:\n            return self._cffi_backend.buffer_", "solution": "\n        res = tuple([\n            self.from_buffer(x) for x in args\n        ])\n\n        if len(res) == 0:\n            return None\n        elif len(res) == 1:\n            return res[0]\n        else:\n            return res"}
{"prompt": "def sourceWatchdog(self):\n        \"\"\"Watchdog timer function. \n\n        Recreates sources which have not generated events in 10*interval if\n        they have watchdog set to true in their configuration\n        \"\"\"", "output": "\n        for source in self.sources:\n            if source.watchdog and source.last_event_time is not None:\n                if time.time() - source.last_event_time > 10 * source.interval:\n                    source.create()\n\n    def start(self):\n        \"\"\"Start the watchdog timer\"\"\"\n        self.timer = threading.Timer(self.interval, self.sourceWatchdog)\n        self.timer.start()\n\n    def stop(self):\n        \"\"\"Stop the watchdog timer\"\"\"\n        self.timer.cancel()\n", "solution": "\n        for i, source in enumerate(self.sources):\n            if not source.config.get('watchdog', False):\n                continue \n            sn = repr(source)\n            last = self.lastEvents.get(source, None)\n            if last:\n                try:\n                    if last < (time.time()-(source.inter*10)):\n                        log.msg(\"Trying to restart stale source %s: %ss\" % (\n                            sn, int(time.time() - last)\n                        ))\n\n                        s = self.sources.pop(i)\n                        try:\n                            s.t.stop()\n                        except Exception as e:\n                            log.msg(\"Could not stop timer for %s: %s\" % (\n                                sn, e))\n\n                        config = copy.deepcopy(s.config)\n\n                        del self.lastEvents[source]\n                        del s, source\n\n                        source = self.createSource(config)\n\n                        reactor.callLater(0, self._startSource, source)\n                except Exception as e:\n                    log.msg(\"Could not reset source %s: %s\" % (\n                        sn, e))"}
{"prompt": "def get_key_for_purpose_and_type(self, purpose, key_type):\n        \"\"\"\n        Gets a list of keys that match the purpose and key_type, and returns the first key in that list\n        Note, if there are many keys that match the criteria, the one you get back will be random from that list\n        :returns: A key object that matches the criteria\n        \"\"\"", "output": "\n        keys = self.get_keys_for_purpose_and_type(purpose, key_type)\n        if keys:\n            return keys[0]\n        else:\n            return None\n\n    def get_keys_for_purpose_and_type(self, purpose, key_type):\n        \"\"\"\n        Gets a list of keys that match the purpose and key_type\n        :returns: A list of key objects that match the criteria\n        \"\"\"\n        keys = []\n        for key in self.keys:\n            if key.purpose == purpose and key.key_type == key_type:\n                keys.append(key", "solution": "\n        key = [key for key in self.keys.values() if key.purpose == purpose and key.key_type == key_type]\n        try:\n            return key[0]\n        except IndexError:\n            return None"}
{"prompt": "def _trj_fill_run_table(self, traj, start, stop):\n        \"\"\"Fills the `run` overview table with information.\n\n        Will also update new information.\n\n        \"\"\"", "output": "\n        # Get run information\n       _run_info = traj.get_run_info(start, stop)\n\n        # Check if run information is already in the run table\n        if_run_info in self.run_table.values():\n            # Update run information\n           _run_info['start'] = start\n           _run_info['stop'] = stop\n           _run_info['___'] =_run_info['_'] +_run_info['stop'] - start\n            # Update run information\n           _", "solution": "\n\n        def _make_row(info_dict):\n            row = (info_dict['idx'],\n                   info_dict['name'],\n                   info_dict['time'],\n                   info_dict['timestamp'],\n                   info_dict['finish_timestamp'],\n                   info_dict['runtime'],\n                   info_dict['parameter_summary'],\n                   info_dict['short_environment_hexsha'],\n                   info_dict['completed'])\n            return row\n\n        runtable = getattr(self._overview_group, 'runs')\n\n        rows = []\n        updated_run_information = traj._updated_run_information\n        for idx in range(start, stop):\n            info_dict = traj._run_information[traj._single_run_ids[idx]]\n            rows.append(_make_row(info_dict))\n            updated_run_information.discard(idx)\n\n        if rows:\n            runtable.append(rows)\n            runtable.flush()\n\n        # Store all runs that are updated and that have not been stored yet\n        rows = []\n        indices = []\n        for idx in updated_run_information:\n            info_dict = traj.f_get_run_information(idx, copy=False)\n            rows.append(_make_row(info_dict))\n            indices.append(idx)\n\n        if rows:\n            runtable.modify_coordinates(indices, rows)\n\n        traj._updated_run_information = set()"}
{"prompt": "def to_service(self, service, version):\n        \"\"\"Sets the service name and version the request should target\n\n        Args:\n            service (str): The name of the service as displayed in the services.json file\n            version (str): The version of the service as displayed in the services.json file\n\n        Returns:\n            The request builder instance in order to chain calls\n        \"\"\"", "output": "\n        self.service = service\n        self.version = version\n        return self\n\n    def with_params(self, **kwargs):\n        \"\"\"Sets the parameters for the request\n\n        Args:\n            **kwargs: The parameters for the request\n\n        Returns:\n            The request builder instance in order to chain calls\n        \"\"\"\n        self.params = kwargs\n        return self\n\n    def with_headers(self, **kwargs):\n        \"\"\"Sets the headers for the request\n\n        Args:\n            **kwargs: The headers for the request\n\n        Returns:\n            The request builder instance in order to chain calls\n        \"\"\"\n        self.headers = kwargs\n        return self\n\n    def with", "solution": "\n        service_url = self._service_locator.get_service_url(service, version)\n        return self.__copy_and_set('service_url', self.__strip_trailing_slashes(service_url))"}
{"prompt": "def _cmd(self, cmd, *args, **kw):\n        \"\"\"\n        write a single command, with variable number of arguments. after the\n        command, the device must return ACK\n        \"\"\"", "output": "\n        self.write(cmd.encode('utf-8') + b' ')\n        for arg in args:\n            self.write(arg.encode('utf-8') + b' ')\n        self.write(b'\\r\\n')\n        return self.read_until(b'ACK').strip()\n\n    def get_version(self):\n        \"\"\"\n        get the version of the device\n        \"\"\"\n        return self._cmd('AT+GMR')\n\n    def get_imei(self):\n        \"\"\"\n        get the IMEI number of the device\n        \"\"\"\n        return self._cmd('AT+CG", "solution": "\n        ok = kw.setdefault('ok', False)\n\n        self._wakeup()\n        if args:\n            cmd = \"%s %s\" % (cmd, ' '.join(str(a) for a in args))\n        for i in xrange(3):\n            log.info(\"send: \" + cmd)\n            self.port.write(cmd + '\\n')\n            if ok:\n                ack = self.port.read(len(self.OK))  # read OK\n                log_raw('read', ack)\n                if ack == self.OK:\n                    return\n            else:\n                ack = self.port.read(len(self.ACK))  # read ACK\n                log_raw('read', ack)\n                if ack == self.ACK:\n                    return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        for line in lines:\n            self.prefilter_line(line, continue_prompt=continue_prompt)\n\n    def prefilter_line(self, line, continue_prompt=False):\n        \"\"\"Prefilter a single input line of text.\n\n        This method is called for each line of input, and is responsible\n        for performing any necessary actions based on the input.  It\n        should return the modified line of input, or None if the line\n        should be ignored.\n\n        :param line: The line of input to be prefiltered.\n        :param continue_prompt: A boolean indicating whether the user has\n           ", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def app_template_filter(self, name=None):\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"", "output": "1. The function should take one argument, the value to be filtered.\n       2. The function should return the filtered value.\n       3. The function should be decorated with the :func:`app_template_filter`\n           decorator.\n       4. The function can be used in templates by calling the filter with the\n           filter name as argument, like ``{{ my_filter(value) }}``.\n       5. The filter will be registered globally for all blueprints.\n       6. The filter can be overridden by a blueprint-specific filter with the\n           same name.\n       7. The filter will be called with the same arguments", "solution": "\n        def decorator(f):\n            self.add_app_template_filter(f, name=name)\n            return f\n        return decorator"}
{"prompt": "def _replace_at_index(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"", "output": "\n  return x[:index] + replacement + x[index+1:]\n\ndef _replace_at_indices(x, indices, replacements):\n  \"\"\"Replaces elements at supplied indices.\"\"\"\n  return [_replace_at_index(x, index, replacement) for index, replacement in zip(indices, replacements)]\n\ndef _replace_at_indices_with_list(x, indices, replacements):\n  \"\"\"Replaces elements at supplied indices with a list.\"\"\"\n  return [_replace_at_index(x, index, replacement) for index, replacement in zip(indices, replacements)]", "solution": "\n  x_new = tf.concat([x[:index], tf.expand_dims(replacement, axis=0),\n                     x[(index + 1):]], axis=0)\n  return x_new"}
{"prompt": "def converted(self):\n        \"\"\"\n        :rtype: {str: object}\n        \"\"\"", "output": "\n        return self._converted\n\n    def __getitem__(self, key):\n        return self._converted[key]\n\n    def __setitem__(self, key, value):\n        self._converted[key] = value\n\n    def __delitem__(self, key):\n        del self._converted[key]\n\n    def __contains__(self, key):\n        return key in self._converted\n\n    def __iter__(self):\n        return iter(self._converted)\n\n    def __len__(self):\n        return len(self._converted)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self._", "solution": "\n        if self._converted is None:\n            self._converted = self.converter(self.name, self.value)\n        return self._converted"}
{"prompt": "def categories(self):\n        \"\"\"\n        Return the list of levels for an enum (categorical) column.\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"", "output": "\n        if self.n_levels > 0:\n            return self.levels\n        else:\n            raise ValueError(\"The column is not a categorical column.\")\n\n    def rename_categories(self, mapping):\n        \"\"\"\n        Rename the categories in an enum (categorical) column.\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"\n        if self.n_levels > 0:\n            self.levels = [mapping.get(level, level) for level in self.levels]\n        else:\n            raise ValueError(\"The column is not a categorical column.\")\n\n    def drop_categories(self, categories):", "solution": "\n        if self.ncols != 1:\n            raise H2OValueError(\"This operation only applies to a single factor column\")\n        if self.types[self.names[0]] != \"enum\":\n            raise H2OValueError(\"Input is not a factor. This operation only applies to a single factor column\")\n        return self.levels()[0]"}
{"prompt": "def mixin(cls, mixin_cls):\n        \"\"\"Decorator for mixing in additional functionality into field type\n\n        Example:\n\n        >>> @Integer.mixin\n        ... class IntegerPostgresExtensions:\n        ...     postgres_type = 'INT'\n        ...\n        ...     def postgres_dump(self, obj):\n        ...         self.dump(obj) + \"::integer\"\n\n        Is roughly equivalent to:\n\n        >>> Integer.postgres_type = 'INT'\n        ...\n        ... def postgres_dump(self, obj):\n        ...     self.dump(obj) + \"::integer\"\n        ...\n        ... Integer.postgres_dump = postgres_dump\n\n        \"\"\"", "output": "dump\n\n        \"\"\"\n        def decorator(cls):\n            for attr in dir(mixin_cls):\n                if not attr.startswith('__'):\n                    setattr(cls, attr, getattr(mixin_cls, attr))\n            return cls\n        return decorator(cls)\n\n\nclass Integer(Field):\n    \"\"\"Field for storing integer values\"\"\"\n    postgres_type = 'INT'\n\n    def __init__(self, *args, **kwargs):\n        super(Integer, self).__init__(*args, **kwargs)\n\n    def dump(self, value):\n        if value is None:\n            return 'NULL'\n        return str(value)\n\n   ", "solution": "\n        for item_name in dir(mixin_cls):\n            if item_name.startswith(\"__\"):\n                # don't copy magic properties\n                continue\n            item = getattr(mixin_cls, item_name)\n\n            if isinstance(item, types.MethodType):\n                # unbound method will cause problems\n                # so get the underlying function instead\n                item = item.im_func\n\n            setattr(cls, item_name, item)\n        return mixin_cls"}
{"prompt": "def get_annotation_data_between_times(self, id_tier, start, end):\n        \"\"\"Gives the annotations within the times.\n        When the tier contains reference annotations this will be returned,\n        check :func:`get_ref_annotation_data_between_times` for the format.\n\n        :param str id_tier: Name of the tier.\n        :param int start: Start time of the annotation.\n        :param int end: End time of the annotation.\n        :returns: List of annotations within that time.\n        :raises KeyError: If the tier is non existent.\n        \"\"\"", "output": "\n        if id_tier not in self.tiers:\n            raise KeyError(\"Tier '{}' does not exist.\".format(id_tier))\n\n        annotations = []\n        for annotation in self.tiers[id_tier]:\n            if annotation.start_time >= start and annotation.end_time <= end:\n                annotations.append(annotation)\n\n        return annotations\n\n    def get_ref_annotation_data_between_times(self, id_tier, start, end):\n        \"\"\"Gives the reference annotations within the times.\n        When the tier contains reference annotations this will be returned,\n        check :func:`get_annotation", "solution": "\n        if self.tiers[id_tier][1]:\n            return self.get_ref_annotation_data_between_times(\n                id_tier, start, end)\n        anns = ((self.timeslots[a[0]], self.timeslots[a[1]], a[2])\n                for a in self.tiers[id_tier][0].values())\n        return sorted(a for a in anns if a[1] >= start and a[0] <= end)"}
{"prompt": "def __deftype_method_recur_to_py_ast(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"", "output": "\n    # TODO: Implement this\n    raise NotImplementedError(\"`recur` inside `deftype*` method is not implemented\")\n\n\ndef __deftype_method_recur_to_py_ast_impl(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"\n    # TODO: Implement this\n    raise NotImplementedError(\"`recur` inside `deftype*` method is not implemented\")\n\n\ndef __deftype_method_recur_to_py_ast_impl_impl(\n    ctx: Generator", "solution": "\n    assert node.op == NodeOp.RECUR\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n    assert this_entry is not None, \"Field type local must have this\"\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain(\n                    [\n                        ast.NameConstant(ctx.recur_point.is_variadic),\n                        ast.Name(id=this_entry.munged, ctx=ast.Load()),\n                    ],\n                    recur_nodes,\n                )\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )"}
{"prompt": "def _random_adjspecies_pair():\n    \"\"\"Return an ordered 2-tuple containing a species and a describer.\"\"\"", "output": "\n   _species =_species_list()\n   _desc =_desc_list()\n   _species_adj = random.choice(list(set(random.sample(list(set(ir_species_list())), 2))))\n   _desc_adj = random.choice(list(set(random.sample(list(set(ir_desc_list())), 2))))\n    return (ir_species_list().index(ir_species_list().index(ir_species_list().index(ir_species_list().index(ir_species_list().index", "solution": "\n    describer, desc_position = random_describer()\n    if desc_position == 'prefix':\n        return (describer, random_species())\n    elif desc_position == 'suffix':\n        return (random_species(), describer)"}
{"prompt": "def _run_cmd_line_code(self):\n        \"\"\"Run code or file specified at the command-line\"\"\"", "output": "\n        if self.args.file:\n            with open(self.args.file, 'r') as f:\n                code = f.read()\n        else:\n            code = self.args.code\n\n        try:\n            exec(code, globals(), locals())\n        except Exception as e:\n            print(f\"Error: {e}\")\n            sys.exit(1)\n\n    def _run_interactive_code(self):\n        \"\"\"Run code interactively\"\"\"\n        while True:\n            code = input(\"Python code: \")\n            if not code:\n                break\n            try:\n                exec(code, globals(), locals())", "solution": "\n        if self.code_to_run:\n            line = self.code_to_run\n            try:\n                self.log.info(\"Running code given at command line (c=): %s\" %\n                              line)\n                self.shell.run_cell(line, store_history=False)\n            except:\n                self.log.warn(\"Error in executing line in user namespace: %s\" %\n                              line)\n                self.shell.showtraceback()\n\n        # Like Python itself, ignore the second if the first of these is present\n        elif self.file_to_run:\n            fname = self.file_to_run\n            try:\n                self._exec_file(fname)\n            except:\n                self.log.warn(\"Error in executing file in user namespace: %s\" %\n                              fname)\n                self.shell.showtraceback()"}
{"prompt": "def _first_glimpse_sensor(self, x_t):\n        \"\"\"\n        Compute first glimpse position using down-sampled image.\n        \"\"\"", "output": "\n        =_downsample(x_t)\n        =_downsample(self.x_t_prev)\n        =.reshape(-1, 1)\n        =_downsample(self.x_t_prev_prev)\n        =_downsample(self.x_t_prev_prev_prev)\n        =.reshape(-1, 1)\n        =_downsample(self.x_t_prev_prev_prev_prev)\n        =_downsample(self.x_t_prev_", "solution": "\n        downsampled_img = theano.tensor.signal.downsample.max_pool_2d(x_t, (4,4))\n        downsampled_img = downsampled_img.flatten()\n        first_l = T.dot(downsampled_img, self.W_f)\n        if self.disable_reinforce:\n            wf_grad = self.W_f\n            if self.random_glimpse:\n                first_l = self.srng.uniform((2,), low=-1.7, high=1.7)\n        else:\n            sampled_l_t = self._sample_gaussian(first_l, self.cov)\n            sampled_pdf = self._multi_gaussian_pdf(disconnected_grad(sampled_l_t), first_l)\n            wf_grad = T.grad(T.log(sampled_pdf), self.W_f)\n            first_l = sampled_l_t\n        return first_l, wf_grad"}
{"prompt": "def makeReturnFormat(self):\n        \"\"\" Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. \"\"\"", "output": "\n        = self.m_req.m_data\n        = bytearray(len(self.m_req.m_data))\n        for i in range(len(self.m_req.m_data)):\n            if i in self.reserved_fields:\n                continue\n            if i in self.crc_fields:\n                continue\n            if i in self.checksum_fields:\n                continue\n            if i in self.checksum_fields:\n                continue\n            if i in self.checksum_fields:\n                continue\n            if i in self.checksum_fields:\n                continue\n            if i in self.checksum_", "solution": "\n        for fld in self.m_blk_a:\n            compare_fld = fld.upper()\n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                self.m_req[fld] = self.m_blk_a[fld]\n        pass"}
{"prompt": "def add_attachment(self, filename, open_file):\n        \"\"\"\n        Adds an attachment to this card.\n        \"\"\"", "output": "\n        self.attachments.append((filename, open_file))\n\n    def add_action(self, action):\n        \"\"\"\n        Adds an action to this card.\n        \"\"\"\n        self.actions.append(action)\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of this card.\n        \"\"\"\n        return self.title + \"\\n\" + self.text + \"\\n\" + \"\\n\".join([str(action) for action in self.actions]) + \"\\n\" + \"\\n\".join([\"- \" + filename for filename, _ in self.attachments])\n", "solution": "\n        fields = {\n            'api_key': self.client.api_key,\n            'token': self.client.user_auth_token\n        }\n\n        content_type, body = self.encode_multipart_formdata(\n            fields=fields,\n            filename=filename,\n            file_values=open_file\n        )\n\n        return self.fetch_json(\n            uri_path=self.base_uri + '/attachments',\n            http_method='POST',\n            body=body,\n            headers={'Content-Type': content_type},\n        )"}
{"prompt": "def configure(self, options, conf):\n        \"\"\"\n        Configure plugin.\n        \"\"\"", "output": "\n        self.options = options\n        self.conf = conf\n\n    def setup(self):\n        \"\"\"\n        Setup plugin.\n        \"\"\"\n        self.log.info(\"Setup plugin\")\n\n    def start(self):\n        \"\"\"\n        Start plugin.\n        \"\"\"\n        self.log.info(\"Start plugin\")\n\n    def stop(self):\n        \"\"\"\n        Stop plugin.\n        \"\"\"\n        self.log.info(\"Stop plugin\")\n\n    def cleanup(self):\n        \"\"\"\n        Cleanup plugin.\n        \"\"\"\n        self.log.info(\"Cleanup plugin\")\n", "solution": "\n        try:\n            self.status.pop('active')\n        except KeyError:\n            pass\n        super(Coverage, self).configure(options, conf)\n        if conf.worker:\n            return\n        if self.enabled:\n            try:\n                import coverage\n            except ImportError:\n                log.error(\"Coverage not available: \"\n                          \"unable to import coverage module\")\n                self.enabled = False\n                return\n        self.conf = conf\n        self.coverErase = options.cover_erase\n        self.coverTests = options.cover_tests\n        self.coverPackages = []\n        if options.cover_packages:\n            for pkgs in [tolist(x) for x in options.cover_packages]:\n                self.coverPackages.extend(pkgs)\n        self.coverInclusive = options.cover_inclusive\n        if self.coverPackages:\n            log.info(\"Coverage report will include only packages: %s\",\n                     self.coverPackages)\n        self.coverHtmlDir = None\n        if options.cover_html:\n            self.coverHtmlDir = options.cover_html_dir\n            log.debug('Will put HTML coverage report in %s', self.coverHtmlDir)\n        self.coverBranches = options.cover_branches\n        self.coverXmlFile = None\n        if options.cover_min_percentage:\n            self.coverMinPercentage = int(options.cover_min_percentage.rstrip('%'))\n        if options.cover_xml:\n            self.coverXmlFile = options.cover_xml_file\n            log.debug('Will put XML coverage report in %s', self.coverXmlFile)\n        if self.enabled:\n            self.status['active'] = True\n            self.coverInstance = coverage.coverage(auto_data=False,\n                branch=self.coverBranches, data_suffix=None)"}
{"prompt": "def _select_next_server(self):\n        \"\"\"\n        Looks up in the server pool for an available server\n        and attempts to connect.\n        \"\"\"", "output": "\n        if not self.servers:\n            raise NoServersAvailable(\"No servers available\")\n\n        server = self.servers.pop(0)\n        try:\n            self.connection = self.connect(server)\n            self.logger.info(\"Connected to server %s\", server)\n            return server\n        except Exception as e:\n            self.logger.error(\"Failed to connect to server %s: %s\", server, e)\n            self.servers.append(server)\n            return self._select_next_server()\n\n    def _select_server(self, server_name):\n        \"\"\"\n        Looks up in the server", "solution": "\n\n        while True:\n            if len(self._server_pool) == 0:\n                self._current_server = None\n                raise ErrNoServers\n\n            now = time.monotonic()\n            s = self._server_pool.pop(0)\n            if self.options[\"max_reconnect_attempts\"] > 0:\n                if s.reconnects > self.options[\"max_reconnect_attempts\"]:\n                    # Discard server since already tried to reconnect too many times\n                    continue\n\n            # Not yet exceeded max_reconnect_attempts so can still use\n            # this server in the future.\n            self._server_pool.append(s)\n            if s.last_attempt is not None and now < s.last_attempt + self.options[\"reconnect_time_wait\"]:\n                # Backoff connecting to server if we attempted recently.\n                yield from asyncio.sleep(self.options[\"reconnect_time_wait\"], loop=self._loop)\n            try:\n                s.last_attempt = time.monotonic()\n                r, w = yield from asyncio.open_connection(\n                    s.uri.hostname,\n                    s.uri.port,\n                    loop=self._loop,\n                    limit=DEFAULT_BUFFER_SIZE)\n                self._current_server = s\n\n                # We keep a reference to the initial transport we used when\n                # establishing the connection in case we later upgrade to TLS\n                # after getting the first INFO message. This is in order to\n                # prevent the GC closing the socket after we send CONNECT\n                # and replace the transport.\n                #\n                # See https://github.com/nats-io/asyncio-nats/issues/43\n                self._bare_io_reader = self._io_reader = r\n                self._bare_io_writer = self._io_writer = w\n                break\n            except Exception as e:\n                s.last_attempt = time.monotonic()\n                s.reconnects += 1\n\n                self._err = e\n                if self._error_cb is not None:\n                    yield from self._error_cb(e)\n                continue"}
{"prompt": "def time_remaining_est(self, completed_iter):\n        \"\"\"Estimate the remaining time left.\n\n        Parameters:\n            completed_iter (int): Number of iterations completed.\n\n        Returns:\n            est_time: Estimated time remaining.\n        \"\"\"", "output": "\n       _iter = self.num_iters - completed_iter\n        est_time = self.time_per_iter * (completed_iter + 1)\n        return est_time\n\n    def estimate_time_remaining(self,_iter):\n        \"\"\"Estimate the remaining time left for a given number of iterations.\n\n        Parameters:\n            completed_iter (int): Number of iterations completed.\n\n        Returns:\n            est_time: Estimated time remaining.\n        \"\"\"\n        if self.num_iters == 0:\n            return 0\n        elif self.num_iters == 1:\n            return self.", "solution": "\n        if completed_iter:\n            t_r_est = (time.time() - self.t_start) / \\\n                completed_iter*(self.iter-completed_iter)\n        else:\n            t_r_est = 0\n        date_time = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=t_r_est)\n        time_string = \"%02d:%02d:%02d:%02d\" % \\\n            (date_time.day - 1, date_time.hour, date_time.minute, date_time.second)\n\n        return time_string"}
{"prompt": "def bkg_subtract(self, analytes=None, errtype='stderr', focus_stage='despiked'):\n        \"\"\"\n        Subtract calculated background from data.\n\n        Must run bkg_calc first!\n\n        Parameters\n        ----------\n        analytes : str or iterable\n            Which analyte(s) to subtract.\n        errtype : str\n            Which type of error to propagate. default is 'stderr'.\n        focus_stage : str\n            Which stage of analysis to apply processing to. \n            Defaults to 'despiked' if present, or 'rawdata' if not. \n            Can be one of:\n            * 'rawdata': raw data, loaded from csv file.\n            * 'despiked': despiked data.\n            * 'signal'/'background': isolated signal and background data.\n              Created by self.separate, after signal and background\n              regions have been identified by self.autorange.\n            * 'bkgsub': background subtracted data, created by \n              self.bkg_correct\n            * 'ratios': element ratio data, created by self.ratio.\n            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.\n        \"\"\"", "output": " 'despiked' - subtract spikes from data\n            * 'rawdata' - subtract data from data\n            * 'both' - subtract spikes and data from data\n\n        Returns\n        -------\n       _data : pandas.DataFrame\n            Data with background subtracted.\n        \"\"\"\n        if analytes is None:\n            analytes = self.analytes\n\n       _data = self.data[self.data['focus_stage'] == focus_stage]\n       _data = self.bkg_calc(analytes, errtype, focus_stage)\n\n        for analyte", "solution": "\n        if analytes is None:\n            analytes = self.analytes\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        if focus_stage == 'despiked':\n            if 'despiked' not in self.stages_complete:\n                focus_stage = 'rawdata'\n\n        # make uncertainty-aware background interpolators\n        bkg_interps = {}\n        for a in analytes:\n            bkg_interps[a] = un_interp1d(x=self.bkg['calc']['uTime'],\n                                         y=un.uarray(self.bkg['calc'][a]['mean'],\n                                                     self.bkg['calc'][a][errtype]))\n        self.bkg_interps = bkg_interps\n\n        # apply background corrections\n        with self.pbar.set(total=len(self.data), desc='Background Subtraction') as prog:\n            for d in self.data.values():\n                # [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), None, focus_stage=focus_stage) for a in analytes]\n                [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), ~d.sig, focus_stage=focus_stage) for a in analytes]\n                d.setfocus('bkgsub')\n\n                prog.update()\n\n        self.stages_complete.update(['bkgsub'])\n        self.focus_stage = 'bkgsub'\n        return"}
{"prompt": "def find_bp(self, filename, lineno, frame):\n        \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n        Called only if we know there is a bpt at this\n        location.  Returns breakpoint that was triggered and a flag\n        that indicates if it is ok to delete a temporary breakpoint.\n\n        \"\"\"", "output": "\n        # Check if this is a temporary breakpoint\n        if.is_temporary_breakpoint(filename, lineno):\n            return None, True\n\n        # Check if this is a breakpoint\n        for bp in.breakpoints:\n            if bp.filename == filename and bp.lineno == lineno:\n                return bp, False\n\n        # If we get here, there is no matching breakpoint\n        return None, False\n\n    def delete_temporary_breakpoint(self, bp):\n        \"\"\"Delete a temporary breakpoint.\"\"\"\n        if bp is not None:\n          .delete_temporary_breakpoint(bp.filename, bp.lineno)\n\n    def delete_", "solution": "\n        possibles = self.bplist[filename, lineno]\n        for i in range(0, len(possibles)):\n            b = possibles[i]\n            if not b.enabled:\n                continue\n            if not checkfuncname(b, frame):\n                continue\n            # Count every hit when bp is enabled\n            b.hits += 1\n            if not b.condition:\n                # If unconditional, and ignoring, go on to next, else\n                # break\n                if b.ignore > 0:\n                    b.ignore = b.ignore -1\n                    continue\n                else:\n                    # breakpoint and marker that's ok to delete if\n                    # temporary\n                    return (b, True)\n            else:\n                # Conditional bp.\n                # Ignore count applies only to those bpt hits where the\n                # condition evaluates to true.\n                try:\n                    val = eval(b.condition, frame.f_globals, frame.f_locals)\n                    if val:\n                        if b.ignore > 0:\n                            b.ignore = b.ignore -1\n                            # continue\n                        else:\n                            return (b, True)\n                    # else:\n                    #   continue\n                except:\n                    # if eval fails, most conservative thing is to\n                    # stop on breakpoint regardless of ignore count.\n                    # Don't delete temporary, as another hint to user.\n                    return (b, False)\n                pass\n            pass\n        return (None, None)"}
{"prompt": "def _to_dataframe(self):\n        \"\"\"Read and return the dataset contents as a pandas DataFrame.\"\"\"", "output": "\n       _df = pd.DataFrame(self.data)\n        return self._transform_dataframe(self.transform_fn, self.transform_args, self.transform_kwargs,\n                                         self.transform_columns, self.transform_index, self.transform_index_name,\n                                         self.transform_index_type, self.transform_index_order, self.transform_index_reverse,\n                                         self.transform_index_drop, self.transform_index_rename, self.transform_index_dtype,\n                                         self.transform_index_errors, self.transform_index_errors_ignore", "solution": "\n        #TODO: figure out why passing in the opened stream directly gives invalid data\n        data = self.read_as_binary()\n        reader = BytesIO(data)\n        return deserialize_dataframe(reader, self.data_type_id)"}
{"prompt": "def create_tfs_git_client(url, token=None):\n    \"\"\"\n    Creates a TFS Git Client to pull Git repo info\n    \"\"\"", "output": "\n    if token:\n        client = TfsGitClient(url, token)\n    else:\n        client = TfsGitClient(url)\n    return client\n\n\ndef get_git_repo_info(git_client, repo_name, branch='master'):\n    \"\"\"\n    Gets the Git repo info for the given repo name and branch\n    \"\"\"\n    repo_info = git_client.get_repository_info(repo_name, branch)\n    return repo_info\n\n\ndef get_git_repo_commits(git_client, repo_name, branch='master', max_commits=None):\n   ", "solution": "\n    if token is None:\n        token = os.environ.get('TFS_API_TOKEN', None)\n\n    tfs_connection = create_tfs_connection(url, token)\n    tfs_git_client = tfs_connection.get_client('vsts.git.v4_1.git_client.GitClient')\n\n    if tfs_git_client is None:\n        msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'\n        raise RuntimeError(msg, url)\n\n    return tfs_git_client"}
{"prompt": "def canFetchMore(self, index):\n        \"\"\"Return if more data available for *index*.\"\"\"", "output": "\n        return self.data[index] is not None\n\n    def fetchMore(self, index):\n        \"\"\"Fetch more data for *index*.\"\"\"\n        if index >= len(self.data):\n            return None\n        data = self.data[index]\n        if data is None:\n            return None\n        self.data[index] = None\n        return data\n\nclass DataFetcher(object):\n    \"\"\"Fetch data from a source.\"\"\"\n\n    def __init__(self, source):\n        self.source = source\n        self.data = DataQueue()\n\n    def fetch(self, index):\n        \"\"\"Fetch data for *index*", "solution": "\n        if not index.isValid():\n            item = self.root\n        else:\n            item = index.internalPointer()\n\n        return item.canFetchMore()"}
{"prompt": "def info_formatter(info):\n    \"\"\"Produce a sequence of formatted lines from info.\n\n    `info` is a sequence of pairs (label, data).  The produced lines are\n    nicely formatted, ready to print.\n\n    \"\"\"", "output": "\n    lines = []\n    for label, data in info:\n        lines.append(f\"{label}: {data}\")\n    return lines\n\n\ndef print_info(info):\n    \"\"\"Print a sequence of formatted lines from info.\"\"\"\n    lines = info_formatter(info)\n    for line in lines:\n        print(line)\n\n\ndef print_info_with_header(header, info):\n    \"\"\"Print a sequence of formatted lines from info with a header.\"\"\"\n    lines = info_formatter(info)\n    for line in lines:\n        print(f\"{header}: {line}\")\n\n\ndef print_info_", "solution": "\n    label_len = max([len(l) for l, _d in info])\n    for label, data in info:\n        if data == []:\n            data = \"-none-\"\n        if isinstance(data, (list, tuple)):\n            prefix = \"%*s:\" % (label_len, label)\n            for e in data:\n                yield \"%*s %s\" % (label_len+1, prefix, e)\n                prefix = \"\"\n        else:\n            yield \"%*s: %s\" % (label_len, label, data)"}
{"prompt": "async def handle_request(self, request: Request\n                             ) -> typing.Tuple[AsyncGeneratorType, Response]:\n        \"\"\"\n        Wrap request with middleware.\n        :param request:\n        :return:\n        \"\"\"", "output": "\n        middleware_response = await self.middleware.handle_request(request)\n        if middleware_response:\n            return middleware_response\n\n        response = await self.handle_request_without_middleware(request)\n        return response\n\n    async def handle_request_without_middleware(self, request: Request) -> Response:\n        \"\"\"\n        Handle request without middleware.\n        :param request:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n", "solution": "\n        callback_result, response = None, None\n\n        await self._run_request_middleware(request)\n        try:\n            callback_result, response = await request.fetch_callback(self.sem)\n        except NotImplementedParseError as e:\n            self.logger.error(e)\n        except NothingMatchedError as e:\n            self.logger.error(f'<Item: {str(e).lower()}>')\n        except Exception as e:\n            self.logger.error(f'<Callback[{request.callback.__name__}]: {e}')\n        await self._run_response_middleware(request, response)\n        await self._process_response(request=request, response=response)\n        return callback_result, response"}
{"prompt": "def get_connection(host='localhost', port=27017, username=None, password=None,\n                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):\n    \"\"\"Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect\n\n    \"\"\"", "output": "\n    if uri:\n        client = MongoClient(uri, connect=False)\n    elif mongodb:\n        client = MongoClient(mongodb, connect=False)\n    else:\n        client = MongoClient(host, port, connect=False)\n\n    if username and password:\n        client.admin.authenticate(username, password)\n\n    if authdb:\n        client.admin.command(\"auth\", authdb, username, password)\n\n    return client[authdb or 'admin'] if authdb else client\n", "solution": "\n    authdb = authdb or mongodb\n    if uri is None:\n        if username and password:\n            uri = (\"mongodb://{}:{}@{}:{}/{}\"\n                   .format(quote_plus(username), quote_plus(password), host, port, authdb))\n            log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n                   .format(quote_plus(username), host, port, authdb))\n        else:\n            log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n            \n\n    LOG.info(\"Try to connect to %s\" % log_uri)\n    try:\n        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)\n    except ServerSelectionTimeoutError as err:\n        LOG.warning(\"Connection Refused\")\n        raise ConnectionFailure\n\n    LOG.info(\"Connection established\")\n    return client"}
{"prompt": "def _get_job(self, project_id, job_id):\n        \"\"\"\n        Gets a MLEngine job based on the job name.\n\n        :return: MLEngine job object if succeed.\n        :rtype: dict\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server\n        \"\"\"", "output": "\n        try:\n            request = self._mlengine_client.projects().jobs().get(name=f\"projects/{project_id}/jobs/{job_id}\")\n            response = request.execute()\n            return response\n        except HttpError as e:\n            raise e\n\n    def _create_job(self, project_id, job_name, job_config):\n        \"\"\"\n        Creates a new MLEngine job.\n\n        :param project_id: The project ID.\n        :type project_id: str\n        :param job_name: The name of the job.\n        :type job_name: str\n        :", "solution": "\n        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    # polling after 30 seconds when quota failure occurs\n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))\n                    raise"}
{"prompt": "def _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list: list[datetime.date]\n    :param before_target: closest before or after the target\n    :type before_target: bool or None\n    :returns: The closest date\n    :rtype: datetime.date or None\n    \"\"\"", "output": " to the target date\n    :rtype: datetime.date\n    \"\"\"\n    if before_target is None:\n        before_target = False\n\n    if before_target:\n        return min(date_list, key=lambda x: abs((x - target_dt).days))\n    else:\n        return max(date_list, key=lambda x: abs((x - target_dt).days))\n\n\ndef _get_date_range(start_date, end_date):\n    \"\"\"\n    This function creates a list of dates between the start and end dates.\n\n    :param start_date: The start date\n   ", "solution": "\n    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max\n    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()"}
{"prompt": "def _has_parent_of_type(node, node_type, statement):\n    \"\"\"Check if the given node has a parent of the given type.\"\"\"", "output": "\n    if node.type == node_type:\n        return True\n    if node.parent is None:\n        return False\n    return _has_parent_of_type(node.parent, node_type, statement)\n\n\ndef _get_parent_of_type(node, node_type):\n    \"\"\"Get the parent of the given node of the given type.\"\"\"\n    if node.type == node_type:\n        return node\n    if node.parent is None:\n        return None\n    return _get_parent_of_type(node.parent, node_type)\n\n\ndef _get_first_", "solution": "\n    parent = node.parent\n    while not isinstance(parent, node_type) and statement.parent_of(parent):\n        parent = parent.parent\n    return isinstance(parent, node_type)"}
{"prompt": "def _wrap_command(cmds, cls, strict=True):\n    \"\"\"Wrap a setup command\n\n    Parameters\n    ----------\n    cmds: list(str)\n        The names of the other commands to run prior to the command.\n    strict: boolean, optional\n        Whether to raise errors when a pre-command fails.\n    \"\"\"", "output": "1. If the command is not found, raise an error.\n   \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            for cmd in cmds:\n                if not.has_command(cmd):\n                    if strict:\n                        raise ValueError(f\"Command '{cmd}' not found.\")\n                    else:\n                        print(f\"Command '{cmd}' not found.\")\n                        return\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef _wrap_commands(cmds, cls, strict=True):\n    \"\"\"Wrap a setup command\n\n    Parameters\n    ----------\n    cmds: list(str", "solution": "\n    class WrappedCommand(cls):\n\n        def run(self):\n            if not getattr(self, 'uninstall', None):\n                try:\n                    [self.run_command(cmd) for cmd in cmds]\n                except Exception:\n                    if strict:\n                        raise\n                    else:\n                        pass\n            # update package data\n            update_package_data(self.distribution)\n\n            result = cls.run(self)\n            return result\n    return WrappedCommand"}
{"prompt": "def as_tuple(self):\n        \"\"\"\n        :rtype: (str, object)\n        \"\"\"", "output": "\n        return (self.name, self.value)\n\n    def __str__(self):\n        return f\"{self.name}: {self.value}\"\n\n\nclass Config:\n    def __init__(self, config_dict):\n        self.config_dict = config_dict\n\n    def __getitem__(self, item):\n        return self.config_dict[item]\n\n    def __iter__(self):\n        return iter(self.config_dict)\n\n    def __len__(self):\n        return len(self.config_dict)\n\n    def __str__(self):\n        return str(self.config_dict)\n\n   ", "solution": "\n        if self._as_tuple is None:\n            self._as_tuple = self.converted.items()[0]\n        return self._as_tuple"}
{"prompt": "def add_virtual_columns_cartesian_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", alpha=\"l\", delta=\"b\", distance=\"distance\", radians=False, center=None, center_name=\"solar_position\"):\n        \"\"\"Convert cartesian to spherical coordinates.\n\n\n\n        :param x:\n        :param y:\n        :param z:\n        :param alpha:\n        :param delta: name for polar angle, ranges from -90 to 90 (or -pi to pi when radians is True).\n        :param distance:\n        :param radians:\n        :param center:\n        :param center_name:\n        :return:\n        \"\"\"", "output": " :param center:\n        :param center_name:\n        :return:\n        \"\"\"\n       _name = \"cartesian_to_spherical\"\n       _description = \"Convert cartesian to spherical coordinates.\"\n\n       _inputs = OrderedDict()\n       _inputs[\"x\"] = x\n        if center is not None:\n            center_name = \"solar_position\"\n            center_description = \"Center of, in.\"\n            center_units = \"rad\"\n            center_default =_inputs[\"center\"] =_inputs[\"center\"] =_inputs[\"center\"] =_inputs", "solution": "\n        transform = \"\" if radians else \"*180./pi\"\n\n        if center is not None:\n            self.add_variable(center_name, center)\n        if center is not None and center[0] != 0:\n            x = \"({x} - {center_name}[0])\".format(**locals())\n        if center is not None and center[1] != 0:\n            y = \"({y} - {center_name}[1])\".format(**locals())\n        if center is not None and center[2] != 0:\n            z = \"({z} - {center_name}[2])\".format(**locals())\n        self.add_virtual_column(distance, \"sqrt({x}**2 + {y}**2 + {z}**2)\".format(**locals()))\n        # self.add_virtual_column(alpha, \"((arctan2({y}, {x}) + 2*pi) % (2*pi)){transform}\".format(**locals()))\n        self.add_virtual_column(alpha, \"arctan2({y}, {x}){transform}\".format(**locals()))\n        self.add_virtual_column(delta, \"(-arccos({z}/{distance})+pi/2){transform}\".format(**locals()))"}
{"prompt": "def unix_ts(dtval):\n    \"\"\"Convert datetime into a unix timestamp.\n    This is the equivalent to Python 3's int(datetime.timestamp()).\n\n    :param dt: datetime to convert\n    \"\"\"", "output": "\n    return int(dtval.timestamp())\n\n\ndef unix_ts_to_datetime(tsval):\n    \"\"\"Convert a unix timestamp into a datetime object.\n\n    :param ts: unix timestamp to convert\n    \"\"\"\n    return datetime.fromtimestamp(tsval)\n\n\ndef datetime_to_unix_ts(dtval):\n    \"\"\"Convert a datetime object into a unix timestamp.\n\n    :param dt: datetime to convert\n    \"\"\"\n    return int(dtval.timestamp())\n\n\ndef datetime_to_str(dtval):\n    \"\"\"Convert a datetime object into a string representation.\n\n    :param dt: datetime", "solution": "\n    epoch = datetime(1970, 1, 1, 0, 0, tzinfo=tzutc())\n    delta = (dtval - epoch)\n    return delta.days * 24 * 3600 + delta.seconds"}
{"prompt": "def path_dispatch_kwarg(mname, path_default, returns_model):\n    \"\"\"\n    Parameterized decorator for methods that accept path as a second\n    argument.\n    \"\"\"", "output": "\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            path = kwargs.pop('path', path_default)\n            if path is None:\n                raise ValueError('path argument is required')\n            if path == path_default:\n                return func(self, *args, **kwargs)\n            if not isinstance(path, str):\n                raise TypeError('path must be a string')\n            if path.startswith('/'):\n                path = path[1:]\n            if path.endswith('/'):\n                path = path[:-1]\n            if not path", "solution": "\n    def _wrapper(self, path=path_default, **kwargs):\n        prefix, mgr, mgr_path = _resolve_path(path, self.managers)\n        result = getattr(mgr, mname)(path=mgr_path, **kwargs)\n        if returns_model and prefix:\n            return _apply_prefix(prefix, result)\n        else:\n            return result\n    return _wrapper"}
{"prompt": "def get_label ( self, object ):\n        \"\"\" Gets the label to display for a specified object.\n        \"\"\"", "output": "\n        return object.label\n\n    def get_icon ( self, object ):\n        \"\"\" Gets the icon to display for a specified object.\n        \"\"\"\n        return object.icon\n\n    def get_tooltip ( self, object ):\n        \"\"\" Gets the tooltip to display for a specified object.\n        \"\"\"\n        return object.tooltip\n\n    def get_children ( self, object ):\n        \"\"\" Gets the children of a specified object.\n        \"\"\"\n        return object.children\n\n    def get_parent ( self, object ):\n        \"\"\" Gets the parent of a specified object.\n        \"\"\"\n        return object.parent\n\n    def get_data ( self, object ):\n       ", "solution": "\n        label = self.label\n        if label[:1] == '=':\n            return label[1:]\n\n        label = xgetattr( object, label, '' )\n\n        if self.formatter is None:\n            return label\n\n        return self.formatter( object, label )"}
{"prompt": "def create(self, request):\n        \"\"\"\n        Log in django staff user\n        \"\"\"", "output": "\n        if request.method == 'POST':\n            form = LoginForm(request.POST)\n            if form.is_valid():\n                username = form.cleaned_data.get('username')\n                password = form.cleaned_data.get('password')\n                user = authenticate(username=username, password=password)\n                if user is not None:\n                    if user.is_active:\n                        login(request, user)\n                        return redirect('dashboard')\n                    else:\n                        messages.error(request, 'Your account is inactive.')\n                else:\n                    messages.error(request, 'Invalid username or password.')\n        else", "solution": "\n        # TODO: Decorate api with sensitive post parameters as Django admin do?\n        # from django.utils.decorators import method_decorator\n        # from django.views.decorators.debug import sensitive_post_parameters\n        # sensitive_post_parameters_m = method_decorator(sensitive_post_parameters())\n\n        login_form = AuthenticationForm(request, data=request.data)\n\n        if not login_form.is_valid():\n            raise serializers.ValidationError(login_form.errors)\n\n        auth_login(request, login_form.get_user())\n\n        serializer = UserSerializer(request.user)\n        return Response(serializer.data, status=status.HTTP_200_OK)"}
{"prompt": "def remove(self, recursive=True, ignore_error=True):\n        \"\"\"\n        Remove the directory.\n        \"\"\"", "output": "\n        if self.exists():\n            if recursive:\n                for item in self.iterdir():\n                    item.unlink()\n            self.rmdir()\n        elif not ignore_error:\n            raise FileNotFoundError(f\"{self} does not exist.\")\n\n    def rename(self, new_name):\n        \"\"\"\n        Rename the directory.\n        \"\"\"\n        if self.exists():\n            self.rename(new_name)\n        else:\n            raise FileNotFoundError(f\"{self} does not exist.\")\n\n    def mkdir(self, mode=0o777, parents=False, exist_ok=False):\n        \"\"\"\n        Create", "solution": "\n        try:\n            if recursive or self._cleanup == 'recursive':\n                shutil.rmtree(self.path)\n            else:\n                os.rmdir(self.path)\n        except Exception as e:\n            if not ignore_error:\n                raise e"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        if status.farthest > self.farthest:\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest == self.farthest:\n            self.expected =.expected\n\n        return self\n\n    def __str__(self) -> str:\n        return f\"Status(farthest={self.farthest}, expected={self.expected})\"\n", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def activity(self, value):\n        \"\"\"\n        Used to set TurnContext._activity when a context object is created. Only takes instances of Activities.\n        :param value:\n        :return:\n        \"\"\"", "output": "\n        if not isinstance(value, Activity):\n            raise TypeError(\"activity must be an instance of Activity\")\n        self._activity = value\n\n    @property\n    def conversation(self):\n        \"\"\"\n        Used to get the Conversation object associated with the TurnContext.\n        :return:\n        \"\"\"\n        return self._activity.conversation\n\n    @property\n    def channel_id(self):\n        \"\"\"\n        Used to get the channel ID associated with the TurnContext.\n        :return:\n        \"\"\"\n        return self._activity.channel_id\n\n    @property\n    def conversation_id(self):\n        \"\"\"\n        Used to get the conversation ID associated with", "solution": "\n        if not isinstance(value, Activity):\n            raise TypeError('TurnContext: cannot set `activity` to a type other than Activity.')\n        else:\n            self._activity = value"}
{"prompt": "def _post_cutout_no_chunking_blosc(self, token, channel,\n                                       x_start, y_start, z_start,\n                                       data, resolution):\n        \"\"\"\n        Accepts data in zyx. !!!\n        \"\"\"", "output": "\n       _data = data.copy()\n       _data = np.rollaxis(np.rollaxis(np.rollaxis(np.rollaxis(\n            np.rollaxis(np.rollaxis(np.rollaxis(\n                np.rollaxis(np.rollaxis(np.rollaxis(\n                    np.rollaxis(np.rollaxis(\n                        np.rollaxis(np.rollaxis(\n                            np.rollaxis(np.rollaxis(\n                                np.rollaxis(np.rollaxis(\n                                    np.rollaxis(np.rollaxis(\n                                        np.rollaxis(np", "solution": "\n        data = numpy.expand_dims(data, axis=0)\n        blosc_data = blosc.pack_array(data)\n        url = self.url(\"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\".format(\n            token, channel,\n            resolution,\n            x_start, x_start + data.shape[3],\n            y_start, y_start + data.shape[2],\n            z_start, z_start + data.shape[1]\n        ))\n\n        req = self.remote_utils.post_url(url, data=blosc_data, headers={\n            'Content-Type': 'application/octet-stream'\n        })\n\n        if req.status_code is not 200:\n            raise RemoteDataUploadError(req.text)\n        else:\n            return True"}
{"prompt": "def current(self):\n        \"\"\" bool: Whether this window is the window in which commands are being executed. \"\"\"", "output": "\n        return self._current\n\n    def set_current(self, current):\n        \"\"\" Set whether this window is the window in which commands are being executed. \"\"\"\n        self._current = current\n\n    def set_title(self, title):\n        \"\"\" Set the title of the window. \"\"\"\n        self._title = title\n\n    def get_title(self):\n        \"\"\" Get the title of the window. \"\"\"\n        return self._title\n\n    def get_id(self):\n        \"\"\" Get the ID of the window. \"\"\"\n        return self._id\n\n    def get_position(self):\n        \"\"\" Get the position of the window. \"\"\"", "solution": "\n        try:\n            return self.driver.current_window_handle == self.handle\n        except self.driver.no_such_window_error:\n            return False"}
{"prompt": "def autodetect_files(self):\n        \"\"\" Attempt to detect requirements files in the current working directory \"\"\"", "output": "\n        for filename in ['requirements.txt','setup.py', 'Pipfile', 'Pipfile.lock']:\n            if os.path.exists(filename):\n                return filename\n        return None\n\n    def install_requirements(self, requirements_file=None):\n        \"\"\" Install requirements from the given requirements file \"\"\"\n        if requirements_file is None:\n            requirements_file = self.autodetect_files()\n            if requirements_file is None:\n                raise ValueError('Could not detect requirements file')\n\n        if not os.path.exists(requirements_file):\n            raise ValueError('Requirements file not found')\n\n        if", "solution": "\n        if self._is_valid_requirements_file('requirements.txt'):\n            self.filenames.append('requirements.txt')\n\n        if self._is_valid_requirements_file('requirements.pip'):  # pragma: nocover\n            self.filenames.append('requirements.pip')\n\n        if os.path.isdir('requirements'):\n            for filename in os.listdir('requirements'):\n                file_path = os.path.join('requirements', filename)\n                if self._is_valid_requirements_file(file_path):\n                    self.filenames.append(file_path)\n        self._check_inclusions_recursively()"}
{"prompt": "def format_extension(self):\n        \"\"\"The format extension of asset.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n        \"\"\"", "output": "\n        return os.path.splitext(self.filename)[1]\n\n    @property\n    def format_name(self):\n        \"\"\"The format name of asset.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_name\n            'js'\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.format_name\n            'external.min'\n        \"\"\"\n        return os.path.splitext(self.filename)[0].split('/')[-1].split('.')[-2]\n\n    @", "solution": "\n        for extension in reversed(self.extensions):\n            compiler = self.environment.compilers.get(extension)\n            if not compiler and self.environment.mimetypes.get(extension):\n                return extension"}
