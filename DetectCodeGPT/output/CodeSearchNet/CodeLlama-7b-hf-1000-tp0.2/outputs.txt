{"prompt": "def print_help(self, classes=False):\n        \"\"\"Print the help for each Configurable class in self.classes.\n\n        If classes=False (the default), only flags and aliases are printed.\n        \"\"\"", "output": "\n        if classes:\n            self.print_classes()\n        else:\n            self.print_flags()\n            self.print_aliases()\n\n    def print_classes(self):\n        \"\"\"Print the help for each Configurable class in self.classes.\"\"\"\n        for name in sorted(self.classes):\n            self.print_class(name)\n\n    def print_class(self, name):\n        \"\"\"Print the help for the Configurable class with the given name.\"\"\"\n        cls = self.classes[name]\n        print(cls.__", "solution": "\n        self.print_subcommands()\n        self.print_options()\n\n        if classes:\n            if self.classes:\n                print \"Class parameters\"\n                print \"----------------\"\n                print\n                for p in wrap_paragraphs(self.keyvalue_description):\n                    print p\n                    print\n\n            for cls in self.classes:\n                cls.class_print_help()\n                print\n        else:\n            print \"To see all available configurables, use `--help-all`\"\n            print"}
{"prompt": "def derive_fallback_fernet_keys(passwords, user_id):\n    \"\"\"\n    Derive a list of per-user Fernet keys from a list of master keys and a\n    username.\n\n    If a None is encountered in ``passwords``, it is forwarded.\n\n    Parameters\n    ----------\n    passwords : list[unicode]\n        List of ascii-encodable keys to derive.\n    user_id : unicode or None\n        ascii-encodable user_id to use as salt\n    \"\"\"", "output": "\n    if user_id is None:\n        return passwords\n    return [\n        derive_fernet_key(password, user_id)\n        for password in passwords\n    ]\n\n\ndef derive_fernet_key(password, user_id):\n    \"\"\"\n    Derive a Fernet key from a password and a username.\n\n    Parameters\n    ----------\n    password : unicode\n        ascii-encodable password to derive.\n    user_id : unicode\n        ascii-encodable user_id to use as salt\n    \"\"\"", "solution": "\n    # Normally I wouldn't advocate for these kinds of assertions, but we really\n    # really really don't want to mess up deriving encryption keys.\n    assert isinstance(passwords, (list, tuple)), \\\n        \"Expected list or tuple of keys, got %s.\" % type(passwords)\n\n    def derive_single_allow_none(k):\n        if k is None:\n            return None\n        return derive_single_fernet_key(k, user_id).decode('ascii')\n\n    return list(map(derive_single_allow_none, passwords))"}
{"prompt": "def format_help(self, checkerref=False):\n        \"\"\"return the help string for the given message id\"\"\"", "output": "\n        if checkerref:\n            return self.checkerref\n        else:\n            return self.help\n\n    def get_checkerref(self):\n        \"\"\"return the checkerref string for the given message id\"\"\"\n        return self.checkerref\n\n    def get_msgid(self):\n        \"\"\"return the message id for the given checkerref\"\"\"\n        return self.msgid\n\n    def get_msgid_short(self):\n        \"\"\"return the short message id for the given checkerref\"\"\"\n        return self.", "solution": "\n        desc = self.descr\n        if checkerref:\n            desc += \" This message belongs to the %s checker.\" % self.checker.name\n        title = self.msg\n        if self.symbol:\n            msgid = \"%s (%s)\" % (self.symbol, self.msgid)\n        else:\n            msgid = self.msgid\n        if self.minversion or self.maxversion:\n            restr = []\n            if self.minversion:\n                restr.append(\"< %s\" % \".\".join([str(n) for n in self.minversion]))\n            if self.maxversion:\n                restr.append(\">= %s\" % \".\".join([str(n) for n in self.maxversion]))\n            restr = \" or \".join(restr)\n            if checkerref:\n                desc += \" It can't be emitted when using Python %s.\" % restr\n            else:\n                desc += \" This message can't be emitted when using Python %s.\" % restr\n        desc = normalize_text(\" \".join(desc.split()), indent=\"  \")\n        if title != \"%s\":\n            title = title.splitlines()[0]\n\n            return \":%s: *%s*\\n%s\" % (msgid, title.rstrip(\" \"), desc)\n        return \":%s:\\n%s\" % (msgid, desc)"}
{"prompt": "def f_get(self, *args):\n        \"\"\"Returns annotations\n\n        If len(args)>1, then returns a list of annotations.\n\n        `f_get(X)` with *X* integer will return the annotation with name `annotation_X`.\n\n        If the annotation contains only a single entry you can call `f_get()` without arguments.\n        If you call `f_get()` and the annotation contains more than one element a ValueError is\n        thrown.\n\n        \"\"\"", "output": "\n        if len(args) == 0:\n            if len(self.annotations) == 1:\n                return self.annotations[0]\n            else:\n                raise ValueError(\"Annotation contains more than one entry\")\n        elif len(args) == 1:\n            return self.annotations[args[0]]\n        else:\n            return [self.annotations[i] for i in args]\n\n    def f_set(self, *args):\n        \"\"\"Sets annotations\n\n        If len(args)>1, then sets the annotations", "solution": "\n\n        if len(args) == 0:\n            if len(self._dict) == 1:\n                return self._dict[list(self._dict.keys())[0]]\n            elif len(self._dict) > 1:\n                raise ValueError('Your annotation contains more than one entry: '\n                                 '`%s` Please use >>f_get<< with one of these.' %\n                                 (str(list(self._dict.keys()))))\n            else:\n                raise AttributeError('Your annotation is empty, cannot access data.')\n\n        result_list = []\n        for name in args:\n            name = self._translate_key(name)\n            try:\n                result_list.append(self._dict[name])\n            except KeyError:\n                raise AttributeError('Your annotation does not contain %s.' % name)\n\n        if len(args) == 1:\n            return result_list[0]\n        else:\n            return tuple(result_list)"}
{"prompt": "def size(full_path):\r\n    \"\"\"Shows file size.\r\n    :param full_path: full path to the file.\r\n    \"\"\"", "output": "\"\"\r\n    return os.path.getsize(full_path)\r\n\r\n\r\ndef create_dir(path):\r\n    \"\"\"Creates a directory.\r\n    :param path: path to the directory.\r\n    \"\"\"\"\"\r\n    if not os.path.exists(path):\r\n        os.makedirs(path)\r\n\r\n\r\ndef create_file(path):\r\n    \"\"\"Creates a file.\r\n    :param path: path to the file.\r\n    \"\"\"\"\"\r\n    if not os.path.exists(path):\r\n       ", "solution": "\r\n\r\n    file_size = os.path.getsize(full_path)\r\n    str_file_size = str(file_size)\r\n    print(str_file_size, 'b')\r\n\r\n    # Show size in b, kb, mb or gb depending on the dimension\r\n    if len(str_file_size) >= 10:\r\n        print('{0:.2f}'.format(file_size / 1073741824), 'gb')\r\n    elif len(str_file_size) >= 7:\r\n        print('{0:.2f}'.format(file_size / 1048576), 'mb')\r\n    elif len(str_file_size) >= 4:\r\n        print('{0:.2f}'.format(file_size / 1024), 'kb')"}
{"prompt": "def init_cinder_consumer(self, mq):\n        \"\"\"\n        Init openstack cinder mq\n\n        1. Check if enable listening cinder notification\n        2. Create consumer\n\n        :param mq: class ternya.mq.MQ\n        \"\"\"", "output": "\n        if not self.conf.enable_cinder_notification:\n            return\n\n        self.cinder_consumer = mq.create_consumer(\n            self.conf.cinder_notification_topic,\n            self.cinder_notification_callback,\n            'cinder-notification-consumer')\n\n    def cinder_notification_callback(self, body):\n        \"\"\"\n        Callback function for cinder notification\n\n        :param body: body of notification\n        \"\"\"\n        try:\n            if body['event_type'] == 'volume", "solution": "\n        if not self.enable_component_notification(Openstack.Cinder):\n            log.debug(\"disable listening cinder notification\")\n            return\n\n        for i in range(self.config.cinder_mq_consumer_count):\n            mq.create_consumer(self.config.cinder_mq_exchange,\n                               self.config.cinder_mq_queue,\n                               ProcessFactory.process(Openstack.Cinder))\n\n        log.debug(\"enable listening openstack cinder notification.\")"}
{"prompt": "def max_global_iteration(self):\n        \"\"\"Return global iterator with last iteration number\"\"\"", "output": "\n        return self.global_iteration\n\n    def max_local_iteration(self):\n        \"\"\"Return local iterator with last iteration number\"\"\"\n        return self.local_iteration\n\n    def max_global_epoch(self):\n        \"\"\"Return global epoch with last epoch number\"\"\"\n        return self.global_epoch\n\n    def max_local_epoch(self):\n        \"\"\"Return local epoch with last epoch number\"\"\"\n        return self.local_epoch\n\n    def max_global_batch(self):\n", "solution": "\n        return self.indices_to_global_iterator({\n            symbol_pos_int(var_name): end-1 for var_name, start, end, incr in self._loop_stack\n        })"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        parser = argparse.ArgumentParser(\n            description='Runs a single test case',\n            prog='test',\n            usage='%(prog)s [options]',\n            add_help=False)\n        parser.add_argument(\n            '-h', '--help',\n            action='help',\n            help='Show this help message and exit.')\n        parser.add_argument(\n            '-v', '--verbose',\n            action='store_true',\n            help='Verbose output')\n        parser.add_argument(\n           ", "solution": "\n        ApiCli.get_arguments(self)\n\n        # Get the host group name\n        if self.args.host_group_name is not None:\n            self.host_group_name = self.args.host_group_name\n\n        # Get the list of sources separated by commas\n        if self.args.sources is not None:\n            self.sources = self.args.sources\n\n        payload = {}\n        if self.host_group_name is not None:\n            payload['name'] = self.host_group_name\n\n        if self.sources is not None:\n            source_list = str.split(self.sources, ',')\n            if 'hostnames' not in payload:\n                payload['hostnames'] = []\n\n            for s in source_list:\n                payload['hostnames'].append(s)\n        self.data = json.dumps(payload, sort_keys=True)\n        self.headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}"}
{"prompt": "def next_retry_datetime(self):\n        \"\"\"\n        Get datetime of the next retry if the task instance fails. For exponential\n        backoff, retry_delay is used as base and will be converted to seconds.\n        \"\"\"", "output": "\n        if self.retry_delay:\n            return (\n                timezone.utcnow() +\n                timedelta(seconds=self.retry_delay * (2 ** (self.retry_count - 1))))\n        else:\n            return None\n\n    @property\n    def next_run_date(self):\n        \"\"\"\n        Return the next scheduled date for the task instance.\n        \"\"\"\n        if self.start_date:\n            if self.start_date > timezone.utcnow():\n                return self.start_date\n       ", "solution": "\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            # deterministic per task instance\n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)\n            modded_hash = min_backoff + hash % min_backoff\n            # timedelta has a maximum representable value. The exponentiation\n            # here means this value can be exceeded after a certain number\n            # of tries (around 50 if the initial delay is 1s, even fewer if\n            # the delay is larger). Cap the value here before creating a\n            # timedelta object so the operation doesn't fail.\n            delay_backoff_in_seconds = min(\n                modded_hash,\n                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay"}
{"prompt": "def showCode(self, width=80):\n        \"\"\"Show all words of the code in a nice format.\n        \"\"\"", "output": "\n        if self.code is None:\n            return\n        print(\"Code:\")\n        for i, word in enumerate(self.code):\n            print(\"{:02d}: {}\".format(i, word))\n\n    def showWords(self, width=80):\n        \"\"\"Show all words of the code in a nice format.\n        \"\"\"\n        if self.words is None:\n            return\n        print(\"Words:\")\n        for i, word in enumerate(self.words):\n            print(\"{:02d}: {}\".format(i", "solution": "\n        #make table of all symbols with binary strings\n        symbolStrings = [\n            (self.bitPattern(s.index), self.mnemonic(s.index))\n            for s in self\n            ]\n        #determine column widths the way Lisp programmers do it\n        leftColWidth, rightColWidth = map(max, map(\n            map,\n            repeat(len),\n            zip(*symbolStrings)\n            ))\n        colwidth = leftColWidth+rightColWidth\n        columns = 81//(colwidth+2)\n        rows = -(-len(symbolStrings)//columns)\n        def justify(bs):\n            b,s = bs\n            return b.rjust(leftColWidth)+':'+s.ljust(rightColWidth)\n        for i in range(rows):\n            print(' '.join(map(justify, symbolStrings[i::rows])).rstrip())"}
{"prompt": "def normalize(ast: Node) -> Node:\n    \"\"\"\n    Normalize an AST nodes.\n\n    all builtins containers are replace by referencable subclasses\n    \"\"\"", "output": "\n    if isinstance(ast, (List, Dict, Set)):\n        return ast.__class__(\n            [normalize(item) for item in ast.items],\n            ast.location,\n        )\n    elif isinstance(ast, (Tuple, Starred, Arguments)):\n        return ast.__class__(\n            [normalize(item) for item in ast.items],\n            ast.location,\n        )\n    elif isinstance(ast, (Name, Constant, Ellipsis, Ellipsis2)):\n        return ast\n    elif isinstance(", "solution": "\n    res = ast\n    typemap = {DictNode, ListNode, TupleNode}\n    if type(ast) is dict:\n        res = DictNode(ast)\n    elif type(ast) is list:\n        res = ListNode(ast)\n    elif type(ast) is tuple:\n        res = TupleNode(ast)\n    # in-depth change\n    if hasattr(res, 'items'):\n        for k, v in res.items():\n            res[k] = normalize(v)\n    elif hasattr(res, '__getitem__'):\n        for idx, v in zip(range(len(res)), res):\n            res[idx] = normalize(v)\n    if type(res) not in typemap and hasattr(res, '__dict__'):\n        subattr = vars(res)\n        for k, v in subattr.items():\n            setattr(res, k, normalize(v))\n    return res"}
{"prompt": "def get_memory_maps(self):\n        \"\"\"Return process's mapped memory regions as a list of nameduples.\n        Fields are explained in 'man proc'; here is an updated (Apr 2012)\n        version: http://goo.gl/fmebo\n        \"\"\"", "output": "\n        # TODO: use /proc/self/smaps instead of /proc/self/maps\n        maps = []\n        with open('/proc/%d/maps' % self.pid) as f:\n            for line in f:\n                try:\n                    addr, perms, offset, dev, inode, pathname = line.split()\n                    maps.append(MemoryMap(addr, perms, offset, dev, inode, pathname))\n                except ValueError:\n                    pass\n        return maps\n\n    def get_memory_map(self", "solution": "\n        f = None\n        try:\n            f = open(\"/proc/%s/smaps\" % self.pid)\n            first_line = f.readline()\n            current_block = [first_line]\n\n            def get_blocks():\n                data = {}\n                for line in f:\n                    fields = line.split(None, 5)\n                    if len(fields) >= 5:\n                        yield (current_block.pop(), data)\n                        current_block.append(line)\n                    else:\n                        data[fields[0]] = int(fields[1]) * 1024\n                yield (current_block.pop(), data)\n\n            if first_line:  # smaps file can be empty\n                for header, data in get_blocks():\n                    hfields = header.split(None, 5)\n                    try:\n                        addr, perms, offset, dev, inode, path = hfields\n                    except ValueError:\n                        addr, perms, offset, dev, inode, path = hfields + ['']\n                    if not path:\n                        path = '[anon]'\n                    else:\n                        path = path.strip()\n                    yield (addr, perms, path,\n                           data['Rss:'],\n                           data['Size:'],\n                           data.get('Pss:', 0),\n                           data['Shared_Clean:'], data['Shared_Clean:'],\n                           data['Private_Clean:'], data['Private_Dirty:'],\n                           data['Referenced:'],\n                           data['Anonymous:'],\n                           data['Swap:'])\n            f.close()\n        except EnvironmentError:\n            # XXX - Can't use wrap_exceptions decorator as we're\n            # returning a generator;  this probably needs some\n            # refactoring in order to avoid this code duplication.\n            if f is not None:\n                f.close()\n            err = sys.exc_info()[1]\n            if err.errno in (errno.ENOENT, errno.ESRCH):\n                raise NoSuchProcess(self.pid, self._process_name)\n            if err.errno in (errno.EPERM, errno.EACCES):\n                raise AccessDenied(self.pid, self._process_name)\n            raise\n        except:\n            if f is not None:\n                f.close()\n            raise"}
{"prompt": "def add_path(path, config=None):\n    \"\"\"Ensure that the path, or the root of the current package (if\n    path is in a package), is in sys.path.\n    \"\"\"", "output": "\n    if path not in sys.path:\n        if config is None:\n            config = Config()\n        if path.startswith(config.package_path):\n            path = config.package_path\n        sys.path.insert(0, path)\n\n\ndef get_config():\n    \"\"\"Get the configuration for the current package.\n\n    This will return the configuration for the current package, or None if\n    the current package is not a package.\n    \"\"\"\n    if not hasattr(sys, 'frozen'):\n        return None\n    if not", "solution": "\n\n    # FIXME add any src-looking dirs seen too... need to get config for that\n    \n    log.debug('Add path %s' % path)    \n    if not path:\n        return []\n    added = []\n    parent = os.path.dirname(path)\n    if (parent\n        and os.path.exists(os.path.join(path, '__init__.py'))):\n        added.extend(add_path(parent, config))\n    elif not path in sys.path:\n        log.debug(\"insert %s into sys.path\", path)\n        sys.path.insert(0, path)\n        added.append(path)\n    if config and config.srcDirs:\n        for dirname in config.srcDirs:\n            dirpath = os.path.join(path, dirname)\n            if os.path.isdir(dirpath):\n                sys.path.insert(0, dirpath)\n                added.append(dirpath)\n    return added"}
{"prompt": "def _kl_pareto_pareto(a, b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(a || b) with a and b Pareto.\n\n  Args:\n    a: instance of a Pareto distribution object.\n    b: instance of a Pareto distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_pareto_pareto\".\n\n  Returns:\n    Batchwise KL(a || b)\n  \"\"\"", "output": "\"\"\"\n  with tf.name_scope(name, \"kl_pareto_pareto\", [a, b]):\n    a_scale = a.scale\n    a_shape = a.shape\n    b_scale = b.scale\n    b_shape = b.shape\n    a_scale_log = tf.log(a_scale)\n    b_scale_log = tf.log(b_scale)\n    a_scale_log_b_scale = a_scale_log - b_scale_log\n    a_shape_log =", "solution": "\n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 55\n    # Terminology is different from source to source for Pareto distributions.\n    # The 'concentration' parameter corresponds to 'a' in that source, and the\n    # 'scale' parameter corresponds to 'm'.\n    final_batch_shape = distribution_util.get_broadcast_shape(\n        a.concentration, b.concentration, a.scale, b.scale)\n    common_type = dtype_util.common_dtype(\n        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)\n    return tf.where(\n        a.scale >= b.scale,\n        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +\n        tf.math.log(a.concentration) - tf.math.log(b.concentration) +\n        b.concentration / a.concentration - 1.0,\n        tf.broadcast_to(tf.cast(np.inf, common_type), final_batch_shape))"}
{"prompt": "def flush(self):\n        \"\"\"Flush incomming socket messages.\"\"\"", "output": "\n        while self.socket.poll(0):\n            self.socket.recv()\n\n    def send(self, message):\n        \"\"\"Send a message to the server.\"\"\"\n        self.socket.send(message)\n\n    def recv(self):\n        \"\"\"Receive a message from the server.\"\"\"\n        return self.socket.recv()\n\n    def close(self):\n        \"\"\"Close the socket.\"\"\"\n        self.socket.close()\n\n    def __enter__(self):\n        \"\"\"Enter the context manager.\"\"\"\n        return self", "solution": "\n        debug('flushing incomming socket messages')\n        try:\n            while True:\n                msg = self.socket.recv(self.buffer_size)\n                debug(b'< ' + msg)\n        except socket.error:\n            pass"}
{"prompt": "def vcf_records(self, format_tags=None, qualified=False):\n        \"\"\"Generates parsed VcfRecord objects.\n\n        Typically called in a for loop to process each vcf record in a\n        VcfReader. VcfReader must be opened in advanced and closed when\n        complete. Skips all headers.\n\n        Args:\n            qualified: When True, sample names are prefixed with file name\n\n        Returns:\n            Parsed VcfRecord\n\n        Raises:\n            StopIteration: when reader is exhausted.\n            TypeError: if reader is closed.\n        \"\"\"", "output": "       \"\"\"\n        if not self.is_open:\n            raise ValueError(\"Reader must be opened before records can be read.\")\n\n        if format_tags is None:\n            format_tags = []\n\n        if qualified:\n            prefix = self.filename + \":\"\n        else:\n            prefix = \"\"\n\n        for line in self.reader:\n            if line.startswith(\"#\"):\n                continue\n            record = self.parse_record(line)\n            record.sample_names = [prefix + sample for sample in record.sample_names]\n           ", "solution": "\n        if qualified:\n            sample_names = self.qualified_sample_names\n        else:\n            sample_names = self.sample_names\n\n        for line in self._file_reader.read_lines():\n            if line.startswith(\"#\"):\n                continue\n            vcf_record = vcf.VcfRecord.parse_record(line, sample_names)\n            if format_tags:\n                vcf_record = self.modify_format_tag(vcf_record, format_tags)\n            yield vcf_record"}
{"prompt": "def main(args,parser,subparser):\n    \"\"\"the list command corresponds with listing images for an external\n       resource. This is different from listing images that are local to the\n       database, which should be done with \"images\"\n    \"\"\"", "output": "\n    parser.add_argument('-r','--resource',\n                        help='the resource to list images from',\n                        required=True)\n    parser.add_argument('-f','--format',\n                        help='the format to list images in',\n                        required=True)\n    parser.add_argument('-o','--output',\n                        help='the output file to write the list to',\n                        required=True)\n    parser.add_argument('-d','--directory',\n                        help='the directory to list images from',\n", "solution": "\n    from sregistry.main import get_client\n    cli = get_client(quiet=args.quiet)\n    \n    for query in args.query:\n        if query in ['','*']:\n            query = None\n\n        cli.ls(query=query)"}
{"prompt": "def parse_hpo_genes(hpo_lines):\n    \"\"\"Parse HPO gene information\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with hgnc symbols as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        line = line.strip()\n        if not line:\n            continue\n        if line.startswith('#'):\n            continue\n        line = line.split('\\t')\n        if len(line) < 4:\n            continue\n        hgnc_symbol = line[0]\n        hpo_id = line[1]\n        hpo_name = line[2]\n        hpo_description = line[3]\n        if hgnc_symbol not in diseases:\n", "solution": "\n    LOG.info(\"Parsing HPO genes ...\")\n    genes = {}\n    for index, line in enumerate(hpo_lines):\n        # First line is header\n        if index == 0:\n            continue\n        if len(line) < 5:\n            continue\n        gene_info = parse_hpo_gene(line)\n        hgnc_symbol = gene_info['hgnc_symbol']\n        description = gene_info['description']\n        \n        if hgnc_symbol not in genes:\n            genes[hgnc_symbol] = {\n                'hgnc_symbol': hgnc_symbol\n            }\n        \n        gene = genes[hgnc_symbol]\n        if description == 'Incomplete penetrance':\n            gene['incomplete_penetrance'] = True\n        if description == 'Autosomal dominant inheritance':\n            gene['ad'] = True\n        if description == 'Autosomal recessive inheritance':\n            gene['ar'] = True\n        if description == 'Mithochondrial inheritance':\n            gene['mt'] = True\n        if description == 'X-linked dominant inheritance':\n            gene['xd'] = True\n        if description == 'X-linked recessive inheritance':\n            gene['xr'] = True\n        if description == 'Y-linked inheritance':\n            gene['x'] = True\n        if description == 'X-linked inheritance':\n            gene['y'] = True\n    LOG.info(\"Parsing done.\")\n    return genes"}
{"prompt": "def start_tag(el):\n    \"\"\"\n    The text representation of the start tag for a tag.\n    \"\"\"", "output": "\n    return \"<\" + el.tag\n\n\ndef end_tag(el):\n    \"\"\"\n    The text representation of the end tag for a tag.\n    \"\"\"\n    return \"</\" + el.tag + \">\"\n\n\ndef text(el):\n    \"\"\"\n    The text representation of the text content for a tag.\n    \"\"\"\n    return el.text\n\n\ndef attr(el, name):\n    \"\"\"\n    The text representation of the attribute for a tag.\n    \"\"\"\n    return el.attrib.get(name)\n\n\ndef children", "solution": "\n    return '<%s%s>' % (\n        el.tag, ''.join([' %s=\"%s\"' % (name, html_escape(value, True))\n                         for name, value in el.attrib.items()]))"}
{"prompt": "def get_file_content(url, comes_from=None, session=None):\n    \"\"\"Gets the content of a file; it may be a filename, file: URL, or\n    http: URL.  Returns (location, content).  Content is unicode.\"\"\"", "output": "\n    if session is None:\n        session = requests.Session()\n    if url.startswith('file:'):\n        url = url[len('file:'):]\n        if url.startswith('/'):\n            url = url[1:]\n        location = url\n        with open(url, 'rb') as f:\n            content = f.read().decode('utf-8')\n    elif url.startswith('http:'):\n        if comes_from is None:\n            raise ValueError(\"comes_from must be set for HTTP URLs\")\n        location = url", "solution": "\n    if session is None:\n        raise TypeError(\n            \"get_file_content() missing 1 required keyword argument: 'session'\"\n        )\n\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if (scheme == 'file' and comes_from and\n                comes_from.startswith('http')):\n            raise InstallationError(\n                'Requirements file %s references URL %s, which is local'\n                % (comes_from, url))\n        if scheme == 'file':\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = match.group(1) + ':' + path.split('|', 1)[1]\n            path = urllib_parse.unquote(path)\n            if path.startswith('/'):\n                path = '/' + path.lstrip('/')\n            url = path\n        else:\n            # FIXME: catch some errors\n            resp = session.get(url)\n            resp.raise_for_status()\n\n            if six.PY3:\n                return resp.url, resp.text\n            else:\n                return resp.url, resp.content\n    try:\n        with open(url) as f:\n            content = f.read()\n    except IOError as exc:\n        raise InstallationError(\n            'Could not open requirements file: %s' % str(exc)\n        )\n    return url, content"}
{"prompt": "def _get_repetition(extractor, text, *, bounds=(0, None), ignore_whitespace=False):\n  \"\"\"Tries to pull text with extractor repeatedly.\n\n  Bounds is a 2-tuple of (lbound, ubound) where lbound is a number and ubound is a number or None.\n  If the ubound is None, this method will execute extractor on text until extrator raises DeadEnd.\n  Otherwise, extractor will be called until it raises DeadEnd, or it has extracted ubound times.\n\n  If the number of children extracted is >= lbound, then a ParseNode with type repetition is\n  returned. Otherwise, DeadEnd is raised.\n\n  Bounds are interpreted as (lbound, ubound]\n\n  This method is used to implement:\n  - option (0, 1)\n  - zero_or_more (0, None)\n  - one_or_more (1, None)\n  - exact_repeat (n, n)\n  \"\"\"", "output": " If ignore_whitespace is True, this method will ignore whitespace when pulling text.\n  \"\"\"\n  if ignore_whitespace:\n    text = text.replace(' ', '')\n  lbound, ubound = bounds\n  if ubound is None:\n    ubound = float('inf')\n  extracted = []\n  for _ in range(lbound, ubound):\n    try:\n      extracted.append(extractor(text))\n    except DeadEnd:\n      break\n  return extracted\n\n\ndef _get_repetition_with_separator", "solution": "\n  minr, maxr = bounds\n  children = []\n\n  while maxr is None or len(children) <= maxr:\n    ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n    try:\n      child = _call_extractor(extractor, use_text)\n      child.add_ignored(ignored_ws)\n    except DeadEnd:\n      break\n\n    if child.is_empty:\n      break\n\n    children.append(child)\n    text = text[child.consumed:]\n\n\n  if len(children) >= minr:\n    return ParseNode(ParseNodeType.repetition,\n                      children=children)\n  else:\n    raise DeadEnd()"}
{"prompt": "def get_query(self, query):\n        \"\"\" Grab all issues matching a github query \"\"\"", "output": "\n        issues = []\n        for page in range(1, 100):\n            url = 'https://api.github.com/search/issues?q={}&page={}&per_page=100'.format(query, page)\n            response = requests.get(url, headers=self.headers)\n            if response.status_code == 200:\n                issues += response.json()['items']\n            else:\n                print('Error: {}'.format(response.status_code))\n                break\n        return issues\n\n    def", "solution": "\n        issues = {}\n        for issue in self.client.get_query(query):\n            url = issue['html_url']\n            try:\n                repo = self.get_repository_from_issue(issue)\n            except ValueError as e:\n                log.critical(e)\n            else:\n                issues[url] = (repo, issue)\n        return issues"}
{"prompt": "def build(self, secret_key):\n        \"\"\"Builds a final copy of the token using the given secret key.\n\n        :param secret_key(string): The secret key that corresponds to this builder's access key.\n        \"\"\"", "output": "\n        if not self.access_key:\n            raise ValueError('Access key must be set before building.')\n        if not self.secret_key:\n            raise ValueError('Secret key must be set before building.')\n        if not self.expiration:\n            raise ValueError('Expiration must be set before building.')\n\n        self.secret_key = secret_key\n        self.token = self.access_key + ':' + self.secret_key + ':' + self.expiration\n        return self\n\n    def get_token(self):", "solution": "\n        key = jwk.JWK(\n            kty='oct',\n            k=base64url_encode(uuid.UUID(secret_key).bytes),\n        )\n\n        header = {\n            'alg': 'dir',\n            'enc': 'A128GCM',\n            'zip': 'DEF',\n            'cty': 'JWT',\n            'kid': self._access_key,\n        }\n\n        now = int(time.time())\n\n        payload = {\n            'iat': now,\n            'nbf': now,\n        }\n\n        if self._expiration is not None:\n            payload['exp'] = int(calendar.timegm(self._expiration.utctimetuple()))\n\n        if len(self._view_identifiers) > 0:\n            payload[VIEW_IDENTIFIERS_CLAIM_NAME] = self._view_identifiers\n\n        if len(self._parameters) > 0:\n            parameters = []\n            for parameter in self._parameters:\n                serialized = {\n                    'field': parameter.field,\n                    'op': parameter.op,\n                }\n\n                if hasattr(parameter, '__iter__'):\n                    serialized['any'] = list(parameter.value)\n                else:\n                    serialized['value'] = parameter.value\n\n                parameters.append(serialized)\n\n            payload[PARAMETERS_CLAIM_NAME] = parameters\n\n        if len(self._attributes) > 0:\n            payload[ATTRIBUTES_CLAIM_NAME] = self._attributes\n\n        tok = jwe.JWE(json_encode(payload), protected=header)\n        tok.add_recipient(key)\n\n        return tok.serialize(compact=True)"}
{"prompt": "def cpu_percent(interval=0.1, percpu=False):\n    \"\"\"Return a float representing the current system-wide CPU\n    utilization as a percentage.\n\n    When interval is > 0.0 compares system CPU times elapsed before\n    and after the interval (blocking).\n\n    When interval is 0.0 or None compares system CPU times elapsed\n    since last call or module import, returning immediately.\n    In this case is recommended for accuracy that this function be\n    called with at least 0.1 seconds between calls.\n\n    When percpu is True returns a list of floats representing the\n    utilization as a percentage for each CPU.\n    First element of the list refers to first CPU, second element\n    to second CPU and so on.\n    The order of the list is consistent across calls.\n    \"\"\"", "output": "When percpu is True returns a list of floats representing\n    CPU utilization as a percentage for each processor.\n\n    \"\"\"\n    if interval is None or interval < 0.0:\n        interval = 0.0\n    if interval > 0.0:\n        if percpu:\n            prev_times = _cpu_times_percpu()\n        else:\n            prev_times = _cpu_times()\n        time.sleep(interval)\n        if percpu:\n            curr_times = _cpu_times_percpu()\n        else:\n", "solution": "\n    global _last_cpu_times\n    global _last_per_cpu_times\n    blocking = interval is not None and interval > 0.0\n\n    def calculate(t1, t2):\n        t1_all = sum(t1)\n        t1_busy = t1_all - t1.idle\n\n        t2_all = sum(t2)\n        t2_busy = t2_all - t2.idle\n\n        # this usually indicates a float precision issue\n        if t2_busy <= t1_busy:\n            return 0.0\n\n        busy_delta = t2_busy - t1_busy\n        all_delta = t2_all - t1_all\n        busy_perc = (busy_delta / all_delta) * 100\n        return round(busy_perc, 1)\n\n    # system-wide usage\n    if not percpu:\n        if blocking:\n            t1 = cpu_times()\n            time.sleep(interval)\n        else:\n            t1 = _last_cpu_times\n        _last_cpu_times = cpu_times()\n        return calculate(t1, _last_cpu_times)\n    # per-cpu usage\n    else:\n        ret = []\n        if blocking:\n            tot1 = cpu_times(percpu=True)\n            time.sleep(interval)\n        else:\n            tot1 = _last_per_cpu_times\n        _last_per_cpu_times = cpu_times(percpu=True)\n        for t1, t2 in zip(tot1, _last_per_cpu_times):\n            ret.append(calculate(t1, t2))\n        return ret"}
{"prompt": "def load_filters():\n    \"\"\"\n    Loads and returns all filters.\n    \"\"\"", "output": "\n    filters = []\n    for file in os.listdir(os.path.join(os.path.dirname(__file__), 'filters')):\n        if file.endswith('.py') and not file.startswith('_'):\n            filters.append(file[:-3])\n    return filters\n\n\ndef load_filter(filter_name):\n    \"\"\"\n    Loads and returns the specified filter.\n    \"\"\"\n    try:\n        module = importlib.import_module('filters.%s' % filter_name)\n        return module.filter\n", "solution": "\n    all_filters = {}\n    for m in JINJA_FILTERS:\n        if hasattr(m, \"filters\"):\n            all_filters.update(m.filters())\n    return all_filters"}
{"prompt": "def weighted(loads):\n    \"\"\"Pick two at random using inverse load as weight.\n\n    Return the less loaded of the two.\n    \"\"\"", "output": "\n    loads = sorted(loads, key=lambda x: x[1], reverse=True)\n    return loads[0]\n\n\ndef weighted_random(loads):\n    \"\"\"Pick two at random using inverse load as weight.\n\n    Return the less loaded of the two.\n    \"\"\"\n    loads = sorted(loads, key=lambda x: x[1], reverse=True)\n    return loads[random.randint(0, len(loads) - 1)]\n\n\ndef weighted_random_with_replacement(loads):\n    \"\"\"P", "solution": "\n    # weight 0 a million times more than 1:\n    weights = 1./(1e-6+numpy.array(loads))\n    sums = weights.cumsum()\n    t = sums[-1]\n    x = random()*t\n    y = random()*t\n    idx = 0\n    idy = 0\n    while sums[idx] < x:\n        idx += 1\n    while sums[idy] < y:\n        idy += 1\n    if weights[idy] > weights[idx]:\n        return idy\n    else:\n        return idx"}
{"prompt": "def print_figure(fig, fmt='png'):\n    \"\"\"Convert a figure to svg or png for inline display.\"\"\"", "output": "\n    fig.patch.set_alpha(0)\n    fig.patch.set_linewidth(0)\n    fig.patch.set_facecolor('white')\n    fig.patch.set_edgecolor('white')\n    fig.patch.set_zorder(-1)\n    fig.patch.set_visible(True)\n    fig.patch.set_rasterized(True)\n    fig.patch.set_animated(True)\n    fig.patch.set_clip_box(None)\n    fig.patch.set_clip_path(", "solution": "\n    # When there's an empty figure, we shouldn't return anything, otherwise we\n    # get big blank areas in the qt console.\n    if not fig.axes and not fig.lines:\n        return\n\n    fc = fig.get_facecolor()\n    ec = fig.get_edgecolor()\n    fig.set_facecolor('white')\n    fig.set_edgecolor('white')\n    try:\n        bytes_io = BytesIO()\n        fig.canvas.print_figure(bytes_io, format=fmt, bbox_inches='tight')\n        data = bytes_io.getvalue()\n    finally:\n        fig.set_facecolor(fc)\n        fig.set_edgecolor(ec)\n    return data"}
{"prompt": "def json_to_bounds(segments_json):\n    \"\"\"Extracts the boundaries from a json file and puts them into\n        an np array.\"\"\"", "output": "\n    bounds = []\n    for segment in segments_json:\n        bounds.append(segment['bounds'])\n    return np.array(bounds)\n\n\ndef json_to_labels(segments_json):\n    \"\"\"Extracts the labels from a json file and puts them into\n        an np array.\"\"\"\n    labels = []\n    for segment in segments_json:\n        labels.append(segment['label'])\n    return np.array(labels)\n\n\ndef json_to_points(segments_json):\n    \"\"\"Extracts the points from", "solution": "\n    f = open(segments_json)\n    segments = json.load(f)[\"segments\"]\n    bounds = []\n    for segment in segments:\n        bounds.append(segment[\"start\"])\n    bounds.append(bounds[-1] + segments[-1][\"duration\"]) # Add last boundary\n    f.close()\n    return np.asarray(bounds)"}
{"prompt": "def _bio_to_string(bio):\n    \"\"\"\n    Copy the contents of an OpenSSL BIO object into a Python byte string.\n    \"\"\"", "output": "\n    bio_len = BIO_ctrl_pending(bio)\n    bio_data = ctypes.create_string_buffer(bio_len)\n    BIO_read(bio, ctypes.byref(bio_data), bio_len)\n    return bio_data.raw\n\n\ndef _bio_to_bytes(bio):\n    \"\"\"\n    Copy the contents of an OpenSSL BIO object into a Python byte string.\n    \"\"\"\n    bio_len = BIO_ctrl_pending(bio)\n    bio_data = ctypes.", "solution": "\n    result_buffer = _ffi.new('char**')\n    buffer_length = _lib.BIO_get_mem_data(bio, result_buffer)\n    return _ffi.buffer(result_buffer[0], buffer_length)[:]"}
{"prompt": "def create_from(cls, backend):\n        \"\"\"\n        Create device specification with values in backend configuration.\n        Args:\n            backend(Backend): backend configuration\n        Returns:\n            DeviceSpecification: created device specification\n        Raises:\n            PulseError: when an invalid backend is specified\n        \"\"\"", "output": "\n        return cls(backend.configuration().n_qubits,\n                   backend.configuration().dt,\n                   backend.configuration().memory,\n                   backend.configuration().n_registers,\n                   backend.configuration().coupling_map,\n                   backend.configuration().basis_gates,\n                   backend.configuration().simulator,\n                   backend.configuration().local,\n                   backend.configuration().conditional,\n                   backend.configuration().open_pulse,\n                   backend.configuration().memory_slots,\n                   backend.", "solution": "\n        backend_config = backend.configuration()\n\n        # TODO : Remove usage of config.defaults when backend.defaults() is updated.\n        try:\n            backend_default = backend.defaults()\n        except ModelValidationError:\n            from collections import namedtuple\n            BackendDefault = namedtuple('BackendDefault', ('qubit_freq_est', 'meas_freq_est'))\n\n            backend_default = BackendDefault(\n                qubit_freq_est=backend_config.defaults['qubit_freq_est'],\n                meas_freq_est=backend_config.defaults['meas_freq_est']\n            )\n\n        # system size\n        n_qubits = backend_config.n_qubits\n        n_registers = backend_config.n_registers\n        n_uchannels = backend_config.n_uchannels\n\n        if n_uchannels > 0 and n_uchannels != n_qubits:\n            raise PulseError(\"This version assumes no U-channels or #U-cannels==#qubits.\")\n\n        # frequency information\n        qubit_lo_freqs = backend_default.qubit_freq_est\n        qubit_lo_ranges = backend_config.qubit_lo_range\n        meas_lo_freqs = backend_default.meas_freq_est\n        meas_lo_ranges = backend_config.meas_lo_range\n\n        # generate channels with assuming their numberings are aligned with qubits\n        drives = [\n            DriveChannel(i, qubit_lo_freqs[i], tuple(qubit_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        measures = [\n            MeasureChannel(i, meas_lo_freqs[i], tuple(meas_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        acquires = [AcquireChannel(i) for i in range(n_qubits)]\n        controls = [ControlChannel(i) for i in range(n_uchannels)]\n\n        qubits = []\n        for i in range(n_qubits):\n            # TODO: get qubits <-> channels relationship from backend\n            qubit = Qubit(i,\n                          drive_channels=[drives[i]],\n                          control_channels=None if n_uchannels == 0 else controls[i],\n                          measure_channels=[measures[i]],\n                          acquire_channels=[acquires[i]])\n            qubits.append(qubit)\n\n        registers = [RegisterSlot(i) for i in range(n_registers)]\n        # TODO: get #mem_slots from backend\n        mem_slots = [MemorySlot(i) for i in range(len(qubits))]\n\n        return DeviceSpecification(qubits, registers, mem_slots)"}
{"prompt": "def remove_memberships(self, team, users):\n        \"\"\"\n        **Description**\n            Remove user memberships from specified team.\n\n        **Arguments**\n            - **team**: the name of the team from which user memberships are removed\n            - **users**: list of usernames which should be removed from team\n\n        **Example**\n            `examples/user_team_mgmt_extended.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt_extended.py>`_\n        \"\"\"", "output": "extended.py>`_\n        \"\"\"\n        if not isinstance(users, list):\n            raise ValueError(\"users must be a list\")\n        if not isinstance(team, str):\n            raise ValueError(\"team must be a string\")\n        if not users:\n            raise ValueError(\"users must not be empty\")\n        if not team:\n            raise ValueError(\"team must not be empty\")\n\n        url = self._url + \"/teams/\" + team + \"/members\"\n        data = {\"users\": users}\n        return self._http_delete(", "solution": "\n\n        res = self.list_memberships(team)\n\n        if res[0] is False:\n            return res\n\n        old_memberships = res[1]\n        new_memberships = {k: v for k, v in old_memberships.items() if k not in users}\n\n        res = self.edit_team(team, new_memberships)\n\n        if res[0] is False:\n            return res\n        else:\n            return [True, None]"}
{"prompt": "def price_converter(obj):\n    \"\"\"Ensures that string prices are converted into Price objects.\"\"\"", "output": "\n    if isinstance(obj, str):\n        return Price(obj)\n    return obj\n\n\nclass Price(object):\n    \"\"\"Represents a price.\"\"\"\n\n    def __init__(self, value):\n        \"\"\"Initializes a new instance of the Price class.\n\n        Args:\n            value: The value of the price.\n        \"\"\"\n        self.value = value\n\n    def __str__(self):\n        \"\"\"Returns a string representation of the price.\"\"\"\n        return self.value\n\n    def __repr__(self):\n", "solution": "\n    if isinstance(obj, str):\n        obj = PriceClass.parse(obj)\n    return obj"}
{"prompt": "def finalize(self, remove_all_handlers=True):\n        \"\"\"Finalizes the manager, closes and removes all handlers if desired.\"\"\"", "output": "\n        if remove_all_handlers:\n            self.remove_all_handlers()\n        self.close()\n\n    def close(self):\n        \"\"\"Closes the manager.\"\"\"\n        self.closed = True\n\n    def remove_all_handlers(self):\n        \"\"\"Removes all handlers from the manager.\"\"\"\n        for handler in self.handlers:\n            self.remove_handler(handler)\n\n    def remove_handler(self, handler):\n        \"\"\"Removes a handler from the manager.\"\"\"\n        if handler in", "solution": "\n        for tool in self._tools:\n            tool.finalize()\n        self._tools = []\n        self._stdout_to_logger = None\n        for config in (self._sp_config, self._mp_config):\n            if hasattr(config, 'close'):\n                config.close()\n        self._sp_config = None\n        self._mp_config = None\n        if remove_all_handlers:\n            self.tabula_rasa()"}
{"prompt": "def _experiments_to_circuits(qobj):\n    \"\"\"Return a list of QuantumCircuit object(s) from a qobj\n\n    Args:\n        qobj (Qobj): The Qobj object to convert to QuantumCircuits\n    Returns:\n        list: A list of QuantumCircuit objects from the qobj\n\n    \"\"\"", "output": "\n    circuits = []\n    for experiment in qobj.experiments:\n        if experiment.config.shots == 1:\n            circuits.append(experiment.instructions)\n        else:\n            for shot in experiment.shots:\n                circuits.append(shot)\n    return circuits\n\n\ndef _experiments_to_counts(qobj):\n    \"\"\"Return a list of dicts of counts from a qobj\n\n    Args:\n        qobj (Qobj): The Qobj object to convert to counts\n", "solution": "\n    if qobj.experiments:\n        circuits = []\n        for x in qobj.experiments:\n            quantum_registers = [QuantumRegister(i[1], name=i[0])\n                                 for i in x.header.qreg_sizes]\n            classical_registers = [ClassicalRegister(i[1], name=i[0])\n                                   for i in x.header.creg_sizes]\n            circuit = QuantumCircuit(*quantum_registers,\n                                     *classical_registers,\n                                     name=x.header.name)\n            qreg_dict = {}\n            creg_dict = {}\n            for reg in quantum_registers:\n                qreg_dict[reg.name] = reg\n            for reg in classical_registers:\n                creg_dict[reg.name] = reg\n            for i in x.instructions:\n                instr_method = getattr(circuit, i.name)\n                qubits = []\n                try:\n                    for qubit in i.qubits:\n                        qubit_label = x.header.qubit_labels[qubit]\n                        qubits.append(\n                            qreg_dict[qubit_label[0]][qubit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                clbits = []\n                try:\n                    for clbit in i.memory:\n                        clbit_label = x.header.clbit_labels[clbit]\n                        clbits.append(\n                            creg_dict[clbit_label[0]][clbit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                params = []\n                try:\n                    params = i.params\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                if i.name in ['snapshot']:\n                    instr_method(\n                        i.label,\n                        snapshot_type=i.snapshot_type,\n                        qubits=qubits,\n                        params=params)\n                elif i.name == 'initialize':\n                    instr_method(params, qubits)\n                else:\n                    instr_method(*params, *qubits, *clbits)\n            circuits.append(circuit)\n        return circuits\n    return None"}
{"prompt": "def visit_assign(self, node):\n        \"\"\"check that if assigning to a function call, the function is\n        possibly returning something valuable\n        \"\"\"", "output": "\n        if isinstance(node.value, ast.Call):\n            self.check_function_call(node.value)\n        return self.generic_visit(node)\n\n    def visit_Call(self, node):\n        \"\"\"check that if calling a function, the function is possibly\n        returning something valuable\n        \"\"\"\n        if isinstance(node.func, ast.Name):\n            self.check_function_call(node)\n        return self.generic_visit(node)\n\n    def visit_FunctionDef(self, node):\n        \"\"\"", "solution": "\n        if not isinstance(node.value, astroid.Call):\n            return\n        function_node = safe_infer(node.value.func)\n        # skip class, generator and incomplete function definition\n        funcs = (astroid.FunctionDef, astroid.UnboundMethod, astroid.BoundMethod)\n        if not (\n            isinstance(function_node, funcs)\n            and function_node.root().fully_defined()\n            and not function_node.decorators\n        ):\n            return\n        if (\n            function_node.is_generator()\n            or function_node.is_abstract(pass_is_abstract=False)\n            or isinstance(function_node, astroid.AsyncFunctionDef)\n        ):\n            return\n        returns = list(\n            function_node.nodes_of_class(astroid.Return, skip_klass=astroid.FunctionDef)\n        )\n        if not returns:\n            self.add_message(\"assignment-from-no-return\", node=node)\n        else:\n            for rnode in returns:\n                if not (\n                    isinstance(rnode.value, astroid.Const)\n                    and rnode.value.value is None\n                    or rnode.value is None\n                ):\n                    break\n            else:\n                self.add_message(\"assignment-from-none\", node=node)"}
{"prompt": "def read_array(self, start, end):\n        \"\"\"Return floats from `start` to `end` inclusive, indexed from 1.\n\n        The entire range of floats is immediately read into memory from\n        the file, making this efficient for small sequences of floats\n        whose values are all needed immediately.\n\n        \"\"\"", "output": "\n        return self.read_array_from_file(start, end, self.file)\n\n    def read_array_from_file(self, start, end, file):\n        \"\"\"Return floats from `start` to `end` inclusive, indexed from 1.\n\n        The entire range of floats is immediately read into memory from\n        the file, making this efficient for small sequences of floats\n        whose values are all needed immediately.\n\n        \"\"\"\n        if start < 1:\n            raise ValueError(\"start must be >= 1\")\n       ", "solution": "\n        f = self.file\n        f.seek(8 * (start - 1))\n        length = 1 + end - start\n        data = f.read(8 * length)\n        return ndarray(length, self.endian + 'd', data)"}
{"prompt": "def batches(dataset):\n    \"\"\"Returns a callable that chooses sequences from netcdf data.\"\"\"", "output": "\n    def _batches(batch_size):\n        \"\"\"Returns a batch of sequences from netcdf data.\"\"\"\n        # Get a random batch of sequences.\n        batch = []\n        for _ in range(batch_size):\n            # Get a random sequence.\n            seq = dataset.random_sequence()\n            # Add it to the batch.\n            batch.append(seq)\n        return batch\n    return _batches\n\n\ndef batches_from_file(filename, batch_size):\n    \"\"\"Returns a callable that chooses sequences from", "solution": "\n    seq_lengths = dataset.variables['seqLengths'].data\n    seq_begins = np.concatenate(([0], np.cumsum(seq_lengths)[:-1]))\n\n    def sample():\n        chosen = np.random.choice(\n            list(range(len(seq_lengths))), BATCH_SIZE, replace=False)\n        return batch_at(dataset.variables['inputs'].data,\n                        dataset.variables['targetClasses'].data,\n                        seq_begins[chosen],\n                        seq_lengths[chosen])\n\n    return sample"}
{"prompt": "def _raise_if_wrong_file_signature(stream):\n    \"\"\" Reads the 4 first bytes of the stream to check that is LASF\"\"\"", "output": "\n    signature = stream.read(4)\n    if signature != b'LASF':\n        raise ValueError('File signature is not LASF')\n\n\ndef _read_header(stream):\n    \"\"\" Reads the header of the stream and returns a Header object\"\"\"\n    header = Header()\n    header.file_signature = stream.read(4)\n    _raise_if_wrong_file_signature(stream)\n    header.file_source_id = stream.read(2)\n    header.global_encoding = stream.read", "solution": "\n    file_sig = stream.read(len(headers.LAS_FILE_SIGNATURE))\n    if file_sig != headers.LAS_FILE_SIGNATURE:\n        raise errors.PylasError(\n            \"File Signature ({}) is not {}\".format(file_sig, headers.LAS_FILE_SIGNATURE)\n        )"}
{"prompt": "def find_data_files(self, package, src_dir):\n        \"\"\"Return filenames for package's data files in 'src_dir'\"\"\"", "output": "\n        data_files = []\n        for dirpath, dirnames, filenames in os.walk(src_dir):\n            for i, dirname in enumerate(dirnames):\n                if dirname.startswith('.'):\n                    del dirnames[i]\n            for filename in filenames:\n                if filename.endswith('.py') or filename.startswith('.'):\n                    continue\n                path = os.path.join(dirpath, filename)\n                data_files.append(path)\n        return data_files\n\n    def find", "solution": "\n        globs = (self.package_data.get('', [])\n                 + self.package_data.get(package, []))\n        files = self.manifest_files.get(package, [])[:]\n        for pattern in globs:\n            # Each pattern has to be converted to a platform-specific path\n            files.extend(glob(os.path.join(src_dir, convert_path(pattern))))\n        return self.exclude_data_files(package, src_dir, files)"}
{"prompt": "def to_id(string: str) -> str:\n    \"\"\"Get a spotify ID from a URI or open.spotify URL.\n\n    Paramters\n    ---------\n    string : str\n        The string to operate on.\n\n    Returns\n    -------\n    id : str\n        The Spotify ID from the string.\n    \"\"\"", "output": "\n    if string.startswith(\"spotify:\"):\n        return string.split(\":\")[2]\n    elif string.startswith(\"open.spotify.com\"):\n        return string.split(\"/\")[-1]\n    else:\n        raise ValueError(\"Invalid string.\")\n\n\ndef to_uri(id: str) -> str:\n    \"\"\"Get a URI from a Spotify ID.\n\n    Paramters\n    ---------\n    id : str\n        The Spotify ID to operate on.\n\n    Returns\n    -------\n    uri", "solution": "\n    string = string.strip()\n\n    match = _URI_RE.match(string)\n\n    if match is None:\n        match = _OPEN_RE.match(string)\n\n        if match is None:\n            return string\n        else:\n            return match.group(2)\n    else:\n        return match.group(1)"}
{"prompt": "def marquee(txt='',width=78,mark='*'):\n    \"\"\"Return the input string centered in a 'marquee'.\n\n    :Examples:\n\n        In [16]: marquee('A test',40)\n        Out[16]: '**************** A test ****************'\n\n        In [17]: marquee('A test',40,'-')\n        Out[17]: '---------------- A test ----------------'\n\n        In [18]: marquee('A test',40,' ')\n        Out[18]: '                 A test                 '\n\n    \"\"\"", "output": "18]: '                 A test                 '\n\n        In [19]: marquee('A test',40,'')\n        Out[19]: 'A test'\n\n    \"\"\"\n    if len(txt) > width:\n        return txt\n    else:\n        return mark*(width-len(txt))+txt\n\n\ndef get_file_size(file_path):\n    \"\"\"Return the size of a file in bytes.\n\n    :Examples:\n\n        In [1]: get_file_size('test.txt')\n        Out[1", "solution": "\n    if not txt:\n        return (mark*width)[:width]\n    nmark = (width-len(txt)-2)//len(mark)//2\n    if nmark < 0: nmark =0\n    marks = mark*nmark\n    return '%s %s %s' % (marks,txt,marks)"}
{"prompt": "def add(self, gate, qargs, cargs):\n        \"\"\"Add an instruction and its context (where it's attached).\"\"\"", "output": "\n        if gate.name == 'measure':\n            self.measure_count += 1\n        if gate.name == 'reset':\n            self.reset_count += 1\n        if gate.name == 'barrier':\n            self.barrier_count += 1\n        if gate.name == 'snapshot':\n            self.snapshot_count += 1\n        if gate.name == 'save_expval':\n            self.save_expval_count += 1\n        if gate.name == 'save_statevector':\n            self.save", "solution": "\n        if not isinstance(gate, Instruction):\n            raise QiskitError(\"attempt to add non-Instruction\" +\n                              \" to InstructionSet\")\n        self.instructions.append(gate)\n        self.qargs.append(qargs)\n        self.cargs.append(cargs)"}
{"prompt": "def read_char(self, c: str) -> bool:\n        \"\"\"\n        Consume the c head byte, increment current index and return True\n        else return False. It use peekchar and it's the same as '' in BNF.\n        \"\"\"", "output": "\n        if self.peekchar() == c:\n            self.index += 1\n            return True\n        return False\n\n    def read_string(self, s: str) -> bool:\n        \"\"\"\n        Consume the s string, increment current index and return True\n        else return False. It use peekstring and it's the same as '' in BNF.\n        \"\"\"\n        if self.peekstring(len(s)) == s:\n            self.index += len(s)\n            return True\n        return False\n\n    def read_", "solution": "\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()"}
{"prompt": "def disassemble(msg, msg_nocr, section, co, lasti=-1, start_line=-1,\n                end_line=None, relative_pos=False, highlight='light',\n                start_offset=0, end_offset=None):\n    \"\"\"Disassemble a code object.\"\"\"", "output": "\n    if end_offset is None:\n        end_offset = len(msg_nocr)\n    if start_line == -1:\n        start_line = 1\n    if end_line is None:\n        end_line = start_line + 1\n    if relative_pos:\n        start_offset = 0\n        end_offset = len(msg_nocr)\n    if lasti == -1:\n        lasti = len(msg_nocr)\n    if start_offset > end_offset:\n        start_offset, end", "solution": "\n    return disassemble_bytes(msg, msg_nocr, co.co_code, lasti, co.co_firstlineno,\n                             start_line, end_line, relative_pos,\n                        co.co_varnames, co.co_names, co.co_consts,\n                        co.co_cellvars, co.co_freevars,\n                        dict(findlinestarts(co)), highlight,\n                        start_offset=start_offset, end_offset=end_offset)"}
{"prompt": "def _unpack_storm_date(date):\n        \"\"\"\n        given a packed storm date field, unpack and return 'YYYY-MM-DD' string.\n        \"\"\"", "output": "\n        year = date >> 9\n        month = (date >> 5) & 0x0f\n        day = date & 0x1f\n        return '{:04d}-{:02d}-{:02d}'.format(year, month, day)\n\n    def _unpack_storm_time(time):\n        \"\"\"\n        given a packed storm time field, unpack and return 'HH:MM' string.\n        \"\"\"\n        hour = time >> 11\n        minute = (time >> 5) & 0x", "solution": "\n        year = (date & 0x7f) + 2000  # 7 bits\n        day = (date >> 7) & 0x01f  # 5 bits\n        month = (date >> 12) & 0x0f  # 4 bits\n        return \"%s-%s-%s\" % (year, month, day)"}
{"prompt": "def poll_operation_until_done(self, name, polling_interval_in_seconds):\n        \"\"\"\n        Poll backup operation state until it's completed.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n        :type polling_interval_in_seconds: int\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"", "output": "\n        operation_id = name\n        operation_url = self._get_operation_url(operation_id)\n        while True:\n            operation_response = self._client.get(operation_url)\n            if operation_response.status_code != 200:\n                raise CloudError(operation_response)\n            operation_result = operation_response.json()\n            if operation_result['status'] == 'Succeeded':\n                return operation_result\n            elif operation_result['status'] == 'Failed':\n                raise CloudError(operation_response", "solution": "\n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result"}
{"prompt": "def cost(self, t_node, branch_length, multiplicity=2.0):\n        \"\"\"\n        returns the cost associated with a branch starting at t_node\n        t_node is time before present, the branch goes back in time\n\n        Args:\n            - t_node:           time of the node\n            - branch_length:    branch length, determines when this branch merges with sister\n            - multiplicity:     2 if merger is binary, higher if this is a polytomy\n        \"\"\"", "output": "\n        return self.time_to_present(t_node) * branch_length * multiplicity\n\n    def time_to_present(self, t_node):\n        \"\"\"\n        returns the time to present for a given node\n\n        Args:\n            - t_node:           time of the node\n        \"\"\"\n        return self.t_present - t_node\n\n    def get_branch_length(self, t_node, t_sister):\n        \"\"\"\n        returns the branch length for a given node\n\n        Args:\n            -", "solution": "\n        merger_time = t_node+branch_length\n        return self.integral_merger_rate(merger_time) - self.integral_merger_rate(t_node)\\\n                 - np.log(self.total_merger_rate(merger_time))*(multiplicity-1.0)/multiplicity"}
{"prompt": "def run_cell_magic(self, magic_name, line, cell):\n        \"\"\"Execute the given cell magic.\n        \n        Parameters\n        ----------\n        magic_name : str\n          Name of the desired magic function, without '%' prefix.\n\n        line : str\n          The rest of the first input line as a single string.\n\n        cell : str\n          The body of the cell as a (possibly multiline) string.\n        \"\"\"", "output": "\n        # Check for the magic function\n        magic_name = magic_name.strip()\n        if magic_name not in self.magics:\n            raise KeyError('Unknown cell magic %s' % magic_name)\n        magic_func = self.magics[magic_name]\n        # Run the magic function\n        try:\n            self.shell.run_cell_magic(magic_name, line, cell)\n        except Exception:\n            self.log.error(\"Error running cell magic %s\", magic_name, exc_info=True", "solution": "\n        fn = self.find_cell_magic(magic_name)\n        if fn is None:\n            lm = self.find_line_magic(magic_name)\n            etpl = \"Cell magic function `%%%%%s` not found%s.\"\n            extra = '' if lm is None else (' (But line magic `%%%s` exists, '\n                                    'did you mean that instead?)' % magic_name )\n            error(etpl % (magic_name, extra))\n        else:\n            # Note: this is the distance in the stack to the user's frame.\n            # This will need to be updated if the internal calling logic gets\n            # refactored, or else we'll be expanding the wrong variables.\n            stack_depth = 2\n            magic_arg_s = self.var_expand(line, stack_depth)\n            with self.builtin_trap:\n                result = fn(line, cell)\n            return result"}
{"prompt": "def update_trace_watch(self):\n        \"\"\"Parses the nextflow trace file and retrieves the path of report JSON\n        files that have not been sent to the service yet.\n        \"\"\"", "output": "\n        if self.trace_file_path is None:\n            return\n\n        if not os.path.exists(self.trace_file_path):\n            return\n\n        with open(self.trace_file_path, 'r') as trace_file:\n            trace_lines = trace_file.readlines()\n\n        if len(trace_lines) == 0:\n            return\n\n        for line in trace_lines:\n            if line.startswith('report'):\n                report_path = line.split(' ')[1]\n                self.", "solution": "\n\n        # Check the size stamp of the tracefile. Only proceed with the parsing\n        # if it changed from the previous size.\n        size_stamp = os.path.getsize(self.trace_file)\n        self.trace_retry = 0\n        if size_stamp and size_stamp == self.trace_sizestamp:\n            return\n        else:\n            logger.debug(\"Updating trace size stamp to: {}\".format(size_stamp))\n            self.trace_sizestamp = size_stamp\n\n        with open(self.trace_file) as fh:\n\n            # Skip potential empty lines at the start of file\n            header = next(fh).strip()\n            while not header:\n                header = next(fh).strip()\n\n            # Get header mappings before parsing the file\n            hm = self._header_mapping(header)\n\n            for line in fh:\n                # Skip empty lines\n                if line.strip() == \"\":\n                    continue\n\n                fields = line.strip().split(\"\\t\")\n\n                # Skip if task ID was already processes\n                if fields[hm[\"task_id\"]] in self.stored_ids:\n                    continue\n\n                if fields[hm[\"process\"]] == \"report\":\n                    self.report_queue.append(\n                        self._expand_path(fields[hm[\"hash\"]])\n                    )\n                    self.send = True\n\n                # Add the processed trace line to the stored ids. It will be\n                # skipped in future parsers\n                self.stored_ids.append(fields[hm[\"task_id\"]])"}
{"prompt": "def permission_check(apikey, endpoint):\n        \"\"\"\n        return (user, seckey) if url end point is in allowed entry point list\n        \"\"\"", "output": "\n        if endpoint in allowed_entry_points:\n            return apikey\n        else:\n            return None\n\n    @app.route('/')\n    def index():\n        \"\"\"\n        return index page\n        \"\"\"\n        return render_template('index.html')\n\n    @app.route('/<endpoint>', methods=['GET', 'POST'])\n    def entry_point(endpoint):\n        \"\"\"\n        return entry point page\n        \"\"\"\n        if request.method == 'GET':\n            return render_template('entry_point.html', endpoint=endpoint)", "solution": "\n        try:\n            ak = APIKeys.objects.get(apikey=apikey)\n            apitree = cPickle.loads(ak.apitree.encode(\"ascii\"))\n            if apitree.match(endpoint):\n                return ak.user if ak.user else AnonymousUser(), ak.seckey\n        except APIKeys.DoesNotExist:\n            pass\n        return None, None"}
{"prompt": "def __create_criterion(self, criterion_str):\n        \"\"\" instantiate criterion from config string \"\"\"", "output": "\n        if criterion_str == \"MSE\":\n            criterion = nn.MSELoss()\n        elif criterion_str == \"MAE\":\n            criterion = nn.L1Loss()\n        elif criterion_str == \"CE\":\n            criterion = nn.CrossEntropyLoss()\n        elif criterion_str == \"BCE\":\n            criterion = nn.BCELoss()\n        elif criterion_str == \"BCEL\":\n            criterion = nn.BCEWithLogitsL", "solution": "\n        parsed = criterion_str.split(\"(\")\n        type_str = parsed[0].strip().lower()\n        parsed[1] = parsed[1].split(\")\")[0].strip()\n\n        for criterion_class in self.custom_criterions:\n            if criterion_class.get_type_string() == type_str:\n                return criterion_class(self, parsed[1])\n        raise ValueError(\n            \"Unsupported autostop criterion type: %s\" % criterion_str)"}
{"prompt": "def verboseRead(self, alphabet, context='', skipExtra=False):\n        \"\"\"Read symbol and extra from stream and explain what happens.\n        Returns the value of the symbol\n        >>> olleke.pos = 0\n        >>> l = Layout(olleke)\n        >>> l.verboseRead(WindowSizeAlphabet())\n        0000  1b                   1011 WSIZE   windowsize=(1<<22)-16=4194288\n        4194288\n        \"\"\"", "output": "\n        if self.pos == len(self.stream):\n            raise EOFError\n        if context:\n            print(context)\n        symbol = self.read(alphabet)\n        if not skipExtra:\n            extra = self.read(ExtraAlphabet())\n            print('%04x  %s' % (self.pos, extra))\n        return symbol\n\n    def verboseReadBits(self, n, context=''):\n        \"\"\"Read n bits and explain what happens.\n        Returns the value of the bits\n        >>> olleke.pos =", "solution": "\n        #TODO 2: verbosity level, e.g. show only codes and maps in header\n        stream = self.stream\n        pos = stream.pos\n        if skipExtra:\n            length, symbol = alphabet.readTuple(stream)\n            extraBits, extra = 0, None\n        else:\n            length, symbol, extraBits, extra = alphabet.readTupleAndExtra(\n                stream)\n        #fields: address, hex data, binary data, name of alphabet, explanation\n        hexdata = self.makeHexData(pos)\n        addressField = '{:04x}'.format(pos+7>>3) if hexdata else ''\n        bitdata = self.formatBitData(pos, length, extraBits)\n        #bitPtr moves bitdata so that the bytes are easier to read\n        #jump back to right if a new byte starts\n        if '|' in bitdata[1:]:\n            #start over on the right side\n            self.bitPtr = self.width\n        fillWidth = self.bitPtr-(len(hexdata)+len(bitdata))\n        if fillWidth<0: fillWidth = 0\n        print('{:<5s} {:<{}s} {:7s} {}'.format(\n            addressField,\n            hexdata+' '*fillWidth+bitdata, self.width,\n            context+alphabet.name,\n            symbol if skipExtra else symbol.explanation(extra),\n            ))\n        #jump to the right if we started with a '|'\n        #because we didn't jump before printing\n        if bitdata.startswith('|'): self.bitPtr = self.width\n        else: self.bitPtr -= len(bitdata)\n        return symbol if skipExtra else symbol.value(extra)"}
{"prompt": "def builtin(cls, name):\n        \"\"\"\n        Generate a default legend.\n\n        Args:\n            name (str): The name of the legend you want. Not case sensitive.\n                 'nsdoe': Nova Scotia Dept. of Energy\n                 'canstrat': Canstrat\n                 'nagmdm__6_2': USGS N. Am. Geol. Map Data Model 6.2\n                 'nagmdm__6_1': USGS N. Am. Geol. Map Data Model 6.1\n                 'nagmdm__4_3': USGS N. Am. Geol. Map Data Model 4.3\n                 'sgmc': USGS State Geologic Map Compilation\n\n            Default 'nagmdm__6_2'.\n\n        Returns:\n            Legend: The legend stored in `defaults.py`.\n        \"\"\"", "output": "nagmdm__6_0': USGS N. Am. Geol. Map Data Model 6.0\n                 'nagmdm__5_2': USGS N. Am. Geol. Map Data Model 5.2\n                 'nagmdm__5_1': USGS N. Am. Geol. Map Data Model 5.1\n                 'nagmdm__5_0': USGS N. Am. Geol. Map Data Model 5.0\n                 'nagmdm__4_2': USGS N. Am. Ge", "solution": "\n        names = {\n                 'nsdoe': LEGEND__NSDOE,\n                 'canstrat': LEGEND__Canstrat,\n                 'nagmdm__6_2': LEGEND__NAGMDM__6_2,\n                 'nagmdm__6_1': LEGEND__NAGMDM__6_1,\n                 'nagmdm__4_3': LEGEND__NAGMDM__4_3,\n                 'sgmc': LEGEND__SGMC,\n                 }\n        return cls.from_csv(text=names[name.lower()])"}
{"prompt": "def parse_email(data, strip_attachment_payloads=False):\n    \"\"\"\n    A simplified email parser\n\n    Args:\n        data: The RFC 822 message string, or MSG binary\n        strip_attachment_payloads (bool): Remove attachment payloads\n\n    Returns (dict): Parsed email data\n    \"\"\"", "output": "\n    if isinstance(data, bytes):\n        data = data.decode('utf-8')\n\n    # Parse the email\n    email = emailparser.parse_from_string(data)\n\n    # Remove attachment payloads\n    if strip_attachment_payloads:\n        for attachment in email.attachments:\n            attachment.payload = None\n\n    # Return the email data\n    return email\n\n\ndef parse_email_attachments(email):\n    \"\"\"\n    Parse email attachments\n\n    Args:\n        email: The email object\n", "solution": "\n\n    if type(data) == bytes:\n        if is_outlook_msg(data):\n            data = convert_outlook_msg(data)\n        data = data.decode(\"utf-8\", errors=\"replace\")\n    parsed_email = mailparser.parse_from_string(data)\n    headers = json.loads(parsed_email.headers_json).copy()\n    parsed_email = json.loads(parsed_email.mail_json).copy()\n    parsed_email[\"headers\"] = headers\n\n    if \"received\" in parsed_email:\n        for received in parsed_email[\"received\"]:\n            if \"date_utc\" in received:\n                if received[\"date_utc\"] is None:\n                    del received[\"date_utc\"]\n                else:\n                    received[\"date_utc\"] = received[\"date_utc\"].replace(\"T\",\n                                                                        \" \")\n\n    if \"from\" not in parsed_email:\n        if \"From\" in parsed_email[\"headers\"]:\n            parsed_email[\"from\"] = parsed_email[\"Headers\"][\"From\"]\n        else:\n            parsed_email[\"from\"] = None\n\n    if parsed_email[\"from\"] is not None:\n        parsed_email[\"from\"] = parse_email_address(parsed_email[\"from\"][0])\n\n    if \"date\" in parsed_email:\n        parsed_email[\"date\"] = parsed_email[\"date\"].replace(\"T\", \" \")\n    else:\n        parsed_email[\"date\"] = None\n    if \"reply_to\" in parsed_email:\n        parsed_email[\"reply_to\"] = list(map(lambda x: parse_email_address(x),\n                                            parsed_email[\"reply_to\"]))\n    else:\n        parsed_email[\"reply_to\"] = []\n\n    if \"to\" in parsed_email:\n        parsed_email[\"to\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"to\"]))\n    else:\n        parsed_email[\"to\"] = []\n\n    if \"cc\" in parsed_email:\n        parsed_email[\"cc\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"cc\"]))\n    else:\n        parsed_email[\"cc\"] = []\n\n    if \"bcc\" in parsed_email:\n        parsed_email[\"bcc\"] = list(map(lambda x: parse_email_address(x),\n                                       parsed_email[\"bcc\"]))\n    else:\n        parsed_email[\"bcc\"] = []\n\n    if \"delivered_to\" in parsed_email:\n        parsed_email[\"delivered_to\"] = list(\n            map(lambda x: parse_email_address(x),\n                parsed_email[\"delivered_to\"])\n        )\n\n    if \"attachments\" not in parsed_email:\n        parsed_email[\"attachments\"] = []\n    else:\n        for attachment in parsed_email[\"attachments\"]:\n            if \"payload\" in attachment:\n                payload = attachment[\"payload\"]\n                try:\n                    if \"content_transfer_encoding\" in attachment:\n                        if attachment[\"content_transfer_encoding\"] == \"base64\":\n                            payload = decode_base64(payload)\n                        else:\n                            payload = str.encode(payload)\n                    attachment[\"sha256\"] = hashlib.sha256(payload).hexdigest()\n                except Exception as e:\n                    logger.debug(\"Unable to decode attachment: {0}\".format(\n                        e.__str__()\n                    ))\n        if strip_attachment_payloads:\n            for attachment in parsed_email[\"attachments\"]:\n                if \"payload\" in attachment:\n                    del attachment[\"payload\"]\n\n    if \"subject\" not in parsed_email:\n        parsed_email[\"subject\"] = None\n\n    parsed_email[\"filename_safe_subject\"] = get_filename_safe_string(\n        parsed_email[\"subject\"])\n\n    if \"body\" not in parsed_email:\n        parsed_email[\"body\"] = None\n\n    return parsed_email"}
{"prompt": "def get_news(self, tickers=[], tags=[], sources=[], startDate=None,\n                 endDate=None, limit=100, offset=0, sortBy=\"publishedDate\",\n                 fmt='json'):\n        \"\"\"Return list of news articles matching given search terms\n            https://api.tiingo.com/docs/tiingo/news\n\n            # Dates are in YYYY-MM-DD Format.\n\n            Args:\n                tickers [string] : List of unique Stock Tickers to search\n                tags [string] : List of topics tagged by Tiingo Algorithms\n                sources [string]: List of base urls to include as news sources\n                startDate, endDate [date]: Boundaries of news search window\n                limit (int): Max results returned. Default 100, max 1000\n                offset (int): Search results offset, used for paginating\n                sortBy (string): \"publishedDate\" OR (#TODO: UPDATE THIS)\n        \"\"\"", "output": ": List of tags to search\n                sources [string] : List of sources to search\n                startDate [string] : Start date for search\n                endDate [string] : End date for search\n                limit [int] : Number of results to return\n                offset [int] : Number of results to skip\n                sortBy [string] : Field to sort by\n                fmt [string] : Format to return results\n\n            Returns:\n                dict : Dictionary of results\n        \"\"\"\n        params = {}\n        if tickers:\n            params['tickers'] = ','.join(tick", "solution": "\n        url = \"tiingo/news\"\n        params = {\n            'limit': limit,\n            'offset': offset,\n            'sortBy': sortBy,\n            'tickers': tickers,\n            'sources': sources,\n            'tags': tags,\n            'startDate': startDate,\n            'endDate': endDate\n        }\n        response = self._request('GET', url, params=params)\n        data = response.json()\n        if fmt == 'json':\n            return data\n        elif fmt == 'object':\n            return [dict_to_object(item, \"NewsArticle\") for item in data]"}
{"prompt": "def _iter_nodes(self, node, recursive=False, max_depth=float('inf'),\n                    with_links=True, in_search=False, predicate=None):\n        \"\"\"Returns an iterator over nodes hanging below a given start node.\n\n        :param node:\n\n            Start node\n\n        :param recursive:\n\n            Whether recursively also iterate over the children of the start node's children\n\n        :param max_depth:\n\n            Maximum depth to search for\n\n        :param in_search:\n\n            if it is used during get search and if detailed info should be returned\n\n        :param with_links:\n\n            If links should be considered\n\n        :param predicate:\n\n            A predicate to filter nodes\n\n        :return: Iterator\n\n        \"\"\"", "output": "is in search\n\n        :param predicate:\n\n            Predicate function to filter nodes\n\n        :return:\n\n            Iterator over nodes\n        \"\"\"\n        if node is None:\n            return\n\n        if in_search:\n            if predicate is not None:\n                if predicate(node):\n                    yield node\n            else:\n                yield node\n\n        if recursive:\n            if max_depth > 1:\n                for child in node.children:\n                    for n in self._iter_nodes(child, recursive=True, max_depth=", "solution": "\n        def _run_predicate(x, run_name_set):\n            branch = x.v_run_branch\n            return branch == 'trajectory' or branch in run_name_set\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if predicate is None:\n            predicate = lambda x: True\n        elif isinstance(predicate, (tuple, list)):\n            # Create a predicate from a list of run names or run indices\n            run_list = predicate\n            run_name_set = set()\n            for item in run_list:\n                if item == -1:\n                    run_name_set.add(self._root_instance.f_wildcard('$', -1))\n                elif isinstance(item, int):\n                    run_name_set.add(self._root_instance.f_idx_to_run(item))\n                else:\n                    run_name_set.add(item)\n            predicate = lambda x: _run_predicate(x, run_name_set)\n\n        if recursive:\n            return NaturalNamingInterface._recursive_traversal_bfs(node,\n                                            self._root_instance._linked_by,\n                                            max_depth, with_links,\n                                            in_search, predicate)\n        else:\n            iterator = (x for x in self._make_child_iterator(node, with_links) if\n                        predicate(x[2]))\n            if in_search:\n                return iterator # Here we return tuples: (depth, name, object)\n            else:\n                return (x[2] for x in iterator)"}
{"prompt": "def du_handler(self, args):\n    \"\"\"Handler for size command\"\"\"", "output": "\n    if len(args) == 0:\n        self.send_message(self.get_size())\n    elif len(args) == 1:\n        if args[0] == \"--help\":\n            self.send_message(\"Usage: size [file]\")\n        else:\n            self.send_message(self.get_size(args[0]))\n    else:\n        self.send_message(\"Usage: size [file]\")\n\n\ndef du_help(self):\n    \"\"\"Help for size command\"\"\"\n    self.send", "solution": "\n    for src, size in self.s3handler().size(args[1:]):\n      message('%s\\t%s' % (size, src))"}
{"prompt": "def gauge(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Gauge metric with the specified value\"\"\"", "output": "\n        self.send_metric(name, value, rate, 'gauge')\n\n    def histogram(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Histogram metric with the specified value\"\"\"\n        self.send_metric(name, value, rate, 'histogram')\n\n    def increment(self, name, rate=1):\n        # type: (str, float) -> None\n        \"\"\"Send an Increment metric\"\"\"\n        self.send_metric(name", "solution": "\n\n        if self._should_send_metric(name, rate):\n            if not is_numeric(value):\n                value = float(value)\n            self._request(\n                Gauge(\n                    self._create_metric_name_for_request(name),\n                    value,\n                    rate\n                ).to_request()\n            )"}
{"prompt": "def watch_port_events(port, chip, pin_function_maps, event_queue,\n                      return_after_kbdint=False):\n    \"\"\"Waits for a port event. When a port event occurs it is placed onto the\n    event queue.\n\n    :param port: The port we are waiting for interrupts on (GPIOA/GPIOB).\n    :type port: int\n    :param chip: The chip we are waiting for interrupts on.\n    :type chip: :class:`pifacecommon.mcp23s17.MCP23S17`\n    :param pin_function_maps: A list of classes that have inheritted from\n        :class:`FunctionMap`\\ s describing what to do with events.\n    :type pin_function_maps: list\n    :param event_queue: A queue to put events on.\n    :type event_queue: :py:class:`multiprocessing.Queue`\n    \"\"\"", "output": "17.MCP23S17`\n    :param pin_function_maps: A dictionary of pin to function mappings.\n    :type pin_function_maps: dict\n    :param event_queue: The queue to place events onto.\n    :type event_queue: :class:`queue.Queue`\n    :param return_after_kbdint: If True, return after a keyboard interrupt\n        occurs.\n    :type return_after_kbdint: bool\n    :return: The event that was placed onto the event queue.\n    :rtype: :", "solution": "\n    # set up epoll\n    gpio25 = open(GPIO_INTERRUPT_DEVICE_VALUE, 'r')  # change to use 'with'?\n    epoll = select.epoll()\n    epoll.register(gpio25, select.EPOLLIN | select.EPOLLET)\n\n    while True:\n        # wait here until input\n        try:\n            events = epoll.poll()\n        except KeyboardInterrupt as e:\n            if return_after_kbdint:\n                return\n            else:\n                raise e\n        except IOError as e:\n            # ignore \"Interrupted system call\" error.\n            # I don't really like this solution. Ignoring problems is bad!\n            if e.errno != errno.EINTR:\n                raise\n\n        # find out where the interrupt came from and put it on the event queue\n        if port == pifacecommon.mcp23s17.GPIOA:\n            interrupt_flag = chip.intfa.value\n        else:\n            interrupt_flag = chip.intfb.value\n\n        if interrupt_flag == 0:\n            continue  # The interrupt has not been flagged on this board\n        else:\n            if port == pifacecommon.mcp23s17.GPIOA:\n                interrupt_capture = chip.intcapa.value\n            else:\n                interrupt_capture = chip.intcapb.value\n            event_queue.add_event(InterruptEvent(\n                interrupt_flag, interrupt_capture, chip, time.time()))\n\n    epoll.close()"}
{"prompt": "def _parse_retry_after(self, response):\n        \"\"\"Parse Retry-After header from response if it is set.\"\"\"", "output": "\n        retry_after = response.headers.get('Retry-After')\n        if retry_after:\n            try:\n                retry_after = int(retry_after)\n            except ValueError:\n                retry_after = None\n        return retry_after\n\n    def _parse_retry_after_header(self, response):\n        \"\"\"Parse Retry-After header from response if it is set.\"\"\"\n        retry_after = response.headers.get('Retry-After')\n        if retry_after:\n           ", "solution": "\n        value = response.headers.get('Retry-After')\n\n        if not value:\n            seconds = 0\n        elif re.match(r'^\\s*[0-9]+\\s*$', value):\n            seconds = int(value)\n        else:\n            date_tuple = email.utils.parsedate(value)\n            if date_tuple is None:\n                seconds = 0\n            else:\n                seconds = time.mktime(date_tuple) - time.time()\n        return max(0, seconds)"}
{"prompt": "def _utf8_params(params):\n    \"\"\"encode a dictionary of URL parameters (including iterables) as utf-8\"\"\"", "output": "\n    return dict([(k.encode('utf-8'), _utf8_param_value(v)) for k, v in params.items()])\n\n\ndef _utf8_param_value(value):\n    \"\"\"encode a URL parameter value (including iterables) as utf-8\"\"\"\n    if isinstance(value, unicode):\n        return value.encode('utf-8')\n    elif isinstance(value, list):\n        return [_utf8_param_value(v) for v in value]\n    else:\n        return value\n\n\ndef", "solution": "\n    assert isinstance(params, dict)\n    encoded_params = []\n    for k, v in params.items():\n        if v is None:\n            continue\n        if isinstance(v, integer_types + (float,)):\n            v = str(v)\n        if isinstance(v, (list, tuple)):\n            v = [to_bytes(x) for x in v]\n        else:\n            v = to_bytes(v)\n        encoded_params.append((k, v))\n    return dict(encoded_params)"}
{"prompt": "def parse_variant_id(chrom, pos, ref, alt, variant_type):\n    \"\"\"Parse the variant id for a variant\n\n    variant_id is used to identify variants within a certain type of\n    analysis. It is not human readable since it is a md5 key.\n\n    Args:\n        chrom(str)\n        pos(str)\n        ref(str)\n        alt(str)\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        variant_id(str): The variant id converted to md5 string\n    \"\"\"", "output": "to a md5 key\n    \"\"\"\n    variant_id = f'{chrom}_{pos}_{ref}_{alt}_{variant_type}'\n    variant_id = variant_id.encode('utf-8')\n    variant_id = hashlib.md5(variant_id).hexdigest()\n    return variant_id\n\n\ndef parse_variant_id_from_vcf(chrom, pos, ref, alt, variant_type, vcf_id):\n    \"\"\"Parse the variant id for a variant\n\n    variant_id is used to identify variants within a certain type", "solution": "\n    return generate_md5_key([chrom, pos, ref, alt, variant_type])"}
{"prompt": "def _construct_schema(elements, nsmap):\n    \"\"\"Consruct fiona schema based on given elements\n\n    :param list Element: list of elements\n    :param dict nsmap: namespace map\n\n    :return dict: schema\n    \"\"\"", "output": "\n    schema = {}\n    for element in elements:\n        name = element.get('name')\n        type = element.get('type')\n        if type == 'geometry':\n            schema['geometry'] = 'Point'\n        elif type == 'attribute':\n            schema[name] = 'str'\n        elif type == 'attribute_int':\n            schema[name] = 'int'\n        elif type == 'attribute_float':\n            schema[name] = 'float'\n        elif type == 'attribute_bool':\n            schema[name] = 'bool'", "solution": "\n\n    schema = {\n        'properties': {},\n        'geometry': None\n    }\n\n    schema_key = None\n    gml_key = None\n\n    # if nsmap is defined, use it\n    if nsmap:\n        for key in nsmap:\n            if nsmap[key] == XS_NAMESPACE:\n                schema_key = key\n            if nsmap[key] in GML_NAMESPACES:\n                gml_key = key\n    # if no nsmap is defined, we have to guess\n    else:\n        gml_key = 'gml'\n        schema_key = 'xsd'\n\n    mappings = {\n        'PointPropertyType': 'Point',\n        'PolygonPropertyType': 'Polygon',\n        'LineStringPropertyType': 'LineString',\n        'MultiPointPropertyType': 'MultiPoint',\n        'MultiLineStringPropertyType': 'MultiLineString',\n        'MultiPolygonPropertyType': 'MultiPolygon',\n        'MultiGeometryPropertyType': 'MultiGeometry',\n        'GeometryPropertyType': 'GeometryCollection',\n        'SurfacePropertyType': '3D Polygon',\n        'MultiSurfacePropertyType': '3D MultiPolygon'\n    }\n\n    for element in elements:\n        data_type = element.attrib['type'].replace(gml_key + ':', '')\n        name = element.attrib['name']\n\n        if data_type in mappings:\n            schema['geometry'] = mappings[data_type]\n            schema['geometry_column'] = name\n        else:\n            schema['properties'][name] = data_type.replace(schema_key+':', '')\n\n    if schema['properties'] or schema['geometry']:\n        return schema\n    else:\n        return None"}
{"prompt": "def _flatten(child, parent):\n    \"\"\"Custom flattening method for the parse tree.\"\"\"", "output": "\n    if isinstance(child, list):\n        for item in child:\n            _flatten(item, parent)\n    else:\n        parent.append(child)\n\n\ndef _parse_tree(tree):\n    \"\"\"Parse the parse tree.\"\"\"\n    if isinstance(tree, list):\n        for item in tree:\n            _parse_tree(item)\n    else:\n        if isinstance(tree, tuple):\n            if tree[0] == 'TOP':\n                _flatten(tree[1], tree)\n            else:\n", "solution": "\n    return parent.is_type(TokenType.expression) and child.node_type == parent.node_type"}
{"prompt": "def read(self, resource_id):\n        \"\"\"Read object in elasticsearch using the resource_id.\n        :param resource_id: id of the object to be read.\n        :return: object value from elasticsearch.\n        \"\"\"", "output": "\n        try:\n            return self.es.get(index=self.index, id=resource_id)\n        except NotFoundError:\n            return None\n\n    def update(self, resource_id, data):\n        \"\"\"Update object in elasticsearch using the resource_id.\n        :param resource_id: id of the object to be updated.\n        :param data: new object value.\n        :return: object value from elasticsearch.\n        \"\"\"\n        try:\n            return self.es.update(index=self.index, id=resource_id", "solution": "\n        self.logger.debug('elasticsearch::read::{}'.format(resource_id))\n        return self.driver._es.get(\n            index=self.driver._index,\n            id=resource_id,\n            doc_type='_doc'\n        )['_source']"}
{"prompt": "def date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):\n        \"\"\"use previously calculated variation of the rate to estimate\n        the uncertainty in a particular numdate due to rate variation.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n            node for which the confidence interval is to be calculated\n        interval : tuple, optional\n            Array of length two, or tuple, defining the bounds of the confidence interval\n\n        \"\"\"", "output": "\n        if node.numdate is None:\n            return None\n        if node.numdate.rate_var is None:\n            return None\n        if node.numdate.rate_var == 0:\n            return None\n        if node.numdate.rate_var < 0:\n            return None\n        if node.numdate.rate_var > 1:\n            return None\n        if node.numdate.rate_var > 0.95:\n            return None\n        if node.numdate.rate_var < 0.05", "solution": "\n        if hasattr(node, \"numdate_rate_variation\"):\n            from scipy.special import erfinv\n            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0\n                    for x in interval]\n            l,c,u = [x[1] for x in node.numdate_rate_variation]\n            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])\n\n        else:\n            return None"}
{"prompt": "def score(self, x, w=None, **kwargs):\n        \"\"\"Compute R^2 coefficient of determination for a given input.\n\n        Parameters\n        ----------\n        x : ndarray (num-examples, num-inputs)\n            An array containing data to be fed into the network. Multiple\n            examples are arranged as rows in this array, with columns containing\n            the variables for each example.\n\n        Returns\n        -------\n        r2 : float\n            The R^2 correlation between the prediction of this netork and its\n            input. This can serve as one measure of the information loss of the\n            autoencoder.\n        \"\"\"", "output": ".\n\n        Notes\n        -----\n        The R^2 coefficient of determination is defined as:\n\n        .. math::\n\n            R^2 = 1 - \\\\frac{1}{n} \\\\sum_{i=1}^n (y_i - \\\\hat{y_i})^2\n\n        where :math:`y_i` is the true value of the target variable for the\n        :math:`i`th example and :math:`\\\\hat{y_i}` is the predicted value.\n        \"\"\"\n        if w is None:\n            w = self", "solution": "\n        return super(Autoencoder, self).score(x, x, w=w, **kwargs)"}
{"prompt": "def set_parent(self, parent):\n        \"\"\"Set parent ``Expression`` for this object.\n\n        Args:\n            parent (Expression): The ``Expression`` which contains this object.\n\n        Raises:\n            FiqlObjectException: Parent must be of type ``Expression``.\n        \"\"\"", "output": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of type Expression\")\n        self._parent = parent\n\n    def get_parent(self):\n        \"\"\"Get parent ``Expression`` for this object.\n\n        Returns:\n            Expression: The ``Expression`` which contains this object.\n        \"\"\"\n        return self._parent\n\n    def get_value(self):\n        \"\"\"Get the value of this object.\n\n        Returns:\n            str: The value of this object.\n        \"\"\"\n        return self._value\n", "solution": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of %s not %s\" % (\n                Expression, type(parent)))\n        self.parent = parent"}
{"prompt": "def _build_kernel_function_declaration(self, name='kernel'):\n        \"\"\"Build and return kernel function declaration\"\"\"", "output": "\n        return self._build_function_declaration(name=name,\n                                               return_type=self.return_type,\n                                               args=self.args)\n\n    def _build_kernel_function_definition(self, name='kernel'):\n        \"\"\"Build and return kernel function definition\"\"\"\n        return self._build_function_definition(name=name,\n                                               return_type=self.return_type,\n                                               args=self.args,\n                                               body=self.body)\n\n    def _build_function", "solution": "\n        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)\n        scalar_declarations = self._build_scalar_declarations(with_init=False)\n        const_declarations = self._build_const_declartions(with_init=False)\n        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +\n                                                          const_declarations),\n                              type=c_ast.TypeDecl(declname=name,\n                                                  quals=[],\n                                                  type=c_ast.IdentifierType(names=['void'])))"}
{"prompt": "def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,\n                                   length=8):\n  \"\"\"Visualizes a qualitative analysis of a given model.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    model: A DisentangledSequentialVAE model.\n    samples: Number of samples to draw from the latent distributions.\n    batch_size: Number of sequences to generate.\n    length: Number of timesteps to generate for each sequence.\n  \"\"\"", "output": "of timesteps to generate.\n  \"\"\"\n  # Get the latent distribution.\n  _, z_mean, z_log_var = model.encode(inputs)\n  z_mean = tf.squeeze(z_mean, axis=1)\n  z_log_var = tf.squeeze(z_log_var, axis=1)\n  z_std = tf.exp(0.5 * z_log_var)\n  z_samples = tf.random.normal(shape=[batch_size, samples, z_mean", "solution": "\n  average = lambda dist: tf.reduce_mean(\n      input_tensor=dist.mean(), axis=0)  # avg over samples\n  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n                                    samples=samples)\n    visualize_reconstruction(inputs, average(reconstruct()))\n    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n                             name=\"static_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n                             name=\"dynamic_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),\n                             name=\"swap_static\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),\n                             name=\"swap_dynamic\")\n\n  with tf.compat.v1.name_scope(\"generation\"):\n    generate = functools.partial(model.generate, batch_size=batch_size,\n                                 length=length, samples=samples)\n    image_summary(average(generate(fix_static=True)), \"fix_static\")\n    image_summary(average(generate(fix_dynamic=True)), \"fix_dynamic\")"}
{"prompt": "def __single_arity_fn_to_py_ast(\n    ctx: GeneratorContext,\n    node: Fn,\n    method: FnMethod,\n    def_name: Optional[str] = None,\n    meta_node: Optional[MetaNode] = None,\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a function with a single arity.\"\"\"", "output": "\n    if def_name is None:\n        def_name = node.name\n\n    if meta_node is None:\n        meta_node = node.meta\n\n    # Create a function definition.\n    fn_def = ast.FunctionDef(\n        name=def_name,\n        args=ast.arguments(\n            args=[],\n            vararg=None,\n            kwonlyargs=[],\n            kw_defaults=[],\n            kwarg=None,\n            defaults=[],\n        ),\n        body=[],\n        decorator_list=[],", "solution": "\n    assert node.op == NodeOp.FN\n    assert method.op == NodeOp.FN_METHOD\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n    with ctx.new_symbol_table(py_fn_name), ctx.new_recur_point(\n        method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n    ):\n        # Allow named anonymous functions to recursively call themselves\n        if lisp_fn_name is not None:\n            ctx.symbol_table.new_symbol(\n                sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n            )\n\n        fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n            ctx, method.params, method.body\n        )\n        meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n        return GeneratedPyAST(\n            node=ast.Name(id=py_fn_name, ctx=ast.Load()),\n            dependencies=list(\n                chain(\n                    meta_deps,\n                    [\n                        py_fn_node(\n                            name=py_fn_name,\n                            args=ast.arguments(\n                                args=fn_args,\n                                kwarg=None,\n                                vararg=varg,\n                                kwonlyargs=[],\n                                defaults=[],\n                                kw_defaults=[],\n                            ),\n                            body=fn_body_ast,\n                            decorator_list=list(\n                                chain(\n                                    meta_decorators,\n                                    [_BASILISP_FN_FN_NAME],\n                                    [_TRAMPOLINE_FN_NAME]\n                                    if ctx.recur_point.has_recur\n                                    else [],\n                                )\n                            ),\n                            returns=None,\n                        )\n                    ],\n                )\n            ),\n        )"}
{"prompt": "def activate(fn=None):\n    \"\"\"\n    Enables the HTTP traffic interceptors.\n\n    This function can be used as decorator.\n\n    Arguments:\n        fn (function|coroutinefunction): Optional function argument\n            if used as decorator.\n\n    Returns:\n        function: decorator wrapper function, only if called as decorator,\n            otherwise ``None``.\n\n    Example::\n\n        # Standard use case\n        pook.activate()\n        pook.mock('server.com/foo').reply(404)\n\n        res = requests.get('server.com/foo')\n        assert res.status_code == 404\n        pook.disable()\n\n        # Decorator use case\n        @pook.activate\n        def test_request():\n            pook.mock('server.com/foo').reply(404)\n\n            res = requests.get('server.com/foo')\n            assert res.status_code == 404\n    \"\"\"", "output": "4)\n        requests.get('http://server.com/foo')\n\n        # Use as decorator\n        @pook.activate\n        def test():\n            pook.mock('server.com/foo').reply(404)\n            requests.get('http://server.com/foo')\n\n        test()\n\n    \"\"\"\n    if fn is None:\n        return _activate\n    return _activate(fn)\n\n\ndef _activate(fn):\n    \"\"\"\n    Activates the HTTP traffic interceptors.\n\n   ", "solution": "\n    # If not used as decorator, activate the engine and exit\n    if not isfunction(fn):\n        _engine.activate()\n        return None\n\n    # If used as decorator for an async coroutine, wrap it\n    if iscoroutinefunction is not None and iscoroutinefunction(fn):\n        return activate_async(fn, _engine)\n\n    @functools.wraps(fn)\n    def wrapper(*args, **kw):\n        _engine.activate()\n        try:\n            fn(*args, **kw)\n        finally:\n            _engine.disable()\n\n    return wrapper"}
{"prompt": "def parse(self, string):\n        \"\"\"\n        Parse lipid definition from string:\n\n            alhead=C P, allink=A A, altail=TCC CCCC, alname=DPSM, charge=0.0\n        \"\"\"", "output": "\n        self.lipid_string = string\n        self.lipid_string = self.lipid_string.replace(' ', '')\n        self.lipid_string = self.lipid_string.replace('=', ':')\n        self.lipid_string = self.lipid_string.replace(',', ':')\n        self.lipid_string = self.lipid_string.replace(';', ':')\n        self.lipid_string = self.lipid_string.replace(';', ':')\n        self.lipid_", "solution": "\n        fields = [i.split(\"=\") for i in string.split(', ')]\n        for what, val in fields:\n            what = what.strip()\n            val  = val.split()\n            if what.endswith(\"head\"):\n                self.head = val\n            elif what.endswith(\"link\"):\n                self.link = val\n            elif what.endswith(\"tail\"):\n                self.tail = val\n            elif what == \"charge\":\n                self.charge = float(val[0])\n            elif what.endswith(\"name\") and not self.name:\n                self.name = val[0]\n        if self.charge is None:\n            # Infer charge from head groups\n            self.charge = sum([headgroup_charges[bead] for bead in self.head])"}
{"prompt": "def combine_columns_aggregation(\n        df,\n        id_cols: List[str],\n        cols_for_combination: Dict[str, str],\n        agg_func: Union[str, List[str], Dict[str, str]] = 'sum'\n):\n    \"\"\"\n    Aggregates data to reproduce \"All\" category for requester\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list*): the columns id to group\n    - `cols_for_combination` (*dict*): colums corresponding to\n       the filters as key and their default value as value\n\n    *optional :*\n    - `agg_func` (*str*, *list* or *dict*): the function(s) to use for aggregating the data.\n       Accepted combinations are:\n       - string function name\n       - list of functions and/or function names, e.g. [np.sum, 'mean']\n       - dict of axis labels -> functions, function names or list of such.\n    \"\"\"", "output": "for_combination` (*dict*): the columns to combine\n    - `agg_func` (*str, list, dict*): the aggregation function to apply\n\n    ---\n\n    ### Example\n\n    ```python\n    combine_columns_aggregation(\n        df,\n        id_cols=['country', 'year'],\n        cols_for_combination={'value': 'value_2019'},\n        agg_func='sum'\n    )\n    ```\n\n    ---\n\n    ### Returns\n\n    *pandas.DataFrame*", "solution": "\n    requesters_cols = list(cols_for_combination.keys())\n    requester_combination = [\n        list(item) for i in range(0, len(requesters_cols) + 1)\n        for item in itertools.combinations(requesters_cols, i)]\n    dfs_result = []\n    for comb in requester_combination:\n        df_tmp = df.groupby(id_cols + comb).agg(agg_func).reset_index()\n        for key in (set(cols_for_combination.keys()) - set(comb)):\n            df_tmp[key] = cols_for_combination[key]\n        dfs_result.append(df_tmp)\n\n    return pd.concat(dfs_result, sort=False, ignore_index=True)"}
{"prompt": "def _backup_file(self, file, patch):\n        \"\"\" Creates a backup of file \"\"\"", "output": "\n        backup_file = file + '.bak'\n        if os.path.exists(backup_file):\n            os.remove(backup_file)\n        os.rename(file, backup_file)\n        if patch:\n            self.patch_file(backup_file)\n\n    def _restore_file(self, file, backup_file):\n        \"\"\" Restores file from backup \"\"\"\n        if os.path.exists(file):\n            os.remove(file)\n        os.rename(backup_file, file)", "solution": "\n        dest_dir = self.quilt_pc + patch.get_name()\n        file_dir = file.get_directory()\n        if file_dir:\n            #TODO get relative path\n            dest_dir = dest_dir + file_dir\n        backup = Backup()\n        backup.backup_file(file, dest_dir, copy_empty=True)"}
{"prompt": "def delete(self, url, **kwargs):\n        \"\"\"Sends a DELETE request.\n\n        Args:\n            url(basestring): The URL of the API endpoint.\n            **kwargs:\n                erc(int): The expected (success) response code for the request.\n                others: Passed on to the requests package.\n\n        Raises:\n            ApiError: If anything other than the expected response code is\n                returned by the Webex Teams API endpoint.\n\n        \"\"\"", "output": "\n        return self._request('DELETE', url, **kwargs)\n\n    def get(self, url, **kwargs):\n        \"\"\"Sends a GET request.\n\n        Args:\n            url(basestring): The URL of the API endpoint.\n            **kwargs:\n                erc(int): The expected (success) response code for the request.\n                others: Passed on to the requests package.\n\n        Raises:\n            ApiError: If anything other than the expected response code is\n                returned by the Webex Teams API endpoint.\n\n       ", "solution": "\n        check_type(url, basestring, may_be_none=False)\n\n        # Expected response code\n        erc = kwargs.pop('erc', EXPECTED_RESPONSE_CODE['DELETE'])\n\n        self.request('DELETE', url, erc, **kwargs)"}
{"prompt": "def meteor_random_id(name=None, length=17):\n    \"\"\"Generate a new ID, optionally using namespace of given `name`.\"\"\"", "output": "\n    if name is None:\n        name = ''\n    else:\n        name = name.lower()\n    return ''.join([name, str(random.randint(0, 0x7fffffff))])[:length]\n\n\ndef meteor_random_id_with_prefix(prefix, length=17):\n    \"\"\"Generate a new ID, optionally using namespace of given `name`.\"\"\"\n    return ''.join([prefix, str(random.randint(0, 0x7fffffff))])[:length]\n\n\ndef meteor", "solution": "\n    if name is None:\n        stream = THREAD_LOCAL.alea_random\n    else:\n        stream = THREAD_LOCAL.random_streams[name]\n    return stream.random_string(length, METEOR_ID_CHARS)"}
{"prompt": "def INIT(self):\n        \"\"\"INIT state.\n\n        [:rfc:`2131#section-4.4.1`]::\n\n            The client SHOULD wait a random time between one and ten\n            seconds to desynchronize the use of DHCP at startup\n\n        .. todo::\n           - The initial delay is implemented, but probably is not in other\n             implementations. Check what other implementations do.\n        \"\"\"", "output": "\n        self.delay_init()\n        self.delay_init_done = True\n        self.delay_init_timer = self.schedule_delay_init()\n        self.state = 'INIT-REBOOT'\n\n    def INIT_REBOOT(self):\n        \"\"\"INIT-REBOOT state.\n\n        [:rfc:`2131#section-4.4.1`]::\n\n            The client SHOULD wait a random time between one and ten\n            seconds to desynchronize the use of DHCP at startup", "solution": "\n        # NOTE: in case INIT is reached from other state, initialize attributes\n        # reset all variables.\n        logger.debug('In state: INIT')\n        if self.current_state is not STATE_PREINIT:\n            self.reset()\n        self.current_state = STATE_INIT\n        # NOTE: see previous TODO, maybe this is not needed.\n        if self.delay_selecting:\n            if self.delay_before_selecting is None:\n                delay_before_selecting = gen_delay_selecting()\n            else:\n                delay_before_selecting = self.delay_before_selecting\n        else:\n            delay_before_selecting = 0\n        self.set_timeout(self.current_state,\n                         self.timeout_delay_before_selecting,\n                         delay_before_selecting)\n        if self.timeout_select is not None:\n            self.set_timeout(STATE_SELECTING,\n                             self.timeout_selecting,\n                             self.timeout_select)"}
{"prompt": "def bisect(seq, func=bool):\n\t\"\"\"\n\tSplit a sequence into two sequences:  the first is elements that\n\treturn False for func(element) and the second for True for\n\tfunc(element).\n\tBy default, func is ``bool``, so uses the truth value of the object.\n\n\t>>> is_odd = lambda n: n%2\n\t>>> even, odd = bisect(range(5), is_odd)\n\t>>> list(odd)\n\t[1, 3]\n\t>>> list(even)\n\t[0, 2, 4]\n\n\t>>> other, zeros = bisect(reversed(range(5)))\n\t>>> list(zeros)\n\t[0]\n\t>>> list(other)\n\t[4, 3, 2, 1]\n\n\t\"\"\"", "output": ")\n\t[0, 2, 4]\n\t\"\"\"\n\t# bisect_left and bisect_right are not used because they would\n\t# require the sequence to be sorted.\n\t# bisect_right(seq, value) returns the index of the first element\n\t# greater than or equal to value.\n\t# bisect_left(seq, value) returns the index of the first element\n\t# greater than value.\n\t# bisect_left(seq, value) is equivalent to bisect_right(seq, value) - 1\n\t", "solution": "\n\tqueues = GroupbySaved(seq, func)\n\treturn queues.get_first_n_queues(2)"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        if default:\n            prompt = '%s [y/N]? ' % prompt\n        else:\n            prompt = '%s [y/N]? ' % prompt\n        while True:\n            sys.stdout.write(prompt)\n            choice = raw_input().lower()\n            if default and not choice:\n                return True\n            if choice in ('y', 'yes'):\n                return True\n            if choice in ('n', 'no'):\n                return False\n\n    def query_yes_no(self, question, default=None):\n        \"\"\"", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def check_type(self, value, attr, data):\n        \"\"\"Validates a value against the correct type of the field.\n\n        It calls ``_expected_types`` to get a list of valid types.\n\n        Subclasses can do one of the following:\n\n            1. They can override the ``valid_types`` property with a tuple with\n            the expected types for this field.\n\n            2. They can override the ``_expected_types`` method to return a\n            tuple of expected types for the field.\n\n            3. They can change ``check_type`` completely to customize\n            validation.\n\n        This method or the overrides must return the ``value`` parameter\n        untouched.\n        \"\"\"", "output": "type`` to do something else.\n\n        :param value: The value to check.\n        :param attr: The attribute name.\n        :param data: The data passed to the ``Schema``.\n        :return: The value.\n        \"\"\"\n        if not isinstance(value, self._expected_types):\n            raise ValidationError(\n                self.error_messages['invalid'],\n                field_names=[attr],\n                validator_value=self._expected_types,\n                value=value\n            )\n        return value\n\n    def _expected_types", "solution": "\n        expected_types = self._expected_types()\n        if not isinstance(value, expected_types):\n            raise self._not_expected_type(\n                value, expected_types, fields=[self], field_names=attr, data=data)\n        return value"}
{"prompt": "def _get_selection_cursor(self, start, end):\n        \"\"\" Convenience method that returns a cursor with text selected between\n            the positions 'start' and 'end'.\n        \"\"\"", "output": "\n        cursor = self.textCursor()\n        cursor.setPosition(start)\n        cursor.movePosition(QtGui.QTextCursor.Right, QtGui.QTextCursor.KeepAnchor, end - start)\n        return cursor\n\n    def _get_selection_text(self, start, end):\n        \"\"\" Convenience method that returns the text selected between the\n            positions 'start' and 'end'.\n        \"\"\"\n        cursor = self._get_selection_cursor(start, end)\n        return cursor.selectedText()\n\n    def _get_", "solution": "\n        cursor = self._control.textCursor()\n        cursor.setPosition(start)\n        cursor.setPosition(end, QtGui.QTextCursor.KeepAnchor)\n        return cursor"}
{"prompt": "def delay(self, sender, receiver, packet, add_delay=0):\n        \"\"\"\n        bandwidths are inaccurate, as we don't account for parallel transfers here\n        \"\"\"", "output": "\n        if self.delay_model == 'exponential':\n            delay = self.delay_exponential(sender, receiver, packet)\n        elif self.delay_model == 'normal':\n            delay = self.delay_normal(sender, receiver, packet)\n        elif self.delay_model == 'poisson':\n            delay = self.delay_poisson(sender, receiver, packet)\n        elif self.delay_model == 'uniform':\n            delay = self.delay_uniform(sender, receiver, packet)\n        else:\n            raise Exception('Unknown", "solution": "\n        bw = min(sender.ul_bandwidth, receiver.dl_bandwidth)\n        delay = sender.base_latency + receiver.base_latency\n        delay += len(packet) / bw\n        delay += add_delay\n        return delay"}
{"prompt": "def look_ahead_match(rating, tokens):\n    \"\"\"iterate through all tokens looking for matches of cleansed tokens\n    or token regexes, skipping tokens left empty by cleansing and\n    coping with Token objects that produce multiple space-separated\n    strings when cleansed.  Yields tokens that match.\n\n    \"\"\"", "output": "\n    for token in tokens:\n        if token.empty:\n            continue\n        if token.cleansed:\n            if token.match(rating):\n                yield token\n        else:\n            for regex in token.regexes:\n                if regex.match(rating):\n                    yield token\n\ndef look_ahead_match_all(rating, tokens):\n    \"\"\"iterate through all tokens looking for matches of cleansed tokens\n    or token regexes, skipping tokens left empty by cleansing and\n    coping with Token objects that produce multiple space-", "solution": "\n    ## this ensures that all cleansed tokens are non-zero length\n    all_mregexes = []\n    for m in rating.mentions:\n        mregexes = []\n        mpatterns = m.decode('utf8').split(' ')\n        for mpat in mpatterns:\n            if mpat.startswith('ur\"^') and mpat.endswith('$\"'): # is not regex\n                ## chop out the meat of the regex so we can reconstitute it below\n                mpat = mpat[4:-2]\n            else:\n                mpat = cleanse(mpat)\n            if mpat:\n                ## make a unicode raw string\n                ## https://docs.python.org/2/reference/lexical_analysis.html#string-literals\n                mpat = ur'^%s$' % mpat\n                logger.debug('look_ahead_match compiling regex: %s', mpat)\n                mregexes.append(re.compile(mpat, re.UNICODE | re.IGNORECASE))\n\n        if not mregexes:\n            logger.warn('got empty cleansed mention: %r\\nrating=%r' % (m, rating))\n\n        all_mregexes.append(mregexes)\n\n    ## now that we have all_mregexes, go through all the tokens\n    for i in range(len(tokens)):\n        for mregexes in all_mregexes:\n            if mregexes[0].match(tokens[i][0][0]):\n                ## found the start of a possible match, so iterate\n                ## through the tuples of cleansed strings for each\n                ## Token while stepping through the cleansed strings\n                ## for this mention.\n                m_j = 1\n                i_j = 0\n                last_token_matched = 0\n                matched = True\n                while m_j < len(mregexes):\n                    i_j += 1\n                    if i_j == len(tokens[i + last_token_matched][0]):\n                        i_j = 0\n                        last_token_matched += 1\n                        if i + last_token_matched == len(tokens):\n                            matched = False\n                            break\n                    target_token = tokens[i + last_token_matched][0][i_j]\n                    ## this next line is the actual string comparison\n                    if mregexes[m_j].match(target_token):\n                        m_j += 1\n                    elif target_token == '':\n                        continue\n                    else:\n                        matched = False\n                        break\n                if matched:\n                    ## yield each matched token only once\n                    toks = set()\n                    for j in xrange(last_token_matched + 1):\n                        toks.add(tokens[i + j][1])\n                    for tok in toks:\n                        yield tok"}
{"prompt": "def stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports):\n    \"\"\"return lines with leading/trailing whitespace and any ignored code\n    features removed\n    \"\"\"", "output": "\n    if ignore_comments:\n        lines = [line for line in lines if not line.lstrip().startswith('#')]\n    if ignore_docstrings:\n        lines = [line for line in lines if not line.lstrip().startswith('\"\"\"')]\n    if ignore_imports:\n        lines = [line for line in lines if not line.lstrip().startswith('import')]\n    return [line.strip() for line in lines]\n\n\ndef count_lines(lines, ignore_comments, ignore_docstrings, ignore_imports", "solution": "\n    if ignore_imports:\n        tree = astroid.parse(\"\".join(lines))\n        node_is_import_by_lineno = (\n            (node.lineno, isinstance(node, (astroid.Import, astroid.ImportFrom)))\n            for node in tree.body\n        )\n        line_begins_import = {\n            lineno: all(is_import for _, is_import in node_is_import_group)\n            for lineno, node_is_import_group in groupby(\n                node_is_import_by_lineno, key=lambda x: x[0]\n            )\n        }\n        current_line_is_import = False\n\n    strippedlines = []\n    docstring = None\n    for lineno, line in enumerate(lines, start=1):\n        line = line.strip()\n        if ignore_docstrings:\n            if not docstring and any(\n                line.startswith(i) for i in ['"}
{"prompt": "def add_field(self, model, field):\n        \"\"\"Ran when a field is added to a model.\"\"\"", "output": "\n        pass\n\n    def remove_field(self, model, field):\n        \"\"\"Ran when a field is removed from a model.\"\"\"\n        pass\n\n    def alter_field(self, model, old_field, new_field, strict=False):\n        \"\"\"Ran when a field is altered on a model.\"\"\"\n        pass\n\n    def alter_unique_together(self, model, old_unique_together, new_unique_together):\n        \"\"\"Ran when the unique_together is altered on a model.\"\"\"\n       ", "solution": "\n\n        for key in self._iterate_required_keys(field):\n            self._create_hstore_required(\n                model._meta.db_table,\n                field,\n                key\n            )"}
{"prompt": "def get_organisation_information(self, query_params=None):\n        \"\"\"\n        Get information fot this organisation. Returns a dictionary of values.\n        \"\"\"", "output": "\n        return self.get_resource(\n            'organisation',\n            query_params=query_params\n        )\n\n    def get_organisation_members(self, query_params=None):\n        \"\"\"\n        Get information fot this organisation. Returns a dictionary of values.\n        \"\"\"\n        return self.get_resource(\n            'organisation/members',\n            query_params=query_params\n        )\n\n    def get_organisation_members_invited(self, query_params=None):\n        \"\"\"\n        Get information fot this organisation. Returns a dictionary of", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )"}
{"prompt": "def verified(context, collaborator, test, outpath=None):\n    \"\"\"Export variants which have been verified for an institute\n        and write them to an excel file.\n\n    Args:\n        collaborator(str): institute id\n        test(bool): True if the function is called for testing purposes\n        outpath(str): path to output file\n\n    Returns:\n        written_files(int): number of written or simulated files\n    \"\"\"", "output": "\n    # Get the institute object\n    institute = context.driver.institute(collaborator)\n\n    # Get the verified variants\n    variants = institute.verified_variants()\n\n    # Write the variants to an excel file\n    written_files = write_verified_variants(variants, outpath)\n\n    # Return the number of written files\n    return written_files\n\n\ndef write_verified_variants(variants, outpath=None):\n    \"\"\"Write verified variants to an excel file.\n\n    Args", "solution": "\n    written_files = 0\n    collaborator = collaborator or 'cust000'\n    LOG.info('Exporting verified variants for cust {}'.format(collaborator))\n\n    adapter = context.obj['adapter']\n    verified_vars = adapter.verified(institute_id=collaborator)\n    LOG.info('FOUND {} verified variants for institute {}'.format(len(verified_vars), collaborator))\n\n\n    if not verified_vars:\n        LOG.warning('There are no verified variants for institute {} in database!'.format(collaborator))\n        return None\n\n    document_lines = export_verified_variants(verified_vars)\n\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n    document_name = '.'.join(['verified_variants', collaborator, today]) + '.xlsx'\n\n    # If this was a test and lines are created return success\n    if test and document_lines:\n        written_files +=1\n        LOG.info('Success. Verified variants file contains {} lines'.format(len(document_lines)))\n        return written_files\n\n    # create workbook and new sheet\n    # set up outfolder\n    if not outpath:\n        outpath = str(os.getcwd())\n    workbook = Workbook(os.path.join(outpath,document_name))\n    Report_Sheet = workbook.add_worksheet()\n\n    # Write the column header\n    row = 0\n    for col,field in enumerate(VERIFIED_VARIANTS_HEADER):\n        Report_Sheet.write(row,col,field)\n\n    # Write variant lines, after header (start at line 1)\n    for row, line in enumerate(document_lines,1): # each line becomes a row in the document\n        for col, field in enumerate(line): # each field in line becomes a cell\n            Report_Sheet.write(row,col,field)\n    workbook.close()\n\n    if os.path.exists(os.path.join(outpath,document_name)):\n        LOG.info('Success. Verified variants file of {} lines was written to disk'. format(len(document_lines)))\n        written_files += 1\n\n    return written_files"}
{"prompt": "def _plot_result(self, traj, result_name):\n        \"\"\"Plots a state variable graph for several neurons into one figure\"\"\"", "output": "\n        # TODO: add a legend\n        # TODO: add a title\n        # TODO: add a colorbar\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a label\n        # TODO: add a", "solution": "\n        result = traj.f_get(result_name)\n        varname = result.record_variables[0]\n        values = result[varname]\n        times = result.t\n\n        record = result.record\n\n        for idx, celia_neuron in enumerate(record):\n            plt.subplot(len(record), 1, idx+1)\n            plt.plot(times, values[idx,:])\n            if idx==0:\n                plt.title('%s' % varname)\n            if idx==1:\n                plt.ylabel('%s' % ( varname))\n            if idx == len(record)-1:\n                plt.xlabel('t')"}
{"prompt": "def unregister_sizer(self, attr_name):\n        \"\"\"\n        Unregister the SizedImage subclass currently assigned to `attr_name`.\n\n        If a SizedImage subclass isn't already registered to `attr_name`\n        NotRegistered will raise.\n        \"\"\"", "output": "\n        if attr_name not in self._sizer_registry:\n            raise NotRegistered(attr_name)\n        del self._sizer_registry[attr_name]\n\n    def register_sizer(self, attr_name, sizer_class):\n        \"\"\"\n        Register a SizedImage subclass to `attr_name`.\n\n        If a SizedImage subclass is already registered to `attr_name`\n        AlreadyRegistered will raise.\n        \"\"\"\n        if attr_name in self._sizer_registry:\n            raise Al", "solution": "\n        if attr_name not in self._sizedimage_registry:\n            raise NotRegistered(\n                'No SizedImage subclass is registered to %s' % attr_name\n            )\n        else:\n            del self._sizedimage_registry[attr_name]"}
{"prompt": "def assert_no_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"", "output": "\n        return self.assert_title(title, **kwargs)\n\n    def assert_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page has the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"\n        return self.", "solution": "\n\n        query = TitleQuery(title, **kwargs)\n\n        @self.synchronize(wait=query.wait)\n        def assert_no_title():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n            return True\n\n        return assert_no_title()"}
{"prompt": "def __fetch(self, url, payload):\n        \"\"\"Fetch requests from groupsio API\"\"\"", "output": "\n        try:\n            r = requests.post(url, data=payload)\n            r.raise_for_status()\n            return r.json()\n        except requests.exceptions.HTTPError as err:\n            raise GroupioError(err)\n\n    def __get_token(self):\n        \"\"\"Get a token from the groupsio API\"\"\"\n        payload = {\n            'username': self.username,\n            'password': self.password,\n            'grant_type': 'password',\n            'client_id': self.client_id,", "solution": "\n\n        r = requests.get(url, params=payload, auth=self.auth, verify=self.verify)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise e\n\n        return r"}
{"prompt": "def f_shrink(self, force=False):\n        \"\"\" Shrinks the trajectory and removes all exploration ranges from the parameters.\n        Only possible if the trajectory has not been stored to disk before or was loaded as new.\n\n        :param force:\n\n            Usually you cannot shrink the trajectory if it has been stored to disk,\n            because there's no guarantee that it is actually shrunk if there\n            still exist explored parameters on disk. In case you are certain that\n            you did not store explored parameters to disk set or you deleted all\n            of them from disk set `force=True`.\n\n        :raises: TypeError if the trajectory was stored before.\n\n        \"\"\"", "output": "\n            explored parameters, you can set force=True to force the shrinking.\n\n        :return:\n\n            True if the trajectory was shrunk, False otherwise.\n        \"\"\"\n        if self.stored:\n            if force:\n                self.shrink()\n                return True\n            else:\n                return False\n        else:\n            self.shrink()\n            return True\n\n    def shrink(self):\n        \"\"\" Shrinks the trajectory and removes all exploration ranges from the parameters.\n        \"\"\"\n        self.traject", "solution": "\n        if self._stored and not force:\n            raise TypeError('Your trajectory is already stored to disk or database, shrinking is '\n                            'not allowed.')\n\n        for param in self._explored_parameters.values():\n            param.f_unlock()\n            try:\n                param._shrink()\n            except Exception as exc:\n                self._logger.error('Could not shrink `%s` because of:`%s`' %\n                                   (param.v_full_name, repr(exc)))\n\n        # If we shrink, we do not have any explored parameters left and we can erase all\n        # run information, and the length of the trajectory is 1 again.\n        self._explored_parameters = {}\n        self._run_information = {}\n        self._single_run_ids = {}\n        self._add_run_info(0)\n        self._test_run_addition(1)"}
{"prompt": "def get_data_disk(self, service_name, deployment_name, role_name, lun):\n        \"\"\"\n        Retrieves the specified data disk from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            The Logical Unit Number (LUN) for the disk.\n        \"\"\"", "output": "\n        return self.get_data_disks(service_name, deployment_name, role_name)[lun]\n\n    def get_data_disks(self, service_name, deployment_name, role_name):\n        \"\"\"\n        Retrieves the data disks from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        \"\"\"\n        return self.get_role(service_name, deployment", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_get(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            DataVirtualHardDisk)"}
{"prompt": "def for_type(typ, func):\n    \"\"\"\n    Add a pretty printer for a given type.\n    \"\"\"", "output": "\n    if typ not in _pretty_printers:\n        _pretty_printers[typ] = []\n    _pretty_printers[typ].append(func)\n\n\ndef for_type_and_subtypes(typ, func):\n    \"\"\"\n    Add a pretty printer for a given type and all its subtypes.\n    \"\"\"\n    if typ not in _pretty_printers:\n        _pretty_printers[typ] = []\n    _pretty_printers[typ].append(func)\n", "solution": "\n    oldfunc = _type_pprinters.get(typ, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _type_pprinters[typ] = func\n    return oldfunc"}
{"prompt": "def structured_mesh(shape = (2,2,2), dim = (1.,1.,1.)):\n    \"\"\"\n    Returns a structured mesh. \n    \n    :arg shape: 2 or 3 integers (eg: shape = (10, 10, 10)).\n    :type shape: tuple\n    :arg dim: 2 or 3 floats (eg: dim = (4., 2., 1.))\n    :type dim: tuple\n    \n    .. note::\n\n       This function does not use GMSH for mesh generation.\n\n\n    \n    >>> import argiope as ag\n    >>> mesh = ag.mesh.structured_mesh(shape =(10,10,10), dim=(1.,1.,1.)))\n    \"\"\"", "output": "msh.\n    \"\"\"\n    from dolfin import *\n    mesh = Mesh(shape, dim)\n    return mesh\n\ndef unstructured_mesh(mesh_file):\n    \"\"\"\n    Returns an unstructured mesh. \n    \n    :arg mesh_file: Path to the mesh file.\n    :type mesh_file: string\n    \n    .. note::\n\n       This function does not use Gmsh.\n    \"\"\"\n    from dolfin import *\n    mesh = Mesh(mesh_file)\n    return mesh", "solution": "\n    # PREPROCESSING\n    shape = np.array(shape)\n    dim   = np.array(dim) \n    Ne = shape.prod()\n    Nn = (shape + 1).prod()\n    # LABELS\n    nindex = np.arange(Nn) + 1\n    eindex = np.arange(Ne) + 1\n    # COORDINATES\n    coords = [ np.linspace(0., dim[i], shape[i] + 1) for i in range(len(shape))]\n    coords = np.array(np.meshgrid(*coords))\n    coords = np.array([c.swapaxes(0,1).flatten(\"F\") for c in coords]).T\n    if len(shape) == 2:\n        c = coords\n        coords = np.zeros((Nn, 3))\n        coords[:, :2] = c  \n    # CONNECTIVITY    \n    conn = _make_conn(shape)\n    # MESH INSTANCE\n    mesh = Mesh(nlabels = nindex,\n                coords  = coords,\n                elabels = eindex,\n                conn = conn,)\n    if len(shape) == 2: mesh.elements[(\"type\", \"argiope\")] = \"quad4\"\n    if len(shape) == 3: mesh.elements[(\"type\", \"argiope\")] = \"hexa8\"    \n    return mesh"}
{"prompt": "def inshape(shape, axes):\n    \"\"\"\n    Checks to see if a list of axes are contained within an array shape.\n\n    Parameters\n    ----------\n    shape : tuple[int]\n        the shape of a BoltArray\n\n    axes : tuple[int]\n        the axes to check against shape\n    \"\"\"", "output": "\n    if len(axes) != len(shape):\n        raise ValueError(\"axes and shape must be the same length\")\n    for i, axis in enumerate(axes):\n        if axis >= shape[i]:\n            raise ValueError(\"axis %d is out of bounds\" % axis)\n\n\ndef check_axes(axes, shape):\n    \"\"\"\n    Checks to see if a list of axes are contained within an array shape.\n\n    Parameters\n    ----------\n    axes : tuple[int]\n        the axes to check against shape", "solution": "\n    valid = all([(axis < len(shape)) and (axis >= 0) for axis in axes])\n    if not valid:\n        raise ValueError(\"axes not valid for an ndarray of shape: %s\" % str(shape))"}
{"prompt": "def desc(value):\n    \"\"\" str: A normalized representation for a user-provided value. \"\"\"", "output": "\n    return value.replace(' ', '_').lower()\n\n\ndef _get_value(value, default):\n    \"\"\"\n    Get a value from a dictionary or a default value.\n\n    Parameters\n    ----------\n    value : str or dict\n        The value to get.\n    default : str\n        The default value.\n\n    Returns\n    -------\n    str\n        The value.\n    \"\"\"\n    if isinstance(value, dict):\n        return value.get('value', default)\n    return value\n\n\ndef _get_values", "solution": "\n\n    def normalize_strings(value):\n        if isinstance(value, list):\n            value = [normalize_strings(e) for e in value]\n\n        if isinstance(value, dict):\n            value = {normalize_strings(k): normalize_strings(v) for k, v in iter(value.items())}\n\n        if isregex(value):\n            value = value.pattern\n\n        if isbytes(value):\n            value = decode_bytes(value)\n\n        if PY2:\n            if isstring(value):\n                # In Python 2, strings (``unicode`` objects) represent as ``u'...'``, so ensure\n                # the string is encoded (as a ``str`` object) for cleaner representation.\n                value = encode_string(value)\n\n        return value\n\n    value = normalize_strings(value)\n\n    return repr(value)"}
{"prompt": "def recommend_k_items_slow(self, test, top_k=10, remove_seen=True):\n        \"\"\"Recommend top K items for all users which are in the test set.\n\n        Args:\n            test: test Spark dataframe\n            top_k: top n items to return\n            remove_seen: remove items test users have already seen in the past from the recommended set.\n        \"\"\"", "output": "\n        # TODO: implement this method\n        raise NotImplementedError\n\n    def recommend_k_items_fast(self, test, top_k=10, remove_seen=True):\n        \"\"\"Recommend top K items for all users which are in the test set.\n\n        Args:\n            test: test Spark dataframe\n            top_k: top n items to return\n            remove_seen: remove items test users have already seen in the past from the recommended set.\n        \"\"\"\n        # TODO: implement this method\n        raise NotImplementedError", "solution": "\n\n        # TODO: remove seen\n        if remove_seen:\n            raise ValueError(\"Not implemented\")\n\n        self.get_user_affinity(test)\\\n            .write.mode(\"overwrite\")\\\n            .saveAsTable(self.f(\"{prefix}user_affinity\"))\n\n        # user_affinity * item_similarity\n        # filter top-k\n        query = self.f(\n            "}
{"prompt": "def basis_state(str_state, num):\n    \"\"\"\n    Return a basis state ndarray.\n\n    Args:\n        str_state (string): a string representing the state.\n        num (int): the number of qubits\n    Returns:\n        ndarray:  state(2**num) a quantum state with basis basis state.\n     Raises:\n        QiskitError: if the dimensions is wrong\n    \"\"\"", "output": "\n    if num < 0:\n        raise QiskitError('Number of qubits must be non-negative.')\n    if len(str_state) != 2 ** num:\n        raise QiskitError('Length of state string must equal 2**num.')\n    state = np.zeros(2 ** num, dtype=complex)\n    for i, state_i in enumerate(str_state):\n        if state_i == '1':\n            state[i] = 1\n        elif state_i == '0':\n            state[i", "solution": "\n    n = int(str_state, 2)\n    if num >= len(str_state):\n        state = np.zeros(1 << num, dtype=complex)\n        state[n] = 1\n        return state\n    else:\n        raise QiskitError('size of bitstring is greater than num.')"}
{"prompt": "def _sparse_tensor_dense_matmul(sp_a, b, **kwargs):\n  \"\"\"Returns (batched) matmul of a SparseTensor with a Tensor.\n\n  Args:\n    sp_a: `SparseTensor` representing a (batch of) matrices.\n    b: `Tensor` representing a (batch of) matrices, with the same batch shape of\n      `sp_a`. The shape must be compatible with the shape of `sp_a` and kwargs.\n    **kwargs: Keyword arguments to `tf.sparse_tensor_dense_matmul`.\n\n  Returns:\n    product: A dense (batch of) matrix-shaped Tensor of the same batch shape and\n    dtype as `sp_a` and `b`. If `sp_a` or `b` is adjointed through `kwargs` then\n    the shape is adjusted accordingly.\n  \"\"\"", "output": "tf.sparse.sparse_dense_matmul`.\n\n  Returns:\n    A `Tensor` with shape `[sp_a.shape[0], b.shape[1]]`.\n  \"\"\"\n  if sp_a.dtype != b.dtype:\n    raise TypeError(\"SparseTensor and Tensor must have the same dtype, got %s \"\n                    \"and %s.\" % (sp_a.dtype, b.dtype))\n  if not sp_a.shape.is_compatible_with(b.shape):\n   ", "solution": "\n  batch_shape = _get_shape(sp_a)[:-2]\n\n  # Reshape the SparseTensor into a rank 3 SparseTensors, with the\n  # batch shape flattened to a single dimension. If the batch rank is 0, then\n  # we add a batch dimension of rank 1.\n  sp_a = tf.sparse.reshape(sp_a, tf.concat([[-1], _get_shape(sp_a)[-2:]],\n                                           axis=0))\n  # Reshape b to stack the batch dimension along the rows.\n  b = tf.reshape(b, tf.concat([[-1], _get_shape(b)[-1:]], axis=0))\n\n  # Convert the SparseTensor to a matrix in block diagonal form with blocks of\n  # matrices [M, N]. This allow us to use tf.sparse_tensor_dense_matmul which\n  # only accepts rank 2 (Sparse)Tensors.\n  out = tf.sparse.sparse_dense_matmul(_sparse_block_diag(sp_a), b, **kwargs)\n\n  # Finally retrieve the original batch shape from the resulting rank 2 Tensor.\n  # Note that we avoid inferring the final shape from `sp_a` or `b` because we\n  # might have transposed one or both of them.\n  return tf.reshape(\n      out,\n      tf.concat([batch_shape, [-1], _get_shape(out)[-1:]], axis=0))"}
{"prompt": "def require_parents(packages):\n        \"\"\"\n        Exclude any apparent package that apparently doesn't include its\n        parent.\n\n        For example, exclude 'foo.bar' if 'foo' is not present.\n        \"\"\"", "output": "\n        for package in packages:\n            if '.' in package:\n                parent = package.rsplit('.', 1)[0]\n                if parent not in packages:\n                    packages.remove(package)\n\n    def require_packages(packages):\n        \"\"\"\n        Exclude any apparent package that apparently doesn't exist.\n\n        For example, exclude 'foo.bar' if 'foo' is not present.\n        \"\"\"\n        for package in packages:\n            if '.' in package:\n                parent = package.rsplit('.', 1)[0]\n", "solution": "\n        found = []\n        for pkg in packages:\n            base, sep, child = pkg.rpartition('.')\n            if base and base not in found:\n                continue\n            found.append(pkg)\n            yield pkg"}
{"prompt": "def make_report(self,traceback):\n        \"\"\"Return a string containing a crash report.\"\"\"", "output": "\n        return traceback\n\n    def get_traceback_text(self,traceback):\n        \"\"\"Return a string containing a traceback.\"\"\"\n        return traceback\n\n    def get_report_label(self):\n        \"\"\"Return a string containing a label for the crash report.\"\"\"\n        return \"Traceback\"\n\n    def get_traceback_label(self):\n        \"\"\"Return a string containing a label for the traceback.\"\"\"\n        return \"Traceback\"\n\n    def get_crash_report_entry_text(self,crash_report", "solution": "\n\n        sec_sep = self.section_sep\n\n        report = ['*'*75+'\\n\\n'+'IPython post-mortem report\\n\\n']\n        rpt_add = report.append\n        rpt_add(sys_info())\n\n        try:\n            config = pformat(self.app.config)\n            rpt_add(sec_sep)\n            rpt_add('Application name: %s\\n\\n' % self.app_name)\n            rpt_add('Current user configuration structure:\\n\\n')\n            rpt_add(config)\n        except:\n            pass\n        rpt_add(sec_sep+'Crash traceback:\\n\\n' + traceback)\n\n        return ''.join(report)"}
{"prompt": "def _fmt(self, string):\n        \"\"\"Helper method to format our name into a string.\"\"\"", "output": "\n        return string.format(name=self.name,\n                             number=self.number,\n                             type=self.type)\n\n    def __str__(self):\n        \"\"\"Returns a string representation of the object.\"\"\"\n        return self._fmt('{name} {number} {type}')\n\n    def __repr__(self):\n        \"\"\"Returns a string representation of the object.\"\"\"\n        return self._fmt('{name} {number} {type}')\n\n    def __eq__(self, other):\n        \"\"\"Returns True if", "solution": "\n        if '{' not in string:\n            string = '{}.' + string\n        return string.format(self.name)"}
{"prompt": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake MNIST-style data for unit testing.\"\"\"", "output": "\n  def fake_input_fn(is_training):\n    \"\"\"Fake input function.\"\"\"\n    # Create a fake dataset that yields 100 images.\n    images = tf.random_uniform([batch_size, 28, 28, 1])\n    labels = tf.random_uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)\n    return tf.data.Dataset.from_tensors((images, labels)).repeat()\n  return fake_input_fn\n\n\n", "solution": "\n  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype(\"float32\")\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn"}
{"prompt": "def marker(self, *args):\n        \"\"\"\n        Defines markers one at a time for your graph\n        args are of the form::\n            <marker type>,\n            <color>,\n            <data set index>,\n            <data point>,\n            <size>,\n            <priority>\n        see the official developers doc for the complete spec\n        APIPARAM: chm\n        \"\"\"", "output": "\n        self._marker_args = args\n        return self\n\n    def marker_size(self, *args):\n        \"\"\"\n        Defines markers one at a time for your graph\n        args are of the form::\n            <marker type>,\n            <color>,\n            <data set index>,\n            <data point>,\n            <size>,\n            <priority>\n        see the official developers doc for the complete spec\n        APIPARAM: chm\n        \"\"\"\n        self._marker_args = args\n        return self\n\n    def marker_priority(", "solution": "\n        if len(args[0]) == 1:\n            assert args[0] in MARKERS, 'Invalid marker type: %s'%args[0]\n        assert len(args) <= 6, 'Incorrect arguments %s'%str(args)\n        args = color_args(args, 1)\n        self.markers.append(','.join(map(str,args)) )\n        return self"}
{"prompt": "def camera_event_motion_enum(self, camera_id, **kwargs):\n        \"\"\"Return motion settings matching camera_id.\"\"\"", "output": "\n        return self.camera_event_motion_enum_by_id(camera_id, **kwargs)\n\n    def camera_event_motion_enum_by_id(self, camera_id, **kwargs):\n        \"\"\"Return motion settings matching camera_id.\"\"\"\n        return self.camera_event_motion_enum_by_name(\n            self.camera_id_to_name(camera_id), **kwargs)\n\n    def camera_event_motion_enum_by_name(self, camera_name, **kwargs):\n       ", "solution": "\n        api = self._api_info['camera_event']\n        payload = dict({\n            '_sid': self._sid,\n            'api': api['name'],\n            'method': 'MotionEnum',\n            'version': api['version'],\n            'camId': camera_id,\n        }, **kwargs)\n        response = self._get_json_with_retry(api['url'], payload)\n\n        return MotionSetting(camera_id, response['data']['MDParam'])"}
{"prompt": "def register(self, managed_object):\n        \"\"\"\n        Register a managed object with a KMIP appliance.\n\n        Args:\n            managed_object (ManagedObject): A managed object to register. An\n                instantiatable subclass of ManagedObject from the Pie API.\n\n        Returns:\n            string: The uid of the newly registered managed object.\n\n        Raises:\n            ClientConnectionNotOpen: if the client connection is unusable\n            KmipOperationFailure: if the operation result is a failure\n            TypeError: if the input argument is invalid\n        \"\"\"", "output": "\n        \"\"\"\n        if not isinstance(managed_object, ManagedObject):\n            raise TypeError(\"managed_object must be a ManagedObject\")\n\n        request = RegisterRequestPayload(\n            object_type=managed_object.object_type,\n            attributes=managed_object.attributes\n        )\n\n        operation = RegisterOperation(request)\n        response = self.send_request(operation)\n\n        if response.result.result_status.value == ResultStatusEnum.SUCCESS:\n            return response.result.object_uuid\n        else:\n           ", "solution": "\n        # Check input\n        if not isinstance(managed_object, pobjects.ManagedObject):\n            raise TypeError(\"managed object must be a Pie ManagedObject\")\n\n        # Extract and create attributes\n        object_attributes = list()\n\n        if hasattr(managed_object, 'cryptographic_usage_masks'):\n            if managed_object.cryptographic_usage_masks is not None:\n                mask_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_USAGE_MASK,\n                    managed_object.cryptographic_usage_masks\n                )\n                object_attributes.append(mask_attribute)\n        if hasattr(managed_object, 'operation_policy_name'):\n            if managed_object.operation_policy_name is not None:\n                opn_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.OPERATION_POLICY_NAME,\n                    managed_object.operation_policy_name\n                )\n                object_attributes.append(opn_attribute)\n        if hasattr(managed_object, 'names'):\n            if managed_object.names:\n                for name in managed_object.names:\n                    name_attribute = self.attribute_factory.create_attribute(\n                        enums.AttributeType.NAME,\n                        name\n                    )\n                    object_attributes.append(name_attribute)\n\n        template = cobjects.TemplateAttribute(attributes=object_attributes)\n        object_type = managed_object.object_type\n\n        # Register the managed object and handle the results\n        secret = self.object_factory.convert(managed_object)\n        result = self.proxy.register(object_type, template, secret)\n\n        status = result.result_status.value\n        if status == enums.ResultStatus.SUCCESS:\n            return result.uuid\n        else:\n            reason = result.result_reason.value\n            message = result.result_message.value\n            raise exceptions.KmipOperationFailure(status, reason, message)"}
{"prompt": "def visit_return(self, node):\n        \"\"\"1 - check is the node has a right sibling (if so, that's some\n        unreachable code)\n        2 - check is the node is inside the finally clause of a try...finally\n        block\n        \"\"\"", "output": "\n        if node.sibling:\n            self.warn(node, \"Unreachable code\")\n        if node.parent and node.parent.type == 'try':\n            if node.parent.finally_block:\n                self.warn(node, \"Unreachable code\")\n\n    def visit_raise(self, node):\n        \"\"\"1 - check if the node is inside a finally clause\n        \"\"\"\n        if node.parent and node.parent.type == 'try':\n            if node.parent.finally_block:\n                self.", "solution": "\n        self._check_unreachable(node)\n        # Is it inside final body of a try...finally bloc ?\n        self._check_not_in_finally(node, \"return\", (astroid.FunctionDef,))"}
{"prompt": "def read(self, size=-1):\n        \"\"\"\n        read([size]) -> read at most size bytes, returned as a string.\n\n        If the size argument is negative or omitted, read until EOF is reached.\n        Notice that when in non-blocking mode, less data than what was\n        requested may be returned, even if no size parameter was given.\n        \"\"\"", "output": "\n        if size < 0:\n            size = None\n        return self._sock.recv(size)\n\n    def readline(self, size=-1):\n        \"\"\"\n        readline([size]) -> read a line, ending in a newline character.\n\n        If the size argument is negative or omitted, read until a newline\n        character is encountered, or until EOF is reached.\n        \"\"\"\n        if size < 0:\n            size = None\n        return self._sock.recv(size)\n\n    def readlines(self, sizehint", "solution": "\n        if self.left is not None:\n            size = min(size, self.left)\n        if self.closed:\n            raise ValueError('I/O operation on closed file')\n        if size < 0:\n            return ''.join(self)\n        elif not size:\n            chunk = ''\n        elif self.buf:\n            chunk = self.buf\n            self.buf = None\n        else:\n            try:\n                chunk = next(self.iterator)\n            except StopIteration:\n                return ''\n        if len(chunk) > size:\n            self.buf = chunk[size:]\n            chunk = chunk[:size]\n        if self.left is not None:\n            self.left -= len(chunk)\n        return chunk"}
{"prompt": "def balanced_rows(n, iterable, fillvalue=None):\n\t\"\"\"\n\tLike grouper, but balance the rows to minimize fill per row.\n\tbalanced_rows(3, 'ABCDEFG', 'x') --> ABC DEx FGx\"\n\t\"\"\"", "output": "\n\targs = [iter(iterable)] * n\n\treturn zip_longest(*args, fillvalue=fillvalue)\n\n\ndef balanced_cols(n, iterable, fillvalue=None):\n\t\"\"\"\n\tLike grouper, but balance the columns to minimize fill per column.\n\tbalanced_cols(3, 'ABCDEFG', 'x') --> AB CDEFGx\"\n\t\"\"\"\n\targs = [iter(iterable)] * n\n\treturn zip(*args, fillvalue=fillvalue)\n\n\ndef balanced", "solution": "\n\titerable, iterable_copy = itertools.tee(iterable)\n\tcount = len(tuple(iterable_copy))\n\tfor allocation in partition_items(count, n):\n\t\trow = itertools.islice(iterable, allocation)\n\t\tif allocation < n:\n\t\t\trow = itertools.chain(row, [fillvalue])\n\t\tyield tuple(row)"}
{"prompt": "def timetree_likelihood(self):\n        \"\"\"\n        Return the likelihood of the data given the current branch length in the tree\n        \"\"\"", "output": "\n        return self.likelihood_function(self.branch_length)\n\n    def timetree_likelihood_gradient(self):\n        \"\"\"\n        Return the gradient of the likelihood of the data given the current branch length in the tree\n        \"\"\"\n        return self.likelihood_gradient_function(self.branch_length)\n\n    def timetree_likelihood_hessian(self):\n        \"\"\"\n        Return the hessian of the likelihood of the data given the current branch length in the tree\n        \"\"\"\n        return self.likelihood_", "solution": "\n        LH = 0\n        for node in self.tree.find_clades(order='preorder'):  # sum the likelihood contributions of all branches\n            if node.up is None: # root node\n                continue\n            LH -= node.branch_length_interpolator(node.branch_length)\n\n        # add the root sequence LH and return\n        if self.aln:\n            LH += self.gtr.sequence_logLH(self.tree.root.cseq, pattern_multiplicity=self.multiplicity)\n        return LH"}
{"prompt": "def read(*paths):\n    \"\"\"Build a file path from *paths* and return the contents.\"\"\"", "output": "\n    with open(os.path.join(*paths), 'r') as f:\n        return f.read()\n\n\ndef get_version(*file_paths):\n    \"\"\"Get version string.\"\"\"\n    version_file = read(*file_paths)\n    version_match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\",\n                              version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(\"Unable to find version string.\")", "solution": "\n    filename = os.path.join(*paths)\n    with codecs.open(filename, mode='r', encoding='utf-8') as handle:\n        return handle.read()"}
{"prompt": "def bootstrap(self, path_or_uri):\n        \"\"\" Initialize a database.\n\n        :param database_path: The absolute path to the database to initialize.\n        \"\"\"", "output": "\n        if not os.path.exists(path_or_uri):\n            os.makedirs(path_or_uri)\n        self.db_path = path_or_uri\n        self.db_uri = 'sqlite:///' + path_or_uri\n        self.engine = create_engine(self.db_uri)\n        self.session_factory = sessionmaker(bind=self.engine)\n        self.session = self.session_factory()\n        self.Base = declarative_base()\n        self.Base.metadata.create_all", "solution": "\n        _logger.debug(\"Bootstrapping new database: %s\", path_or_uri)\n        self.database_uri = _urify_db(path_or_uri)\n        db = sa.create_engine(self.database_uri)\n        Base.metadata.create_all(db)"}
{"prompt": "def apply_mirroring(storage, debug):\n    \"\"\"Applies L4: mirroring\n\n    See: http://unicode.org/reports/tr9/#L4\n\n    \"\"\"", "output": "\n    if debug:\n        print(\"Applying L4: mirroring\")\n\n    # 1. For each character in the input,\n    for char in storage:\n        # 2. if the character has a mirrored glyph,\n        if char.mirrored:\n            # 3. add the mirrored glyph to the output.\n            storage.append(char.mirrored)\n        else:\n            # 4. otherwise, add the character to the output.\n            storage.append(char)\n\n\ndef apply_combining_", "solution": "\n    # L4. A character is depicted by a mirrored glyph if and only if (a) the\n    # resolved directionality of that character is R, and (b) the\n    # Bidi_Mirrored property value of that character is true.\n    for _ch in storage['chars']:\n        unichar = _ch['ch']\n        if mirrored(unichar) and \\\n                _embedding_direction(_ch['level']) == 'R':\n            _ch['ch'] = MIRRORED.get(unichar, unichar)\n\n    if debug:\n        debug_storage(storage)"}
{"prompt": "def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool:\n    \"\"\"return True if <name> is a method overridden from an ancestor\"\"\"", "output": "\n    for ancestor in class_node.ancestors():\n        if ancestor.name == name:\n            return True\n    return False\n\n\ndef get_overridden_methods(class_node: astroid.node_classes.NodeNG) -> Set[str]:\n    \"\"\"return a set of method names overridden from an ancestor\"\"\"\n    return {\n        method.name\n        for method in class_node.body\n        if isinstance(method, astroid.node_classes.FunctionDef)\n        and overrides_a_method(class", "solution": "\n    for ancestor in class_node.ancestors():\n        if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef):\n            return True\n    return False"}
{"prompt": "def _add_empty_child(self, parent, code, final=False):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 \u0441 \u043a\u043e\u0434\u043e\u043c code\n        \"\"\"", "output": "\n        child = self.add_child(parent, code)\n        if final:\n            self.set_final(child)\n        return child\n\n    def _add_empty_children(self, parent, codes):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u043e\u0432 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0430\u043c \u0441 \u043a\u043e\u0434\u0430\u043c\u0438 codes\n        \"\"\"\n        for code in codes:\n            self._add_empty_child(parent, code)\n\n    def _add_empty_siblings(self, parent, codes):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432", "solution": "\n        self.graph[parent][code] = self.nodes_number\n        self.graph.append(self._make_default_node())\n        self.data.append(None)\n        self.final.append(final)\n        self.nodes_number += 1\n        return (self.nodes_number - 1)"}
{"prompt": "def _check_year(year, month, error, error_msg):\n    \"\"\"Checks that the year is within 50 years from now.\"\"\"", "output": "\n    if year < datetime.now().year - 50:\n        raise error(error_msg.format(year))\n    if year > datetime.now().year + 50:\n        raise error(error_msg.format(year))\n    if month < 1 or month > 12:\n        raise error(error_msg.format(month))\n\n\ndef _check_month(year, month, error, error_msg):\n    \"\"\"Checks that the month is within 12 months from now.\"\"\"\n    if month < 1 or", "solution": "\n    if year not in xrange((now.year - 50), (now.year + 51)):\n        year = now.year\n        month = now.month\n        error = error_msg\n    return year, month, error"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n        return self.adjoint()\n\n    def is_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if the QuantumChannel is completely-positive trace-preserving.\n\n        Args:\n            atol (float): absolute tolerance value\n            rtol (float): relative tolerance value\n\n        Returns:\n            bool: is completely-positive and trace-preserving\n        \"\"\"\n        return True\n\n    def is_hermitian(self, atol=None, rtol=None):\n", "solution": "\n        din, dout = self.dim\n        dtr = self._data[0].shape[0] // dout\n        stine = [None, None]\n        for i, mat in enumerate(self._data):\n            if mat is not None:\n                stine[i] = np.reshape(\n                    np.transpose(np.reshape(mat, (dout, dtr, din)), (2, 1, 0)),\n                    (din * dtr, dout))\n        return Stinespring(\n            tuple(stine),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def newick(self):\n        \"\"\"The representation of the Node in Newick format.\"\"\"", "output": "\n        return self.newick_str\n\n    @property\n    def newick_str(self):\n        \"\"\"The representation of the Node in Newick format.\"\"\"\n        return self._newick_str\n\n    @newick_str.setter\n    def newick_str(self, newick_str):\n        \"\"\"Set the representation of the Node in Newick format.\"\"\"\n        self._newick_str = newick_str\n\n    @property\n    def name(self):\n        \"\"\"The name of the Node.\"\"\"\n        return self._", "solution": "\n        label = self.name or ''\n        if self._length:\n            label += ':' + self._length\n        descendants = ','.join([n.newick for n in self.descendants])\n        if descendants:\n            descendants = '(' + descendants + ')'\n        return descendants + label"}
{"prompt": "async def get_postcode_random() -> Postcode:\n    \"\"\"\n    Gets a random postcode object..\n    Acts as a middleware between us and the API, caching results.\n    :return: The PostCode object else None if the postcode does not exist.\n    \"\"\"", "output": "\n    postcode = Postcode.objects.all().order_by('?').first()\n    if postcode:\n        return postcode\n    else:\n        return None\n\n\ndef get_postcode_by_id(postcode_id: int) -> Postcode:\n    \"\"\"\n    Gets a postcode object by its id.\n    Acts as a middleware between us and the API, caching results.\n    :param postcode_id: The id of the postcode to get.\n    :return: The PostCode object else None if the postcode", "solution": "\n    try:\n        postcode = await fetch_postcode_random()\n    except (ApiError, CircuitBreakerError):\n        raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n\n    if postcode is not None:\n        postcode.save()\n    return postcode"}
{"prompt": "def tasks(self, from_date=DEFAULT_DATETIME):\n        \"\"\"Retrieve tasks.\n\n        :param from_date: retrieve tasks that where updated from that date;\n            dates are converted epoch time.\n        \"\"\"", "output": "\n        return self._get(\n            'tasks',\n            params={'from_date': from_date}\n        )\n\n    def task(self, task_id):\n        \"\"\"Retrieve a task.\n\n        :param task_id: the task id.\n        \"\"\"\n        return self._get('tasks/%s' % task_id)\n\n    def create_task(self, task):\n        \"\"\"Create a task.\n\n        :param task: the task to create.\n        \"\"\"\n        return self._post('tasks', task)\n\n", "solution": "\n        # Convert 'from_date' to epoch timestamp.\n        # Zero value (1970-01-01 00:00:00) is not allowed for\n        # 'modifiedStart' so it will be set to 1, by default.\n        ts = int(datetime_to_utc(from_date).timestamp()) or 1\n\n        consts = {\n            self.PMODIFIED_START: ts\n        }\n\n        attachments = {\n            self. PPROJECTS: True\n        }\n\n        params = {\n            self.PCONSTRAINTS: consts,\n            self.PATTACHMENTS: attachments,\n            self.PORDER: self.VOUTDATED,\n        }\n\n        while True:\n            r = self._call(self.MANIPHEST_TASKS, params)\n            yield r\n            j = json.loads(r)\n            after = j['result']['cursor']['after']\n            if not after:\n                break\n            params[self.PAFTER] = after"}
{"prompt": "def ensure_dir(path):\n    \"\"\"Ensure that a needed directory exists, creating it if it doesn't\"\"\"", "output": "\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef get_file_list(path):\n    \"\"\"Get a list of files in a directory\"\"\"\n    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\ndef get_file_list_with_extension(path, extension):\n    \"\"\"Get a list of files in a directory with a given extension\"\"\"\n    return [f for f in os.listdir(", "solution": "\n    try:\n        log.info('Ensuring directory exists: %s' % path)\n        os.makedirs(path)\n    except OSError:\n        if not os.path.isdir(path):\n            raise"}
{"prompt": "def parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"", "output": "phenotype: Is a phenotype entry\n        predominantly phenotypes: Is a phenotype entry\n\n    Args:\n        lines (list): A list of lines from the file\n\n    Returns:\n        dict: A dictionary with the mim number as key and the entry type as value\n    \"\"\"\n    mim2gene = {}\n    for line in lines:\n        line = line.strip()\n        if line.startswith('#'):\n            continue\n        if line.startswith('MIM'):\n            continue\n        if line.startswith", "solution": "\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry"}
{"prompt": "def directive(directname=None):\n    \"\"\"Attach a class to a parsing class and register it as a parser directive.\n\n        The class is registered with its name unless directname is provided.\n    \"\"\"", "output": "\n    def decorator(cls):\n        if directname is None:\n            directname = cls.__name__\n        cls.directname = directname\n        cls.directive = True\n        return cls\n    return decorator\n\n\ndef directive_parser(directname=None):\n    \"\"\"Attach a function to a parsing class and register it as a parser directive.\n\n        The function is registered with its name unless directname is provided.\n    \"\"\"\n    def decorator(func):\n        if directname is None:\n            directname = func", "solution": "\n    global _directives\n    class_dir_list = _directives\n\n    def wrapper(f):\n        nonlocal directname\n        if directname is None:\n            directname = f.__name__\n        f.ns_name = directname\n        set_one(class_dir_list, directname, f)\n        return f\n    return wrapper"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a Redis connection.\n        \"\"\"", "output": "\n        return self.connection\n\n    def get_key(self, key):\n        \"\"\"\n        Returns a key from the Redis connection.\n        \"\"\"\n        return self.connection.get(key)\n\n    def set_key(self, key, value):\n        \"\"\"\n        Sets a key in the Redis connection.\n        \"\"\"\n        return self.connection.set(key, value)\n\n    def delete_key(self, key):\n        \"\"\"\n        Deletes a key from the Redis connection.\n        \"\"\"\n        return self.connection", "solution": "\n        conn = self.get_connection(self.redis_conn_id)\n        self.host = conn.host\n        self.port = conn.port\n        self.password = None if str(conn.password).lower() in ['none', 'false', ''] else conn.password\n        self.db = conn.extra_dejson.get('db', None)\n\n        if not self.redis:\n            self.log.debug(\n                'Initializing redis object for conn_id \"%s\" on %s:%s:%s',\n                self.redis_conn_id, self.host, self.port, self.db\n            )\n            self.redis = Redis(\n                host=self.host,\n                port=self.port,\n                password=self.password,\n                db=self.db)\n\n        return self.redis"}
{"prompt": "def decode(self, packet):\n        \"\"\"\n        Decode a PUBREL control packet. \n        \"\"\"", "output": "\n        self.logger.debug(\"PUBREL received\")\n        packet.logger.debug(\"PUBREL received\")\n        packet.logger.debug(\"Packet: %s\" % packet)\n        packet.logger.debug(\"Packet.data: %s\" % packet.data)\n        packet.logger.debug(\"Packet.data.decode('utf-8'): %s\" % packet.data.decode('utf-8'))\n        packet.logger.debug(\"Packet.data.decode('utf-8').split(' '): %s\" % packet.", "solution": "\n        self.encoded = packet\n        lenLen = 1\n        while packet[lenLen] & 0x80:\n            lenLen += 1\n        packet_remaining = packet[lenLen+1:]\n        self.msgId  = decode16Int(packet_remaining)\n        self.dup = (packet[0] & 0x08) == 0x08"}
{"prompt": "def init_log( quiet, debug ):\n   \"\"\"\n   setup system logging to desired verbosity.\n   \"\"\"", "output": "\n   if debug:\n      log_level = logging.DEBUG\n   elif quiet:\n      log_level = logging.ERROR\n   else:\n      log_level = logging.INFO\n\n   logging.basicConfig(\n      level=log_level,\n      format='%(asctime)s %(levelname)s %(message)s',\n      datefmt='%Y-%m-%d %H:%M:%S'\n   )\n\n\ndef get_logger( name ):\n   \"\"\"\n   get a logger by name.\n   \"\"\"\n  ", "solution": "\n   from logging.handlers import SysLogHandler\n   fmt = logging.Formatter( os.path.basename(sys.argv[0]) +\n         \".%(name)s %(levelname)s - %(message)s\")\n   facility = SysLogHandler.LOG_DAEMON\n   syslog = SysLogHandler(address='/dev/log',facility=facility)\n   syslog.setFormatter( fmt )\n   log.addHandler(syslog)\n   if not quiet:\n      console = logging.StreamHandler()\n      console.setFormatter( fmt )\n      log.addHandler(console)\n      log.setLevel(logging.INFO)\n      if debug:\n         log.setLevel(logging.DEBUG)"}
{"prompt": "def loadPlugins(self):\n        \"\"\"Load plugins in nose.plugins.builtin\n        \"\"\"", "output": "\n        from nose.plugins.builtin import (\n            CapturePlugin, CoveragePlugin, DiffPlugin, ExcludePlugin,\n            FileNamePlugin, FileNameAndLineNumberPlugin,\n            FileNameAndDescriptionPlugin, FileNameAndIdPlugin,\n            FileNameAndFuncNamePlugin, FileNameAndModulePlugin,\n            FileNameAndClassPlugin, FileNameAndMethodPlugin,\n            FileNameAndLineNumberAndDescriptionPlugin,\n            FileNameAndLineNumberAndIdPlugin,\n            FileNameAndLineNumberAndFuncNamePlugin,\n            FileNameAndLineNumberAndModule", "solution": "\n        from nose.plugins import builtin\n        for plug in builtin.plugins:\n            self.addPlugin(plug())\n        super(BuiltinPluginManager, self).loadPlugins()"}
{"prompt": "def verify(self, pkey):\n        \"\"\"\n        Verifies the signature on this certificate signing request.\n\n        :param PKey key: A public key.\n\n        :return: ``True`` if the signature is correct.\n        :rtype: bool\n\n        :raises OpenSSL.crypto.Error: If the signature is invalid or there is a\n            problem verifying the signature.\n        \"\"\"", "output": "\n        return self._cert_req.verify(pkey)\n\n    def set_pubkey(self, pkey):\n        \"\"\"\n        Sets the public key for this certificate signing request.\n\n        :param PKey pkey: The public key.\n        \"\"\"\n        self._cert_req.set_pubkey(pkey)\n\n    def get_pubkey(self):\n        \"\"\"\n        Returns the public key for this certificate signing request.\n\n        :return: The public key.\n        :rtype: PKey\n        \"\"\"\n        return self", "solution": "\n        if not isinstance(pkey, PKey):\n            raise TypeError(\"pkey must be a PKey instance\")\n\n        result = _lib.X509_REQ_verify(self._req, pkey._pkey)\n        if result <= 0:\n            _raise_current_error()\n\n        return result"}
{"prompt": "def add_card(self, query_params=None):\n        \"\"\"\n        Create a card for this list. Returns a Card object.\n        \"\"\"", "output": "\n        return self._create_card(query_params=query_params)\n\n    def add_comment(self, comment, query_params=None):\n        \"\"\"\n        Add a comment to this list. Returns a Comment object.\n        \"\"\"\n        return self._create_comment(comment, query_params=query_params)\n\n    def add_label(self, label, query_params=None):\n        \"\"\"\n        Add a label to this list. Returns a Label object.\n        \"\"\"\n        return self._create_label(label, query_params=query", "solution": "\n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)"}
{"prompt": "def get_default_value(self):\n        \"\"\"Instantiate a default value instance.\n\n        This is called when the containing HasTraits classes'\n        :meth:`__new__` method is called to ensure that a unique instance\n        is created for each HasTraits instance.\n        \"\"\"", "output": "\n        return self.default_value()\n\n    def __get__(self, obj, cls):\n        \"\"\"Get the value of the trait from the HasTraits instance.\n\n        This is called when the trait is accessed as an attribute of the\n        HasTraits instance.\n        \"\"\"\n        if obj is None:\n            return self\n        return obj.get(self.name)\n\n    def __set__(self, obj, value):\n        \"\"\"Set the value of the trait on the HasTraits instance.\n\n        This is called when the trait is assigned to", "solution": "\n        dv  = self.default_value\n        if isinstance(dv, DefaultValueGenerator):\n            return dv.generate(self.klass)\n        else:\n            return dv"}
{"prompt": "def forward(self, input, target):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n        Takes an input object, and computes the corresponding loss of the criterion,\n        compared with `target`\n\n        :param input: ndarray or list of ndarray\n        :param target: ndarray or list of ndarray\n        :return: value of loss\n        \"\"\"", "output": "\n        if isinstance(input, list):\n            input = [Variable(i) for i in input]\n        else:\n            input = [Variable(input)]\n        if isinstance(target, list):\n            target = [Variable(i) for i in target]\n        else:\n            target = [Variable(target)]\n        return self.forward_batch(input, target)\n\n    def forward_batch(self, input, target):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n", "solution": "\n        jinput, input_is_table = Layer.check_input(input)\n        jtarget, target_is_table = Layer.check_input(target)\n        output = callBigDlFunc(self.bigdl_type,\n                               \"criterionForward\",\n                               self.value,\n                               jinput,\n                               input_is_table,\n                               jtarget,\n                               target_is_table)\n        return output"}
{"prompt": "def split_line(self, line, cursor_pos=None):\n        \"\"\"Split a line of text with a cursor at the given position.\n        \"\"\"", "output": "\n        if cursor_pos is None:\n            cursor_pos = len(line)\n        if cursor_pos == 0:\n            return [line], 0\n        if cursor_pos == len(line):\n            return [line], len(line)\n        left, right = line[:cursor_pos], line[cursor_pos:]\n        return [left, right], cursor_pos\n\n    def join_lines(self, lines):\n        \"\"\"Join a list of lines into a single line.\n        \"\"\"\n        return ''.join(lines)\n\n    def", "solution": "\n        l = line if cursor_pos is None else line[:cursor_pos]\n        return self._delim_re.split(l)[-1]"}
{"prompt": "def signing_keys_as_jwks(self):\n        \"\"\"\n        Build a JWKS from the signing keys belonging to the self signer\n\n        :return: Dictionary\n        \"\"\"", "output": "\n        jwks = {\n            \"keys\": []\n        }\n        for key in self.signing_keys:\n            jwks[\"keys\"].append(key.as_jwk())\n        return jwks\n\n    def signing_keys_as_pem(self):\n        \"\"\"\n        Build a PEM from the signing keys belonging to the self signer\n\n        :return: Dictionary\n        \"\"\"\n        pem = {\n            \"keys\": []\n        }\n        for key in self.signing_keys:\n            pem[\"", "solution": "\n        _l = [x.serialize() for x in self.self_signer.keyjar.get_signing_key()]\n        if not _l:\n            _l = [x.serialize() for x in\n                  self.self_signer.keyjar.get_signing_key(owner=self.iss)]\n        return {'keys': _l}"}
{"prompt": "def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a fallback import block.\"\"\"", "output": "\n    return node.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent", "solution": "\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n\n    if isinstance(context, astroid.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable(\n            handler.body for handler in context.handlers\n        )\n        handlers = context.handlers\n\n    has_fallback_imports = any(\n        isinstance(import_node, (astroid.ImportFrom, astroid.Import))\n        for import_node in other_body\n    )\n    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)\n    return ignores_import_error or has_fallback_imports"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report generated model in human readable form.\"\"\"", "output": "\n        if self.model is None:\n            raise ValueError(\"Model is not generated yet.\")\n        if self.model.is_empty():\n            raise ValueError(\"Model is empty.\")\n        if not self.model.is_valid():\n            raise ValueError(\"Model is not valid.\")\n        if self.model.is_empty():\n            raise ValueError(\"Model is empty.\")\n        if self.model.is_valid():\n            raise ValueError(\"Model is not valid.\")\n        if self.model.is_empty():\n            raise ValueError(\"Model is empty", "solution": "\n        if self._args and self._args.verbose > 2:\n            pprint(self.results)\n\n        for dimension, lc_info in self.results['dimensions'].items():\n            print(\"{}D layer condition:\".format(dimension), file=output_file)\n            for cache, lc_solution in sorted(lc_info['caches'].items()):\n                print(cache+\": \", end='', file=output_file)\n                if lc_solution['lt'] is sympy.true:\n                    print(\"unconditionally fulfilled\", file=output_file)\n                else:\n                    if lc_solution['eq'] is None:\n                        print(\"{}\".format(lc_solution['lt']), file=output_file)\n                    elif type(lc_solution['eq']) is not list:\n                        print(\"{}\".format(lc_solution['eq']), file=output_file)\n                    else:\n                        for solu in lc_solution['eq']:\n                            for s, v in solu.items():\n                                print(\"{} <= {}\".format(s, v), file=output_file)"}
{"prompt": "def _draw(self, mode, vertex_list=None):\n        \"\"\"Draw vertices in the domain.\n\n        If `vertex_list` is not specified, all vertices in the domain are\n        drawn.  This is the most efficient way to render primitives.\n\n        If `vertex_list` specifies a `VertexList`, only primitives in that\n        list will be drawn.\n\n        :Parameters:\n            `mode` : int\n                OpenGL drawing mode, e.g. ``GL_POINTS``, ``GL_LINES``, etc.\n            `vertex_list` : `VertexList`\n                Vertex list to draw, or ``None`` for all lists in this domain.\n\n        \"\"\"", "output": ".\n            `vertex_list` : `VertexList`\n                The `VertexList` to draw.\n\n        \"\"\"\n        if vertex_list is None:\n            vertex_list = self._vertex_list\n        vertex_list.draw(mode)\n\n    def _draw_with_colors(self, mode, vertex_list=None):\n        \"\"\"Draw vertices in the domain with colors.\n\n        If `vertex_list` is not specified, all vertices in the domain are\n        drawn.  This is the most efficient way to render primitives.\n", "solution": "\n        glPushClientAttrib(GL_CLIENT_VERTEX_ARRAY_BIT)\n        for buffer, attributes in self.buffer_attributes:\n            buffer.bind()\n            for attribute in attributes:\n                attribute.enable()\n                attribute.set_pointer(attribute.buffer.ptr)\n        if vertexbuffer._workaround_vbo_finish:\n            glFinish()\n\n        if vertex_list is not None:\n            glDrawArrays(mode, vertex_list.start, vertex_list.count)\n        else:\n            starts, sizes = self.allocator.get_allocated_regions()\n            primcount = len(starts)\n            if primcount == 0:\n                pass\n            elif primcount == 1:\n                # Common case\n                glDrawArrays(mode, starts[0], int(sizes[0]))\n            elif gl_info.have_version(1, 4):\n                starts = (GLint * primcount)(*starts)\n                sizes = (GLsizei * primcount)(*sizes)\n                glMultiDrawArrays(mode, starts, sizes, primcount)\n            else:\n                for start, size in zip(starts, sizes):\n                    glDrawArrays(mode, start, size)\n\n        for buffer, _ in self.buffer_attributes:\n            buffer.unbind()\n        glPopClientAttrib()"}
{"prompt": "def parse_rrset(e_rrset, connection, zone_id):\n    \"\"\"\n    This a parser that allows the passing of any valid ResourceRecordSet\n    tag. It will spit out the appropriate ResourceRecordSet object for the tag.\n\n    :param lxml.etree._Element e_rrset: The root node of the etree parsed\n        response from the API.\n    :param Route53Connection connection: The connection instance used to\n        query the API.\n    :param str zone_id: The zone ID of the HostedZone these rrsets belong to.\n    :rtype: ResourceRecordSet\n    :returns: An instantiated ResourceRecordSet object.\n    \"\"\"", "output": "sets belong\n        to.\n    :rtype: Route53ResourceRecordSet\n    :return: The ResourceRecordSet object that corresponds to the tag.\n    \"\"\"\n    rrset = Route53ResourceRecordSet(connection, zone_id)\n    rrset.name = e_rrset.find('Name').text\n    rrset.type = e_rrset.find('Type').text\n    rrset.ttl = int(e_rrset.find('TTL').text)\n    rrset.resource_records = []\n", "solution": "\n\n    # This dict will be used to instantiate a ResourceRecordSet instance to yield.\n    kwargs = {\n        'connection': connection,\n        'zone_id': zone_id,\n    }\n    rrset_type = None\n\n    for e_field in e_rrset:\n        # Cheesy way to strip off the namespace.\n        tag_name = e_field.tag.split('}')[1]\n        field_text = e_field.text\n\n        if tag_name == 'Type':\n            # Need to store this to determine which ResourceRecordSet\n            # subclass to instantiate.\n            rrset_type = field_text\n            continue\n        elif tag_name == 'AliasTarget':\n            # A records have some special field values we need.\n            alias_hosted_zone_id, alias_dns_name = parse_rrset_alias(e_field)\n            kwargs['alias_hosted_zone_id'] = alias_hosted_zone_id\n            kwargs['alias_dns_name'] = alias_dns_name\n            # Alias A entries have no TTL.\n            kwargs['ttl'] = None\n            continue\n        elif tag_name == 'ResourceRecords':\n            kwargs['records'] = parse_rrset_record_values(e_field)\n            continue\n\n        # Map the XML tag name to a kwarg name.\n        kw_name = RRSET_TAG_TO_KWARG_MAP[tag_name]\n        # This will be the key/val pair used to instantiate the\n        # ResourceRecordSet instance.\n        kwargs[kw_name] = field_text\n\n    if not rrset_type:\n        raise Route53Error(\"No Type tag found in ListResourceRecordSetsResponse.\")\n\n    if 'records' not in kwargs:\n        # Not all rrsets have records.\n        kwargs['records'] = []\n\n    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP[rrset_type]\n    return RRSetSubclass(**kwargs)"}
{"prompt": "def get_params(degrees, translate, scale_ranges, shears, img_size):\n        \"\"\"Get parameters for affine transformation\n\n        Returns:\n            sequence: params to be passed to the affine transformation\n        \"\"\"", "output": "\n        # 0.5 is added to the degrees to correct the rotation direction.\n        if degrees is not None:\n            # get_params() will convert degrees to radians for us\n            degrees = get_params(degrees, 0.5)\n\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n\n            translations = (np.random.uniform(-max_dx, max_dx),\n                            np.random", "solution": "\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = random.uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            shear = random.uniform(shears[0], shears[1])\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear"}
{"prompt": "def topic(self, topic_id):\n        \"\"\"Retrive the topic with `topic_id` identifier.\n\n        :param topic_id: identifier of the topic to retrieve\n        \"\"\"", "output": "\n        return self.get_topic(topic_id)\n\n    def topics(self, **kwargs):\n        \"\"\"Retrieve the list of topics.\n\n        :param kwargs: additional filters\n        \"\"\"\n        return self.get_topics(**kwargs)\n\n    def topic_posts(self, topic_id, **kwargs):\n        \"\"\"Retrieve the list of posts for a topic.\n\n        :param topic_id: identifier of the topic\n        :param kwargs: additional filters\n        \"\"\"\n        return self.get_topic_posts(", "solution": "\n        params = {\n            self.PKEY: self.api_key\n        }\n\n        # http://example.com/t/8.json\n        response = self._call(self.TOPIC, topic_id,\n                              params=params)\n\n        return response"}
{"prompt": "def decode(data):\n    \"\"\"\n    Decodes a PackBit encoded data.\n    \"\"\"", "output": "\n    if not isinstance(data, bytes):\n        raise TypeError(\"data must be bytes\")\n\n    if len(data) < 1:\n        raise ValueError(\"data must be at least 1 byte long\")\n\n    result = []\n    i = 0\n    while i < len(data):\n        if data[i] == 0:\n            # 0x00: 0x00\n            result.append(0)\n            i += 1\n        elif data[i] == 1:\n            # 0x01", "solution": "\n    data = bytearray(data) # <- python 2/3 compatibility fix\n    result = bytearray()\n    pos = 0\n    while pos < len(data):\n        header_byte = data[pos]\n        if header_byte > 127:\n            header_byte -= 256\n        pos += 1\n\n        if 0 <= header_byte <= 127:\n            result.extend(data[pos:pos+header_byte+1])\n            pos += header_byte+1\n        elif header_byte == -128:\n            pass\n        else:\n            result.extend([data[pos]] * (1 - header_byte))\n            pos += 1\n\n    return bytes(result)"}
{"prompt": "def get_conn(self):\n        \"\"\"Return the FileService object.\"\"\"", "output": "\n        return self.conn\n\n    def get_container(self, container_name):\n        \"\"\"Return the container object.\"\"\"\n        return self.conn.get_container(container_name)\n\n    def get_object(self, container_name, object_name):\n        \"\"\"Return the object object.\"\"\"\n        return self.conn.get_object(container_name, object_name)\n\n    def get_object_metadata(self, container_name, object_name):\n        \"\"\"Return the object metadata.\"\"\"\n        return self.conn.head_", "solution": "\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)"}
{"prompt": "def createClient(self):\n        \"\"\"Create a UDP connection to Riemann\"\"\"", "output": "\n        if self.client is None:\n            self.client = RiemannClient(host=self.host, port=self.port)\n\n    def send(self, event):\n        \"\"\"Send an event to Riemann\"\"\"\n        self.createClient()\n        self.client.send(event)\n\n    def send_events(self, events):\n        \"\"\"Send a list of events to Riemann\"\"\"\n        self.createClient()\n        self.client.send_events(events)\n\n    def send_state(self, state):\n        \"\"\"Send a state", "solution": "\n        server = self.config.get('server', '127.0.0.1')\n        port = self.config.get('port', 5555)\n\n        def connect(ip):\n            self.protocol = riemann.RiemannUDP(ip, port)\n            self.endpoint = reactor.listenUDP(0, self.protocol)\n\n        d = reactor.resolve(server)\n        d.addCallback(connect)\n        return d"}
{"prompt": "def _multi_gamma_sequence(self, a, p, name=\"multi_gamma_sequence\"):\n    \"\"\"Creates sequence used in multivariate (di)gamma; shape = shape(a)+[p].\"\"\"", "output": "\n    with tf.name_scope(name):\n      a = tf.convert_to_tensor(a, name=\"a\")\n      p = tf.convert_to_tensor(p, name=\"p\")\n      return tf.concat([a, p], axis=0)\n\n  def _multi_gamma_log_prob(self, x, a, p, name=\"multi_gamma_log_prob\"):\n    \"\"\"Computes log prob for multivariate (di)gamma distribution.\"\"\"\n    with tf.name_scope(name):\n      a = tf", "solution": "\n    with self._name_scope(name):\n      # Linspace only takes scalars, so we'll add in the offset afterwards.\n      seq = tf.linspace(\n          tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(\n              p, tf.int32))\n      return seq + tf.expand_dims(a, [-1])"}
{"prompt": "def skus(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-06-01: :class:`SkusOperations<azure.mgmt.storage.v2017_06_01.operations.SkusOperations>`\n           * 2017-10-01: :class:`SkusOperations<azure.mgmt.storage.v2017_10_01.operations.SkusOperations>`\n           * 2018-02-01: :class:`SkusOperations<azure.mgmt.storage.v2018_02_01.operations.SkusOperations>`\n           * 2018-03-01-preview: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01_preview.operations.SkusOperations>`\n           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage.v2018_07_01.operations.SkusOperations>`\n        \"\"\"", "output": "-02-01: :class:`SkusOperations<azure.mgmt.storage.v2018_02_01.operations.SkusOperations>`\n           * 2018-03-01-preview: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01_preview.operations.SkusOperations>`\n           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage", "solution": "\n        api_version = self._get_api_version('skus')\n        if api_version == '2017-06-01':\n            from .v2017_06_01.operations import SkusOperations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-03-01-preview':\n            from .v2018_03_01_preview.operations import SkusOperations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import SkusOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def delete_local_file(file_name):\n    \"\"\"\n    Deletes the file associated with the file_name passed from local storage.\n    \n    :param str file_name: Filename of the file to be deleted\n    :return str: Filename of the file that was just deleted\n    \"\"\"", "output": "\n    if os.path.exists(file_name):\n        os.remove(file_name)\n        return file_name\n    else:\n        return None\n\n\ndef delete_remote_file(file_name):\n    \"\"\"\n    Deletes the file associated with the file_name passed from remote storage.\n    \n    :param str file_name: Filename of the file to be deleted\n    :return str: Filename of the file that was just deleted\n    \"\"\"\n    if os.path.exists(file_name):\n        os.remove(", "solution": "\n\n    try:\n        os.remove(file_name)\n        log.info(f\"Deletion for {file_name} has finished\")\n        return file_name\n    except OSError:\n        pass"}
{"prompt": "def output_json(self, file_name='/tmp/ND.json'):\n        \"\"\"\n        Arguments:\n            file_name(str : '/tmp/ND.json'): The file name to store the json to\n\n        Returns:\n            None\n        \"\"\"", "output": "\n        with open(file_name, 'w') as f:\n            json.dump(self.to_dict(), f)\n\n    def to_dict(self):\n        \"\"\"\n        Returns:\n            dict: A dictionary representation of the ND object\n        \"\"\"\n        return {\n            'name': self.name,\n            'description': self.description,\n            'version': self.version,\n            'author': self.author,\n            'email': self.email,\n            'url': self.url,\n            'license': self.lic", "solution": "\n        complete_example = (\n            self.dataset, self.project, self.channels, self.metadata)\n        data = json.loads(self.nd_json(*complete_example))\n\n        # self.verify_json(data)\n        self.verify_path(data, VERIFY_BY_SLICE)\n\n        f = open(file_name, 'w')\n        f.write(str(data))\n        f.close()"}
{"prompt": "def open_graph(self, **args):\n        \"\"\"open a vcg graph\n        \"\"\"", "output": "\n        if args.get('filename') is None:\n            raise Exception('filename not specified')\n        filename = args.get('filename')\n        if args.get('format') is None:\n            raise Exception('format not specified')\n        format = args.get('format')\n        if args.get('flags') is None:\n            raise Exception('flags not specified')\n        flags = args.get('flags')\n        if args.get('mask') is None:\n            raise Exception('mask not specified')\n        mask = args.get('mask')\n        if args", "solution": "\n        self._stream.write(\"%sgraph:{\\n\" % self._indent)\n        self._inc_indent()\n        self._write_attributes(GRAPH_ATTRS, **args)"}
{"prompt": "def remove(cls, name: sym.Symbol) -> Optional[\"Namespace\"]:\n        \"\"\"Remove the namespace bound to the symbol `name` in the global\n        namespace cache and return that namespace.\n        Return None if the namespace did not exist in the cache.\"\"\"", "output": "\n        return cls._cache.pop(name, None)\n\n    @classmethod\n    def get(cls, name: sym.Symbol) -> Optional[\"Namespace\"]:\n        \"\"\"Get the namespace bound to the symbol `name` in the global\n        namespace cache. Return None if the namespace did not exist in the\n        cache.\"\"\"\n        return cls._cache.get(name)\n\n    @classmethod\n    def clear(cls) -> None:\n        \"\"\"Clear the global namespace cache.\"\"\"\n        cls._cache.clear()\n\n    @classmethod", "solution": "\n        while True:\n            oldval: lmap.Map = cls._NAMESPACES.deref()\n            ns: Optional[Namespace] = oldval.entry(name, None)\n            newval = oldval\n            if ns is not None:\n                newval = oldval.dissoc(name)\n            if cls._NAMESPACES.compare_and_set(oldval, newval):\n                return ns"}
{"prompt": "def einsum_vecmul_index(gate_indices, number_of_qubits):\n    \"\"\"Return the index string for Numpy.eignsum matrix-vector multiplication.\n\n    The returned indices are to perform a matrix multiplication A.v where\n    the matrix A is an M-qubit matrix, vector v is an N-qubit vector, and\n    M <= N, and identity matrices are implied on the subsystems where A has no\n    support on v.\n\n    Args:\n        gate_indices (list[int]): the indices of the right matrix subsystems\n                                  to contract with the left matrix.\n        number_of_qubits (int): the total number of qubits for the right matrix.\n\n    Returns:\n        str: An indices string for the Numpy.einsum function.\n    \"\"\"", "output": "the gate to be applied\n        number_of_qubits (int): the number of qubits in the system\n\n    Returns:\n        str: the index string for the einsum call\n    \"\"\"\n    # pylint: disable=invalid-name\n    M = len(gate_indices)\n    N = number_of_qubits\n    if M > N:\n        raise ValueError(\"The number of qubits in the gate is larger than the number of qubits in the system.\")\n\n    # The indices of the gate\n    indices = []\n    for i", "solution": "\n\n    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,\n                                                                    number_of_qubits)\n\n    # Combine indices into matrix multiplication string format\n    # for numpy.einsum function\n    return \"{mat_l}{mat_r}, \".format(mat_l=mat_l, mat_r=mat_r) + \\\n           \"{tens_lin}->{tens_lout}\".format(tens_lin=tens_lin,\n                                            tens_lout=tens_lout)"}
{"prompt": "def get_client(self):\n        \"\"\"\n        Obtains a client for use, whether an existing unused client\n        or a brand new one if none are available.\n        \"\"\"", "output": "\n        if self.clients:\n            return self.clients.pop()\n        else:\n            return self.create_client()\n\n    def release_client(self, client):\n        \"\"\"\n        Releases a client back to the pool.\n        \"\"\"\n        self.clients.append(client)\n\n    def create_client(self):\n        \"\"\"\n        Creates a new client.\n        \"\"\"\n        client = self.client_class(self.host, self.port,\n                                   self.username, self.password,\n                                ", "solution": "\n        client = None\n        try:\n            client = self.clients.get(block=False)\n        except queue.Empty:\n            pass\n        if not client:\n            self.client_id += 1\n            kwargs = dict(self.kwargs)\n            kwargs['verbose_id'] = kwargs.get(\n                'verbose_id', '') + str(self.client_id)\n            client = self.client_class(*self.args, **kwargs)\n        return client"}
{"prompt": "def _sem_open(name, value=None):\n    \"\"\" Construct or retrieve a semaphore with the given name\n\n    If value is None, try to retrieve an existing named semaphore.\n    Else create a new semaphore with the given value\n    \"\"\"", "output": "\n    if value is None:\n        return _sem_open_existing(name)\n    else:\n        return _sem_open_new(name, value)\n\n\ndef _sem_open_existing(name):\n    \"\"\" Construct a semaphore with the given name\n\n    If the named semaphore already exists, raise an exception.\n    \"\"\"\n    sem_handle = ctypes.windll.kernel32.OpenSemaphoreW(\n        SEMAPHORE_ALL_ACCESS,\n        False,\n        name\n    )", "solution": "\n    if value is None:\n        handle = pthread.sem_open(ctypes.c_char_p(name), 0)\n    else:\n        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,\n                                  ctypes.c_int(value))\n\n    if handle == SEM_FAILURE:\n        e = ctypes.get_errno()\n        if e == errno.EEXIST:\n            raise FileExistsError(\"a semaphore named %s already exists\" % name)\n        elif e == errno.ENOENT:\n            raise FileNotFoundError('cannot find semaphore named %s' % name)\n        elif e == errno.ENOSYS:\n            raise NotImplementedError('No semaphore implementation on this '\n                                      'system')\n        else:\n            raiseFromErrno()\n\n    return handle"}
{"prompt": "def delete(self, request, *args, **kwargs):\n        \"\"\"Delete auth token when `delete` request was issued.\"\"\"", "output": "\n        self.object = self.get_object()\n        self.object.delete()\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n\nclass UserViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    API endpoint that allows users to be viewed or edited.\n    \"\"\"\n    queryset = User.objects.all().order_by('-date_joined')\n    serializer_class = UserSerializer\n    permission_classes = (permissions.IsAuthenticated,)\n\n    def get_queryset", "solution": "\n        # Logic repeated from DRF because one cannot easily reuse it\n        auth = get_authorization_header(request).split()\n\n        if not auth or auth[0].lower() != b'token':\n            return response.Response(status=status.HTTP_400_BAD_REQUEST)\n\n        if len(auth) == 1:\n            msg = 'Invalid token header. No credentials provided.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n        elif len(auth) > 2:\n            msg = 'Invalid token header. Token string should not contain spaces.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n\n        try:\n            token = self.model.objects.get(key=auth[1])\n        except self.model.DoesNotExist:\n            pass\n        else:\n            token.delete()\n            signals.user_logged_out.send(\n                type(self),\n                user=token.user,\n                request=request,\n            )\n        return response.Response(status=status.HTTP_204_NO_CONTENT)"}
{"prompt": "def load_pkcs7_data(type, buffer):\n    \"\"\"\n    Load pkcs7 data from the string *buffer* encoded with the type\n    *type*.\n\n    :param type: The file type (one of FILETYPE_PEM or FILETYPE_ASN1)\n    :param buffer: The buffer with the pkcs7 data.\n    :return: The PKCS7 object\n    \"\"\"", "output": "\n    if type == FILETYPE_PEM:\n        return load_pkcs7_pem(buffer)\n    elif type == FILETYPE_ASN1:\n        return load_pkcs7_asn1(buffer)\n    else:\n        raise ValueError(\"Unknown file type\")\n\n\ndef load_pkcs7_pem(buffer):\n    \"\"\"\n    Load pkcs7 data from the string *buffer* encoded with pem.\n\n    :param buffer: The buffer with the pkcs7 data.\n    :return: The PKCS", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        pkcs7 = _lib.PEM_read_bio_PKCS7(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        pkcs7 = _lib.d2i_PKCS7_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if pkcs7 == _ffi.NULL:\n        _raise_current_error()\n\n    pypkcs7 = PKCS7.__new__(PKCS7)\n    pypkcs7._pkcs7 = _ffi.gc(pkcs7, _lib.PKCS7_free)\n    return pypkcs7"}
{"prompt": "def create_sprites_dataset(characters, actions, directions, channels=3,\n                           length=8, shuffle=False, fake_data=False):\n  \"\"\"Creates a tf.data pipeline for the sprites dataset.\n\n  Args:\n    characters: A list of (skin, hair, top, pants) tuples containing\n      relative paths to the sprite png image for each attribute.\n    actions: A list of Actions.\n    directions: A list of Directions.\n    channels: Number of image channels to yield.\n    length: Desired length of the sequences.\n    shuffle: Whether or not to shuffle the characters and sequences\n      start frame.\n    fake_data: Boolean for whether or not to yield synthetic data.\n\n  Returns:\n    A tf.data.Dataset yielding (seq, skin label index, hair label index,\n    top label index, pants label index, action label index, skin label\n    name, hair label_name, top label name, pants label name, action\n    label name) tuples.\n  \"\"\"", "output": "length: Number of frames to yield.\n    shuffle: Whether to shuffle the dataset.\n    fake_data: Whether to use fake data.\n\n  Returns:\n    A tf.data.Dataset.\n  \"\"\"\n  if fake_data:\n    return _create_fake_sprites_dataset(characters, actions, directions,\n                                        channels, length, shuffle)\n  else:\n    return _create_real_sprites_dataset(characters, actions, directions,\n                                        channels, length, shuffle)\n\n\ndef", "solution": "\n  if fake_data:\n    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])\n  else:\n    basedir = download_sprites()\n\n  action_names = [action.name for action in actions]\n  action_metadata = [(action.start_row, action.frames) for action in actions]\n\n  direction_rows = [direction.row_offset for direction in directions]\n\n  chars = tf.data.Dataset.from_tensor_slices(characters)\n  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()\n  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()\n  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()\n\n  if shuffle:\n    chars = chars.shuffle(len(characters))\n\n  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))\n\n  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))\n  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))\n  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))\n  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))\n  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))\n\n  def process_example(attrs, act_name, act_metadata, dir_row_offset):\n    "}
{"prompt": "def _prepare_subsection(data, sections, wrapper):\n    \"\"\"\n    Creates a subsection ready to be filled.\n    :param data: Semi-parsed dictionary.\n    :param sections: A list of sections.\n    :param wrapper: A wrapping object for key-value pairs.\n    :return: A newly created subsection.\n    \"\"\"", "output": "\n    subsection = {}\n    for key, value in data.items():\n        if key in sections:\n            subsection[key] = _prepare_subsection(value, sections, wrapper)\n        else:\n            subsection[key] = wrapper(key, value)\n    return subsection\n\n\ndef _prepare_section(data, sections, wrapper):\n    \"\"\"\n    Creates a section ready to be filled.\n    :param data: Semi-parsed dictionary.\n    :param sections: A list of sections.\n    :param wrapper: A wrapping object", "solution": "\n    current = data\n    for i in sections[:-1]:\n        current = current[i]\n\n    current[sections[-1]] = wrapper()\n    return current[sections[-1]]"}
{"prompt": "def render_tag(self, context, *tag_args, **tag_kwargs):\n        \"\"\"\n        Rendering of the tag. It either assigns the value as variable, or renders it.\n        \"\"\"", "output": "\n        # get the value\n        value = self.value.resolve(context)\n        # get the name\n        name = self.name.resolve(context)\n        # get the type\n        type = self.type.resolve(context)\n        # get the options\n        options = self.options.resolve(context)\n        # get the id\n        id = self.id.resolve(context)\n        # get the class\n        class_ = self.class_.resolve(context)\n        # get the style\n        style = self.style.resolve(context)\n", "solution": "\n        # Be very explicit about which base functionality is used:\n        # Using super() for mixin support will not work nicely anyway here.\n        if self.as_var:\n            # Assign the value in the parent context\n            return BaseAssignmentNode.render_tag(self, context, *tag_args, **tag_kwargs)\n        else:\n            # Render the output using the BaseInclusionNode features\n            return BaseInclusionNode.render_tag(self, context, *tag_args, **tag_kwargs)"}
{"prompt": "def verify_token(self, request, *args, **kwargs):\n        \"\"\"\n        Use `token` to allow one-time access to a view.\n\n        Set the user as a class attribute or raise an `InvalidExpiredToken`.\n\n        Token expiry can be set in `settings` with `VERIFY_ACCOUNT_EXPIRY` and is\n        set in seconds.\n        \"\"\"", "output": "\n        token = kwargs.get(\"token\")\n        if not token:\n            raise InvalidExpiredToken(\"Token not provided\")\n        try:\n            payload = jwt.decode(token, settings.SECRET_KEY)\n        except jwt.ExpiredSignatureError:\n            raise InvalidExpiredToken(\"Token has expired\")\n        except jwt.InvalidTokenError:\n            raise InvalidExpiredToken(\"Invalid token\")\n        user = get_user_model().objects.get(pk=payload[\"user_id\"])\n        if user.is_active", "solution": "\n        User = get_user_model()\n\n        try:\n            max_age = settings.VERIFY_ACCOUNT_EXPIRY\n        except AttributeError:\n            max_age = self.DEFAULT_VERIFY_ACCOUNT_EXPIRY\n\n        try:\n            email_data = signing.loads(kwargs['token'], max_age=max_age)\n        except signing.BadSignature:\n            raise self.invalid_exception_class\n\n        email = email_data['email']\n\n        try:\n            self.user = User.objects.get_by_natural_key(email)\n        except User.DoesNotExist:\n            raise self.invalid_exception_class\n\n        if self.user.email_verified:\n            raise self.permission_denied_class"}
{"prompt": "def partitions(l, partition_size):\n    \"\"\"\n    >>> list(partitions([], 10))\n    []\n    >>> list(partitions([1,2,3,4,5], 1))\n    [[1], [2], [3], [4], [5]]\n    >>> list(partitions([1,2,3,4,5], 2))\n    [[1, 2], [3, 4], [5]]\n    >>> list(partitions([1,2,3,4,5], 5))\n    [[1, 2, 3, 4, 5]]\n\n    :param list l: List to be partitioned\n    :param int partition_size: Size of partitions\n    \"\"\"", "output": "[[1, 2, 3, 4, 5]]\n    >>> list(partitions([1,2,3,4,5], 6))\n    []\n    >>> list(partitions([1,2,3,4,5], 3))\n    [[1, 2, 3], [4, 5]]\n    >>> list(partitions([1,2,3,4,5], 4))\n    [[1, 2, 3, 4], [5]]\n    >>> list(partitions([1,2,3", "solution": "\n    for i in xrange(0, len(l), partition_size):\n        yield l[i:i + partition_size]"}
{"prompt": "def download_layers(self, repo_name, digest=None, destination=None):\n    \"\"\" download layers is a wrapper to do the following for a client loaded\n        with a manifest for an image:\n      \n        1. use the manifests to retrieve list of digests (get_digests)\n        2. atomically download the list to destination (get_layers)\n\n        This function uses the MultiProcess client to download layers\n        at the same time.\n    \"\"\"", "output": "\n    # get the list of digests\n    digests = self.get_digests(repo_name, digest)\n    # download the layers\n    self.get_layers(repo_name, digests, destination)\n\ndef get_digests(self, repo_name, digest=None):\n    \"\"\" get_digests is a wrapper to do the following for a client loaded\n        with a manifest for an image:\n      \n        1. use the manifests to retrieve list of digests (get_digests)\n        2. atomically download the list", "solution": "\n    from sregistry.main.workers import ( Workers, download_task )\n\n    # 1. Get manifests if not retrieved\n    if not hasattr(self, 'manifests'):\n        self._get_manifests(repo_name, digest)\n\n    # Obtain list of digets, and destination for download\n    digests = self._get_digests()\n    destination = self._get_download_cache(destination)\n\n    # Create multiprocess download client\n    workers = Workers()\n\n    # Download each layer atomically\n    tasks = []\n    layers = []\n    for digest in digests:\n\n        targz = \"%s/%s.tar.gz\" % (destination, digest)\n\n        # Only download if not in cache already\n        if not os.path.exists(targz):\n            url = \"%s/%s/blobs/%s\" % (self.base, repo_name, digest)\n            tasks.append((url, self.headers, targz))\n        layers.append(targz)\n\n    # Download layers with multiprocess workers\n    if len(tasks) > 0:\n        download_layers = workers.run(func=download_task,\n                                      tasks=tasks)\n    # Create the metadata tar\n    metadata = self._create_metadata_tar(destination)\n    if metadata is not None:\n        layers.append(metadata)\n\n\n    return layers"}
{"prompt": "def process_item(self, item, spider):\n        \"\"\"\n        Process single item. Add item to items and then upload to S3 if size of items\n        >= max_chunk_size.\n        \"\"\"", "output": "\n        self.items.append(item)\n        if len(self.items) >= self.max_chunk_size:\n            self.upload_items()\n\n    def upload_items(self):\n        \"\"\"\n        Upload items to S3.\n        \"\"\"\n        if not self.items:\n            return\n        try:\n            self.s3_client.put_object(\n                Bucket=self.bucket,\n                Key=self.key,\n                Body=json.dumps(self.items, cls=DjangoJSONEncoder),", "solution": "\n        self.items.append(item)\n        if len(self.items) >= self.max_chunk_size:\n            self._upload_chunk(spider)\n\n        return item"}
{"prompt": "def handle_print(self, signame, set_print):\n        \"\"\"Set whether we print or not when this signal is caught.\"\"\"", "output": "\n        if signame == 'SIGUSR1':\n            self.print_on_sigusr1 = set_print\n        elif signame == 'SIGUSR2':\n            self.print_on_sigusr2 = set_print\n        else:\n            raise ValueError('Unknown signal name: %s' % signame)\n\n    def handle_signal(self, signame, set_print):\n        \"\"\"Set whether we print or not when this signal is caught.\"\"\"\n        if signame == 'SIGUSR1':\n            self.print", "solution": "\n        if set_print:\n            self.sigs[signame].print_method = self.dbgr.intf[-1].msg\n        else:\n            self.sigs[signame].print_method = None\n            pass\n        return set_print"}
{"prompt": "def _trj_check_version(self, version, python, force):\n        \"\"\"Checks for version mismatch\n\n        Raises a VersionMismatchError if version of loaded trajectory and current pypet version\n        do not match. In case of `force=True` error is not raised only a warning is emitted.\n\n        \"\"\"", "output": "\n        if not force:\n            if version != self.version:\n                raise VersionMismatchError(\n                    'Version mismatch: Trajectory was saved with pypet version %s, '\n                    'but current pypet version is %s' % (version, self.version))\n\n    def _trj_check_python(self, python, force):\n        \"\"\"Checks for python version mismatch\n\n        Raises a VersionMismatchError if python version of loaded trajectory and current python\n        version do not match. In case of `force", "solution": "\n        curr_python = pypetconstants.python_version_string\n\n        if (version != VERSION or curr_python != python) and not force:\n            raise pex.VersionMismatchError('Current pypet version is %s used under python %s '\n                                           '  but your trajectory'\n                                           ' was created with version %s and python %s.'\n                                           ' Use >>force=True<< to perform your load regardless'\n                                           ' of version mismatch.' %\n                                           (VERSION, curr_python, version, python))\n        elif version != VERSION or curr_python != python:\n            self._logger.warning('Current pypet version is %s with python %s but your trajectory'\n                                 ' was created with version %s under python %s.'\n                                 ' Yet, you enforced the load, so I will'\n                                 ' handle the trajectory despite the'\n                                 ' version mismatch.' %\n                                 (VERSION, curr_python, version, python))"}
{"prompt": "def get_branch_mutation_matrix(self, node, full_sequence=False):\n        \"\"\"uses results from marginal ancestral inference to return a joint\n        distribution of the sequence states at both ends of the branch.\n\n        Parameters\n        ----------\n        node : Phylo.clade\n            node of the tree\n        full_sequence : bool, optional\n            expand the sequence to the full sequence, if false (default)\n            the there will be one mutation matrix for each column in the\n            reduced alignment\n\n        Returns\n        -------\n        numpy.array\n            an Lxqxq stack of matrices (q=alphabet size, L (reduced)sequence length)\n        \"\"\"", "output": "       mutation_matrix : ndarray\n            joint distribution of the sequence states at both ends of the branch\n        \"\"\"\n        # get the marginal ancestral inference results\n        marginal_ancestral_inference_results = self.get_marginal_ancestral_inference_results(node)\n        # get the mutation matrix\n        mutation_matrix = self.get_mutation_matrix(marginal_ancestral_inference_results, full_sequence)\n        return mutation_matrix\n\n    def get_mutation_matrix", "solution": "\n        pp,pc = self.marginal_branch_profile(node)\n\n        # calculate pc_i [e^Qt]_ij pp_j for each site\n        expQt = self.gtr.expQt(self._branch_length_to_gtr(node))\n        if len(expQt.shape)==3: # site specific model\n            mut_matrix_stack = np.einsum('ai,aj,ija->aij', pc, pp, expQt)\n        else:\n            mut_matrix_stack = np.einsum('ai,aj,ij->aij', pc, pp, expQt)\n\n        # normalize this distribution\n        normalizer = mut_matrix_stack.sum(axis=2).sum(axis=1)\n        mut_matrix_stack = np.einsum('aij,a->aij', mut_matrix_stack, 1.0/normalizer)\n\n        # expand to full sequence if requested\n        if full_sequence:\n            return mut_matrix_stack[self.full_to_reduced_sequence_map]\n        else:\n            return mut_matrix_stack"}
{"prompt": "def meas_gate(self, circuit, qreg, op):\n        \"\"\"\n        Add measurement gates to a circuit.\n\n        Args:\n            circuit (QuantumCircuit): circuit to add measurement to.\n            qreg (tuple(QuantumRegister,int)): quantum register being measured.\n            op (str): the basis label for the measurement.\n        \"\"\"", "output": "\n        if op == 'X':\n            circuit.measure(qreg[0], qreg[1])\n        elif op == 'Y':\n            circuit.measure(qreg[0], qreg[1])\n        elif op == 'Z':\n            circuit.measure(qreg[0], qreg[1])\n        else:\n            raise ValueError(\"Invalid measurement basis.\")\n\n    def meas_circuit(self, qregs, ops):\n        \"\"\"\n        Create a circuit with measurement gates.\n\n        Args:\n            qregs (", "solution": "\n        if self.meas_fun is None:\n            pass\n        else:\n            self.meas_fun(circuit, qreg, op)"}
{"prompt": "def log(wave):\n    r\"\"\"\n    Return the natural logarithm of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.log\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": "\n    .. [[[end]]]\n    \"\"\"\n    return log(wave)\n\n\ndef log10(wave):\n    r\"\"\"\n    Return the base 10 logarithm of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sph", "solution": "\n    pexdoc.exh.addex(\n        ValueError, \"Math domain error\", bool((min(wave._dep_vector) <= 0))\n    )\n    return _operation(wave, \"log\", \"\", np.log)"}
{"prompt": "async def save_tracks(self, *tracks):\n        \"\"\"Save one or more tracks to the current user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        tracks : Sequence[Union[Track, str]]\n            A sequence of track objects or spotify IDs\n        \"\"\"", "output": "\n        if not tracks:\n            raise ValueError('No tracks provided')\n\n        tracks = [self._get_id(track) for track in tracks]\n        return self._put(\n            'me/tracks',\n            json={'ids': tracks},\n            params={'market': self.country},\n        )\n\n    def remove_tracks(self, *tracks):\n        \"\"\"Remove one or more tracks from the current user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        tracks : Sequence[Union[Track", "solution": "\n        _tracks = [(obj if isinstance(obj, str) else obj.id) for obj in tracks]\n        await self.user.http.save_tracks(','.join(_tracks))"}
{"prompt": "def get_logging_level():\n    \"\"\"get_logging_level will configure a logging to standard out based on the user's\n    selected level, which should be in an environment variable called\n    MESSAGELEVEL. if MESSAGELEVEL is not set, the maximum level\n    (5) is assumed (all messages).\n    \"\"\"", "output": "\n    level = 5\n    try:\n        level = int(os.environ[\"MESSAGELEVEL\"])\n    except KeyError:\n        pass\n    except ValueError:\n        pass\n    return level\n\n\ndef get_logging_level_name(level):\n    \"\"\"get_logging_level_name will return the name of the logging level\n    \"\"\"\n    if level == 0:\n        return \"CRITICAL\"\n    elif level == 1:\n        return \"ERROR\"\n    elif level == 2:\n        return \"W", "solution": "\n    level = os.environ.get(\"MESSAGELEVEL\", INFO)\n\n    # User knows logging levels and set one\n    if isinstance(level, int):\n        return level\n\n    # Otherwise it's a string\n    if level == \"CRITICAL\":\n        return CRITICAL\n    elif level == \"ABORT\":\n        return ABORT\n    elif level == \"ERROR\":\n        return ERROR\n    elif level == \"WARNING\":\n        return WARNING\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"INFO\":\n        return INFO\n    elif level == \"QUIET\":\n        return QUIET\n    elif level.startswith(\"VERBOSE\"):\n        return VERBOSE3\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"DEBUG\":\n        return DEBUG\n\n    return level"}
{"prompt": "def format_task_numbers_with_links(tasks):\n    \"\"\"Returns formatting for the tasks section of asana.\"\"\"", "output": "\n    formatted_tasks = []\n    for task in tasks:\n        formatted_tasks.append(f\"[{task['name']}]({task['permalink']})\")\n    return formatted_tasks\n\n\ndef format_section_header(section_name):\n    \"\"\"Returns formatting for the section headers of asana.\"\"\"\n    return f\"**{section_name}**\"\n\n\ndef format_section_body(section_name, section_body):\n    \"\"\"Returns formatting for the section bodies of asana.\"\"\"\n    return f\"{section_", "solution": "\n\n    project_id = data.get('asana-project', None)\n\n    def _task_format(task_id):\n        if project_id:\n            asana_url = tool.ToolApp.make_asana_url(project_id, task_id)\n            return \"[#%d](%s)\" % (task_id, asana_url)\n        else:\n            return \"#%d\" % task_id\n\n    return \"\\n\".join([_task_format(tid) for tid in tasks])"}
{"prompt": "def _mode(self):\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"", "output": "\n    return self.mean()\n\n  def _mean(self):\n    \"\"\"The mean of the von Mises-Fisher distribution is the mean direction.\"\"\"\n    return self.mean_direction\n\n  def _variance(self):\n    \"\"\"The variance of the von Mises-Fisher distribution is the variance of the\n    mean direction.\n    \"\"\"\n    return self.mean_direction.variance()\n\n  def _stddev(self):\n    \"\"\"The standard deviation of the von Mises-Fisher distribution is the\n    standard deviation of the mean direction", "solution": "\n    return (self.mean_direction +\n            tf.zeros_like(self.concentration)[..., tf.newaxis])"}
{"prompt": "def is_detached(self):\n        \"\"\"Check if the repo is in a detached state.\n\n        The repository is in a detached state when HEAD is not a symbolic\n        reference.\n\n        :returns: whether the repository is detached or not\n\n        :raises RepositoryError: when an error occurs checking the state\n            of the repository\n        \"\"\"", "output": "\n        try:\n            return self.repo.head.is_detached\n        except (AttributeError, ValueError):\n            raise RepositoryError(\n                \"Unable to check if repository is detached\"\n            )\n\n    def is_dirty(self):\n        \"\"\"Check if the repo is dirty.\n\n        The repository is dirty when there are uncommitted changes.\n\n        :returns: whether the repository is dirty or not\n\n        :raises RepositoryError: when an error occurs checking the state\n            of the repository\n        \"\"\"\n        try:", "solution": "\n        cmd_sym = ['git', 'symbolic-ref', 'HEAD']\n\n        try:\n            self._exec(cmd_sym, cwd=self.dirpath, env=self.gitenv)\n        except RepositoryError as e:\n            if e.msg.find(\"ref HEAD is not a symbolic ref\") == -1:\n                raise e\n            return True\n        else:\n            return False"}
{"prompt": "def publish_events(\n            self, topic_hostname, events, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Publishes a batch of events to an Azure Event Grid topic.\n\n        :param topic_hostname: The host name of the topic, e.g.\n         topic1.westus2-1.eventgrid.azure.net\n        :type topic_hostname: str\n        :param events: An array of events to be published to Event Grid.\n        :type events: list[~azure.eventgrid.models.EventGridEvent]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`\n        \"\"\"", "output": "GridEvent]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`\n        \"\"\"\n       ", "solution": "\n        # Construct URL\n        url = self.publish_events.metadata['url']\n        path_format_arguments = {\n            'topicHostname': self._serialize.url(\"topic_hostname\", topic_hostname, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(events, '[EventGridEvent]')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise HttpOperationError(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response"}
{"prompt": "def get_imap_capabilities(server):\n    \"\"\"\n    Returns a list of an IMAP server's capabilities\n\n    Args:\n        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient\n\n    Returns (list): A list of capabilities\n    \"\"\"", "output": "\n    return server.capabilities\n\n\ndef get_imap_status(server):\n    \"\"\"\n    Returns the status of an IMAP server\n\n    Args:\n        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient\n\n    Returns (dict): A dictionary of status\n    \"\"\"\n    return server.status()\n\n\ndef get_imap_folders(server):\n    \"\"\"\n    Returns a list of folders on an IMAP server\n\n    Args:\n        server (imapclient.IMAPClient", "solution": "\n\n    capabilities = list(map(str, list(server.capabilities())))\n    for i in range(len(capabilities)):\n        capabilities[i] = str(capabilities[i]).replace(\"b'\",\n                                                       \"\").replace(\"'\",\n                                                                   \"\")\n    logger.debug(\"IMAP server supports: {0}\".format(capabilities))\n\n    return capabilities"}
{"prompt": "def update_path(self, board, color, path):\n        \"\"\" update win/loss count along path \"\"\"", "output": "\n        for i in range(len(path) - 1):\n            if path[i] == path[i + 1]:\n                if path[i] == color:\n                    board[path[i]][path[i + 1]] += 1\n                else:\n                    board[path[i]][path[i + 1]] -= 1\n\n    def check_win(self, board, color):\n        \"\"\" check if the color has won \"\"\"\n        for i in range(self.size):\n            if board[i][i]", "solution": "\n        wins = board.score(BLACK) >= board.score(WHITE)\n        for node in path:\n            if color == BLACK:\n                color = WHITE\n            else:\n                color = BLACK\n            if wins == (color == BLACK):\n                node.wins += 1\n            else:\n                node.losses += 1\n            if node.parent:\n                node.parent.bestchild = node.parent.best_child()"}
{"prompt": "def read(self):\n        \"\"\"\n        Load the metrics file from the given path\n        \"\"\"", "output": "\n        if not self.path:\n            return\n\n        try:\n            with open(self.path, 'r') as f:\n                self.metrics = json.load(f)\n        except IOError:\n            self.metrics = {}\n\n    def write(self):\n        \"\"\"\n        Save the metrics file to the given path\n        \"\"\"\n        if not self.path:\n            return\n\n        try:\n            with open(self.path, 'w') as f:\n                json.dump(self.metrics, f)\n       ", "solution": "\n        f = open(self.path, \"r\")\n        self.manifest_json = f.read()"}
{"prompt": "def update(self, teamId, name=None, **request_parameters):\n        \"\"\"Update details for a team, by ID.\n\n        Args:\n            teamId(basestring): The team ID.\n            name(basestring): A user-friendly name for the team.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Team: A Team object with the updated Webex Teams team details.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "input is invalid.\n            ApiError: If the API returns an error.\n        \"\"\"\n        if not teamId:\n            raise TypeError(\"Missing required parameter: teamId\")\n\n        if not name:\n            raise TypeError(\"Missing required parameter: name\")\n\n        if not isinstance(teamId, basestring):\n            raise TypeError(\"Expected a string for parameter 'teamId'\")\n\n        if not isinstance(name, basestring):\n            raise TypeError(\"Expected a string for parameter 'name'\")\n\n        url = self.build_url", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(name, basestring)\n\n        put_data = dict_from_items_with_values(\n            request_parameters,\n            name=name,\n        )\n\n        # API request\n        json_data = self._session.put(API_ENDPOINT + '/' + teamId,\n                                      json=put_data)\n\n        # Return a team object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def _odds_val(self):\n        \"\"\"Determine a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.odds` value.\n\n        :returns: The derived value\n        \"\"\"", "output": "\n        if self.odds is None:\n            return None\n        if self.odds == 0:\n            return None\n        if self.odds == 1:\n            return self.value\n        if self.odds == 2:\n            return self.value\n        if self.odds == 3:\n            return self.value\n        if self.odds == 4:\n            return self.value\n        if self.odds == 5:\n            return self.value\n        if self.odds == 6:\n            return", "solution": "\n        if len(self.odds) == 0:\n            self.odds = [(1.00, [self.min, self.max])]\n\n        rand_val = rand.random()\n        total = 0\n        for percent,v in self.odds:\n            if total <= rand_val < total+percent:\n                found_v = v\n                break\n            total += percent\n\n        res = None\n        if isinstance(v, (tuple,list)):\n            rand_func = rand.randfloat if type(v[0]) is float else rand.randint\n\n            if len(v) == 2:\n                res = rand_func(v[0], v[1])\n            elif len(v) == 1:\n                res = v[0]\n        else:\n            res = v\n\n        return res"}
{"prompt": "def _check_key_value_types(obj, key_type, value_type, key_check=isinstance, value_check=isinstance):\n    \"\"\"Ensures argument obj is a dictionary, and enforces that the keys/values conform to the types\n    specified by key_type, value_type.\n    \"\"\"", "output": "\n    if not isinstance(obj, dict):\n        raise TypeError(\"obj must be a dictionary\")\n    for key, value in obj.items():\n        if not key_check(key, key_type):\n            raise TypeError(\"obj keys must be of type {}\".format(key_type))\n        if not value_check(value, value_type):\n            raise TypeError(\"obj values must be of type {}\".format(value_type))\n\n\ndef _check_key_value_types_or_none(obj, key_type, value_type, key_", "solution": "\n    if not isinstance(obj, dict):\n        raise_with_traceback(_type_mismatch_error(obj, dict))\n\n    if key_type is str:\n        key_type = string_types\n\n    if value_type is str:\n        value_type = string_types\n\n    for key, value in obj.items():\n        if key_type and not key_check(key, key_type):\n            raise_with_traceback(\n                CheckError(\n                    'Key in dictionary mismatches type. Expected {key_type}. Got {obj_repr}'.format(\n                        key_type=repr(key_type), obj_repr=repr(key)\n                    )\n                )\n            )\n        if value_type and not value_check(value, value_type):\n            raise_with_traceback(\n                CheckError(\n                    'Value in dictionary mismatches expected type for key {key}. Expected value '\n                    'of type {vtype}. Got value {value} of type {obj_type}.'.format(\n                        vtype=repr(value_type), obj_type=type(value), key=key, value=value\n                    )\n                )\n            )\n    return obj"}
{"prompt": "def _clear_temp(self):\n        \"\"\"\n        Clear temp directory from created csv and ods files during\n        communicator operations.\n        \"\"\"", "output": "\n        if os.path.exists(self.temp_dir):\n            for file in os.listdir(self.temp_dir):\n                if file.endswith(\".csv\") or file.endswith(\".ods\"):\n                    os.remove(os.path.join(self.temp_dir, file))\n\n    def _get_temp_file(self, file_type):\n        \"\"\"\n        Get a temporary file name for a given file type.\n\n        :param file_type: File type to get a temporary file name for.\n        :type file", "solution": "\n        temp_files = [LOCAL_ODS, GDOCS_TRANS_CSV, GDOCS_META_CSV,\n                      LOCAL_TRANS_CSV, LOCAL_META_CSV]\n        for temp_file in temp_files:\n            file_path = os.path.join(self.temp_path, temp_file)\n            if os.path.exists(file_path):\n                os.remove(file_path)"}
{"prompt": "def to_zyz(self):\n        \"\"\"Converts a unit-length quaternion to a sequence\n        of ZYZ Euler angles.\n\n        Returns:\n            ndarray: Array of Euler angles.\n        \"\"\"", "output": "\n        q = self.data\n        return np.array([np.arctan2(2 * (q[0] * q[1] + q[2] * q[3]),\n                                    1 - 2 * (q[1] * q[1] + q[2] * q[2])),\n                         np.arcsin(2 * (q[0] * q[2] - q[3] * q[1])),\n                         np.arctan2(2 * (q[0] * q[3] + q[", "solution": "\n        mat = self.to_matrix()\n        euler = np.zeros(3, dtype=float)\n        if mat[2, 2] < 1:\n            if mat[2, 2] > -1:\n                euler[0] = math.atan2(mat[1, 2], mat[0, 2])\n                euler[1] = math.acos(mat[2, 2])\n                euler[2] = math.atan2(mat[2, 1], -mat[2, 0])\n            else:\n                euler[0] = -math.atan2(mat[1, 0], mat[1, 1])\n                euler[1] = np.pi\n        else:\n            euler[0] = math.atan2(mat[1, 0], mat[1, 1])\n        return euler"}
{"prompt": "def export_variants(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"Export causative variants for a collaborator\n\n    Args:\n        adapter(MongoAdapter)\n        collaborator(str)\n        document_id(str): Search for a specific variant\n        case_id(str): Search causative variants for a case\n\n    Yields:\n        variant_obj(scout.Models.Variant): Variants marked as causative ordered by position.\n    \"\"\"", "output": "\n    if document_id:\n        variant_obj = adapter.variant_collection.find_one(\n            {\"_id\": document_id, \"case_id\": case_id}\n        )\n        if variant_obj:\n            yield variant_obj\n        return\n\n    if case_id:\n        variants = adapter.variant_collection.find(\n            {\"case_id\": case_id, \"causative\": True},\n            sort=[(\"chromosome\", 1), (\"position\", 1)],\n        )\n    else:\n        variants", "solution": "\n\n    # Store the variants in a list for sorting\n    variants = []\n    if document_id:\n        yield adapter.variant(document_id)\n        return\n\n    variant_ids = adapter.get_causatives(\n        institute_id=collaborator,\n        case_id=case_id\n        )\n    ##TODO add check so that same variant is not included more than once\n    for document_id in variant_ids:\n\n        variant_obj = adapter.variant(document_id)\n        chrom = variant_obj['chromosome']\n        # Convert chromosome to integer for sorting\n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.info(\"Unknown chromosome %s\", chrom)\n            continue\n\n        # Add chromosome and position to prepare for sorting\n        variants.append((chrom_int, variant_obj['position'], variant_obj))\n\n    # Sort varants based on position\n    variants.sort(key=lambda x: (x[0], x[1]))\n\n    for variant in variants:\n        variant_obj = variant[2]\n        yield variant_obj"}
{"prompt": "def delete_uneeded(self):\n        \"\"\"\n        Delete the directory which are not registered into our structure.\n        \"\"\"", "output": "\n        for directory in os.listdir(self.directory):\n            if directory not in self.structure:\n                shutil.rmtree(os.path.join(self.directory, directory))\n\n    def create_structure(self):\n        \"\"\"\n        Create the directory structure.\n        \"\"\"\n        for directory in self.structure:\n            os.mkdir(os.path.join(self.directory, directory))\n\n    def move_files(self):\n        \"\"\"\n        Move the files into the directory structure.\n        \"\"\"\n        for file in os.", "solution": "\n\n        # We get the structure we have to apply.\n        structure = self._get_structure()\n\n        # We get the list of key which is implicitly the list of directory we do not bave to delete.\n        list_of_key = list(structure.keys())\n\n        # We move to the content of the parent as we know that we are creating only one directory.\n        # Note: if one day we will have to create multiple directory, we will have to change\n        # the following.\n        structure = structure[list_of_key[0]]\n\n        # We also set the parent directory as we are going to construct its childen.\n        parent_path = list_of_key[0]\n\n        if not parent_path.endswith(PyFunceble.directory_separator):\n            parent_path += PyFunceble.directory_separator\n\n        for root, _, _ in PyFunceble.walk(parent_path):\n            # We loop through each directories of the parent path.\n\n            # We fix the path in order to avoid issues.\n            root = Directory(root).fix_path()\n\n            if root.replace(parent_path, \"\") not in structure:\n                # The currently read directory is not in our structure.\n\n                # We delete it.\n                PyFunceble.rmtree(root)"}
{"prompt": "def is_effective_member(self, group_id, netid):\n        \"\"\"\n        Returns True if the netid is in the group, False otherwise.\n        \"\"\"", "output": "\n        return self.is_member(group_id, netid) and self.is_active(group_id, netid)\n\n    def is_member(self, group_id, netid):\n        \"\"\"\n        Returns True if the netid is in the group, False otherwise.\n        \"\"\"\n        return self.is_in_group(group_id, netid)\n\n    def is_active(self, group_id, netid):\n        \"\"\"\n        Returns True if the netid is in the group, False otherwise.\n        \"\"\"\n       ", "solution": "\n        self._valid_group_id(group_id)\n\n        # GWS doesn't accept EPPNs on effective member checks, for UW users\n        netid = re.sub('@washington.edu', '', netid)\n\n        url = \"{}/group/{}/effective_member/{}\".format(self.API,\n                                                       group_id,\n                                                       netid)\n\n        try:\n            data = self._get_resource(url)\n            return True  # 200\n        except DataFailureException as ex:\n            if ex.status == 404:\n                return False\n            else:\n                raise"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for dirpath, dirnames, filenames in os.walk(rootpath):\r\n        # Ignore certain directories and files\r\n        for skip_path in skip:\r\n            if os.path.join(dirpath, skip_path) in dirnames:\r\n                dirnames.remove(skip_path)\r\n        for filename in filenames:\r\n            if filename.endswith('.py') and filename != '__init__.py':\r\n                module_name = filename[:-3]\r\n                module_path =", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def Rock(*args, **kwargs):\n    \"\"\"\n    Graceful deprecation for old class name.\n    \"\"\"", "output": "\n    from .Rock import Rock\n    return Rock(*args, **kwargs)\n\n\ndef Rock_v2(*args, **kwargs):\n    \"\"\"\n    Graceful deprecation for old class name.\n    \"\"\"\n    from .Rock_v2 import Rock_v2\n    return Rock_v2(*args, **kwargs)\n\n\ndef Rock_v3(*args, **kwargs):\n    \"\"\"\n    Graceful deprecation for old class name.\n    \"\"\"\n    from .Rock_v3 import Rock_v3\n    return Rock_v3", "solution": "\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        w = \"The 'Rock' class was renamed 'Component'. \"\n        w += \"Please update your code.\"\n        warnings.warn(w, DeprecationWarning, stacklevel=2)\n\n    return Component(*args, **kwargs)"}
{"prompt": "def metadata(self):\n        \"\"\"Get metadata information in XML format.\"\"\"", "output": "\n        return self.request('metadata')\n\n    def metadata_json(self):\n        \"\"\"Get metadata information in JSON format.\"\"\"\n        return self.request('metadata', 'json')\n\n    def metadata_xml(self):\n        \"\"\"Get metadata information in XML format.\"\"\"\n        return self.request('metadata', 'xml')\n\n    def metadata_csv(self):\n        \"\"\"Get metadata information in CSV format.\"\"\"\n        return self.request('metadata', 'csv')\n\n    def metadata_tsv(self):\n        \"\"\"Get metadata information in TSV", "solution": "\n\n        params = {\n            self.PCTYPE: self.CTYPE_XML\n        }\n\n        response = self.call(self.CGI_BUG, params)\n\n        return response"}
{"prompt": "def put(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a PUT request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"", "output": "\n        raise NotImplementedError\n\n    def delete(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a DELETE request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"\n        raise NotImplementedError\n\n    def options(self, thing_id='0', action_name", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        self.set_status(200)"}
{"prompt": "def safe_infer(\n    node: astroid.node_classes.NodeNG, context=None\n) -> Optional[astroid.node_classes.NodeNG]:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"", "output": "\n    if context is None:\n        context = astroid.InferenceContext()\n    inferred = context.infer(node)\n    if len(inferred) == 1:\n        return inferred[0]\n    return None\n\n\ndef safe_infer_string(node: astroid.node_classes.NodeNG) -> Optional[str]:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"\n    infer", "solution": "\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except astroid.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value"}
{"prompt": "def IsNotNone(*fields, default=None):\n    \"\"\"Selects whichever field is not None, in the specified order.\n\n    Arguments:\n        fields:\n            The fields to attempt to get a value from,\n            in order.\n\n        default:\n            The value to return in case all values are None.\n\n    Returns:\n        A Case-When expression that tries each field and\n        returns the specified default value when all of\n        them are None.\n    \"\"\"", "output": "\n    return Case(\n        *[When(field, field) for field in fields],\n        default=default,\n    )\n\n\ndef IsNotNoneOrEmpty(*fields, default=None):\n    \"\"\"Selects whichever field is not None or empty, in the specified order.\n\n    Arguments:\n        fields:\n            The fields to attempt to get a value from,\n            in order.\n\n        default:\n            The value to return in case all values are None\n            or empty.\n\n    Returns:\n        A Case-When expression that tries", "solution": "\n\n    when_clauses = [\n        expressions.When(\n            ~expressions.Q(**{field: None}),\n            then=expressions.F(field)\n        )\n        for field in reversed(fields)\n    ]\n\n    return expressions.Case(\n        *when_clauses,\n        default=expressions.Value(default),\n        output_field=CharField()\n    )"}
{"prompt": "def authorize_redirect(self, oauth_scope, callback_uri=None,\n                           ax_attrs=[\"name\",\"email\",\"language\",\"username\"]):\n        \"\"\"Authenticates and authorizes for the given Google resource.\n\n        Some of the available resources are:\n\n        * Gmail Contacts - http://www.google.com/m8/feeds/\n        * Calendar - http://www.google.com/calendar/feeds/\n        * Finance - http://finance.google.com/finance/feeds/\n\n        You can authorize multiple resources by separating the resource\n        URLs with a space.\n        \"\"\"", "output": "Args:\n          oauth_scope: The scope of the resource to authorize for.\n          callback_uri: The callback URI to use for the authorization.\n          ax_attrs: The list of attributes to request from the user.\n\n        Returns:\n          The URL to redirect the user to in order to authorize the request.\n        \"\"\"\n        if callback_uri is None:\n            callback_uri = self.request.uri\n\n        # Generate the OAuth request token.\n        request_token = self.get_request_token(oauth_scope, callback", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs,\n                                 oauth_scope=oauth_scope)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def main(self,argv=None):\n        \"\"\"Run as a command-line script.\"\"\"", "output": "\n        if argv is None:\n            argv = sys.argv[1:]\n        parser = argparse.ArgumentParser(description=__doc__)\n        parser.add_argument('--version',action='version',version=__version__)\n        parser.add_argument('-v','--verbose',action='store_true',\n            help='Print verbose output.')\n        parser.add_argument('-q','--quiet',action='store_true',\n            help='Print only errors.')\n        parser.add_argument('-c','--", "solution": "\n\n        parser = optparse.OptionParser(usage=USAGE % self.__class__.__name__)\n        newopt = parser.add_option\n        newopt('-i','--interact',action='store_true',default=False,\n               help='Interact with the program after the script is run.')\n\n        opts,args = parser.parse_args(argv)\n\n        if len(args) != 1:\n            print >> sys.stderr,\"You must supply exactly one file to run.\"\n            sys.exit(1)\n\n        self.run_file(args[0],opts.interact)"}
{"prompt": "def init_s3_keys(opt):\n    \"\"\"Initialize s3 access keys from environment variable or s3cfg config file.\"\"\"", "output": "\n    if opt.s3cfg_path is not None:\n        s3cfg_path = opt.s3cfg_path\n    else:\n        s3cfg_path = os.path.join(os.path.expanduser('~'), '.s3cfg')\n    if not os.path.exists(s3cfg_path):\n        raise ValueError('S3 config file not found: {}'.format(s3cfg_path))\n    with open(s3cfg_path, 'r') as f:\n        s3cfg = f.read()\n", "solution": "\n    S3Handler.S3_KEYS = S3Handler.s3_keys_from_cmdline(opt) or S3Handler.s3_keys_from_env() \\\n                        or S3Handler.s3_keys_from_s3cfg(opt)"}
{"prompt": "def _reshuffle(mat, shape):\n    \"\"\"Reshuffle the indicies of a bipartite matrix A[ij,kl] -> A[lj,ki].\"\"\"", "output": "\n    mat = mat.reshape(shape)\n    mat = mat.transpose(1, 0, 3, 2)\n    mat = mat.reshape(shape[1], shape[0], shape[3], shape[2])\n    return mat\n\n\ndef _reshape_bipartite(mat, shape):\n    \"\"\"Reshape a bipartite matrix A[ij,kl] -> A[ij,kl].\"\"\"\n    mat = mat.reshape(shape)\n    mat = mat.transpose(1, 0, 3", "solution": "\n    return np.reshape(\n        np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)),\n        (shape[3] * shape[1], shape[0] * shape[2]))"}
{"prompt": "def get_by_symbol_name(self, name: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"", "output": "\n        return self.get_by_symbol(name)\n\n    def get_by_symbol(self, symbol: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol \"\"\"\n        return self.get_by_symbol_type(symbol, None)\n\n    def get_by_symbol_type(self, symbol: str, symbol_type: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol and symbol type \"\"\"\n        return self.get_by_symbol_type_and_scope(symbol,", "solution": "\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope"}
{"prompt": "def to_ndarray(self):\n        \"\"\"\n        Transfer JTensor to ndarray.\n        As SparseTensor may generate an very big ndarray, so we don't support this function for SparseTensor.\n        :return: a ndarray\n        \"\"\"", "output": "\n        if self.is_sparse:\n            raise RuntimeError(\"SparseTensor can't be transfer to ndarray.\")\n        return self.data.asnumpy()\n\n    def to_tensor(self):\n        \"\"\"\n        Transfer JTensor to Tensor.\n        :return: a Tensor\n        \"\"\"\n        return Tensor(self.data)\n\n    def __str__(self):\n        return self.data.__str__()\n\n    def __repr__(self):\n        return self.data.__repr__()\n\n    def", "solution": "\n        assert self.indices is None, \"sparseTensor to ndarray is not supported\"\n        return np.array(self.storage, dtype=get_dtype(self.bigdl_type)).reshape(self.shape)"}
{"prompt": "def deprecated(msg=''):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\n\n    :param msg:\n\n        Additional message added to the warning.\n\n    \"\"\"", "output": "\n\n    def deprecated_decorator(func):\n        \"\"\"Decorator function.\n\n        :param func:\n\n            Function to be decorated.\n\n        \"\"\"\n\n        def new_func(*args, **kwargs):\n            \"\"\"Decorated function.\n\n            :param args:\n\n                Function arguments.\n\n            :param kwargs:\n\n                Function keyword arguments.\n\n            \"\"\"\n\n            warnings.warn(\n                \"Call to deprecated function {}.\".format(func.__name__) +\n                (msg and \" {}\".format(msg", "solution": "\n\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warning_string = \"Call to deprecated function or property `%s`.\" % func.__name__\n            warning_string = warning_string + ' ' + msg\n            warnings.warn(\n                warning_string,\n                category=DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return wrapper"}
{"prompt": "def uncache_zipdir(path):\n    \"\"\"Ensure that the importer caches dont have stale info for `path`\"\"\"", "output": "\n    import zipfile\n    import sys\n    import os\n    import importlib.util\n\n    if not os.path.exists(path):\n        raise FileNotFoundError(path)\n\n    if os.path.isdir(path):\n        path = os.path.join(path, '__init__.py')\n\n    if not os.path.exists(path):\n        raise FileNotFoundError(path)\n\n    if not path.endswith('.zip'):\n        raise ValueError(f'path must be a zip file, not {path}')\n", "solution": "\n    from zipimport import _zip_directory_cache as zdc\n    _uncache(path, zdc)\n    _uncache(path, sys.path_importer_cache)"}
{"prompt": "def adjust_frame(self, pos, absolute_pos):\n        \"\"\"Adjust stack frame by pos positions. If absolute_pos then\n        pos is an absolute number. Otherwise it is a relative number.\n\n        A negative number indexes from the other end.\"\"\"", "output": "\n        if absolute_pos:\n            self.frame = pos\n        else:\n            self.frame += pos\n\n    def get_frame(self):\n        \"\"\"Return the current frame number.\"\"\"\n        return self.frame\n\n    def get_frame_count(self):\n        \"\"\"Return the number of frames in the stack.\"\"\"\n        return self.frame_count\n\n    def get_frame_rate(self):\n        \"\"\"Return the frame rate of the stack.\"\"\"\n        return self.frame_rate\n\n    def get_frame_time(self):", "solution": "\n        if not self.curframe:\n            Mmsg.errmsg(self, \"No stack.\")\n            return\n\n        # Below we remove any negativity. At the end, pos will be\n        # the new value of self.curindex.\n        if absolute_pos:\n            if pos >= 0:\n                pos = len(self.stack)-pos-1\n            else:\n                pos = -pos-1\n        else:\n            pos += self.curindex\n\n        if pos < 0:\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the oldest frame.\")\n            return\n        elif pos >= len(self.stack):\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the newest frame.\")\n            return\n\n        self.curindex = pos\n        self.curframe = self.stack[self.curindex][0]\n        self.print_location()\n        self.list_lineno = None\n        return"}
{"prompt": "def filter_unused_variable(line, previous_line=''):\n    \"\"\"Return line if used, otherwise return None.\"\"\"", "output": "\n    if line.startswith('var ') and not line.startswith('var _'):\n        return line\n    if line.startswith('var ') and previous_line.startswith('var '):\n        return line\n    return None\n\n\ndef filter_unused_function(line, previous_line=''):\n    \"\"\"Return line if used, otherwise return None.\"\"\"\n    if line.startswith('function '):\n        return line\n    if line.startswith('function '):\n        return line\n    return None\n\n\ndef filter_unused_class", "solution": "\n    if re.match(EXCEPT_REGEX, line):\n        return re.sub(r' as \\w+:$', ':', line, count=1)\n    elif multiline_statement(line, previous_line):\n        return line\n    elif line.count('=') == 1:\n        split_line = line.split('=')\n        assert len(split_line) == 2\n        value = split_line[1].lstrip()\n        if ',' in split_line[0]:\n            return line\n\n        if is_literal_or_name(value):\n            # Rather than removing the line, replace with it \"pass\" to avoid\n            # a possible hanging block with no body.\n            value = 'pass' + get_line_ending(line)\n\n        return get_indentation(line) + value\n    else:\n        return line"}
{"prompt": "def initialize(cls):\n        \"\"\"\n        Creates an **\"Example\"** entity of kind **\"NDBConfig\"** in the\n        datastore if the model is empty and raises and error to inform you that\n        you should populate the model with data.\n\n        .. note::\n\n            The *Datastore Viewer* in the ``_ah/admin/`` won't let you add\n            properties to a model if there is not an entity with that\n            property already. Therefore it is a good idea to keep the\n            **\"Example\"** entity (which has all possible properties set) in\n            the datastore.\n\n        \"\"\"", "output": "in the datastore.\n\n        \"\"\"\n        if not cls.query().get():\n            cls(name='Example').put()\n            raise Exception('Please populate the model with data.')\n\n    @classmethod\n    def get_config(cls, name):\n        \"\"\"\n        Returns the configuration for the given **name**.\n\n        :param name: The name of the configuration.\n        :type name: str\n\n        :return: The configuration for the given **name**.\n        :rtype: :class:`~ndb.model.Model", "solution": "\n\n        if not len(cls.query().fetch()):\n\n            example = cls.get_or_insert('Example')\n\n            example.class_ = 'Provider class e.g. ' + \\\n                             '\"authomatic.providers.oauth2.Facebook\".'\n            example.provider_name = 'Your custom provider name e.g. \"fb\".'\n\n            # AuthorizationProvider\n            example.consumer_key = 'Consumer key.'\n            example.consumer_secret = 'Consumer secret'\n            example.provider_id = 1\n\n            # OAuth2\n            example.scope = 'coma, separated, list, of, scopes'\n\n            # AuthenticationProvider\n            example.identifier_param = 'Querystring parameter for claimed ' + \\\n                                       'id. default is \"id\"'\n\n            # Save the example\n            example.put()\n\n            # Raise an information error.\n            raise GAEError(\n                'A NDBConfig data model was created! Go to Datastore Viewer '\n                'in your dashboard and populate it with data!')"}
{"prompt": "def _config_session():\n        \"\"\"\n        Configure session for particular device\n\n        Returns:\n            tensorflow.Session\n        \"\"\"", "output": "\n        config = tf.ConfigProto(\n            device_count={'GPU': 0},\n            allow_soft_placement=True,\n            log_device_placement=False\n        )\n        return tf.Session(config=config)\n\n    def _get_model_path(self):\n        \"\"\"\n        Get path to model\n\n        Returns:\n            str\n        \"\"\"\n        return os.path.join(self.model_dir, self.model_name)\n\n    def _get_model_path_latest(self):\n", "solution": "\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.visible_device_list = '0'\n        return tf.Session(config=config)"}
{"prompt": "def _ensure_executor_running(self):\n        \"\"\"ensures all workers and management thread are running\n        \"\"\"", "output": "\n        if not self._executor_running:\n            self._executor_running = True\n            self._start_executor()\n\n    def _start_executor(self):\n        \"\"\"starts the executor thread\n        \"\"\"\n        self._executor_thread = Thread(target=self._executor_thread_main)\n        self._executor_thread.daemon = True\n        self._executor_thread.start()\n\n    def _executor_thread_main(self):\n        \"\"\"main loop of the executor thread\n        \"\"\"\n       ", "solution": "\n        with self._processes_management_lock:\n            if len(self._processes) != self._max_workers:\n                self._adjust_process_count()\n            self._start_queue_management_thread()"}
{"prompt": "def getPos(self):\n        \"\"\"\n        Returns the absolute position and size of the layer.\n        \n        This method is intended for use in vertex position calculation, as the border and offset have already been applied.\n        \n        The returned value is a 4-tuple of ``(sx,sy,ex,ey)``\\ .\n        The two values starting with an s are the \"start\" position, or the lower-left corner.\n        The second pair of values signify the \"end\" position, or upper-right corner.\n        \"\"\"", "output": "\n        return self.sx,self.sy,self.ex,self.ey\n\n    def getSize(self):\n        \"\"\"\n        Returns the size of the layer.\n        \n        This method is intended for use in vertex position calculation, as the border and offset have already been applied.\n        \n        The returned value is a 2-tuple of ``(width,height)``\\ .\n        \"\"\"\n        return self.ex-self.sx,self.ey-self.sy\n\n    def getBorder(self):\n        \"\"\"\n        Returns the", "solution": "\n        # Returns sx,sy,ex,ey\n        # sx,sy are bottom-left/lowest\n        # ex,ey are top-right/highest\n        sx,sy = self.widget.pos[0]+self.border[0]+self.offset[0],                       self.widget.pos[1]+self.border[1]+self.offset[1]\n        ex,ey = self.widget.pos[0]+self.widget.size[0]-self.border[0]+self.offset[0],   self.widget.pos[1]+self.widget.size[1]-self.border[1]+self.offset[1]\n        return sx,sy,ex,ey"}
{"prompt": "def users(self):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"", "output": "\n        return self.get_users()\n\n    def channels(self):\n        \"\"\"\n        List of channels of this slack team\n        \"\"\"\n        return self.get_channels()\n\n    def groups(self):\n        \"\"\"\n        List of groups of this slack team\n        \"\"\"\n        return self.get_groups()\n\n    def ims(self):\n        \"\"\"\n        List of direct messages of this slack team\n        \"\"\"\n        return self.get_ims()\n\n    def get_users(self):\n        \"\"\"\n        List of", "solution": "\n        if not self._users:\n            self._users = self._call_api('users.list')['members']\n        return self._users"}
{"prompt": "def decode_instruction(instruction):\n        \"\"\"\n        Decode whole instruction and return list of args.\n        Usually, returned arg[0] is the instruction opcode.\n\n        example:\n        >> args = decode_instruction('4.size,4.1024;')\n        >> args == ['size', '1024']\n        >> True\n\n        :param instruction: Instruction string.\n\n        :return: list\n        \"\"\"", "output": "\n        args = []\n        for arg in instruction.split(','):\n            if arg.find('.') != -1:\n                arg = arg.split('.')\n                args.append(arg[0])\n                args.append(arg[1])\n            else:\n                args.append(arg)\n        return args\n\n\ndef decode_instruction_args(instruction):\n        \"\"\"\n        Decode instruction args and return list of args.\n        Usually, returned arg[0] is the instruction opcode.\n\n        example:\n        >> args = decode", "solution": "\n        if not instruction.endswith(INST_TERM):\n            raise InvalidInstruction('Instruction termination not found.')\n\n        # Use proper encoding\n        instruction = utf8(instruction)\n\n        # Get arg size\n        elems = instruction.split(ELEM_SEP, 1)\n\n        try:\n            arg_size = int(elems[0])\n        except Exception:\n            # Expected ValueError\n            raise InvalidInstruction(\n                'Invalid arg length.' +\n                ' Possibly due to missing element separator!')\n\n        arg_str = elems[1][:arg_size]\n\n        remaining = elems[1][arg_size:]\n\n        args = [arg_str]\n\n        if remaining.startswith(ARG_SEP):\n            # Ignore the ARG_SEP to parse next arg.\n            remaining = remaining[1:]\n        elif remaining == INST_TERM:\n            # This was the last arg!\n            return args\n        else:\n            # The remaining is neither starting with ARG_SEP nor INST_TERM.\n            raise InvalidInstruction(\n                'Instruction arg (%s) has invalid length.' % arg_str)\n\n        next_args = GuacamoleInstruction.decode_instruction(remaining)\n\n        if next_args:\n            args = args + next_args\n\n        return args"}
{"prompt": "def handle_combo(self,combo,symbol,modifiers,release=False,mod=True):\n        \"\"\"\n        Handles a key combination and dispatches associated events.\n        \n        First, all keybind handlers registered via :py:meth:`add` will be handled,\n        then the pyglet event :peng3d:pgevent:`on_key_combo` with params ``(combo,symbol,modifiers,release,mod)`` is sent to the :py:class:`Peng()` instance.\n        \n        Also sends the events :peng3d:event:`peng3d:keybind.combo`\\, :peng3d:event:`peng3d:keybind.combo.press` and :peng3d:event`peng3d:keybind.combo.release`\\ .\n        \n        :params str combo: Key combination pressed\n        :params int symbol: Key pressed, passed from the same argument within pyglet\n        :params int modifiers: Modifiers held while the key was pressed\n        :params bool release: If the combo was released\n        :params bool mod: If the combo was sent without mods\n        \"\"\"", "output": ":peng3d:pgevent:`on_key_combo_<combo>` and :peng3d:pgevent:`on_key_combo_<combo>_<symbol>` to the :py:class:`Peng()` instance.\n        \n        :param combo: The key combination.\n        :type combo: str\n        :param symbol: The symbol of the key.\n        :type symbol: str\n        :param modifiers: The modifiers of the key.\n        :type modifiers: int\n        :param release: Whether", "solution": "\n        if self.peng.cfg[\"controls.keybinds.debug\"]:\n            print(\"combo: nm=%s %s\"%(mod,combo))\n        if mod:\n            for kbname in self.keybinds.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        else:\n            for kbname in self.keybinds_nm.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        self.peng.sendPygletEvent(\"on_key_combo\",(combo,symbol,modifiers,release,mod))\n        self.peng.sendEvent(\"peng3d:keybind.combo\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        if release:\n            self.peng.sendEvent(\"peng3d:keybind.combo.release\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        else:\n            self.peng.sendEvent(\"peng3d:keybind.combo.press\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})"}
{"prompt": "def expand_tokens(tokens, equal=False):\n    \"\"\"Given a list of tokens, return a generator of the chunks of\n    text for the data in the tokens.\n    \"\"\"", "output": "\n    for token in tokens:\n        if token.is_text:\n            yield token.text\n        elif token.is_space:\n            yield ' '\n        elif token.is_newline:\n            yield '\\n'\n        elif token.is_tab:\n            yield '\\t'\n        elif token.is_equal and equal:\n            yield '='\n        elif token.is_equal and not equal:\n            yield ' '\n        elif token.is_open_paren:\n            yield '('\n        elif token.is_close_paren", "solution": "\n    for token in tokens:\n        for pre in token.pre_tags:\n            yield pre\n        if not equal or not token.hide_when_equal:\n            if token.trailing_whitespace:\n                yield token.html() + token.trailing_whitespace\n            else:\n                yield token.html()\n        for post in token.post_tags:\n            yield post"}
{"prompt": "def replay_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use replay detection for messages protected by\n        :meth:`get_mic` and :meth:`wrap`. False if replay detection cannot be used.\n        \"\"\"", "output": "\n        return self._replay_detection_negotiated\n\n    @property\n    def replay_detection_supported(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use replay detection for messages protected by\n        :meth:`get_mic` and :meth:`wrap`. False if replay detection cannot be used.\n        \"\"\"\n        return self._replay_detection_supported\n\n    @property\n    def replay_detection", "solution": "\n        return (\n            self.flags & C.GSS_C_REPLAY_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"", "output": "\n        if not layout.table:\n            return\n        if layout.table.caption:\n            self.write(layout.table.caption)\n            self.write('\\n')\n        self.write('|')\n        for col in layout.table.columns:\n            self.write(' %s |' % col)\n        self.write('\\n')\n        self.write('|')\n        for col in layout.table.columns:\n            self.write(' --- |')\n        self.write('\\n')\n        for row in layout.table.rows:\n", "solution": "\n        table_content = self.get_table_content(layout)\n        # get columns width\n        cols_width = [0] * len(table_content[0])\n        for row in table_content:\n            for index, col in enumerate(row):\n                cols_width[index] = max(cols_width[index], len(col))\n        self.default_table(layout, table_content, cols_width)\n        self.writeln()"}
{"prompt": "def variants(context, collaborator, document_id, case_id, json):\n    \"\"\"Export causatives for a collaborator in .vcf format\"\"\"", "output": "\n    # TODO: Add collaborator to the context\n    # TODO: Add collaborator to the document\n    # TODO: Add collaborator to the case\n    # TODO: Add collaborator to the case's collaborators\n    # TODO: Add collaborator to the case's collaborators\n    # TODO: Add collaborator to the case's collaborators\n    # TODO: Add collaborator to the case's collaborators\n    # TODO: Add collaborator to the case's collaborators\n    # TODO: Add collaborator to the case's collaborators\n    # TODO:", "solution": "\n    LOG.info(\"Running scout export variants\")\n    adapter = context.obj['adapter']\n    collaborator = collaborator or 'cust000'\n\n    variants = export_variants(\n        adapter,\n        collaborator,\n        document_id=document_id,\n        case_id=case_id\n    )\n\n    if json:\n        click.echo(dumps([var for var in variants]))\n        return\n\n    vcf_header = VCF_HEADER\n\n    #If case_id is given, print more complete vcf entries, with INFO,\n    #and genotypes\n    if case_id:\n        vcf_header[-1] = vcf_header[-1] + \"\\tFORMAT\"\n        case_obj = adapter.case(case_id=case_id)\n        for individual in case_obj['individuals']:\n            vcf_header[-1] = vcf_header[-1] + \"\\t\" + individual['individual_id']\n\n    #print header\n    for line in vcf_header:\n        click.echo(line)\n\n    for variant_obj in variants:\n        variant_string = get_vcf_entry(variant_obj, case_id=case_id)\n        click.echo(variant_string)"}
{"prompt": "def _plot_formatting(title, est_file, algo_ids, last_bound, N, output_file):\n    \"\"\"Formats the plot with the correct axis labels, title, ticks, and\n    so on.\"\"\"", "output": "\n    plt.title(title)\n    plt.xlabel('Number of samples')\n    plt.ylabel('Estimated value')\n    plt.xticks(np.arange(0, N, 10))\n    plt.yticks(np.arange(0, last_bound, 10))\n    plt.legend(algo_ids)\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef _plot_estimates(est_file, algo_ids, last_bound, N, output_", "solution": "\n    import matplotlib.pyplot as plt\n    if title is None:\n        title = os.path.basename(est_file).split(\".\")[0]\n    plt.title(title)\n    plt.yticks(np.arange(0, 1, 1 / float(N)) + 1 / (float(N) * 2))\n    plt.gcf().subplots_adjust(bottom=0.22)\n    plt.gca().set_yticklabels(algo_ids)\n    plt.xlabel(\"Time (seconds)\")\n    plt.xlim((0, last_bound))\n    plt.tight_layout()\n    if output_file is not None:\n        plt.savefig(output_file)\n    plt.show()"}
{"prompt": "def f_remove(self, recursive=True, predicate=None):\n        \"\"\"Recursively removes the group and all it's children.\n\n        :param recursive:\n\n            If removal should be applied recursively. If not, node can only be removed\n            if it has no children.\n\n        :param predicate:\n\n            In case of recursive removal, you can selectively remove nodes in the tree.\n            Predicate which can evaluate for each node to ``True`` in order to remove the node or\n            ``False`` if the node should be kept. Leave ``None`` if you want to remove all nodes.\n\n        \"\"\"", "output": "None`` to remove all nodes.\n\n        :return:\n\n            ``True`` if the group was removed, ``False`` if the group was not removed.\n        \"\"\"\n        if not self.is_group():\n            return False\n\n        if not recursive:\n            if self.children:\n                return False\n\n        if predicate is None:\n            predicate = lambda node: True\n\n        for child in self.children:\n            if predicate(child):\n                child.remove(recursive=recursive, predicate=predicate)\n\n        self.parent.remove", "solution": "\n        parent = self.f_get_parent()\n        parent.f_remove_child(self.v_name, recursive=recursive, predicate=predicate)"}
{"prompt": "def _wrap(text, wrap_at=120, indent=4):\n    \"\"\"\n    Return piece of text, wrapped around if needed.\n\n    :param text: text that may be too long and then needs to be wrapped.\n    :param wrap_at: the maximum line length.\n    :param indent: number of spaces to prepend to all subsequent lines after the first.\n    \"\"\"", "output": "\n    if len(text) <= wrap_at:\n        return text\n    else:\n        return '\\n'.join(\n            ' ' * indent + line\n            for line in textwrap.wrap(text, wrap_at - indent)\n        )\n\n\ndef _get_docstring(obj):\n    \"\"\"\n    Return docstring for given object.\n\n    :param obj: object to get docstring for.\n    \"\"\"\n    if hasattr(obj, '__doc__'):\n        return obj.__doc__\n    else:\n        return ''\n\n", "solution": "\n    out = \"\"\n    curr_line_length = indent\n    space_needed = False\n    for word in text.split():\n        if curr_line_length + len(word) > wrap_at:\n            out += \"\\n\" + \" \" * indent\n            curr_line_length = indent\n            space_needed = False\n        if space_needed:\n            out += \" \"\n            curr_line_length += 1\n        out += word\n        curr_line_length += len(word)\n        space_needed = True\n    return out"}
{"prompt": "def sanitize_for_archive(url, headers, payload):\n        \"\"\"Sanitize payload of a HTTP request by removing the login, password and token information\n        before storing/retrieving archived items\n\n        :param: url: HTTP url request\n        :param: headers: HTTP headers request\n        :param: payload: HTTP payload request\n\n        :returns url, headers and the sanitized payload\n        \"\"\"", "output": "\n        if url.startswith('https://'):\n            url = url.replace('https://', '')\n        if url.startswith('http://'):\n            url = url.replace('http://', '')\n        if url.startswith('www.'):\n            url = url.replace('www.', '')\n        if url.endswith('/'):\n            url = url[:-1]\n        if ':' in url:\n            url = url.split(':')[0]\n        if '?' in url:\n            url = url.split('?')[0]\n", "solution": "\n        if BugzillaRESTClient.PBUGZILLA_LOGIN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_LOGIN)\n\n        if BugzillaRESTClient.PBUGZILLA_PASSWORD in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_PASSWORD)\n\n        if BugzillaRESTClient.PBUGZILLA_TOKEN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_TOKEN)\n\n        return url, headers, payload"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"", "output": "\n        self.logger.info(\"Starting cycle.\")\n        while True:\n            self.cycle_once()\n\n    def cycle_once(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"\n        self.logger.info(\"Starting cycle once.\")\n        self.logger.info(\"Getting notifications.\")\n        notifications = self.get_notifications()\n        self.logger.info(\"Got notifications.\")\n        self.logger.info(\"Getting data feeds.\")\n        data_feeds = self.get_", "solution": "\n        messages = self.poll_datafeeds()\n        notifications = self.process_notifications(messages)\n\n        self.draw_notifications(notifications)"}
{"prompt": "def user_agent():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"", "output": "\n    return \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\"\n\n\ndef get_page(url):\n    \"\"\"\n    Return the page at the given URL.\n    \"\"\"\n    headers = {'User-Agent': user_agent()}\n    response = requests.get(url, headers=headers)\n    return response.", "solution": "\n    data = {\n        \"installer\": {\"name\": \"pip\", \"version\": pip.__version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == 'CPython':\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'PyPy':\n        if sys.pypy_version_info.releaselevel == 'final':\n            pypy_version_info = sys.pypy_version_info[:3]\n        else:\n            pypy_version_info = sys.pypy_version_info\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == 'Jython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'IronPython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        distro = dict(filter(\n            lambda x: x[1],\n            zip([\"name\", \"version\", \"id\"], platform.linux_distribution()),\n        ))\n        libc = dict(filter(\n            lambda x: x[1],\n            zip([\"lib\", \"version\"], platform.libc_ver()),\n        ))\n        if libc:\n            distro[\"libc\"] = libc\n        if distro:\n            data[\"distro\"] = distro\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"OS X\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )"}
{"prompt": "def headers_present(self, headers):\n        \"\"\"\n        Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .headers_present(['content-type', 'Authorization']))\n        \"\"\"", "output": "               .reply(200)\n        \"\"\"\n        self.matchers.append(\n            lambda req: all(\n                [req.headers.get(header) for header in headers]\n            )\n        )\n        return self\n\n    def headers_absent(self, headers):\n        \"\"\"\n        Defines a list of headers that must not be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers", "solution": "\n        headers = {name: re.compile('(.*)') for name in headers}\n        self.add_matcher(matcher('HeadersMatcher', headers))"}
{"prompt": "def resolve_path(self, path, root_id='me/skydrive', objects=False, listdir_limit=500):\n\t\t\"\"\"Return id (or metadata) of an object, specified by chain\n\t\t\t\t(iterable or fs-style path string) of \"name\" attributes\n\t\t\t\tof its ancestors, or raises DoesNotExists error.\n\n\t\t\tRequires many calls to resolve each name in path, so use with care.\n\t\t\t\troot_id parameter allows to specify path relative to some folder_id\n\t\t\t\t(default: me/skydrive).\"\"\"", "output": "\n\t\t\t\t(default is root of SkyDrive).\n\n\t\t\tIf objects=True, returns metadata of each object in path,\n\t\t\t\totherwise returns id of the last object in path.\n\n\t\t\tIf listdir_limit is set, it will be used as limit parameter\n\t\t\t\tfor listdir calls.\n\t\t\"\"\"\n\t\tif isinstance(path, basestring):\n\t\t\tpath = path.split('/')\n\t\tif not path:\n\t\t\traise ValueError('path is empty')\n\t", "solution": "\n\t\tif path:\n\t\t\tif isinstance(path, types.StringTypes):\n\t\t\t\tif not path.startswith('me/skydrive'):\n\t\t\t\t\t# Split path by both kinds of slashes\n\t\t\t\t\tpath = filter(None, it.chain.from_iterable(p.split('\\\\') for p in path.split('/')))\n\t\t\t\telse: root_id, path = path, None\n\t\t\tif path:\n\t\t\t\ttry:\n\t\t\t\t\tfor i, name in enumerate(path):\n\t\t\t\t\t\toffset = None\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tobj_list = self.listdir(root_id, offset=offset, limit=listdir_limit)\n\t\t\t\t\t\t\ttry: root_id = dict(it.imap(op.itemgetter('name', 'id'), obj_list))[name]\n\t\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\t\tif len(obj_list) < listdir_limit: raise # assuming that it's the last page\n\t\t\t\t\t\t\t\toffset = (offset or 0) + listdir_limit\n\t\t\t\t\t\t\telse: break\n\t\t\t\texcept (KeyError, ProtocolError) as err:\n\t\t\t\t\tif isinstance(err, ProtocolError) and err.code != 404: raise\n\t\t\t\t\traise DoesNotExists(root_id, path[i:])\n\t\treturn root_id if not objects else self.info(root_id)"}
{"prompt": "def require(self, key):\n        \"\"\"\n        Raises an exception if value for ``key`` is empty.\n        \"\"\"", "output": "\n        value = self.get(key)\n        if value is None:\n            raise ValueError(\"Missing required key: %s\" % key)\n        return value\n\n    def __getitem__(self, key):\n        \"\"\"\n        Returns the value for the given key.\n        \"\"\"\n        value = self._data.get(key)\n        if value is None:\n            raise KeyError(key)\n        return value\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Sets the value for the given key.\n        \"\"\"", "solution": "\n        value = self.get(key)\n        if not value:\n            raise ValueError('\"{}\" is empty.'.format(key))\n        return value"}
{"prompt": "def Rconverter(Robj, dataframe=False):\n    \"\"\"\n    Convert an object in R's namespace to one suitable\n    for ipython's namespace.\n\n    For a data.frame, it tries to return a structured array.\n    It first checks for colnames, then names.\n    If all are NULL, it returns np.asarray(Robj), else\n    it tries to construct a recarray\n\n    Parameters\n    ----------\n\n    Robj: an R object returned from rpy2\n    \"\"\"", "output": "\n    import numpy as np\n    if isinstance(Robj, np.ndarray):\n        return Robj\n    elif isinstance(Robj, list):\n        return Robj\n    elif isinstance(Robj, np.matrix):\n        return Robj\n    elif isinstance(Robj, np.recarray):\n        return Robj\n    elif isinstance(Robj, np.rec.recarray):\n        return Robj\n    elif isinstance(Robj, np.rec.array):\n        return Robj\n    elif isinstance(Robj,", "solution": "\n    is_data_frame = ro.r('is.data.frame')\n    colnames = ro.r('colnames')\n    rownames = ro.r('rownames') # with pandas, these could be used for the index\n    names = ro.r('names')\n\n    if dataframe:\n        as_data_frame = ro.r('as.data.frame')\n        cols = colnames(Robj)\n        _names = names(Robj)\n        if cols != ri.NULL:\n            Robj = as_data_frame(Robj)\n            names = tuple(np.array(cols))\n        elif _names != ri.NULL:\n            names = tuple(np.array(_names))\n        else: # failed to find names\n            return np.asarray(Robj)\n        Robj = np.rec.fromarrays(Robj, names = names)\n    return np.asarray(Robj)"}
{"prompt": "def pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"", "output": "\n    if pid < 0:\n        return False\n    try:\n        os.kill(pid, 0)\n    except OSError:\n        return False\n    else:\n        return True\n\n\ndef kill_process(pid):\n    \"\"\"Kill a process by pid.\"\"\"\n    if pid_exists(pid):\n        os.kill(pid, signal.SIGTERM)\n\n\ndef get_pid_by_port(port):\n    \"\"\"Get pid by port.\"\"\"\n    pid = 0\n    for p in psutil.process", "solution": "\n    if pid < 0:\n        return False\n    try:\n        os.kill(pid, 0)\n    except OSError as exc:\n        logging.debug(\"No process[%s]: %s\", exc.errno, exc)\n        return exc.errno == errno.EPERM\n    else:\n        p = psutil.Process(pid)\n        return p.status != psutil.STATUS_ZOMBIE"}
{"prompt": "def start_hb(self, callback):\n        \"\"\"Start the heartbeating and call the callback if the kernel dies.\"\"\"", "output": "\n        if self.hb_thread is not None:\n            return\n        self.hb_thread = threading.Thread(target=self._heartbeat, args=(callback,))\n        self.hb_thread.daemon = True\n        self.hb_thread.start()\n\n    def stop_hb(self):\n        \"\"\"Stop the heartbeating.\"\"\"\n        if self.hb_thread is None:\n            return\n        self.hb_thread.join()\n        self.hb_thread = None\n\n    def", "solution": "\n        if not self._beating:\n            self._kernel_alive = True\n\n            def ping_or_dead():\n                self.hb_stream.flush()\n                if self._kernel_alive:\n                    self._kernel_alive = False\n                    self.hb_stream.send(b'ping')\n                    # flush stream to force immediate socket send\n                    self.hb_stream.flush()\n                else:\n                    try:\n                        callback()\n                    except:\n                        pass\n                    finally:\n                        self.stop_hb()\n\n            def beat_received(msg):\n                self._kernel_alive = True\n\n            self.hb_stream.on_recv(beat_received)\n            loop = ioloop.IOLoop.instance()\n            self._hb_periodic_callback = ioloop.PeriodicCallback(ping_or_dead, self.time_to_dead*1000, loop)\n            loop.add_timeout(time.time()+self.first_beat, self._really_start_hb)\n            self._beating= True"}
{"prompt": "def _chunk_noise(noise):\n    \"\"\"Chunk input noise data into valid Touchstone file rows.\"\"\"", "output": "\n    # TODO: This is a hack.\n    # The Touchstone format requires the data to be chunked into 16-bit\n    # words, but the data is stored as 32-bit floats.\n    # This function converts the data to 16-bit words.\n    # The Touchstone format also requires the data to be in the range\n    # [-1, 1].\n    # This function converts the data to [-1, 1].\n    # The Touchstone format also requires the data to be in the order\n    # [real, imag", "solution": "\n    data = zip(\n        noise[\"freq\"],\n        noise[\"nf\"],\n        np.abs(noise[\"rc\"]),\n        np.angle(noise[\"rc\"]),\n        noise[\"res\"],\n    )\n    for freq, nf, rcmag, rcangle, res in data:\n        yield freq, nf, rcmag, rcangle, res"}
{"prompt": "def is_lesser(a, b):\n    \"\"\"\n    Verify that an item *a* is <= then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"", "output": "\n    return a <= b\n\n\ndef is_greater(a, b):\n    \"\"\"\n    Verify that an item *a* is >= then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"\n    return a >= b\n\n\ndef is_equal(a, b):\n    \"\"\"\n    Verify that an item *a* is == then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n   ", "solution": "\n\n    if type(a) != type(b):\n        return False\n\n    if isinstance(a, str) and isinstance(b, str):\n        return a == b\n    elif isinstance(a, bool) and isinstance(b, bool):\n        return a == b\n    elif isinstance(a, list) and isinstance(b, list):\n        for element in a:\n            flag = 0\n            for e in b:\n                if is_lesser(element, e):\n                    flag = 1\n                    break\n            if not flag:\n                return False\n        return True\n    elif isinstance(a, dict) and isinstance(b, dict):\n        if is_lesser(list(a.keys()), list(b.keys())):\n            for key, val in a.items():\n                if not is_lesser(val, b[key]):\n                    return False\n            return True\n        return False\n    elif isinstance(a, int) and isinstance(b, int):\n        return a <= b\n    elif isinstance(a, float) and isinstance(b, float):\n        return a <= b\n\n    return False"}
{"prompt": "def isreshapeable(new, old):\n    \"\"\"\n    Check to see if a proposed tuple of axes is a valid reshaping of\n    the old axes by ensuring that they can be factored.\n\n    Parameters\n    ----------\n    new : tuple\n        tuple of proposed axes\n\n    old : tuple\n        tuple of old axes\n    \"\"\"", "output": "\n    if len(new) != len(old):\n        return False\n    if len(new) == 0:\n        return True\n    if len(new) == 1:\n        return new[0] == old[0]\n    if len(new) == 2:\n        return new[0] == old[0] and new[1] == old[1]\n    if len(new) == 3:\n        return new[0] == old[0] and new[1] == old[1] and new[2] == old[", "solution": "\n\n    new, old = tupleize(new), tupleize(old)\n\n    if not prod(new) == prod(old):\n        raise ValueError(\"Total size of new keys must remain unchanged\")"}
{"prompt": "def _simple_ast_generator(gen_ast):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"", "output": "\n    def wrapper(*args, **kwargs):\n        return GeneratedPyAST(gen_ast(*args, **kwargs))\n    return wrapper\n\n\ndef _simple_ast_visitor(visit_ast):\n    \"\"\"Wrap simpler AST visitors to return a GeneratedPyAST.\"\"\"\n    def wrapper(*args, **kwargs):\n        return GeneratedPyAST(visit_ast(*args, **kwargs))\n    return wrapper\n\n\ndef _simple_ast_transformer(transform_ast):\n    \"\"\"Wrap simpler AST transformers to return a Gener", "solution": "\n\n    @wraps(gen_ast)\n    def wrapped_ast_generator(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:\n        return GeneratedPyAST(node=gen_ast(ctx, form))\n\n    return wrapped_ast_generator"}
{"prompt": "def next_opcode(code, offset):\n    \"\"\"Return the next opcode and offset as a tuple. Tuple (-100,\n    -1000) is returned when reaching the end.\"\"\"", "output": "\n    if offset >= len(code):\n        return (-100, -1000)\n    opcode = code[offset]\n    if opcode == 0:\n        return (opcode, offset + 1)\n    if opcode == 1:\n        return (opcode, offset + 2)\n    if opcode == 2:\n        return (opcode, offset + 3)\n    if opcode == 3:\n        return (opcode, offset + 1)\n    if opcode == 4:\n        return", "solution": "\n    n = len(code)\n    while offset < n:\n        op = code[offset]\n        offset += 1\n        if op >= HAVE_ARGUMENT:\n            offset += 2\n            pass\n        yield op, offset\n        pass\n    yield -100, -1000\n    pass"}
{"prompt": "def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,\n                        name=None):\n  \"\"\"Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):\n    output = tf.reduce_sum(x)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"", "output": ":  Scalar integer `Tensor`.  Rank must be <= rank(x).\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).  Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has rank equal to `rank`", "solution": "\n  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):\n    return tf.compat.v1.assert_less_equal(\n        tf.rank(x), rank, data=data, summarize=summarize, message=message)"}
{"prompt": "def insert_instance_template(self, body, request_id=None, project_id=None):\n        \"\"\"\n        Inserts instance template using body specified\n        Must be called with keyword arguments rather than positional.\n\n        :param body: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again)\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"", "output": "call is retried).\n        :type request_id: str\n        :param project_id: Optional, project ID.\n        :type project_id: str\n        :return: Instance template resource\n        :rtype: dict\n        \"\"\"\n        return self.insert_resource(\n            body=body,\n            request_id=request_id,\n            project_id=project_id,\n            resource_path=self.RESOURCE_PATH,\n            resource_name='instanceTemplate',\n            method_name='insert',\n        )\n\n    def", "solution": "\n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"}
{"prompt": "def _process_op_err(self, e):\n        \"\"\"\n        Process errors which occured while reading or parsing\n        the protocol. If allow_reconnect is enabled it will\n        try to switch the server to which it is currently connected\n        otherwise it will disconnect.\n        \"\"\"", "output": "\n        if self.allow_reconnect and e.args[0] == 'Connection reset by peer':\n            self.logger.warning('Connection reset by peer, trying to reconnect')\n            self.reconnect()\n        else:\n            self.logger.error('Disconnecting from server due to error: %s' % e)\n            self.disconnect()\n\n    def _process_op_data(self, data):\n        \"\"\"\n        Process data which was received from the server.\n        \"\"\"\n        if self.protocol_version == 0:\n           ", "solution": "\n        if self.is_connecting or self.is_closed or self.is_reconnecting:\n            return\n\n        if self.options[\"allow_reconnect\"] and self.is_connected:\n            self._status = Client.RECONNECTING\n            self._ps.reset()\n\n            if self._reconnection_task is not None and not self._reconnection_task.cancelled():\n                # Cancel the previous task in case it may still be running.\n                self._reconnection_task.cancel()\n\n            self._reconnection_task = self._loop.create_task(self._attempt_reconnect())\n        else:\n            self._process_disconnect()\n            self._err = e\n            yield from self._close(Client.CLOSED, True)"}
{"prompt": "def get_generator(self, path, *args, **kw_args):\n        \"\"\"\n        Get a generator that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the generator one row at a time.\n        Unlike the direct access queue, this generator also returns the remainder elements.\n        Additional arguments are forwarded to get_queue.\n        See the get_queue method for documentation of these parameters.\n\n        :param path:\n        :return: A generator that iterates over the rows in the dataset.\n        \"\"\"", "output": "\n        return self.get_queue(path, *args, **kw_args)\n\n    def get_queue(self, path, *args, **kw_args):\n        \"\"\"\n        Get a queue that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the queue one row at a time.\n        Unlike the direct access queue, this queue also returns the remainder elements.\n        Additional arguments are forwarded to get_queue.\n        See the get_queue method for documentation of these parameters.\n\n        :param path:\n       ", "solution": "\n        q = self.get_queue(path=path, *args, **kw_args)\n\n        try:\n            # This generator just implements a standard access pattern for the direct access queue.\n            for guard in q.iter():\n                with guard as batch:\n                    batch_copy = batch.copy()\n\n                for row in batch_copy:\n                    yield row\n\n            last_batch = self.get_remainder(path, q.block_size)\n            for row in last_batch:\n                yield row\n\n        finally:\n            q.close()"}
{"prompt": "def running_under_virtualenv():\n    \"\"\"\n    Return True if we're running inside a virtualenv, False otherwise.\n\n    \"\"\"", "output": "\n    return hasattr(sys, 'real_prefix')\n\n\ndef get_virtualenv_path():\n    \"\"\"\n    Return the path to the virtualenv we're running inside.\n\n    \"\"\"\n    return sys.prefix\n\n\ndef get_virtualenv_site_packages_path():\n    \"\"\"\n    Return the path to the virtualenv's site-packages directory.\n\n    \"\"\"\n    return os.path.join(get_virtualenv_path(), 'lib', 'python%s' % sys.version[:3], 'site-packages')\n\n", "solution": "\n    if hasattr(sys, 'real_prefix'):\n        return True\n    elif sys.prefix != getattr(sys, \"base_prefix\", sys.prefix):\n        return True\n\n    return False"}
{"prompt": "def build(self, pre=None, shortest=False):\n        \"\"\"Build the String instance\n\n        :param list pre: The prerequisites list (optional, default=None)\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"", "output": "\n        if pre is None:\n            pre = []\n        if not isinstance(pre, list):\n            raise TypeError('pre must be a list')\n        if not isinstance(shortest, bool):\n            raise TypeError('shortest must be a boolean')\n        if not self.is_built:\n            self.is_built = True\n            self.pre = pre\n            self.shortest = shortest\n            self.build_string()\n\n    def build_string(self):\n        \"\"\"Build the String instance\n\n        :raises NotImplemented", "solution": "\n        if pre is None:\n            pre = []\n\n        if self.value is not None and rand.maybe():\n            return utils.val(self.value, pre, shortest=shortest)\n\n        length = super(String, self).build(pre, shortest=shortest)\n        res = rand.data(length, self.charset)\n        return res"}
{"prompt": "def chimera_block_quotient(G, blocks):\n    \"\"\"\n    Extract the blocks from a graph, and returns a\n    block-quotient graph according to the acceptability\n    functions block_good and eblock_good\n\n    Inputs:\n        G: a networkx graph\n        blocks: a tuple of tuples\n\n    \"\"\"", "output": "\n    # Create a new graph\n    H = nx.Graph()\n\n    # Add the nodes\n    for block in blocks:\n        H.add_nodes_from(block)\n\n    # Add the edges\n    for block in blocks:\n        for node in block:\n            for neighbor in G.neighbors(node):\n                if neighbor in block:\n                    H.add_edge(node, neighbor)\n\n    return H\n\n\ndef chimera_block_quotient_with_labels(G, blocks):\n    \"\"\"\n    Ext", "solution": "\n    from networkx import Graph\n    from itertools import product\n\n    BG = Graph()\n    blockid = {}\n    for i, b in enumerate(blocks):\n        BG.add_node(i)\n        if not b or not all(G.has_node(x) for x in b):\n            continue\n        for q in b:\n            if q in blockid:\n                raise(RuntimeError, \"two blocks overlap\")\n            blockid[q] = i\n\n    for q, u in blockid.items():\n        ublock = blocks[u]\n        for p in G[q]:\n            if p not in blockid:\n                continue\n            v = blockid[p]\n            if BG.has_edge(u, v) or u == v:\n                continue\n            vblock = blocks[v]\n\n            if ublock[0][2] == vblock[0][2]:\n                block_edges = zip(ublock, vblock)\n            else:\n                block_edges = product(ublock, vblock)\n\n            if all(G.has_edge(x, y) for x, y in block_edges):\n                BG.add_edge(u, v)\n\n    return BG"}
{"prompt": "def process_commands(self):\n        \"\"\"Handle debugger commands.\"\"\"", "output": "\n        while True:\n            try:\n                self.prompt()\n                cmd = self.get_command()\n                if cmd is None:\n                    break\n                self.process_command(cmd)\n            except KeyboardInterrupt:\n                self.print_(\"KeyboardInterrupt\")\n                self.print_(\"\")\n                self.prompt()\n\n    def prompt(self):\n        \"\"\"Print the debugger prompt.\"\"\"\n        self.print_(\"(Pdb)\")\n\n    def get_command(self):\n        \"\"\"Get a command from the", "solution": "\n        if self.core.execution_status != 'No program':\n            self.setup()\n            self.location()\n            pass\n        leave_loop = run_hooks(self, self.preloop_hooks)\n        self.continue_running = False\n\n        while not leave_loop:\n            try:\n                run_hooks(self, self.precmd_hooks)\n                # bdb had a True return to leave loop.\n                # A more straight-forward way is to set\n                # instance variable self.continue_running.\n                leave_loop = self.process_command()\n                if leave_loop or self.continue_running: break\n            except EOFError:\n                # If we have stacked interfaces, pop to the next\n                # one.  If this is the last one however, we'll\n                # just stick with that.  FIXME: Possibly we should\n                # check to see if we are interactive.  and not\n                # leave if that's the case. Is this the right\n                # thing?  investigate and fix.\n                if len(self.debugger.intf) > 1:\n                    del self.debugger.intf[-1]\n                    self.last_command = ''\n                else:\n                    if self.debugger.intf[-1].output:\n                        self.debugger.intf[-1].output.writeline('Leaving')\n                        raise Mexcept.DebuggerQuit\n                        pass\n                    break\n                pass\n            pass\n        return run_hooks(self, self.postcmd_hooks)"}
{"prompt": "def table(name, auth=None, eager=True):\n    \"\"\"Returns a given table for the given user.\"\"\"", "output": "\n    if auth is None:\n        auth = request.auth\n    if auth.is_admin:\n        return Table.query.get(name)\n    else:\n        return Table.query.filter_by(name=name, user_id=auth.id).first()\n\n\ndef table_exists(name, auth=None):\n    \"\"\"Returns True if a given table exists for the given user.\"\"\"\n    if auth is None:\n        auth = request.auth\n    if auth.is_admin:\n        return Table.query.get(name)", "solution": "\n    auth = auth or []\n    dynamodb = boto.connect_dynamodb(*auth)\n\n    table = dynamodb.get_table(name)\n    return Table(table=table, eager=eager)"}
{"prompt": "def _is_raising(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node raise an exception\"\"\"", "output": "\n    if body[0] == \"raise\":\n        return True\n    elif body[0] == \"try\":\n        return _is_raising(body[1])\n    elif body[0] == \"with\":\n        return _is_raising(body[2])\n    elif body[0] == \"if\":\n        return _is_raising(body[2]) or _is_raising(body[3])\n    elif body[0] == \"for\":\n        return _is_raising(body[3])\n    elif body[0", "solution": "\n    for node in body:\n        if isinstance(node, astroid.Raise):\n            return True\n    return False"}
{"prompt": "def get_assembly_size(assembly_file):\n    \"\"\"Returns the number of nucleotides and the size per contig for the\n    provided assembly file path\n\n    Parameters\n    ----------\n    assembly_file : str\n        Path to assembly file.\n\n    Returns\n    -------\n    assembly_size : int\n        Size of the assembly in nucleotides\n    contig_size : dict\n        Length of each contig (contig name as key and length as value)\n\n    \"\"\"", "output": "\n    assembly_size = 0\n    contig_size = {}\n    with open(assembly_file, 'r') as f:\n        for line in f:\n            if line.startswith('>'):\n                contig_name = line.strip().split(' ')[0][1:]\n            else:\n                contig_size[contig_name] = len(line.strip())\n                assembly_size += len(line.strip())\n    return assembly_size, contig_size\n\n\ndef get_assembly_size_from_fasta(", "solution": "\n\n    assembly_size = 0\n    contig_size = {}\n    header = \"\"\n\n    with open(assembly_file) as fh:\n        for line in fh:\n\n            # Skip empty lines\n            if line.strip() == \"\":\n                continue\n\n            if line.startswith(\">\"):\n                header = line.strip()[1:]\n                contig_size[header] = 0\n\n            else:\n                line_len = len(line.strip())\n                assembly_size += line_len\n                contig_size[header] += line_len\n\n    return assembly_size, contig_size"}
{"prompt": "def compute_index_key(self, to_instance):\n        \"\"\"\n        Compute the index key that can be used to identify an instance\n        on the link.\n        \"\"\"", "output": "\n        return self.compute_index_key_from_instance(to_instance)\n\n    def compute_index_key_from_instance(self, to_instance):\n        \"\"\"\n        Compute the index key that can be used to identify an instance\n        on the link.\n        \"\"\"\n        return to_instance.get_index_key()\n\n    def compute_index_key_from_index_key(self, to_index_key):\n        \"\"\"\n        Compute the index key that can be used to identify an instance\n        on the link.\n", "solution": "\n        kwargs = dict()\n        for attr in self.key_map.values():\n            if _is_null(to_instance, attr):\n                return None\n            \n            if attr in to_instance.__dict__:\n                kwargs[attr] = to_instance.__dict__[attr]\n            else:\n                kwargs[attr] = getattr(to_instance, attr)\n\n        return frozenset(tuple(kwargs.items()))"}
{"prompt": "def p_boolean_expression(self, p):\n        \"\"\"\n        expression : expression LE          expression\n                   | expression LESSTHAN    expression\n                   | expression DOUBLEEQUAL expression\n                   | expression NOTEQUAL    expression\n                   | expression GE          expression\n                   | expression GT          expression\n                   | expression AND         expression\n                   | expression OR          expression\n        \"\"\"", "output": "\n        p[0] = BooleanExpression(p[1], p[2], p[3])\n\n    def p_boolean_expression_not(self, p):\n        \"\"\"\n        expression : NOT expression\n        \"\"\"\n        p[0] = BooleanExpression(p[1], p[2])\n\n    def p_boolean_expression_paren(self, p):\n        \"\"\"\n        expression : LPAREN expression RPAREN\n        \"\"\"\n        p[0] = p[2]\n\n    def p_boolean_expression_literal(self,", "solution": "\n        p[0] = BinaryOperationNode(left=p[1],\n                                   operator=p[2],\n                                   right=p[3])"}
{"prompt": "def pairwise_reproducibility(df, plot=False):\n    \"\"\"\n    Calculate the reproducibility of LA-ICPMS based on unique pairs of repeat analyses.\n    \n    Pairwise differences are fit with a half-Cauchy distribution, and the median and \n    95% confidence limits are returned for each analyte.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataset\n    \n    plot : bool\n        Whether or not to plot the resulting error distributions.\n    \n    Returns\n    -------\n    pdiffs : pandas.DataFrame\n        Unique pairwise differences for all analytes.\n    rep_dists : dict of scipy.stats.halfcauchy\n        Half-Cauchy distribution objects fitted to the\n        differences.\n    rep_stats : dict of tuples\n        The 50% and 95% quantiles of the half-cauchy\n        distribution.\n    (fig, axs) : matplotlib objects\n        The figure. If not made, returnes (None, None) placeholder\n    \n    \"\"\"", "output": "Returns\n    -------\n    pandas.DataFrame\n        A dataframe with the median and 95% confidence limits for each analyte.\n    \"\"\"\n    # Get the analytes\n    analytes = df.columns.values[1:]\n    \n    # Get the unique pairs of repeat analyses\n    repeats = df.index.values\n    pairs = []\n    for i in range(len(repeats)):\n        for j in range(i+1, len(repeats)):\n            pairs.append((repeats[i], repeats[j]))", "solution": "\n    \n    ans = df.columns.values\n    pdifs = []\n    \n    # calculate differences between unique pairs\n    for ind, d in df.groupby(level=0):\n        d.index = d.index.droplevel(0)\n\n        difs = []\n        for i, r in d.iterrows():\n            t = d.loc[i+1:, :]\n            difs.append(t[ans] - r[ans])\n\n        pdifs.append(pd.concat(difs))\n    pdifs = pd.concat(pdifs).abs()\n\n    # calculate stats\n    rep_stats = {}\n    rep_dists = {}\n    errfn = stats.halfcauchy\n    \n    for a in ans:\n        d = pdifs.loc[:, a].dropna().values\n        hdist = errfn.fit(d, floc=0)\n        rep_dists[a] = errfn(*hdist)\n        rep_stats[a] = rep_dists[a].ppf((0.5, 0.95))\n    \n    # make plot\n    if not plot:\n        return pdifs, rep_dists, rep_stats, (None, None)\n    \n    fig, axs = plt.subplots(1, len(ans), figsize=[len(ans) * 2, 2])\n    for a, ax in zip(ans, axs):\n        d = pdifs.loc[:, a].dropna().values\n        hist, edges, _ = ax.hist(d, 30)\n        ax.plot(edges, rep_dists[a].pdf(edges) * (sum(hist) * np.mean(np.diff(edges))))\n        ax.set_title(a, loc='left')\n\n    return pdifs, rep_dists, rep_stats, (fig, axs)"}
{"prompt": "def remove_redundant_nodes(self, preserve_lengths=True):\n        \"\"\"\n        Remove all nodes which have only a single child, and attach their\n        grandchildren to their parent.  The resulting tree has the minimum\n        number of internal nodes required for the number of leaves.\n        :param preserve_lengths: If true, branch lengths of removed nodes are \\\n        added to those of their children.\n        \"\"\"", "output": "\n        # TODO: This is a very inefficient implementation.  It should be\n        # replaced with something more efficient.\n        # TODO: This is not a very good implementation.  It does not preserve\n        # branch lengths.\n        # TODO: This is not a very good implementation.  It does not preserve\n        # the order of leaves.\n        # TODO: This is not a very good implementation.  It does not preserve\n        # the order of internal nodes.\n        # TODO: This is not a very good implementation.  It does not preserve\n        # the order of leaves", "solution": "\n        for n in self.walk(mode='postorder'):\n            while n.ancestor and len(n.ancestor.descendants) == 1:\n                grandfather = n.ancestor.ancestor\n                father = n.ancestor\n                if preserve_lengths:\n                    n.length += father.length\n\n                if grandfather:\n                    for i, child in enumerate(grandfather.descendants):\n                        if child is father:\n                            del grandfather.descendants[i]\n                    grandfather.add_descendant(n)\n                    father.ancestor = None\n                else:\n                    self.descendants = n.descendants\n                    if preserve_lengths:\n                        self.length = n.length"}
{"prompt": "def _set_start(self):\n        \"\"\" Sets the start timestamp and formatted time to the current time. \"\"\"", "output": "\n        self.start = time.time()\n        self.start_formatted = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(self.start))\n\n    def _set_end(self):\n        \"\"\" Sets the end timestamp and formatted time to the current time. \"\"\"\n        self.end = time.time()\n        self.end_formatted = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(self.end))\n\n    def _set_duration", "solution": "\n        init_time = time.time()\n        formatted_time = datetime.datetime.fromtimestamp(init_time).strftime('%Y_%m_%d_%Hh%Mm%Ss')\n        run_info_dict = self._run_information[self.v_crun]\n        run_info_dict['timestamp'] = init_time\n        run_info_dict['time'] = formatted_time\n        if self._environment_hexsha is not None:\n            run_info_dict['short_environment_hexsha'] = self._environment_hexsha[0:7]"}
{"prompt": "def return_type(type_name, formatter=None):\n    \"\"\"Specify that this function returns a typed value.\n\n    Args:\n        type_name (str): A type name known to the global typedargs type system\n        formatter (str): An optional name of a formatting function specified\n            for the type given in type_name.\n    \"\"\"", "output": "\n    def decorator(func):\n        func.__return_type__ = type_name\n        func.__return_formatter__ = formatter\n        return func\n    return decorator\n\n\ndef typed_args(func):\n    \"\"\"Decorator for functions that take typed arguments.\n\n    This decorator is used to specify the types of arguments that a function\n    takes.  The decorated function must be defined in a module that has been\n    imported by the typedargs module.  The decorated function must take a\n    single argument, which is a dictionary mapping argument names", "solution": "\n\n    def _returns(func):\n        annotated(func)\n        func.metadata.typed_returnvalue(type_name, formatter)\n        return func\n\n    return _returns"}
{"prompt": "def load_metamodel(resource=None, load_globals=True):\n    \"\"\"\n    Load and return a metamodel expressed in ooaofooa from a *resource*.\n    The resource may be either a filename, a path, or a list of filenames\n    and/or paths.\n    \"\"\"", "output": "\n    from pymod import load_model\n    from pymod.model import Model\n\n    if isinstance(resource, list):\n        model = Model()\n        for r in resource:\n            model.load(r)\n    else:\n        model = load_model(resource)\n\n    if load_globals:\n        model.load_globals()\n\n    return model\n\n\ndef load_metamodel_from_string(string, load_globals=True):\n    \"\"\"\n    Load and return a metamodel expressed in ooa", "solution": "\n    loader = _mk_loader(resource, load_globals)\n    return loader.build_metamodel()"}
{"prompt": "def set_s3_credentials(secret_key_id, secret_access_key):\n    \"\"\"Creates a new Amazon S3 client internally with specified credentials.\n    There are no validations done to the credentials. Incorrect credentials are thus revealed with first S3 import call.\n    \n    secretKeyId Amazon S3 Secret Key ID (provided by Amazon)\n    secretAccessKey Amazon S3 Secret Access Key (provided by Amazon)\n    \"\"\"", "output": "\n    global s3_client\n    s3_client = boto3.client('s3', aws_access_key_id=secret_key_id, aws_secret_access_key=secret_access_key)\n\n\ndef get_s3_client():\n    \"\"\"Returns the Amazon S3 client.\n    \"\"\"\n    return s3_client\n\n\ndef get_s3_bucket_names():\n    \"\"\"Returns a list of all buckets in the account.\n    \"\"\"\n    return s3_client.list_buckets()['", "solution": "\n    if(secret_key_id is None):\n        raise H2OValueError(\"Secret key ID must be specified\")\n\n    if(secret_access_key is None):\n        raise H2OValueError(\"Secret access key must be specified\")\n    \n    if(not secret_key_id):\n        raise H2OValueError(\"Secret key ID must not be empty\")\n    \n    if(not secret_access_key):\n        raise H2OValueError(\"Secret access key must not be empty\")\n    \n    \n    params = {\"secret_key_id\": secret_key_id,\n              \"secret_access_key\": secret_access_key\n              }\n    \n    h2o.api(endpoint=\"POST /3/PersistS3\", data=params)\n    print(\"Credentials successfully set.\")"}
{"prompt": "def tar_and_s3_upload(self, path, key, bucket):\n        \"\"\"\n        Tar the local file or directory and upload to s3\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str\n        :param bucket: s3 bucket\n        :type bucket: str\n        :return: None\n        \"\"\"", "output": "\n        if not os.path.exists(path):\n            raise ValueError('Path does not exist: {}'.format(path))\n\n        # tar the file or directory\n        tar_file = self.tar_file(path)\n\n        # upload the tar file to s3\n        self.s3_upload(tar_file, key, bucket)\n\n    def tar_file(self, path):\n        \"\"\"\n        Tar the local file or directory\n\n        :param path: local file or directory\n        :type path: str\n        :return: tar file", "solution": "\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)"}
{"prompt": "def _make_default_operation_costs(self, allow_spaces=False):\n        \"\"\"\n        sets 1.0 cost for every replacement, insertion, deletion and transposition\n        \"\"\"", "output": "\n        if allow_spaces:\n            self.insertion_cost = 1.0\n            self.deletion_cost = 1.0\n            self.substitution_cost = 1.0\n            self.transposition_cost = 1.0\n        else:\n            self.insertion_cost = 1.0\n            self.deletion_cost = 1.0\n            self.substitution_cost = 1.0\n            self.transposition_cost = 1.0\n\n    def _make_default_operation_", "solution": "\n        self.operation_costs = dict()\n        self.operation_costs[\"\"] = {c: 1.0 for c in list(self.alphabet) + [' ']}\n        for a in self.alphabet:\n            current_costs = {c: 1.0 for c in self.alphabet}\n            current_costs[a] = 0.0\n            current_costs[\"\"] = 1.0\n            if allow_spaces:\n                current_costs[\" \"] = 1.0\n            self.operation_costs[a] = current_costs\n        # \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0437\u0438\u0446\u0438\u0438\n        for a, b in itertools.permutations(self.alphabet, 2):\n            self.operation_costs[a + b] = {b + a: 1.0}\n        # \u043f\u0440\u043e\u0431\u0435\u043b\u044b\n        if allow_spaces:\n            self.operation_costs[\" \"] = {c: 1.0 for c in self.alphabet}\n            self.operation_costs[\" \"][\"\"] = 1.0"}
{"prompt": "def _dir_exists(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of dir_exists.\n\n    Expects a db-style path name.\n    \"\"\"", "output": "\n    return db.exists(user_id, db_dirname)\n\n\ndef _dir_create(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of dir_create.\n\n    Expects a db-style path name.\n    \"\"\"\n    return db.create(user_id, db_dirname)\n\n\ndef _dir_delete(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of dir_delete.\n\n    Expects a db-style path name.\n    \"\"\"\n   ", "solution": "\n    return db.execute(\n        select(\n            [func.count(directories.c.name)],\n        ).where(\n            and_(\n                directories.c.user_id == user_id,\n                directories.c.name == db_dirname,\n            ),\n        )\n    ).scalar() != 0"}
{"prompt": "def create(self, name, description=None, color=None):\n        \"\"\"\n        Creates a new label and returns the response\n\n        :param name: The label name\n        :type name: str\n\n        :param description: An optional description for the label. The name is\n            used if no description is provided.\n        :type description: str\n\n        :param color: The hex color for the label (ex: 'ff0000' for red). If no\n            color is provided, a random one will be assigned.\n        :type color: str\n\n        :returns: The response of your post\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n\n        :returns: The label object\n        :rtype: dict\n        \"\"\"\n        data = {\n            'name': name,\n        }\n\n        if description:\n            data['description'] = description\n\n        if color:\n            data['color'] = color\n\n        return self._post(self.ENDPOINT, data=data)\n\n    def update(self, label_id, name=None, description=None, color=None):\n        \"\"\"\n        Updates an existing label and returns the response\n\n        :param label_id:", "solution": "\n        data = {\n            'name': name,\n            'title': name,\n            'description': description or name,\n            'appearance': {\n                'color': color or random_color()\n            }\n        }\n        # Yes, it's confusing. the `/tags/` endpoint is used for labels\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.TAGS.value,\n            params=data\n        )"}
{"prompt": "def create(self, teamId, personId=None, personEmail=None,\n               isModerator=False, **request_parameters):\n        \"\"\"Add someone to a team by Person ID or email address.\n\n        Add someone to a team by Person ID or email address; optionally making\n        them a moderator.\n\n        Args:\n            teamId(basestring): The team ID.\n            personId(basestring): The person ID.\n            personEmail(basestring): The email address of the person.\n            isModerator(bool): Set to True to make the person a team moderator.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            TeamMembership: A TeamMembership object with the details of the\n            created team membership.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "bool): Whether the person should be a moderator.\n\n        Returns:\n            TeamMember: The created team member.\n\n        Raises:\n            :class:`pydro.core.exceptions.ApiError`: If the API returns an error.\n        \"\"\"\n        if personId is None and personEmail is None:\n            raise ValueError(\"Must provide either personId or personEmail\")\n\n        if personId is not None and personEmail is not None:\n            raise ValueError(\"Must provide either personId or personEmail, not both\")\n\n        if person", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(personId, basestring)\n        check_type(personEmail, basestring)\n        check_type(isModerator, bool)\n\n        post_data = dict_from_items_with_values(\n            request_parameters,\n            teamId=teamId,\n            personId=personId,\n            personEmail=personEmail,\n            isModerator=isModerator,\n        )\n\n        # API request\n        json_data = self._session.post(API_ENDPOINT, json=post_data)\n\n        # Return a team membership object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def visit(self, visit_uri):\n        \"\"\"\n        Navigate to the given URL. The URL can either be a relative URL or an absolute URL. The\n        behavior of either depends on the driver. ::\n\n            session.visit(\"/foo\")\n            session.visit(\"http://google.com\")\n\n        For drivers which can run against an external application, such as the Selenium driver,\n        giving an absolute URL will navigate to that page. This allows testing applications running\n        on remote servers. For these drivers, setting :data:`capybara.app_host` will make the\n        remote server the default. For example::\n\n            capybara.app_host = \"http://google.com\"\n            session.visit(\"/\")  # visits the Google homepage\n\n        Args:\n            visit_uri (str): The URL to navigate to.\n        \"\"\"", "output": "host` will allow\n        relative URLs to be used.\n\n        For drivers which are designed to run against a single application, such as the RackTest\n        driver, giving an absolute URL will raise an error. This is because the application under\n        test is known and should be given explicitly.\n\n        :param visit_uri: the URL to visit\n        :type visit_uri: str\n        \"\"\"\n        raise NotImplementedError()\n\n    def current_url(self):\n        \"\"\"\n        Returns the current URL.\n\n        :return: the current URL\n       ", "solution": "\n\n        self.raise_server_error()\n\n        visit_uri = urlparse(visit_uri)\n\n        if capybara.app_host:\n            uri_base = urlparse(capybara.app_host)\n        elif self.server:\n            uri_base = urlparse(\"http://{}:{}\".format(self.server.host, self.server.port))\n        else:\n            uri_base = None\n\n        visit_uri = ParseResult(\n            scheme=visit_uri.scheme or (uri_base.scheme if uri_base else \"\"),\n            netloc=visit_uri.netloc or (uri_base.netloc if uri_base else \"\"),\n            path=visit_uri.path,\n            params=visit_uri.params,\n            query=visit_uri.query,\n            fragment=visit_uri.fragment)\n\n        self.driver.visit(visit_uri.geturl())"}
{"prompt": "def from_join(cls, join: Join) -> 'ConditionalJoin':\n        \"\"\"Creates a new :see:ConditionalJoin from the\n        specified :see:Join object.\n\n        Arguments:\n            join:\n                The :see:Join object to create the\n                :see:ConditionalJoin object from.\n\n        Returns:\n            A :see:ConditionalJoin object created from\n            the :see:Join object.\n        \"\"\"", "output": "\n        return cls(\n            join.left,\n            join.right,\n            join.condition,\n            join.join_type,\n            join.join_type_name,\n            join.join_type_description,\n            join.join_type_abbreviation,\n            join.join_type_symbol,\n            join.join_type_symbol_abbreviation,\n            join.join_type_symbol_description,\n            join.join_type_symbol_abbreviation_description,\n            join.join_type_symbol_abbre", "solution": "\n\n        return cls(\n            join.table_name,\n            join.parent_alias,\n            join.table_alias,\n            join.join_type,\n            join.join_field,\n            join.nullable\n        )"}
{"prompt": "def process_tokens(self, tokens):\n        \"\"\"process tokens from the current module to search for module/block\n        level options\n        \"\"\"", "output": "\n        for token in tokens:\n            if token.type == 'NAME' and token.value == 'options':\n                self.process_options(tokens)\n            elif token.type == 'NAME' and token.value == 'block':\n                self.process_block(tokens)\n            elif token.type == 'NAME' and token.value == 'module':\n                self.process_module(tokens)\n            elif token.type == 'NAME' and token.value == 'include':\n                self.process_include(tokens)\n            elif token.", "solution": "\n        control_pragmas = {\"disable\", \"enable\"}\n        for (tok_type, content, start, _, _) in tokens:\n            if tok_type != tokenize.COMMENT:\n                continue\n            match = OPTION_RGX.search(content)\n            if match is None:\n                continue\n\n            first_group = match.group(1)\n            if (\n                first_group.strip() == \"disable-all\"\n                or first_group.strip() == \"skip-file\"\n            ):\n                if first_group.strip() == \"disable-all\":\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(\"disable-all\", \"skip-file\"),\n                    )\n                self.add_message(\"file-ignored\", line=start[0])\n                self._ignore_file = True\n                return\n            try:\n                opt, value = first_group.split(\"=\", 1)\n            except ValueError:\n                self.add_message(\n                    \"bad-inline-option\", args=first_group.strip(), line=start[0]\n                )\n                continue\n            opt = opt.strip()\n            if opt in self._options_methods or opt in self._bw_options_methods:\n                try:\n                    meth = self._options_methods[opt]\n                except KeyError:\n                    meth = self._bw_options_methods[opt]\n                    # found a \"(dis|en)able-msg\" pragma deprecated suppression\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(opt, opt.replace(\"-msg\", \"\")),\n                    )\n                for msgid in utils._splitstrip(value):\n                    # Add the line where a control pragma was encountered.\n                    if opt in control_pragmas:\n                        self._pragma_lineno[msgid] = start[0]\n\n                    try:\n                        if (opt, msgid) == (\"disable\", \"all\"):\n                            self.add_message(\n                                \"deprecated-pragma\",\n                                line=start[0],\n                                args=(\"disable=all\", \"skip-file\"),\n                            )\n                            self.add_message(\"file-ignored\", line=start[0])\n                            self._ignore_file = True\n                            return\n                        meth(msgid, \"module\", start[0])\n                    except exceptions.UnknownMessageError:\n                        self.add_message(\"bad-option-value\", args=msgid, line=start[0])\n            else:\n                self.add_message(\"unrecognized-inline-option\", args=opt, line=start[0])"}
{"prompt": "def _flags_changed(self, name, old, new):\n        \"\"\"ensure flags dict is valid\"\"\"", "output": "\n        if new is None:\n            new = {}\n        elif not isinstance(new, dict):\n            raise TypeError(\"flags must be a dict\")\n        if old is None:\n            old = {}\n        elif not isinstance(old, dict):\n            raise TypeError(\"flags must be a dict\")\n        if set(old.keys()) != set(new.keys()):\n            raise ValueError(\"flags must have the same keys\")\n        for k, v in new.items():\n            if not isinstance(v, bool):\n                raise TypeError(\"flags must be", "solution": "\n        for key,value in new.iteritems():\n            assert len(value) == 2, \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[0], (dict, Config)), \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[1], basestring), \"Bad flag: %r:%s\"%(key,value)"}
{"prompt": "def images(self, query=None):\n    \"\"\"List local images in the database, optionally with a query.\n\n       Paramters\n       =========\n       query: a string to search for in the container or collection name|tag|uri\n\n    \"\"\"", "output": "\n    if query is None:\n        query = ''\n    query = query.lower()\n    return [i for i in self.images if query in i.lower()]\n\ndef containers(self, query=None):\n    \"\"\"List local containers in the database, optionally with a query.\n\n       Paramters\n       =========\n       query: a string to search for in the container or collection name|tag|uri\n\n    \"\"\"\n    if query is None:\n        query = ''\n    query = query.lower()\n    return [i for i in self", "solution": "\n    from sregistry.database.models import Collection, Container\n\n    rows = []\n    if query is not None:   \n        like = \"%\" + query + \"%\"\n        containers = Container.query.filter(or_(Container.name == query,\n                                                Container.tag.like(like),\n                                                Container.uri.like(like),\n                                                Container.name.like(like))).all() \n    else:\n        containers = Container.query.all()\n\n    if len(containers) > 0:\n        message = \"  [date]   [client]\\t[uri]\"\n        bot.custom(prefix='Containers:', message=message, color=\"RED\")\n        for c in containers:\n            uri = c.get_uri()\n            created_at = c.created_at.strftime('%B %d, %Y')\n            rows.append([created_at, \"   [%s]\" %c.client, uri])\n        bot.table(rows) \n    return containers"}
{"prompt": "def load_tiff_multipage(tiff_filename, dtype='float32'):\n    \"\"\"\n    Load a multipage tiff into a single variable in x,y,z format.\n\n    Arguments:\n        tiff_filename:     Filename of source data\n        dtype:             data type to use for the returned tensor\n\n    Returns:\n        Array containing contents from input tiff file in xyz order\n    \"\"\"", "output": "\n    import tifffile\n    import numpy as np\n\n    with tifffile.TiffFile(tiff_filename) as tif:\n        data = np.zeros((tif.series[0].shape[0], tif.series[0].shape[1], len(tif.series)), dtype=dtype)\n        for i, page in enumerate(tif.series):\n            data[:, :, i] = page.asarray()\n\n    return data\n\n\ndef load_tiff_stack(tiff_filename,", "solution": "\n    if not os.path.isfile(tiff_filename):\n        raise RuntimeError('could not find file \"%s\"' % tiff_filename)\n\n    # load the data from multi-layer TIF files\n    data = tiff.imread(tiff_filename)\n\n    im = []\n\n    while True:\n\n        Xi = numpy.array(data, dtype=dtype)\n        if Xi.ndim == 2:\n            Xi = Xi[numpy.newaxis, ...]  # add slice dimension\n        im.append(Xi)\n\n        try:\n            data.seek(data.tell()+1)\n        except EOFError:\n            break  # this just means hit end of file (not really an error)\n\n    im = numpy.concatenate(im, axis=0)  # list of 2d -> tensor\n    im = numpy.rollaxis(im, 1)\n    im = numpy.rollaxis(im, 2)\n\n    return im"}
{"prompt": "def with_setup(setup=None, teardown=None):\n    \"\"\"Decorator to add setup and/or teardown methods to a test function::\n\n      @with_setup(setup, teardown)\n      def test_something():\n          \" ... \"\n\n    Note that `with_setup` is useful *only* for test functions, not for test\n    methods or inside of TestCase subclasses.\n    \"\"\"", "output": "\n    def decorate(func):\n        if setup is not None:\n            func.setUp = setup\n        if teardown is not None:\n            func.tearDown = teardown\n        return func\n    return decorate\n\n\ndef with_teardown(teardown):\n    \"\"\"Decorator to add a teardown method to a test function::\n\n      @with_teardown(teardown)\n      def test_something():\n          \" ... \"\n\n    Note that `with_teardown` is useful *only", "solution": "\n    def decorate(func, setup=setup, teardown=teardown):\n        if setup:\n            if hasattr(func, 'setup'):\n                _old_s = func.setup\n                def _s():\n                    setup()\n                    _old_s()\n                func.setup = _s\n            else:\n                func.setup = setup\n        if teardown:\n            if hasattr(func, 'teardown'):\n                _old_t = func.teardown\n                def _t():\n                    _old_t()\n                    teardown()\n                func.teardown = _t\n            else:\n                func.teardown = teardown\n        return func\n    return decorate"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idx : np.array(N)\n            Estimated indeces for the segment boundaries in frames.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n        # Initialize\n        est_idx = np.zeros(self.n_frames, dtype=np.int)\n        est_labels = np.zeros(self.n_frames - 1, dtype=np.int)\n\n        # Process\n        for i in range(self.n_frames):\n            if self.flat[i] == 0:\n                est_idx[i] = 1\n            else:\n                est_idx[i] = 0\n\n        # Post-process\n        est_idx = est_idx.ast", "solution": "\n        # Preprocess to obtain features, times, and input boundary indeces\n        F = self._preprocess()\n\n        # Normalize\n        F = U.normalize(F, norm_type=self.config[\"label_norm_feats\"],\n                        floor=self.config[\"label_norm_floor\"],\n                        min_db=self.config[\"label_norm_min_db\"])\n\n        # Find the labels using 2D-FMCs\n        est_labels = compute_similarity(F, self.in_bound_idxs,\n                                        dirichlet=self.config[\"dirichlet\"],\n                                        xmeans=self.config[\"xmeans\"],\n                                        k=self.config[\"k\"],\n                                        offset=self.config[\"2dfmc_offset\"])\n\n        # Post process estimations\n        self.in_bound_idxs, est_labels = self._postprocess(self.in_bound_idxs,\n                                                           est_labels)\n\n        return self.in_bound_idxs, est_labels"}
{"prompt": "def find_handfile(names=None):\n    \"\"\"\n    \u5c1d\u8bd5\u5b9a\u4f4d ``handfile`` \u6587\u4ef6\uff0c\u660e\u786e\u6307\u5b9a\u6216\u9010\u7ea7\u641c\u7d22\u7236\u8def\u5f84\n\n    :param str names: \u53ef\u9009\uff0c\u5f85\u67e5\u627e\u7684\u6587\u4ef6\u540d\uff0c\u4e3b\u8981\u7528\u4e8e\u8c03\u8bd5\uff0c\u9ed8\u8ba4\u4f7f\u7528\u7ec8\u7aef\u4f20\u5165\u7684\u914d\u7f6e\n    :return: ``handfile`` \u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a None\n    :rtype: str\n    \"\"\"", "output": ":rtype: str\n    \"\"\"\n    if names is None:\n        names = sys.argv[1:]\n    if len(names) == 0:\n        return None\n    if len(names) == 1:\n        return names[0]\n    for name in names:\n        if os.path.exists(name):\n            return name\n    return find_handfile(names[:-1])\n\n\ndef find_handfile_by_name(name):\n    \"\"\"\n    \u5c1d\u8bd5\u5b9a\u4f4d ``handfile`` \u6587\u4ef6\uff0c\u6839", "solution": "\n    # \u5982\u679c\u6ca1\u6709\u660e\u786e\u6307\u5b9a\uff0c\u5219\u5305\u542b env \u4e2d\u7684\u503c\n    names = names or [env.handfile]\n\n    # \u82e5\u65e0 ``.py`` \u6269\u5c55\u540d\uff0c\u5219\u4f5c\u4e3a\u5f85\u67e5\u8be2\u540d\u79f0\uff0c\u8ffd\u52a0\u5230 names \u672b\u5c3e\n    if not names[0].endswith('.py'):\n        names += [names[0] + '.py']\n\n    # name \u4e2d\u662f\u5426\u5305\u542b\u8def\u5f84\u5143\u7d20\n    if os.path.dirname(names[0]):\n        # \u82e5\u5b58\u5728\uff0c\u5219\u6269\u5c55 Home \u8def\u5f84\u6807\u5fd7\uff0c\u5e76\u6d4b\u8bd5\u662f\u5426\u5b58\u5728\n        for name in names:\n            expanded = os.path.expanduser(name)\n            if os.path.exists(expanded):\n                if name.endswith('.py') or _is_package(expanded):\n                    return os.path.abspath(expanded)\n    else:\n        # \u5426\u5219\uff0c\u9010\u7ea7\u5411\u4e0a\u641c\u7d22\uff0c\u76f4\u5230\u6839\u8def\u5f84\n        path = '.'\n\n        # \u5728\u5230\u7cfb\u7edf\u6839\u8def\u5f84\u4e4b\u524d\u505c\u6b62\n        while os.path.split(os.path.abspath(path))[1]:\n            for name in names:\n                joined = os.path.join(path, name)\n                if os.path.exists(joined):\n                    if name.endswith('.py') or _is_package(joined):\n                        return os.path.abspath(joined)\n            path = os.path.join('..', path)\n\n    return None"}
{"prompt": "def remote_upload(self, remote_url, folder_id=None, headers=None):\n        \"\"\"Used to make a remote file upload to openload.co\n\n        Note:\n            If folder_id is not provided, the file will be uploaded to ``Home`` folder.\n\n        Args:\n            remote_url (str): direct link of file to be remotely downloaded.\n            folder_id (:obj:`str`, optional): folder-ID to upload to.\n            headers (:obj:`dict`, optional): additional HTTP headers (e.g. Cookies or HTTP Basic-Auth)\n\n        Returns:\n            dict: dictionary containing (\"id\": uploaded file id, \"folderid\"). ::\n\n                {\n                    \"id\": \"12\",\n                    \"folderid\": \"4248\"\n                }\n\n        \"\"\"", "output": ").\n\n        Returns:\n            :obj:`dict`: response from openload.co.\n\n        Raises:\n            :obj:`ValueError`: if ``remote_url`` is not a valid URL.\n            :obj:`ValueError`: if ``folder_id`` is not a valid folder-ID.\n            :obj:`Exception`: if the response from openload.co is not 200.\n\n        \"\"\"\n        if not is_url(remote_url):\n            raise ValueError(\"remote_url must be a valid URL.\")\n\n        if folder_", "solution": "\n\n        kwargs = {'folder': folder_id, 'headers': headers}\n        params = {'url': remote_url}\n        params.update({key: value for key, value in kwargs.items() if value})\n\n        return self._get('remotedl/add', params=params)"}
{"prompt": "def list(self, **request_parameters):\n        \"\"\"List all roles.\n\n        Args:\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            GeneratorContainer: A GeneratorContainer which, when iterated,\n            yields the roles returned by the Webex Teams query.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        return self.list_roles(**request_parameters)\n\n    def list_roles(self, **request_parameters):\n        \"\"\"List all roles.\n\n        Args:\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            GeneratorContainer: A GeneratorContainer which, when iterated,\n            yields the roles returned by the Webex Teams query.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError:", "solution": "\n        # API request - get items\n        items = self._session.get_items(\n            API_ENDPOINT,\n            params=request_parameters\n        )\n\n        # Yield role objects created from the returned JSON objects\n        for item in items:\n            yield self._object_factory(OBJECT_TYPE, item)"}
{"prompt": "def delete_all_checkpoints(self, path):\n        \"\"\"Delete all checkpoints for the given path.\"\"\"", "output": "\n        for f in os.listdir(path):\n            if f.startswith(\"checkpoint\"):\n                os.remove(os.path.join(path, f))\n\n    def _get_checkpoint_path(self, path):\n        \"\"\"Get the path to the latest checkpoint.\"\"\"\n        checkpoints = [f for f in os.listdir(path) if f.startswith(\"checkpoint\")]\n        if len(checkpoints) == 0:\n            return None\n        return os.path.join(path, max(checkpoints, key", "solution": "\n        with self.engine.begin() as db:\n            delete_remote_checkpoints(db, self.user_id, path)"}
{"prompt": "def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"", "output": "\n        self._run.addHdlProc(trigger, proc)\n\n    def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"\n        self._run.addHdlProc(trigger, proc)\n\n    def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:\n        \"\"\"\n        Add hdl", "solution": "\n        # first process in time has to plan executing of apply values on the\n        # end of this time\n        if not self._applyValPlaned:\n            # (apply on end of this time to minimalize process reevaluation)\n            self._scheduleApplyValues()\n\n        if isEvDependentOn(trigger, proc):\n            if self.now == 0:\n                return  # pass event dependent on startup\n            self._seqProcsToRun.append(proc)\n        else:\n            self._combProcsToRun.append(proc)"}
{"prompt": "def convert(self, amount, currency, new_currency='EUR', date=None):\n        \"\"\"Convert amount from a currency to another one.\n\n        :param float amount: The amount of `currency` to convert.\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :param datetime.date date: Use the conversion rate of this date. If this\n            is not given, the most recent rate is used.\n\n        :return: The value of `amount` in `new_currency`.\n        :rtype: float\n\n        >>> from datetime import date\n        >>> c = CurrencyConverter()\n        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n        72.67...\n        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n        Traceback (most recent call last):\n        RateNotFoundError: BGN has no rate for 2010-11-21\n        \"\"\"", "output": "\n        :rtype: float\n        \"\"\"\n        if currency == new_currency:\n            return amount\n\n        if date is None:\n            date = datetime.date.today()\n\n        rate = self.get_rate(currency, new_currency, date)\n        return amount * rate\n\n    def get_rate(self, currency, new_currency, date):\n        \"\"\"Get the conversion rate for a currency pair.\n\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :", "solution": "\n        for c in currency, new_currency:\n            if c not in self.currencies:\n                raise ValueError('{0} is not a supported currency'.format(c))\n\n        if date is None:\n            date = self.bounds[currency].last_date\n        else:\n            try:\n                date = date.date()  # fallback if input was a datetime object\n            except AttributeError:\n                pass\n\n        r0 = self._get_rate(currency, date)\n        r1 = self._get_rate(new_currency, date)\n\n        return float(amount) / r0 * r1"}
{"prompt": "def user_config_file(self):\n        \"\"\"Get the absolute path to the user config file.\"\"\"", "output": "\n        return os.path.join(self.user_dir, 'config.json')\n\n    @property\n    def user_data_dir(self):\n        \"\"\"Get the absolute path to the user data directory.\"\"\"\n        return os.path.join(self.user_dir, 'data')\n\n    @property\n    def user_dir(self):\n        \"\"\"Get the absolute path to the user directory.\"\"\"\n        return os.path.join(self.home_dir, '.config', 'sos')\n\n    @property\n    def user_log", "solution": "\n        return os.path.join(\n            get_user_config_dir(self.app_name, self.app_author),\n            self.filename)"}
{"prompt": "def build_url_field(self, field_name, model_class):\n        \"\"\"\n        This is needed due to DRF's model serializer uses the queryset to build url name\n\n        # TODO: Move this to own serializer mixin or fix problem elsewhere?\n        \"\"\"", "output": "\n        field = self.fields[field_name]\n        if isinstance(field, serializers.HyperlinkedRelatedField):\n            field.view_name = model_class._meta.model_name + '-detail'\n\n    def build_url_fields(self, model_class):\n        \"\"\"\n        This is needed due to DRF's model serializer uses the queryset to build url name\n\n        # TODO: Move this to own serializer mixin or fix problem elsewhere?\n        \"\"\"\n        for field_name in self.fields:\n           ", "solution": "\n        field, kwargs = super().build_url_field(field_name, model_class)\n\n        view = self.root.context[\"view\"]\n        kwargs[\"view_name\"] = view.get_url_name(\"detail\")\n\n        return field, kwargs"}
{"prompt": "def list_templates(self, extensions=None, filter_func=None):\n        \"\"\"Returns a list of templates for this environment.  This requires\n        that the loader supports the loader's\n        :meth:`~BaseLoader.list_templates` method.\n\n        If there are other files in the template folder besides the\n        actual templates, the returned list can be filtered.  There are two\n        ways: either `extensions` is set to a list of file extensions for\n        templates, or a `filter_func` can be provided which is a callable that\n        is passed a template name and should return `True` if it should end up\n        in the result list.\n\n        If the loader does not support that, a :exc:`TypeError` is raised.\n\n        .. versionadded:: 2.4\n        \"\"\"", "output": "       takes a string and returns a boolean.  If the string is a template\n        name, the callable should return True if the name should be included\n        in the list.\n\n        :param extensions: a list of file extensions for templates\n        :param filter_func: a callable that takes a string and returns a\n                            boolean\n        :return: a list of template names\n        \"\"\"\n        return self.loader.list_templates(extensions, filter_func)\n\n    def get_template(self, template):\n        \"\"\"Returns a template object for the given template", "solution": "\n        x = self.loader.list_templates()\n        if extensions is not None:\n            if filter_func is not None:\n                raise TypeError('either extensions or filter_func '\n                                'can be passed, but not both')\n            filter_func = lambda x: '.' in x and \\\n                                    x.rsplit('.', 1)[1] in extensions\n        if filter_func is not None:\n            x = ifilter(filter_func, x)\n        return x"}
{"prompt": "def find_debugged_frame(frame):\n    \"\"\"Find the first frame that is a debugged frame. We do this\n    Generally we want traceback information without polluting it with\n    debugger frames. We can tell these because those are frames on the\n    top which don't have f_trace set. So we'll look back from the top\n    to find the fist frame where f_trace is set.\n    \"\"\"", "output": "\n    while frame.f_trace is None:\n        frame = frame.f_back\n    return frame\n\n\ndef get_traceback_info(frame):\n    \"\"\"Get the traceback info for the given frame.\n    \"\"\"\n    frame = find_debugged_frame(frame)\n    filename = frame.f_code.co_filename\n    lineno = frame.f_lineno\n    return filename, lineno\n\n\ndef get_traceback_info_for_exception(exc_info):\n    \"\"\"Get the traceback info for the given", "solution": "\n    f_prev = f = frame\n    while f is not None and f.f_trace is None:\n        f_prev = f\n        f = f.f_back\n        pass\n    if f_prev:\n        val = f_prev.f_locals.get('tracer_func_frame')\n        if val == f_prev:\n            if f_prev.f_back:\n                f_prev = f_prev.f_back\n                pass\n            pass\n        pass\n    else:\n        return frame\n    return f_prev"}
{"prompt": "def parent_frame_arguments():\n  \"\"\"Returns parent frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's function\n  arguments. These are positional arguments and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function argument names are overloaded before invoking\n  this method, then values will reflect the overloaded value. For this reason,\n  we recommend calling `parent_frame_arguments` at the beginning of the\n  function.\n  \"\"\"", "output": "parent_frame_arguments` at the end of the function\n  body.\n\n  Returns:\n    A dictionary with caller function arguments.\n  \"\"\"\n  frame = inspect.currentframe().f_back\n  args, _, _, values = inspect.getargvalues(frame)\n  return dict(zip(args, values))\n\n\ndef get_caller_frame_arguments():\n  \"\"\"Returns caller frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's caller\n  function arguments. These are positional arguments and keyword arguments\n  (**", "solution": "\n  # All arguments and the names used for *varargs, and **kwargs\n  arg_names, variable_arg_name, keyword_arg_name, local_vars = (\n      tf_inspect._inspect.getargvalues(  # pylint: disable=protected-access\n          # Get the first frame of the caller of this method.\n          tf_inspect._inspect.stack()[1][0]))  # pylint: disable=protected-access\n\n  # Remove the *varargs, and flatten the **kwargs. Both are\n  # nested lists.\n  local_vars.pop(variable_arg_name, {})\n  keyword_args = local_vars.pop(keyword_arg_name, {})\n\n  final_args = {}\n  # Copy over arguments and their values. In general, local_vars\n  # may contain more than just the arguments, since this method\n  # can be called anywhere in a function.\n  for arg_name in arg_names:\n    final_args[arg_name] = local_vars.pop(arg_name)\n  final_args.update(keyword_args)\n\n  return final_args"}
{"prompt": "def _process_bit_id(self, node):\n        \"\"\"Process an Id or IndexedId node as a bit or register type.\n\n        Return a list of tuples (Register,index).\n        \"\"\"", "output": "\n        if isinstance(node, Id):\n            return [(self._process_id(node), None)]\n        elif isinstance(node, IndexedId):\n            return [(self._process_id(node.id), node.index)]\n        else:\n            raise ValueError(\"Unexpected node type: %s\" % type(node))\n\n    def _process_id(self, node):\n        \"\"\"Process an Id node as a bit or register type.\n\n        Return a Register object.\n        \"\"\"\n        if isinstance(node, Id):\n           ", "solution": "\n        # pylint: disable=inconsistent-return-statements\n        reg = None\n        if node.name in self.dag.qregs:\n            reg = self.dag.qregs[node.name]\n        elif node.name in self.dag.cregs:\n            reg = self.dag.cregs[node.name]\n        else:\n            raise QiskitError(\"expected qreg or creg name:\",\n                              \"line=%s\" % node.line,\n                              \"file=%s\" % node.file)\n\n        if node.type == \"indexed_id\":\n            # An indexed bit or qubit\n            return [(reg, node.index)]\n        elif node.type == \"id\":\n            # A qubit or qreg or creg\n            if not self.bit_stack[-1]:\n                # Global scope\n                return [(reg, j) for j in range(reg.size)]\n            else:\n                # local scope\n                if node.name in self.bit_stack[-1]:\n                    return [self.bit_stack[-1][node.name]]\n                raise QiskitError(\"expected local bit name:\",\n                                  \"line=%s\" % node.line,\n                                  \"file=%s\" % node.file)\n        return None"}
{"prompt": "def _backup(self):\n        \"\"\"\n        Save the current database into the inactive-db.json file.\n        \"\"\"", "output": "\n        self.logger.info(\"Backing up database...\")\n        self.db.backup()\n\n    def _restore(self):\n        \"\"\"\n        Restore the database from the inactive-db.json file.\n        \"\"\"\n        self.logger.info(\"Restoring database...\")\n        self.db.restore()\n\n    def _create_db(self):\n        \"\"\"\n        Create a new database.\n        \"\"\"\n        self.logger.info(\"Creating database...\")\n        self.db.create()\n\n    def _delete", "solution": "\n\n        if PyFunceble.CONFIGURATION[\"inactive_database\"]:\n            # The database subsystem is activated.\n\n            # We save the current database state into the database file.\n            Dict(PyFunceble.INTERN[\"inactive_db\"]).to_json(self.inactive_db_path)"}
{"prompt": "def get_pandas_df(self, hql, schema='default'):\n        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype: DataFrame\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> len(df.index)\n        100\n\n        :return: pandas.DateFrame\n        \"\"\"", "output": "100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> df.head()\n        \"\"\"\n        df = self.get_pandas_df_from_query(hql, schema)\n        return df\n\n    def get_pandas_df_from_query(self, hql, schema='default'):\n        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to '", "solution": "\n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df"}
{"prompt": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"", "output": "\n        raise NotImplementedError\n\n    def get_item_content(self, identifier):\n        \"\"\"Return item content.\n\n        :param identifier: item identifier\n        :returns: item content\n        \"\"\"\n        raise NotImplementedError\n\n    def get_item_metadata(self, identifier):\n        \"\"\"Return item metadata.\n\n        :param identifier: item identifier\n        :returns: item metadata\n        \"\"\"\n        raise NotImplementedError\n\n    def get_item_properties(self, identifier):\n        \"\"\"Return item properties.\n\n       ", "solution": "\n        admin_metadata = self.get_admin_metadata()\n        uuid = admin_metadata[\"uuid\"]\n        # Create directory for the specific dataset.\n        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)\n        mkdir_parents(dataset_cache_abspath)\n\n        bucket_fpath = self.data_key_prefix + identifier\n        obj = self.s3resource.Object(self.bucket, bucket_fpath)\n        relpath = obj.get()['Metadata']['handle']\n        _, ext = os.path.splitext(relpath)\n\n        local_item_abspath = os.path.join(\n            dataset_cache_abspath,\n            identifier + ext\n        )\n        if not os.path.isfile(local_item_abspath):\n\n            tmp_local_item_abspath = local_item_abspath + \".tmp\"\n            self.s3resource.Bucket(self.bucket).download_file(\n                bucket_fpath,\n                tmp_local_item_abspath\n            )\n            os.rename(tmp_local_item_abspath, local_item_abspath)\n\n        return local_item_abspath"}
{"prompt": "def record(self, func):\n        \"\"\"Registers a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.\n        \"\"\"", "output": "\n\n        self.record_once(func)\n        return func\n\n    def record_once(self, func):\n        \"\"\"Registers a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.  The function will only be called once.\n        \"\"\"\n\n        if func not in self.record_once_funcs:\n            self.record_once_funcs.append(func)\n\n    def before_", "solution": "\n        if self._got_registered_once and self.warn_on_modifications:\n            from warnings import warn\n            warn(Warning('The blueprint was already registered once '\n                         'but is getting modified now.  These changes '\n                         'will not show up.'))\n        self.deferred_functions.append(func)"}
{"prompt": "def find_sourcemap_comment(filepath, block_size=100):\n    \"\"\"\n    Seeks and removes the sourcemap comment. If found, the sourcemap line is\n    returned.\n\n    Bundled output files can have massive amounts of lines, and the sourceMap\n    comment is always at the end. So, to extract it efficiently, we read out the\n    lines of the file starting from the end. We look back at most 2 lines.\n\n    :param:filepath: path to output bundle file containing the sourcemap comment\n    :param:blocksize: integer saying how many bytes to read at once\n    :return:string with the sourcemap comment or None\n    \"\"\"", "output": "cemap comment\n    :param:block_size: number of lines to read at a time\n    :return: sourcemap line if found, None otherwise\n    \"\"\"\n    with open(filepath, 'r') as f:\n        lines = []\n        for line in f:\n            lines.append(line)\n        if len(lines) == 0:\n            return None\n        for i in range(len(lines) - 1, -1, -1):\n            if lines[i].startswith('//# sourceMappingURL='):\n                return", "solution": "\n\n    MAX_TRACKBACK = 2  # look back at most 2 lines, catching potential blank line at the end\n\n    block_number = -1\n    # blocks of size block_size, in reverse order starting from the end of the file\n    blocks = []\n    sourcemap = None\n\n    try:\n        # open file in binary read+write mode, so we can seek with negative offsets\n        of = io.open(filepath, 'br+')\n        # figure out what's the end byte\n        of.seek(0, os.SEEK_END)\n        block_end_byte = of.tell()\n\n        # track back for maximum MAX_TRACKBACK lines and while we can track back\n        while block_end_byte > 0 and MAX_TRACKBACK > 0:\n            if (block_end_byte - block_size > 0):\n                # read the last block we haven't yet read\n                of.seek(block_number*block_size, os.SEEK_END)\n                blocks.append(of.read(block_size))\n            else:\n                # file too small, start from begining\n                of.seek(0, os.SEEK_SET)\n                # only read what was not read\n                blocks = [of.read(block_end_byte)]\n\n            # update variables that control while loop\n            content = b''.join(reversed(blocks))\n            lines_found = content.count(b'\\n')\n            MAX_TRACKBACK -= lines_found\n            block_end_byte -= block_size\n            block_number -= 1\n\n            # early check and bail out if we found the sourcemap comment\n            if SOURCEMAPPING_URL_COMMENT in content:\n                offset = 0\n                # splitlines eats the last \\n if its followed by a blank line\n                lines = content.split(b'\\n')\n                for i, line in enumerate(lines):\n                    if line.startswith(SOURCEMAPPING_URL_COMMENT):\n                        offset = len(line)\n                        sourcemap = line\n                        break\n                while i+1 < len(lines):\n                    offset += 1  # for the newline char\n                    offset += len(lines[i+1])\n                    i += 1\n                # track back until the start of the comment, and truncate the comment\n                if sourcemap:\n                    offset += 1  # for the newline before the sourcemap comment\n                    of.seek(-offset, os.SEEK_END)\n                    of.truncate()\n                return force_text(sourcemap)\n    finally:\n        of.close()\n    return sourcemap"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        if default:\n            prompt = '%s [y/n]? ' % prompt\n        else:\n            prompt = '%s [y/N]? ' % prompt\n        while True:\n            try:\n                return raw_input(prompt).lower() == 'y'\n            except KeyboardInterrupt:\n                print 'Interrupted'\n            except EOFError:\n                print 'Interrupted'\n                return False\n\n    def _get_config(self, config_file):\n        \"\"\" Reads the config file and returns a dictionary of the settings.", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def delete(self, tag_id):\n        \"\"\"\n        Delete the specified InactivityAlert\n\n        :param tag_id: The tag ID to delete\n        :type tag_id: str\n\n        :raises: This will raise a\n            :class:`ServerException <logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        self.client.delete(self.url + tag_id)\n\n    def get(self, tag_id):\n        \"\"\"\n        Get the specified InactivityAlert\n\n        :param tag_id: The tag ID to get\n        :type tag_id: str\n\n        :return: The InactivityAlert\n        :rtype: :class:`InactivityAlert <logentries_api.models.InactivityAlert>`\n\n        :raises: This will raise a\n            :class:`ServerException <logentries_api.exceptions.ServerException>`\n           ", "solution": "\n        tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}'\n\n        self._api_delete(\n            url=tag_url.format(\n                account_id=self.account_id,\n                tag_id=tag_id\n            )\n        )"}
{"prompt": "def normal_left_dclick(self, event):\n        \"\"\" Handles the left mouse button being double-clicked when the tool\n        is in the 'normal' state.\n\n        If the event occurred on this tool's component (or any contained\n        component of that component), the method opens a Traits UI view on the\n        object referenced by the 'element' trait of the component that was\n        double-clicked, setting the tool as the active tool for the duration\n        of the view.\n\n        \"\"\"", "output": "\n        if self.component is not None:\n            self.component.normal_left_dclick(event)\n\n    def normal_left_press(self, event):\n        \"\"\" Handles the left mouse button being pressed when the tool is in\n        the 'normal' state.\n\n        If the event occurred on this tool's component (or any contained\n        component of that component), the method opens a Traits UI view on the\n        object referenced by the 'element' trait of the component that was\n        double-clicked, setting the tool as the active tool for the", "solution": "\n        x = event.x\n        y = event.y\n\n        # First determine what component or components we are going to hittest\n        # on.  If our component is a container, then we add its non-container\n        # components to the list of candidates.\n#        candidates = []\n        component = self.component\n#        if isinstance(component, Container):\n#            candidates = get_nested_components(self.component)\n#        else:\n#            # We don't support clicking on unrecognized components\n#            return\n#\n#        # Hittest against all the candidate and take the first one\n#        item = None\n#        for candidate, offset in candidates:\n#            if candidate.is_in(x-offset[0], y-offset[1]):\n#                item = candidate\n#                break\n\n        if hasattr(component, \"element\"):\n            if component.element is not None:\n                component.active_tool = self\n                component.element.edit_traits(kind=\"livemodal\")\n                event.handled = True\n                component.active_tool = None\n                component.request_redraw()\n        return"}
{"prompt": "def _read_compressed_points_data(self, laszip_vlr, point_format):\n        \"\"\" reads the compressed point record\n        \"\"\"", "output": "\n        # read the compressed point record\n        self.laszip_reader.seek(laszip_vlr.offset_to_point_data)\n        self.laszip_reader.readinto(self.compressed_data)\n\n        # decompress the point record\n        self.laszip_reader.seek(laszip_vlr.offset_to_point_data + laszip_vlr.uncompressed_size)\n        self.laszip_reader.readinto(self.uncompressed_data)\n\n        # read the point", "solution": "\n        offset_to_chunk_table = struct.unpack(\"<q\", self.stream.read(8))[0]\n        size_of_point_data = offset_to_chunk_table - self.stream.tell()\n\n        if offset_to_chunk_table <= 0:\n            logger.warning(\n                \"Strange offset to chunk table: {}, ignoring it..\".format(\n                    offset_to_chunk_table\n                )\n            )\n            size_of_point_data = -1  # Read everything\n\n        points = record.PackedPointRecord.from_compressed_buffer(\n            self.stream.read(size_of_point_data),\n            point_format,\n            self.header.point_count,\n            laszip_vlr,\n        )\n        return points"}
{"prompt": "def projector(state, flatten=False):\n    \"\"\"\n    maps a pure state to a state matrix\n\n    Args:\n        state (ndarray): the number of qubits\n        flatten (bool): determine if state matrix of column work\n    Returns:\n        ndarray:  state_mat(2**num, 2**num) if flatten is false\n        ndarray:  state_mat(4**num) if flatten is true stacked on by the column\n    \"\"\"", "output": "\n    num = int(np.log2(state.shape[0]))\n    state_mat = np.zeros((2 ** num, 2 ** num))\n    for i in range(2 ** num):\n        state_mat[i, state[i]] = 1\n    if flatten:\n        state_mat = np.reshape(state_mat, (2 ** (2 * num), 1))\n    return state_mat\n\n\ndef projector_to_state(state_mat, flatten=False):\n    \"\"\"\n    maps a", "solution": "\n    density_matrix = np.outer(state.conjugate(), state)\n    if flatten:\n        return density_matrix.flatten(order='F')\n    return density_matrix"}
{"prompt": "def export_transcripts(adapter, build='37'):\n    \"\"\"Export all transcripts from the database\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        build(str)\n    \n    Yields:\n        transcript(scout.models.Transcript)\n    \"\"\"", "output": "\n    for gene in adapter.gene_collection.find({\"build\": build}):\n        for transcript in adapter.transcript_collection.find({\"gene_id\": gene[\"_id\"]}):\n            yield transcript\n\n\ndef export_genes(adapter, build='37'):\n    \"\"\"Export all genes from the database\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        build(str)\n    \n    Yields:\n        gene(scout.models.Gene)\n    \"\"\"", "solution": "\n    LOG.info(\"Exporting all transcripts\")\n\n    for tx_obj in adapter.transcripts(build=build):\n        yield tx_obj"}
{"prompt": "def write_table(self, table):\n        \"\"\"Send DDL to create the specified `table`\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n\n        Returns None\n        \"\"\"", "output": "\n        self.write_ddl(table.get_ddl())\n\n    def write_ddl(self, ddl):\n        \"\"\"Send DDL to create the specified `table`\n\n        :Parameters:\n          - `ddl`: a string representing the DDL to send to the server.\n\n        Returns None\n        \"\"\"\n        self.cursor.execute(ddl)\n\n    def write_data(self, table, data):\n        \"\"\"Send data to create the specified `table`\n\n        :Parameters:\n          - `table`: an instance of", "solution": "\n        table_sql, serial_key_sql = super(PostgresDbWriter, self).write_table(table)\n        for sql in serial_key_sql + table_sql:\n            self.execute(sql)"}
{"prompt": "def visit_module(self, node):\n        \"\"\"visit module : update consumption analysis variable\n        checks globals doesn't overrides builtins\n        \"\"\"", "output": "\n        self.visit_block(node)\n        self.update_consumption_analysis_variable()\n\n    def visit_classdef(self, node):\n        \"\"\"visit classdef : update consumption analysis variable\n        checks globals doesn't overrides builtins\n        \"\"\"\n        self.visit_block(node)\n        self.update_consumption_analysis_variable()\n\n    def visit_block(self, node):\n        \"\"\"visit block : update consumption analysis variable\n        checks globals doesn't overrides builtins\n        \"\"\"\n       ", "solution": "\n        self._to_consume = [NamesConsumer(node, \"module\")]\n        self._postponed_evaluation_enabled = is_postponed_evaluation_enabled(node)\n\n        for name, stmts in node.locals.items():\n            if utils.is_builtin(name) and not utils.is_inside_except(stmts[0]):\n                if self._should_ignore_redefined_builtin(stmts[0]) or name == \"__doc__\":\n                    continue\n                self.add_message(\"redefined-builtin\", args=name, node=stmts[0])"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n        return self._evolve(self._data.transpose())\n\n    def conjugate(self):\n        \"\"\"Return the conjugate of the QuantumChannel.\"\"\"\n        return self._evolve(self._data.conj())\n\n    def adjoint(self):\n        \"\"\"Return the adjoint of the QuantumChannel.\"\"\"\n        return self._evolve(self._data.conj().T)\n\n    def is_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if the QuantumChannel", "solution": "\n        return SuperOp(\n            np.transpose(self._data),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def _prepare_hiveconf(d):\n        \"\"\"\n        This function prepares a list of hiveconf params\n        from a dictionary of key value pairs.\n\n        :param d:\n        :type d: dict\n\n        >>> hh = HiveCliHook()\n        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n        >>> hh._prepare_hiveconf(hive_conf)\n        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\\n \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n        \"\"\"", "output": "iveconf\", \"hive.exec.dynamic.partition=true\",\n        \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n        \"\"\"\n        if not d:\n            return []\n        return ['-hiveconf', '{0}={1}'.format(key, value) for key, value in d.items()]\n\n    def _prepare_params(self, hql, hive_conf=None, **kwargs):\n        \"\"\"\n        This function prepares a list of parameters to be passed to the\n        h", "solution": "\n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )"}
{"prompt": "def trimmed_pred_default(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"", "output": "\n  return node.is_visible()\n\n\nclass Node(object):\n  \"\"\"A node in the DOM tree.\"\"\"\n\n  def __init__(self, name, parent=None, predicate=None):\n    \"\"\"Creates a new node.\n\n    Args:\n      name: The name of the node.\n      parent: The parent node.\n      predicate: A predicate function that returns True if the node should be\n          included in the trimmed tree.\n    \"\"\"\n    self.name = name\n    self.parent = parent\n    self.predicate =", "solution": "\n  return isinstance(node, ParseNode) and (node.is_empty or node.is_type(ParseNodeType.terminal))"}
{"prompt": "def _verify_compatibility(wave_a, wave_b, check_dep_units=True):\n    \"\"\"Verify that two waveforms can be combined with various mathematical functions.\"\"\"", "output": "\n    if check_dep_units:\n        if wave_a.dep_units != wave_b.dep_units:\n            raise ValueError(\n                \"Waveforms have different dependent units: %s and %s\"\n                % (wave_a.dep_units, wave_b.dep_units)\n            )\n\n    if wave_a.indep_units != wave_b.indep_units:\n        raise ValueError(\n            \"Waveforms have different independent units: %s and %s\"\n           ", "solution": "\n    exobj = pexdoc.exh.addex(RuntimeError, \"Waveforms are not compatible\")\n    ctuple = (\n        bool(wave_a.indep_scale != wave_b.indep_scale),\n        bool(wave_a.dep_scale != wave_b.dep_scale),\n        bool(wave_a.indep_units != wave_b.indep_units),\n        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),\n        bool(wave_a.interp != wave_b.interp),\n    )\n    exobj(any(ctuple))"}
{"prompt": "def flowwrite(flow, filename, quantize=False, concat_axis=0, *args, **kwargs):\n    \"\"\"Write optical flow to file.\n\n    If the flow is not quantized, it will be saved as a .flo file losslessly,\n    otherwise a jpeg image which is lossy but of much smaller size. (dx and dy\n    will be concatenated horizontally into a single image if quantize is True.)\n\n    Args:\n        flow (ndarray): (h, w, 2) array of optical flow.\n        filename (str): Output filepath.\n        quantize (bool): Whether to quantize the flow and save it to 2 jpeg\n            images. If set to True, remaining args will be passed to\n            :func:`quantize_flow`.\n        concat_axis (int): The axis that dx and dy are concatenated,\n            can be either 0 or 1. Ignored if quantize is False.\n    \"\"\"", "output": "       filename (str): name of the file to save the flow to.\n        quantize (bool): if True, save the flow as a jpeg image.\n        concat_axis (int): if quantize is True, the axis to concatenate dx and dy\n            on.\n        args: additional args passed to cv2.imwrite.\n        kwargs: additional kwargs passed to cv2.imwrite.\n\n    \"\"\"\n    if not quantize:\n        # write flow file in Middlebury .flo format\n        flow_write_middlebury(flow, filename", "solution": "\n    if not quantize:\n        with open(filename, 'wb') as f:\n            f.write('PIEH'.encode('utf-8'))\n            np.array([flow.shape[1], flow.shape[0]], dtype=np.int32).tofile(f)\n            flow = flow.astype(np.float32)\n            flow.tofile(f)\n            f.flush()\n    else:\n        assert concat_axis in [0, 1]\n        dx, dy = quantize_flow(flow, *args, **kwargs)\n        dxdy = np.concatenate((dx, dy), axis=concat_axis)\n        imwrite(dxdy, filename)"}
{"prompt": "def get_singularity_version(singularity_version=None):\n    \"\"\"get_singularity_version will determine the singularity version for a\n       build first, an environmental variable is looked at, followed by \n       using the system version.\n\n       Parameters\n       ==========\n       singularity_version: if not defined, look for in environment. If still\n       not find, try finding via executing --version to Singularity. Only return\n       None if not set in environment or installed.\n    \"\"\"", "output": "\n    if singularity_version is None:\n        singularity_version = os.environ.get('SINGULARITY_VERSION')\n    if singularity_version is None:\n        singularity_version = get_singularity_version_from_system()\n    return singularity_version\n\n\ndef get_singularity_version_from_system():\n    \"\"\"get_singularity_version_from_system will determine the singularity version\n       for a build by executing --version to Singularity.\n    \"\"\"\n    try:\n        singularity", "solution": "\n\n    if singularity_version is None:        \n        singularity_version = os.environ.get(\"SINGULARITY_VERSION\")\n        \n    if singularity_version is None:\n        try:\n            cmd = ['singularity','--version']\n            output = run_command(cmd)\n\n            if isinstance(output['message'],bytes):\n                output['message'] = output['message'].decode('utf-8')\n            singularity_version = output['message'].strip('\\n')\n            bot.info(\"Singularity %s being used.\" % singularity_version)\n            \n        except:\n            singularity_version = None\n            bot.warning(\"Singularity version not found, so it's likely not installed.\")\n\n    return singularity_version"}
{"prompt": "def sign(pkey, data, digest):\n    \"\"\"\n    Sign a data string using the given key and message digest.\n\n    :param pkey: PKey to sign with\n    :param data: data to be signed\n    :param digest: message digest to use\n    :return: signature\n\n    .. versionadded:: 0.11\n    \"\"\"", "output": "\n    return pkey.sign(data, digest)\n\n\ndef verify(pkey, signature, data, digest):\n    \"\"\"\n    Verify a signature against a data string using the given key and message digest.\n\n    :param pkey: PKey to verify with\n    :param signature: signature to verify\n    :param data: data to be verified\n    :param digest: message digest to use\n    :return: True if signature is valid, False otherwise\n\n    .. versionadded:: 0.11\n    \"\"\"\n    return pkey", "solution": "\n    data = _text_to_bytes_and_warn(\"data\", data)\n\n    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))\n    if digest_obj == _ffi.NULL:\n        raise ValueError(\"No such digest method\")\n\n    md_ctx = _lib.Cryptography_EVP_MD_CTX_new()\n    md_ctx = _ffi.gc(md_ctx, _lib.Cryptography_EVP_MD_CTX_free)\n\n    _lib.EVP_SignInit(md_ctx, digest_obj)\n    _lib.EVP_SignUpdate(md_ctx, data, len(data))\n\n    length = _lib.EVP_PKEY_size(pkey._pkey)\n    _openssl_assert(length > 0)\n    signature_buffer = _ffi.new(\"unsigned char[]\", length)\n    signature_length = _ffi.new(\"unsigned int *\")\n    final_result = _lib.EVP_SignFinal(\n        md_ctx, signature_buffer, signature_length, pkey._pkey)\n    _openssl_assert(final_result == 1)\n\n    return _ffi.buffer(signature_buffer, signature_length[0])[:]"}
{"prompt": "def p_navigation_step_2(self, p):\n        \"\"\"navigation_step : ARROW identifier LSQBR identifier DOT phrase RSQBR\"\"\"", "output": "\n        p[0] = NavigationStep(p[2], p[4], p[6])\n\n    def p_navigation_step_3(self, p):\n        \"\"\"navigation_step : ARROW identifier LSQBR identifier DOT phrase RSQBR LSQBR identifier DOT phrase RSQBR\"\"\"\n        p[0] = NavigationStep(p[2], p[4], p[6], p[8], p[10])\n\n    def p_navigation_step_4(self, p):\n        \"\"\"navigation_step :", "solution": "\n        p[0] = NavigationStepNode(key_letter=p[2],\n                                  rel_id=p[4],\n                                  phrase=p[6])"}
{"prompt": "def _get_request(self, endpoint):\n        \"\"\"Do actual GET request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for GET request\n        \"\"\"", "output": "\n        response = self.session.get(endpoint)\n        if response.status_code != 200:\n            raise Exception(\n                \"Error: {}\".format(response.text))\n        return response\n\n    def _post_request(self, endpoint, data):\n        \"\"\"Do actual POST request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for POST request\n        data -- data to be sent in POST request\n        \"\"\"\n        response = self.session.post(endpoint, data=data)\n", "solution": "\n        try:\n            response = requests.get(endpoint)\n        except requests.exceptions.RequestException:\n            raise GoldenCheetahNotAvailable(endpoint)\n        \n        if response.text.startswith('unknown athlete'):\n            match = re.match(\n                pattern='unknown athlete (?P<athlete>.+)',\n                string=response.text)\n            raise AthleteDoesNotExist(\n                athlete=match.groupdict()['athlete'])\n\n        elif response.text == 'file not found':\n            match = re.match(\n                pattern='.+/activity/(?P<filename>.+)',\n                string=endpoint)\n            raise ActivityDoesNotExist(\n                filename=match.groupdict()['filename'])\n\n        return response"}
{"prompt": "def _line_segment_with_colons(linefmt, align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\"\"\"", "output": "\n    if align is None:\n        return linefmt\n    else:\n        return linefmt.replace(':', ':' + ':' * (colwidth - 1))\n\n\ndef _line_segment_with_colons_and_dashes(linefmt, align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons and\n    dashes which indicate column's alignment and width (as in `simple`\n    output format).\"\"\"\n    if align is None:\n        return linefmt\n    else:\n        return linefmt.", "solution": "\n    fill = linefmt.hline\n    w = colwidth\n    if align in [\"right\", \"decimal\"]:\n        return (fill[0] * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + (fill[0] * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + (fill[0] * (w - 1))\n    else:\n        return fill[0] * w"}
{"prompt": "def report(self):\n        \"\"\"\n        Print network statistics.\n        \"\"\"", "output": "\n        self.print_stats()\n\n    def print_stats(self):\n        \"\"\"\n        Print network statistics.\n        \"\"\"\n        print('Network statistics:')\n        print('  Number of nodes: %d' % self.num_nodes)\n        print('  Number of edges: %d' % self.num_edges)\n        print('  Number of triangles: %d' % self.num_triangles)\n        print('  Number of quadrilaterals: %d' % self.num_quadrilaterals)\n        print(' ", "solution": "\n        logging.info(\"network inputs: %s\", \" \".join(map(str, self.input_variables)))\n        logging.info(\"network targets: %s\", \" \".join(map(str, self.target_variables)))\n        logging.info(\"network parameters: %s\", \" \".join(map(str, self.all_parameters)))\n        logging.info(\"parameter count: %d\", self.parameter_count)"}
{"prompt": "def init_handlers(self):\n        \"\"\"Create the default handlers.\"\"\"", "output": "\n        self.handlers = {\n            'default': {\n                'level': 'INFO',\n                'class': 'logging.StreamHandler',\n                'formatter': 'default',\n                'stream': 'ext://sys.stdout',\n            },\n            'file': {\n                'level': 'INFO',\n                'class': 'logging.handlers.RotatingFileHandler',\n                'formatter': 'default',\n                'filename': 'logs/app.log',\n                'maxBytes': 1024 * 1024 * 1", "solution": "\n        self._handlers = {}\n        self._esc_handlers = {}\n        for handler in _default_handlers:\n            handler(\n                shell=self.shell, prefilter_manager=self, config=self.config\n            )"}
{"prompt": "def average_data(counts, observable):\n    \"\"\"Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n        counts (dict): a dict of outcomes from an experiment\n        observable (dict or matrix or list): The observable to be averaged over.\n        As an example, ZZ on qubits can be given as:\n        * dict: {\"00\": 1, \"11\": 1, \"01\": -1, \"10\": -1}\n        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]\n        * matrix diagonal (list): [1, -1, -1, 1]\n\n    Returns:\n        Double: Average of the observable\n    \"\"\"", "output": "a list of Pauli matrices\n        [Pauli.Z, Pauli.Z] or as a matrix [[1,0],[0,-1]]\n\n    Returns:\n        float: The mean value of the observable.\n    \"\"\"\n    if isinstance(observable, dict):\n        observable = observable.values()\n    if isinstance(observable, list):\n        observable = np.array(observable)\n    if isinstance(observable, np.ndarray):\n        observable = np.diag(observable)\n    if not isinstance(observable,", "solution": "\n    if not isinstance(observable, dict):\n        observable = make_dict_observable(observable)\n    temp = 0\n    tot = sum(counts.values())\n    for key in counts:\n        if key in observable:\n            temp += counts[key] * observable[key] / tot\n    return temp"}
{"prompt": "def _get_object_name(self, line):\n        \"\"\" Get second token in line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_object_name(\"  def func():  \")\n        'func'\n        >>> docwriter._get_object_name(\"  class Klass(object):  \")\n        'Klass'\n        >>> docwriter._get_object_name(\"  class Klass:  \")\n        'Klass'\n        \"\"\"", "output": "\n        return line.split()[1]\n\n    def _get_object_type(self, line):\n        \"\"\" Get first token in line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_object_type(\"  def func():  \")\n        'def'\n        >>> docwriter._get_object_type(\"  class Klass(object):  \")\n        'class'\n        >>> docwriter._get_object_type(\"  class Klass:  \")\n        'class'\n        \"\"\"\n        return line.", "solution": "\n        name = line.split()[1].split('(')[0].strip()\n        # in case we have classes which are not derived from object\n        # ie. old style classes\n        return name.rstrip(':')"}
{"prompt": "def _build_metrics(func_name, namespace):\n    \"\"\"\n    Builds metrics dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics\n    \"\"\"", "output": "\n    metrics = {}\n    if namespace.dag_id:\n        metrics[\"dag_id\"] = namespace.dag_id\n    if namespace.task_id:\n        metrics[\"task_id\"] = namespace.task_id\n    if namespace.execution_date:\n        metrics[\"execution_date\"] = namespace.execution_date\n    return metrics\n\n\ndef _build_tags(func_name, namespace):\n    \"\"\"\n    Builds tags dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has", "solution": "\n\n    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\n               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\n\n    assert isinstance(namespace, Namespace)\n    tmp_dic = vars(namespace)\n    metrics['dag_id'] = tmp_dic.get('dag_id')\n    metrics['task_id'] = tmp_dic.get('task_id')\n    metrics['execution_date'] = tmp_dic.get('execution_date')\n    metrics['host_name'] = socket.gethostname()\n\n    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))\n    log = Log(\n        event='cli_{}'.format(func_name),\n        task_instance=None,\n        owner=metrics['user'],\n        extra=extra,\n        task_id=metrics.get('task_id'),\n        dag_id=metrics.get('dag_id'),\n        execution_date=metrics.get('execution_date'))\n    metrics['log'] = log\n    return metrics"}
{"prompt": "def get_func_sourcecode(func):\n    \"\"\"\n    Try to get sourcecode using standard inspect.getsource().\n    If the function comes from a module which has been created dynamically\n    (not from the filesystem), then it tries to read the sourcecode on the\n    filesystem anyway.\n    WARNING: can do weird things if the filesystem code slightly differs from\n             the original module code.\n    \"\"\"", "output": "\n    try:\n        return inspect.getsource(func)\n    except OSError:\n        # the function comes from a module which has been created dynamically\n        # (not from the filesystem), so try to read the sourcecode on the\n        # filesystem anyway\n        try:\n            filename = inspect.getsourcefile(func)\n            with open(filename, 'r') as f:\n                return f.read()\n        except (TypeError, IOError):\n            # the function comes from a module which has been created\n            # dynamically and which has no filename\n            return", "solution": "\n\n    def getsource(func):\n        lines, lnum = getsourcelines(func)\n        return ''.join(lines)\n\n    def getsourcelines(func):\n        lines, lnum = findsource(func)\n        return inspect.getblock(lines[lnum:]), lnum + 1\n\n    def findsource(func):\n        file = getfile(func)  # file path\n        module = inspect.getmodule(func, file)\n        lines = linecache.getlines(file, module.__dict__)\n        code = func.__code__\n        lnum = code.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]):\n                break\n            lnum = lnum - 1  # pragma: no cover\n        return lines, lnum\n\n    def getfile(func):\n        module = inspect.getmodule(func)\n        return module.__file__\n\n    try:\n        return inspect.getsource(func)\n    except Exception:\n        return getsource(func)"}
{"prompt": "def x_fit(self, test_length):\r\n        \"\"\" Test to see if the line can has enough space for the given length. \"\"\"", "output": "\r\n        return self.length >= test_length\r\n\r\n    def x_fit_all(self, test_length):\r\n        \"\"\" Test to see if the line can has enough space for the given length. \"\"\"\r\n        return self.length >= test_length\r\n\r\n    def x_fit_any(self, test_length):\r\n        \"\"\" Test to see if the line can has enough space for the given length. \"\"\"\r\n        return self.length >= test_length\r\n\r\n    def x_fit_not(self, test_length):\r", "solution": "\r\n        if (self.x + test_length) >= self.xmax:\r\n            return False\r\n        else:\r\n            return True"}
{"prompt": "def list_files(self, extensions=None):\n        \"\"\"\n        List the ports contents by file type or all.\n        :param extensions: string extensions, single string or list of extensions.\n        :return: A list of full path names of each file.\n        \"\"\"", "output": "\n        if extensions is None:\n            return self.list_files_all()\n        elif isinstance(extensions, str):\n            return self.list_files_by_extension(extensions)\n        elif isinstance(extensions, list):\n            return self.list_files_by_extensions(extensions)\n        else:\n            raise TypeError(\"extensions must be a string or list of strings.\")\n\n    def list_files_all(self):\n        \"\"\"\n        List the ports contents by file type or all.\n        :return: A list of full path names of each file", "solution": "\n        if self.type.lower() != 'directory':\n            raise ValueError(\"Port type is not == directory\")\n\n        filesystem_location = self.path\n\n        for root, dirs, files in os.walk(filesystem_location):\n            if extensions is None:\n                return [os.path.join(root, f) for f in files]\n            elif not isinstance(extensions, list):\n                extensions = [extensions]\n\n            subset_files = []\n\n            for f in files:\n                for extension in extensions:\n                    if f.lower().endswith(extension.lower()):\n                        subset_files.append(os.path.join(root, f))\n                        break\n            return subset_files"}
{"prompt": "def get_image(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get image list from ImageFrame\n        \"\"\"", "output": "\n        if self.is_cuda:\n            return self.data[float_key].cpu().numpy()\n        else:\n            return self.data[float_key].numpy()\n\n    def get_label(self, int_key=\"ints\"):\n        \"\"\"\n        get label list from ImageFrame\n        \"\"\"\n        if self.is_cuda:\n            return self.data[int_key].cpu().numpy()\n        else:\n            return self.data[int_key].numpy()\n\n    def get_float(self, float_key", "solution": "\n        tensors = callBigDlFunc(self.bigdl_type,\n                                   \"localImageFrameToImageTensor\", self.value, float_key, to_chw)\n        return map(lambda tensor: tensor.to_ndarray(), tensors)"}
{"prompt": "def generate_key(self, type, bits):\n        \"\"\"\n        Generate a key pair of the given type, with the given number of bits.\n\n        This generates a key \"into\" the this object.\n\n        :param type: The key type.\n        :type type: :py:data:`TYPE_RSA` or :py:data:`TYPE_DSA`\n        :param bits: The number of bits.\n        :type bits: :py:data:`int` ``>= 0``\n        :raises TypeError: If :py:data:`type` or :py:data:`bits` isn't\n            of the appropriate type.\n        :raises ValueError: If the number of bits isn't an integer of\n            the appropriate size.\n        :return: ``None``\n        \"\"\"", "output": "type` is not :py:data:`TYPE_RSA` or\n            :py:data:`TYPE_DSA`.\n        :raises ValueError: If :py:data:`bits` is less than 0.\n        \"\"\"\n        if type == self.TYPE_RSA:\n            self._key = RSA.generate(bits)\n        elif type == self.TYPE_DSA:\n            self._key = DSA.generate(bits)\n        else:\n            raise TypeError(\"type must be TYPE_RSA or TYPE_DSA\")\n\n", "solution": "\n        if not isinstance(type, int):\n            raise TypeError(\"type must be an integer\")\n\n        if not isinstance(bits, int):\n            raise TypeError(\"bits must be an integer\")\n\n        if type == TYPE_RSA:\n            if bits <= 0:\n                raise ValueError(\"Invalid number of bits\")\n\n            # TODO Check error return\n            exponent = _lib.BN_new()\n            exponent = _ffi.gc(exponent, _lib.BN_free)\n            _lib.BN_set_word(exponent, _lib.RSA_F4)\n\n            rsa = _lib.RSA_new()\n\n            result = _lib.RSA_generate_key_ex(rsa, bits, exponent, _ffi.NULL)\n            _openssl_assert(result == 1)\n\n            result = _lib.EVP_PKEY_assign_RSA(self._pkey, rsa)\n            _openssl_assert(result == 1)\n\n        elif type == TYPE_DSA:\n            dsa = _lib.DSA_new()\n            _openssl_assert(dsa != _ffi.NULL)\n\n            dsa = _ffi.gc(dsa, _lib.DSA_free)\n            res = _lib.DSA_generate_parameters_ex(\n                dsa, bits, _ffi.NULL, 0, _ffi.NULL, _ffi.NULL, _ffi.NULL\n            )\n            _openssl_assert(res == 1)\n\n            _openssl_assert(_lib.DSA_generate_key(dsa) == 1)\n            _openssl_assert(_lib.EVP_PKEY_set1_DSA(self._pkey, dsa) == 1)\n        else:\n            raise Error(\"No such key type\")\n\n        self._initialized = True"}
{"prompt": "def v2_runner_on_skipped(self, result, **kwargs):\n        \"\"\"Run when a task is skipped.\"\"\"", "output": "\n        self._display.display(\n            'skipping: %s' % result._host.get_name(),\n            color=C.COLOR_SKIP,\n            stderr=True\n        )\n\n    def v2_runner_on_ok(self, result, **kwargs):\n        \"\"\"Run when a task passes.\"\"\"\n        self._display.display(\n            'ok: %s' % result._host.get_name(),\n            color=C.COLOR_OK,\n            stderr=True\n        )\n\n    def v2", "solution": "\n        if self._display.verbosity > 1:\n            self._print_task()\n            self.last_skipped = False\n\n            line_length = 120\n            spaces = \" \" * (31 - len(result._host.name) - 4)\n\n            line = \"  * {}{}- {}\".format(\n                colorize(result._host.name, \"not_so_bold\"),\n                spaces,\n                colorize(\"skipped\", \"skipped\"),\n            )\n\n            reason = result._result.get(\"skipped_reason\", \"\") or result._result.get(\n                \"skip_reason\", \"\"\n            )\n            if len(reason) < 50:\n                line += \" -- {}\".format(reason)\n                print(\"{} {}---------\".format(line, \"-\" * (line_length - len(line))))\n            else:\n                print(\"{} {}\".format(line, \"-\" * (line_length - len(line))))\n                print(self._indent_text(reason, 8))\n                print(reason)"}
{"prompt": "def _estimate_gas(self,\n                      master_copy: str,\n                      initializer: bytes,\n                      funder: str,\n                      payment_token: str) -> int:\n        \"\"\"\n        Gas estimation done using web3 and calling the node\n        Payment cannot be estimated, as no ether is in the address. So we add some gas later.\n        :param master_copy: Master Copy of Gnosis Safe already deployed\n        :param initializer: Data initializer to send to GnosisSafe setup method\n        :param funder: Address that should get the payment (if payment set)\n        :param payment_token: Address if a token is used. If not set, 0x0 will be ether\n        :return: Total gas estimation\n        \"\"\"", "output": "funder: Address that will be used to fund the Safe\n        :param payment_token: Address of the token used to pay for the gas\n        :return: Estimated gas\n        \"\"\"\n        # Estimate gas\n        gas_estimate = self.w3.eth.estimate_gas(\n            {\n                \"from\": funder,\n                \"to\": master_copy,\n                \"data\": initializer\n            }\n        )\n        # Add gas to cover the payment\n        gas_estimate += 100000\n        return gas", "solution": "\n\n        # Estimate the contract deployment. We cannot estimate the refunding, as the safe address has not any fund\n        gas: int = self._build_proxy_contract_creation_constructor(\n            master_copy,\n            initializer,\n            funder,\n            payment_token,\n            0).estimateGas()\n\n        # We estimate the refund as a new tx\n        if payment_token == NULL_ADDRESS:\n            # Same cost to send 1 ether than 1000\n            gas += self.w3.eth.estimateGas({'to': funder, 'value': 1})\n        else:\n            # Top should be around 52000 when storage is needed (funder no previous owner of token),\n            # we use value 1 as we are simulating an internal call, and in that calls you don't pay for the data.\n            # If it was a new tx sending 5000 tokens would be more expensive than sending 1 because of data costs\n            try:\n                gas += get_erc20_contract(self.w3,\n                                          payment_token).functions.transfer(funder, 1).estimateGas({'from':\n                                                                                                    payment_token})\n            except ValueError as exc:\n                raise InvalidERC20Token from exc\n\n        return gas"}
{"prompt": "def reg_on_abort(self, callable_object, *args, **kwargs):\n        \"\"\" Register a function/method to be called when execution is aborted\"\"\"", "output": "\n        self._abort_callbacks.append((callable_object, args, kwargs))\n\n    def abort(self, msg=None):\n        \"\"\" Abort execution of the current test.\n\n        :param msg: Optional message to be printed before aborting.\n        \"\"\"\n        if msg:\n            print(msg)\n        for callable_object, args, kwargs in self._abort_callbacks:\n            callable_object(*args, **kwargs)\n        raise AbortTest()\n\n    def _run_test(self, test_", "solution": "\n        persistent = kwargs.pop('persistent', False)\n        event = self._create_event(callable_object, 'abort', persistent, *args, **kwargs)\n        self.abort_callbacks.append(event)\n        return event"}
{"prompt": "def wa(client, event, channel, nick, rest):\n\t\"\"\"\n\tA free-text query resolver by Wolfram|Alpha. Returns the first\n\tresult, if available.\n\t\"\"\"", "output": "\n\tif not rest:\n\t\treturn\n\ttry:\n\t\tresult = wolfram.query(rest)\n\t\tif result:\n\t\t\treturn result[0]['subpods'][0]['plaintext']\n\texcept:\n\t\treturn\n\ndef wiki(client, event, channel, nick, rest):\n\t\"\"\"\n\tA free-text query resolver by Wikipedia. Returns the first\n\tresult, if available.\n\t\"\"\"\n\tif not rest:\n\t\treturn\n\ttry:\n\t\tresult = wikipedia.", "solution": "\n\tclient = wolframalpha.Client(pmxbot.config['Wolfram|Alpha API key'])\n\tres = client.query(rest)\n\treturn next(res.results).text"}
{"prompt": "def update_(self, sct_dict, conf_arg=True):\n        \"\"\"Update values of configuration section with dict.\n\n        Args:\n            sct_dict (dict): dict indexed with option names. Undefined\n                options are discarded.\n            conf_arg (bool): if True, only options that can be set in a config\n                file are updated.\n        \"\"\"", "output": "\n        if conf_arg:\n            conf_dict = self.conf_dict\n        else:\n            conf_dict = self.dict\n\n        for option, value in sct_dict.items():\n            if option in conf_dict:\n                conf_dict[option] = value\n\n    def update(self, sct_dict, conf_arg=True):\n        \"\"\"Update values of configuration section with dict.\n\n        Args:\n            sct_dict (dict): dict indexed with option names. Undefined\n                options are discarded.\n            conf", "solution": "\n        for opt, val in sct_dict.items():\n            if opt not in self.def_:\n                continue\n            if not conf_arg or self.def_[opt].conf_arg:\n                self[opt] = val"}
{"prompt": "def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"", "output": "\n        if self.regex is None:\n            return False\n\n        if self.path is None:\n            return False\n\n        if self.path.startswith(\"~\"):\n            self.path = os.path.expanduser(self.path)\n\n        if not os.path.exists(self.path):\n            return False\n\n        if not os.path.isdir(self.path):\n            return False\n\n        for root, dirs, files in os.walk(self.path):\n            for name in files:\n                if self.", "solution": "\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)"}
{"prompt": "def breadcrumbs(self, tree_alias, context):\n        \"\"\"Builds and returns breadcrumb trail structure for 'sitetree_breadcrumbs' tag.\n\n        :param str|unicode tree_alias:\n        :param Context context:\n        :rtype: list|str\n        \"\"\"", "output": "\n        tree = self.get_tree(tree_alias)\n        if not tree:\n            return ''\n\n        # Get current node\n        current_node = self.get_current_node(tree_alias, context)\n        if not current_node:\n            return ''\n\n        # Get current node's parents\n        parents = self.get_parents(tree_alias, current_node.id)\n\n        # Get current node's children\n        children = self.get_children(tree_alias, current_node.id)\n\n        #", "solution": "\n        tree_alias, sitetree_items = self.init_tree(tree_alias, context)\n\n        if not sitetree_items:\n            return ''\n\n        current_item = self.get_tree_current_item(tree_alias)\n\n        breadcrumbs = []\n\n        if current_item is not None:\n\n            context_ = self.current_page_context\n            check_access = self.check_access\n            get_item_by_id = self.get_item_by_id\n\n            def climb(base_item):\n                "}
{"prompt": "def getCSV(self):\n        \"\"\"\n        Returns\n        -------\n        filename: str\n        \"\"\"", "output": "\n        return self.csv\n\n    def getCSVHeader(self):\n        \"\"\"\n        Returns\n        -------\n        header: list\n        \"\"\"\n        return self.header\n\n    def getCSVData(self):\n        \"\"\"\n        Returns\n        -------\n        data: list\n        \"\"\"\n        return self.data\n\n    def getCSVDataAsDict(self):\n        \"\"\"\n        Returns\n        -------\n        data: list\n        \"\"\"\n        return self.dataAsDict\n\n    def getCSVDataAsDictByColumn(", "solution": "\n        import getpass\n        import gspread\n\n        user = raw_input(\"Insert Google username:\")\n        password = getpass.getpass(prompt=\"Insert password:\")\n        name = raw_input(\"SpreadSheet filename on Drive:\")\n        sheet = raw_input(\"Sheet name (first sheet is default):\")\n\n        cl = gspread.login(user, password)\n        sh = cl.open(name)\n\n        if not (sheet.strip()):\n            ws = sh.sheet1\n            sheet = \"1\"\n        else:\n            ws = sh.worksheet(sheet)\n\n        filename = name + '-worksheet_' + sheet + '.csv'\n        with open(filename, 'wb') as f:\n            writer = UnicodeWriter(f)\n            writer.writerows(ws.get_all_values())\n\n        return filename"}
{"prompt": "def hunt_repeated_yaml_keys(data):\n    \"\"\"Parses yaml and returns a list of repeated variables and\n       the line on which they occur\n    \"\"\"", "output": "\n    repeated_keys = []\n    for key in data.keys():\n        if key in repeated_keys:\n            repeated_keys.append(key)\n        else:\n            repeated_keys.append(key)\n    return repeated_keys\n\n\ndef hunt_repeated_yaml_keys_in_file(file_path):\n    \"\"\"Parses yaml file and returns a list of repeated variables and\n       the line on which they occur\n    \"\"\"\n    with open(file_path, 'r') as stream:\n        try:\n            data", "solution": "\n    loader = yaml.Loader(data)\n\n    def compose_node(parent, index):\n        # the line number where the previous token has ended (plus empty lines)\n        line = loader.line\n        node = Composer.compose_node(loader, parent, index)\n        node.__line__ = line + 1\n        return node\n\n    def construct_mapping(node, deep=False):\n        mapping = dict()\n        errors = dict()\n        for key_node, value_node in node.value:\n            key = key_node.value\n            if key in mapping:\n                if key in errors:\n                    errors[key].append(key_node.__line__)\n                else:\n                    errors[key] = [mapping[key], key_node.__line__]\n\n            mapping[key] = key_node.__line__\n\n        return errors\n\n    loader.compose_node = compose_node\n    loader.construct_mapping = construct_mapping\n    data = loader.get_single_data()\n    return data"}
{"prompt": "def deserialize_dataframe(reader, data_type_id):\n    \"\"\"\n    Deserialize a dataframe.\n\n    Parameters\n    ----------\n    reader : file\n        File-like object to read from. Must be opened in binary mode.\n    data_type_id : dict\n        Serialization format of the raw data.\n        See the azureml.DataTypeIds class for constants.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Dataframe object.\n    \"\"\"", "output": "\n    if data_type_id == azureml.DataTypeIds.PANDAS_DF_CSV:\n        return pandas.read_csv(reader)\n    elif data_type_id == azureml.DataTypeIds.PANDAS_DF_JSON:\n        return pandas.read_json(reader)\n    elif data_type_id == azureml.DataTypeIds.PANDAS_DF_PARQUET:\n        return pandas.read_parquet(reader)\n    elif data_type_id == azureml.DataTypeIds.PANDAS", "solution": "\n    _not_none('reader', reader)\n    _not_none_or_empty('data_type_id', data_type_id)\n\n    serializer = _SERIALIZERS.get(data_type_id)\n    if serializer is None:\n        raise UnsupportedDatasetTypeError(data_type_id)\n    return serializer[1](reader=reader)"}
{"prompt": "def asin(wave):\n    r\"\"\"\n    Return the arc sine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.asin\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": "math.asin.\n    .. [exobj_eng.get_sphinx_autodoc()]\n    .. [exobj_eng.get_sphinx_autodoc()]\n    .. [[[end]]]\n    \"\"\"\n    return wave.copy(func=np.arcsin)\n\n\ndef acos(wave):\n    r\"\"\"\n    Return the arc cosine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng", "solution": "\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"asin\", \"rad\", np.arcsin)"}
{"prompt": "def _on_parent_event_dependent(self):\n        \"\"\"\n        After parrent statement become event dependent\n        propagate event dependency flag to child statements\n        \"\"\"", "output": "\n        if self.parent_statement.event_dependent:\n            self.event_dependent = True\n\n    def _on_parent_event_independent(self):\n        \"\"\"\n        After parrent statement become event independent\n        propagate event independence flag to child statements\n        \"\"\"\n        if not self.parent_statement.event_dependent:\n            self.event_dependent = False\n\n    def _on_parent_event_independent_and_dependent(self):\n        \"\"\"\n        After parrent statement become event independent and dependent\n        propagate event independence and dependency", "solution": "\n        if not self._is_completly_event_dependent:\n            self._is_completly_event_dependent = True\n            for stm in self._iter_stms():\n                stm._on_parent_event_dependent()"}
{"prompt": "def allclose(a, b):\n    \"\"\"\n    Test that a and b are close and match in shape.\n\n    Parameters\n    ----------\n    a : ndarray\n        First array to check\n\n    b : ndarray\n        First array to check\n    \"\"\"", "output": "\n    return np.allclose(a, b) and a.shape == b.shape\n\n\ndef assert_allclose(a, b, rtol=1e-7, atol=0):\n    \"\"\"\n    Test that a and b are close and match in shape.\n\n    Parameters\n    ----------\n    a : ndarray\n        First array to check\n\n    b : ndarray\n        First array to check\n\n    rtol : float\n        Relative tolerance\n\n    atol : float\n        Absolute tolerance\n   ", "solution": "\n    from numpy import allclose\n    return (a.shape == b.shape) and allclose(a, b)"}
{"prompt": "def signFix(val, width):\n    \"\"\"\n    Convert negative int to positive int which has same bits set\n    \"\"\"", "output": "\n    if val < 0:\n        val = val + (1 << width)\n    return val\n\n\ndef signFix_int(val, width):\n    \"\"\"\n    Convert negative int to positive int which has same bits set\n    \"\"\"\n    if val < 0:\n        val = val + (1 << width)\n    return val\n\n\ndef signFix_float(val, width):\n    \"\"\"\n    Convert negative int to positive int which has same bits set\n    \"\"\"\n    if val < 0:\n        val = val + (1", "solution": "\n    if val > 0:\n        msb = 1 << (width - 1)\n        if val & msb:\n            val -= mask(width) + 1\n    return val"}
{"prompt": "def list(self):\n        \"\"\"\n        Get all current labels\n\n        :return: The Logentries API response\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        return self.get(self.url)\n\n    def create(self, name):\n        \"\"\"\n        Create a new label\n\n        :param name: The name of the label\n        :type name: str\n\n        :return: The Logentries API response\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        return self.post(self.url, {'name': name})\n\n", "solution": "\n        return self._post(\n            request='list',\n            uri=ApiUri.TAGS.value,\n        ).get('tags')"}
{"prompt": "def add_menu_action(self, menu, action, defer_shortcut=False):\n        \"\"\"Add action to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will avoid conflict with shortcuts already bound to the\n        widgets themselves.\n        \"\"\"", "output": "\n        menu.addAction(action)\n        if defer_shortcut:\n            action.setShortcutContext(QtCore.Qt.WidgetWithChildrenShortcut)\n        self.addAction(action)\n\n    def add_menu_actions(self, menu, actions, defer_shortcut=False):\n        \"\"\"Add actions to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will", "solution": "\n        menu.addAction(action)\n        self.addAction(action)\n\n        if defer_shortcut:\n            action.setShortcutContext(QtCore.Qt.WidgetShortcut)"}
{"prompt": "def from_string(bnf: str, entry=None, *optional_inherit) -> Grammar:\n    \"\"\"\n    Create a Grammar from a string\n    \"\"\"", "output": "\n    if entry is None:\n        entry = bnf\n    return Grammar(bnf, entry, *optional_inherit)\n\n\ndef from_file(filename: str, entry=None, *optional_inherit) -> Grammar:\n    \"\"\"\n    Create a Grammar from a file\n    \"\"\"\n    if entry is None:\n        entry = filename\n    return Grammar(filename, entry, *optional_inherit)\n\n\ndef from_grammar(grammar: Grammar, entry=None, *optional_inherit) -> Grammar", "solution": "\n    inherit = [Grammar] + list(optional_inherit)\n    scope = {'grammar': bnf, 'entry': entry}\n    return build_grammar(tuple(inherit), scope)"}
{"prompt": "def parse_statement(self):\n    \"\"\"Parse a single statement.\n\n    Returns:\n      Either a `BindingStatement`, `ImportStatement`, `IncludeStatement`, or\n      `None` if no more statements can be parsed (EOF reached).\n    \"\"\"", "output": "\n    # TODO: This is a bit of a hack. We should probably have a more\n    # sophisticated way of parsing statements.\n    if self.peek_token().type == TokenType.EOF:\n      return None\n    if self.peek_token().type == TokenType.IDENTIFIER:\n      if self.peek_token().value == 'import':\n        return self.parse_import_statement()\n      if self.peek_token().value == 'include':\n        return self.parse_include_statement()\n     ", "solution": "\n    self._skip_whitespace_and_comments()\n    if self._current_token.kind == tokenize.ENDMARKER:\n      return None\n\n    # Save off location, but ignore char_num for any statement-level errors.\n    stmt_loc = self._current_location(ignore_char_num=True)\n    binding_key_or_keyword = self._parse_selector()\n    statement = None\n    if self._current_token.value != '=':\n      if binding_key_or_keyword == 'import':\n        module = self._parse_selector(scoped=False)\n        statement = ImportStatement(module, stmt_loc)\n      elif binding_key_or_keyword == 'include':\n        str_loc = self._current_location()\n        success, filename = self._maybe_parse_basic_type()\n        if not success or not isinstance(filename, str):\n          self._raise_syntax_error('Expected file path as string.', str_loc)\n        statement = IncludeStatement(filename, stmt_loc)\n      else:\n        self._raise_syntax_error(\"Expected '='.\")\n    else:  # We saw an '='.\n      self._advance_one_token()\n      value = self.parse_value()\n      scope, selector, arg_name = parse_binding_key(binding_key_or_keyword)\n      statement = BindingStatement(scope, selector, arg_name, value, stmt_loc)\n\n    assert statement, 'Internal parsing error.'\n\n    if (self._current_token.kind != tokenize.NEWLINE and\n        self._current_token.kind != tokenize.ENDMARKER):\n      self._raise_syntax_error('Expected newline.')\n    elif self._current_token.kind == tokenize.NEWLINE:\n      self._advance_one_token()\n\n    return statement"}
{"prompt": "def add_xpaths_to_stream_item(si):\n    \"\"\"Mutably tag tokens with xpath offsets.\n\n    Given some stream item, this will tag all tokens from all taggings\n    in the document that contain character offsets. Note that some\n    tokens may not have computable xpath offsets, so an xpath offset\n    for those tokens will not be set. (See the documentation and\n    comments for ``char_offsets_to_xpaths`` for what it means for a\n    token to have a computable xpath.)\n\n    If a token can have its xpath offset computed, it is added to its\n    set of offsets with a ``OffsetType.XPATH_CHARS`` key.\n    \"\"\"", "output": "   :param si: The stream item to tag.\n    :type si: :class:`stream_item.StreamItem`\n    :returns: The stream item with tokens tagged with xpath offsets.\n    :rtype: :class:`stream_item.StreamItem`\n    \"\"\"\n    for tagging in si.taggings:\n        for token in tagging.tokens:\n            if token.char_offsets is not None:\n                token.xpath_offsets = char_offsets_to_xpaths(\n                    token.char_offsets", "solution": "\n    def sentences_to_xpaths(sentences):\n        tokens = sentences_to_char_tokens(sentences)\n        offsets = char_tokens_to_char_offsets(tokens)\n        return char_offsets_to_xpaths(html, offsets)\n\n    def xprange_to_offset(xprange):\n        return Offset(type=OffsetType.XPATH_CHARS,\n                      first=xprange.start_offset, length=0,\n                      xpath=xprange.start_xpath,\n                      content_form='clean_html', value=None,\n                      xpath_end=xprange.end_xpath,\n                      xpath_end_offset=xprange.end_offset)\n\n    html = unicode(si.body.clean_html, 'utf-8')\n    for sentences in si.body.sentences.itervalues():\n        tokens = sentences_to_char_tokens(sentences)\n        for token, xprange in izip(tokens, sentences_to_xpaths(sentences)):\n            if xprange is None:\n                continue\n            offset = xprange_to_offset(xprange)\n            token.offsets[OffsetType.XPATH_CHARS] = offset"}
{"prompt": "def fill_parameters(self, path, blocks, exclude_free_params=False, check_parameters=False):\n        \"\"\"\n        Load parameters from file to fill all blocks sequentially.\n        :type blocks: list of deepy.layers.Block\n        \"\"\"", "output": "\n        if not os.path.exists(path):\n            raise IOError(\"File not found: {}\".format(path))\n        with open(path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                if line.startswith('['):\n                    block_name = line[1:-1]\n                    block = self.get_block(block_name)\n                    if block is None:\n                        raise ValueError(\"Block not found", "solution": "\n        if not os.path.exists(path):\n            raise Exception(\"model {} does not exist\".format(path))\n        # Decide which parameters to load\n        normal_params = sum([nn.parameters for nn in blocks], [])\n        all_params = sum([nn.all_parameters for nn in blocks], [])\n        # Load parameters\n        if path.endswith(\".gz\"):\n            opener = gzip.open if path.lower().endswith('.gz') else open\n            handle = opener(path, 'rb')\n            saved_params = pickle.load(handle)\n            handle.close()\n            # Write parameters\n            if len(all_params) != len(saved_params):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(saved_params)))\n            for target, source in zip(all_params, saved_params):\n                if not exclude_free_params or target not in normal_params:\n                    target.set_value(source)\n        elif path.endswith(\".npz\"):\n            arrs = np.load(path)\n            # Write parameters\n            if len(all_params) != len(arrs.keys()):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(arrs.keys())))\n            for target, idx in zip(all_params, range(len(arrs.keys()))):\n                if not exclude_free_params or target not in normal_params:\n                    source = arrs['arr_%d' % idx]\n                    target.set_value(source)\n        else:\n            raise Exception(\"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path)"}
{"prompt": "def get_endpoint_path(self, endpoint_id):\n    \"\"\"return the first fullpath to a folder in the endpoint based on\n       expanding the user's home from the globus config file. This\n       function is fragile but I don't see any other way to do it.\n    \n       Parameters\n       ==========\n       endpoint_id: the endpoint id to look up the path for\n\n    \"\"\"", "output": "\n    endpoint_info = self.get_endpoint(endpoint_id)\n    if endpoint_info is None:\n        return None\n    else:\n        return endpoint_info['path']\n\ndef get_endpoint_path_from_name(self, endpoint_name):\n    \"\"\"return the first fullpath to a folder in the endpoint based on\n       expanding the user's home from the globus config file. This\n       function is fragile but I don't see any other way to do it.\n    \n       Parameters\n       ==========\n       endpoint_name", "solution": "\n    config = os.path.expanduser(\"~/.globusonline/lta/config-paths\")\n    if not os.path.exists(config):\n        bot.error('%s not found for a local Globus endpoint.')\n        sys.exit(1)\n\n    path = None\n\n    # Read in the config and get the root path\n\n    config = [x.split(',')[0] for x in read_file(config)]\n    for path in config:\n        if os.path.exists(path):\n            break\n\n    # If we don't have an existing path, exit\n\n    if path is None:\n        bot.error('No path was found for a local Globus endpoint.')\n        sys.exit(1)\n\n    return path"}
{"prompt": "def _request_activity_list(self, athlete):\n        \"\"\"Actually do the request for activity list\n        This call is slow and therefore this method is memory cached.\n\n        Keyword arguments:\n        athlete -- Full name of athlete\n        \"\"\"", "output": "\n        if athlete not in self.activity_list_cache:\n            self.activity_list_cache[athlete] = self._request_activity_list_raw(athlete)\n        return self.activity_list_cache[athlete]\n\n    def _request_activity_list_raw(self, athlete):\n        \"\"\"Actually do the request for activity list\n        This call is slow and therefore this method is memory cached.\n\n        Keyword arguments:\n        athlete -- Full name of athlete\n        \"\"\"\n        url = self.activity_list", "solution": "\n        response = self._get_request(self._athlete_endpoint(athlete))\n        response_buffer = StringIO(response.text)\n\n        activity_list = pd.read_csv(\n            filepath_or_buffer=response_buffer,\n            parse_dates={'datetime': ['date', 'time']},\n            sep=',\\s*',\n            engine='python'\n        )\n        activity_list.rename(columns=lambda x: x.lower(), inplace=True)\n        activity_list.rename(\n            columns=lambda x: '_' + x if x[0].isdigit() else x, inplace=True)\n\n        activity_list['has_hr'] = activity_list.average_heart_rate.map(bool)\n        activity_list['has_spd'] = activity_list.average_speed.map(bool)\n        activity_list['has_pwr'] = activity_list.average_power.map(bool)\n        activity_list['has_cad'] = activity_list.average_heart_rate.map(bool)\n        activity_list['data'] = pd.Series(dtype=np.dtype(\"object\"))\n        return activity_list"}
{"prompt": "def reconnectUnitSignalsToModel(synthesisedUnitOrIntf, modelCls):\n    \"\"\"\n    Reconnect model signals to unit to run simulation with simulation model\n    but use original unit interfaces for communication\n\n    :param synthesisedUnitOrIntf: interface where should be signals\n        replaced from signals from modelCls\n    :param modelCls: simulation model form where signals\n        for synthesisedUnitOrIntf should be taken\n    \"\"\"", "output": "\n    if isinstance(synthesisedUnitOrIntf, int):\n        synthesisedUnitOrIntf = modelCls.getUnit(synthesisedUnitOrIntf)\n    for signal in synthesisedUnitOrIntf.getSignals():\n        if signal.isInput():\n            signal.setValue(modelCls.getInputSignal(signal.getName()))\n        elif signal.isOutput():\n            signal.setValue(modelCls.getOutputSignal(signal.getName()))\n\n\ndef reconnectUnitSignalsToModelWithoutReset", "solution": "\n    obj = synthesisedUnitOrIntf\n    subInterfaces = obj._interfaces\n\n\n    if subInterfaces:\n        for intf in subInterfaces:\n            # proxies are destroyed on original interfaces and only proxies on\n            # array items will remain\n            reconnectUnitSignalsToModel(intf, modelCls)\n    else:\n        # reconnect signal from model\n        s = synthesisedUnitOrIntf\n        s._sigInside = getattr(modelCls, s._sigInside.name)"}
{"prompt": "def lines_without_stdlib(self):\n        \"\"\"Filters code from standard library from self.lines.\"\"\"", "output": "\n        return [line for line in self.lines if not line.startswith('from ')]\n\n    def lines_without_tests(self):\n        \"\"\"Filters code from tests from self.lines.\"\"\"\n        return [line for line in self.lines if not line.startswith('import ')]\n\n    def lines_without_comments(self):\n        \"\"\"Filters code from comments from self.lines.\"\"\"\n        return [line for line in self.lines if not line.startswith('#')]\n\n    def lines_without_docstrings(self):\n", "solution": "\n        prev_line = None\n        current_module_path = inspect.getabsfile(inspect.currentframe())\n        for module_path, lineno, runtime in self.lines:\n            module_abspath = os.path.abspath(module_path)\n            if not prev_line:\n                prev_line = [module_abspath, lineno, runtime]\n            else:\n                if (not check_standard_dir(module_path) and\n                        module_abspath != current_module_path):\n                    yield prev_line\n                    prev_line = [module_abspath, lineno, runtime]\n                else:\n                    prev_line[2] += runtime\n        yield prev_line"}
{"prompt": "def from_ascii(path, seperator=None, names=True, skip_lines=0, skip_after=0, **kwargs):\n    \"\"\"\n    Create an in memory DataFrame from an ascii file (whitespace seperated by default).\n\n    >>> ds = vx.from_ascii(\"table.asc\")\n    >>> ds = vx.from_ascii(\"table.csv\", seperator=\",\", names=[\"x\", \"y\", \"z\"])\n\n    :param path: file path\n    :param seperator: value seperator, by default whitespace, use \",\" for comma seperated values.\n    :param names: If True, the first line is used for the column names, otherwise provide a list of strings with names\n    :param skip_lines: skip lines at the start of the file\n    :param skip_after: skip lines at the end of the file\n    :param kwargs:\n    :rtype: DataFrame\n    \"\"\"", "output": ": seperator for whitespace seperated files\n    :param names: column names\n    :param skip_lines: skip lines at the start of the file\n    :param skip_after: skip lines at the end of the file\n    :param kwargs:\n    :return: DataFrame\n    \"\"\"\n    if seperator is None:\n        seperator = \" \"\n    with open(path, \"r\") as f:\n        lines = f.readlines()\n    if skip_lines > 0:\n        lines = lines[skip_lines:]\n   ", "solution": "\n\n    import vaex.ext.readcol as rc\n    ds = vaex.dataframe.DataFrameArrays(path)\n    if names not in [True, False]:\n        namelist = names\n        names = False\n    else:\n        namelist = None\n    data = rc.readcol(path, fsep=seperator, asdict=namelist is None, names=names, skipline=skip_lines, skipafter=skip_after, **kwargs)\n    if namelist:\n        for name, array in zip(namelist, data.T):\n            ds.add_column(name, array)\n    else:\n        for name, array in data.items():\n            ds.add_column(name, array)\n    return ds"}
{"prompt": "def ndtr(x, name=\"ndtr\"):\n  \"\"\"Normal distribution function.\n\n  Returns the area under the Gaussian probability density function, integrated\n  from minus infinity to x:\n\n  ```\n                    1       / x\n     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt\n                sqrt(2 pi)  /-inf\n\n              = 0.5 (1 + erf(x / sqrt(2)))\n              = 0.5 erfc(x / sqrt(2))\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtr\").\n\n  Returns:\n    ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"", "output": "\n  ```\n\n  Args:\n    x: Floating-point `Tensor`. Must be broadcastable with `a`.\n    name: Python `str`. The name prefixed to the ops created by this function.\n      Default value: `None` (i.e., 'ndtr').\n\n  Returns:\n    norm_cdf: `Tensor` of the same shape as `x`.\n  \"\"\"\n  with tf.name_scope(name or \"ndtr\"):\n    x = tf.convert_to_tensor(x, name=\"x\")", "solution": "\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"x.dtype=%s is not handled, see docstring for supported types.\"\n          % x.dtype)\n    return _ndtr(x)"}
{"prompt": "def _all_dirs(base_path):\n        \"\"\"\n        Return all dirs in base_path, relative to base_path\n        \"\"\"", "output": "\n        return [os.path.relpath(x[0], base_path) for x in os.walk(base_path)]\n\n    def _all_files(base_path):\n        \"\"\"\n        Return all files in base_path, relative to base_path\n        \"\"\"\n        return [os.path.relpath(x[0], base_path) for x in os.walk(base_path)]\n\n    def _all_files_in_dir(base_path, dir_name):\n        \"\"\"\n        Return all files in dir_name,", "solution": "\n        for root, dirs, files in os.walk(base_path, followlinks=True):\n            for dir in dirs:\n                yield os.path.relpath(os.path.join(root, dir), base_path)"}
{"prompt": "def dump_encoding( file, encoding_name, encoding_list ):\n  \"\"\"dump a given encoding\"\"\"", "output": "\n  print( \"Encoding: %s\" % encoding_name )\n  print( \"  %s\" % encoding_list )\n  print( \"\" )\n\ndef dump_encodings( file, encodings ):\n  \"\"\"dump a list of encodings\"\"\"\n  for encoding_name in encodings:\n    encoding_list = encodings[ encoding_name ]\n    dump_encoding( file, encoding_name, encoding_list )\n\ndef dump_encoding_list( file, encoding_list ):\n  \"\"\"dump a list of enc", "solution": "\n\n  write = file.write\n  write( \"  /* the following are indices into the SID name table */\\n\" )\n  write( \"  static const unsigned short  \" + encoding_name +\n         \"[\" + repr( len( encoding_list ) ) + \"] =\\n\" )\n  write( \"  {\\n\" )\n\n  line  = \"    \"\n  comma = \"\"\n  col   = 0\n  for value in encoding_list:\n    line += comma\n    line += \"%3d\" % value\n    comma = \",\"\n    col  += 1\n    if col == 16:\n      col = 0\n      comma = \",\\n    \"\n\n  write( line + \"\\n  };\\n\\n\\n\" )"}
{"prompt": "def _set_kernel_manager(self, kernel_manager):\n        \"\"\" Disconnect from the current kernel manager (if any) and set a new\n            kernel manager.\n        \"\"\"", "output": "\n        if self._kernel_manager is not None:\n            self._kernel_manager.shutdown_kernel()\n        self._kernel_manager = kernel_manager\n        self._kernel_manager.start_kernel()\n        self._kernel_manager.kernel.gui_name = 'qt'\n        self._kernel_manager.kernel.gui_version = __version__\n        self._kernel_manager.kernel.protocol_version = '5.0'\n        self._kernel_manager.kernel.banner = 'Jupyter Qt Console'\n        self._kernel_", "solution": "\n        # Disconnect the old kernel manager, if necessary.\n        old_manager = self._kernel_manager\n        if old_manager is not None:\n            old_manager.started_kernel.disconnect(self._started_kernel)\n            old_manager.started_channels.disconnect(self._started_channels)\n            old_manager.stopped_channels.disconnect(self._stopped_channels)\n\n            # Disconnect the old kernel manager's channels.\n            old_manager.sub_channel.message_received.disconnect(self._dispatch)\n            old_manager.shell_channel.message_received.disconnect(self._dispatch)\n            old_manager.stdin_channel.message_received.disconnect(self._dispatch)\n            old_manager.hb_channel.kernel_died.disconnect(\n                self._handle_kernel_died)\n\n            # Handle the case where the old kernel manager is still listening.\n            if old_manager.channels_running:\n                self._stopped_channels()\n\n        # Set the new kernel manager.\n        self._kernel_manager = kernel_manager\n        if kernel_manager is None:\n            return\n\n        # Connect the new kernel manager.\n        kernel_manager.started_kernel.connect(self._started_kernel)\n        kernel_manager.started_channels.connect(self._started_channels)\n        kernel_manager.stopped_channels.connect(self._stopped_channels)\n\n        # Connect the new kernel manager's channels.\n        kernel_manager.sub_channel.message_received.connect(self._dispatch)\n        kernel_manager.shell_channel.message_received.connect(self._dispatch)\n        kernel_manager.stdin_channel.message_received.connect(self._dispatch)\n        kernel_manager.hb_channel.kernel_died.connect(self._handle_kernel_died)\n\n        # Handle the case where the kernel manager started channels before\n        # we connected.\n        if kernel_manager.channels_running:\n            self._started_channels()"}
{"prompt": "def subscribe_to_trades(self, pair, **kwargs):\n        \"\"\"Subscribe to the passed pair's trades channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"", "output": "\n        return self.ws.subscribe_to_trades(pair, **kwargs)\n\n    def subscribe_to_orderbook(self, pair, **kwargs):\n        \"\"\"Subscribe to the passed pair's orderbook channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"\n        return self.ws.subscribe_to_orderbook(pair, **kwargs)\n\n    def subscribe_to_orderbook_diffs(self, pair, **kwargs):\n        \"\"\"Sub", "solution": "\n        identifier = ('trades', pair)\n        self._subscribe('trades', identifier, symbol=pair, **kwargs)"}
{"prompt": "def interact(self, display_banner=None):\n        \"\"\"Closely emulate the interactive Python console.\"\"\"", "output": "\n        if display_banner is None:\n            display_banner = self.display_banner\n        if display_banner:\n            self.write(self.banner)\n        more = 0\n        while 1:\n            try:\n                if more:\n                    prompt = '> '\n                else:\n                    prompt = '>>> '\n                try:\n                    line = self.raw_input(prompt)\n                except EOFError:\n                    self.write('\\n')\n                    break\n                else:\n                ", "solution": "\n\n        # batch run -> do not interact\n        if self.exit_now:\n            return\n\n        if display_banner is None:\n            display_banner = self.display_banner\n\n        if isinstance(display_banner, basestring):\n            self.show_banner(display_banner)\n        elif display_banner:\n            self.show_banner()\n\n        more = False\n\n        if self.has_readline:\n            self.readline_startup_hook(self.pre_readline)\n            hlen_b4_cell = self.readline.get_current_history_length()\n        else:\n            hlen_b4_cell = 0\n        # exit_now is set by a call to %Exit or %Quit, through the\n        # ask_exit callback.\n\n        while not self.exit_now:\n            self.hooks.pre_prompt_hook()\n            if more:\n                try:\n                    prompt = self.prompt_manager.render('in2')\n                except:\n                    self.showtraceback()\n                if self.autoindent:\n                    self.rl_do_indent = True\n\n            else:\n                try:\n                    prompt = self.separate_in + self.prompt_manager.render('in')\n                except:\n                    self.showtraceback()\n            try:\n                line = self.raw_input(prompt)\n                if self.exit_now:\n                    # quick exit on sys.std[in|out] close\n                    break\n                if self.autoindent:\n                    self.rl_do_indent = False\n\n            except KeyboardInterrupt:\n                #double-guard against keyboardinterrupts during kbdint handling\n                try:\n                    self.write('\\nKeyboardInterrupt\\n')\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n                    more = False\n                except KeyboardInterrupt:\n                    pass\n            except EOFError:\n                if self.autoindent:\n                    self.rl_do_indent = False\n                    if self.has_readline:\n                        self.readline_startup_hook(None)\n                self.write('\\n')\n                self.exit()\n            except bdb.BdbQuit:\n                warn('The Python debugger has exited with a BdbQuit exception.\\n'\n                     'Because of how pdb handles the stack, it is impossible\\n'\n                     'for IPython to properly format this particular exception.\\n'\n                     'IPython will resume normal operation.')\n            except:\n                # exceptions here are VERY RARE, but they can be triggered\n                # asynchronously by signal handlers, for example.\n                self.showtraceback()\n            else:\n                self.input_splitter.push(line)\n                more = self.input_splitter.push_accepts_more()\n                if (self.SyntaxTB.last_syntax_error and\n                    self.autoedit_syntax):\n                    self.edit_syntax_error()\n                if not more:\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    self.run_cell(source_raw, store_history=True)\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n\n        # Turn off the exit flag, so the mainloop can be restarted if desired\n        self.exit_now = False"}
{"prompt": "async def add(self, dialog: Dialog):\n        \"\"\"\n        Adds a new dialog to the set and returns the added dialog.\n        :param dialog: The dialog to add.\n        \"\"\"", "output": "\n        self.dialogs.append(dialog)\n        return dialog\n\n    def get_dialog(self, name: str) -> Dialog:\n        \"\"\"\n        Returns the dialog with the given name.\n        :param name: The name of the dialog.\n        \"\"\"\n        for dialog in self.dialogs:\n            if dialog.name == name:\n                return dialog\n        raise ValueError(\"Dialog not found\")\n\n    def get_dialogs(self) -> List[Dialog]:\n        \"\"\"\n        Returns the dialogs in the set.\n        \"\"\"\n        return", "solution": "\n        if dialog is None or not isinstance(dialog, Dialog):\n            raise TypeError('DialogSet.add(): dialog cannot be None and must be a Dialog or derived class.')\n\n        if dialog.id in self._dialogs:\n            raise TypeError(\"DialogSet.add(): A dialog with an id of '%s' already added.\" % dialog.id)\n\n        # dialog.telemetry_client = this._telemetry_client;\n        self._dialogs[dialog.id] = dialog\n\n        return self"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Request one batch of events from Skype, calling :meth:`onEvent` with each event in turn.\n\n        Subclasses may override this method to alter loop functionality.\n        \"\"\"", "output": "\n        self.onEvent(self.skype.ReadEvent())\n\n    def onEvent(self, event):\n        \"\"\"\n        Called with each event from Skype.\n\n        Subclasses may override this method to alter event handling.\n        \"\"\"\n        if event.EventType == SKYPEEVENTTYPE.USER_STATE_CHANGED:\n            if event.EventObject.UserState == SKYPEUSERSTATE.ONLINE:\n                self.onUserOnline(event.EventObject.Handle)\n            elif event.EventObject.UserState == SKYPEUSER", "solution": "\n        try:\n            events = self.getEvents()\n        except requests.ConnectionError:\n            return\n        for event in events:\n            self.onEvent(event)\n            if self.autoAck:\n                event.ack()"}
{"prompt": "def _maybe_validate_perm(perm, validate_args, name=None):\n  \"\"\"Checks that `perm` is valid.\"\"\"", "output": "\n  with tf.name_scope(name, 'maybe_validate_perm', [perm]):\n    if validate_args:\n      assertions = [\n          tf.assert_rank(perm, 2, message='perm must be a matrix.'),\n          tf.assert_equal(\n              tf.shape(perm)[0], tf.shape(perm)[1],\n              message='perm must be a square matrix.')\n      ]\n      with tf.control_dependencies(assertions):\n        return tf.identity(perm)\n    else:\n      return perm\n", "solution": "\n  with tf.name_scope(name or 'maybe_validate_perm'):\n    assertions = []\n    if not dtype_util.is_integer(perm.dtype):\n      raise TypeError('`perm` must be integer type')\n\n    msg = '`perm` must be a vector.'\n    if tensorshape_util.rank(perm.shape) is not None:\n      if tensorshape_util.rank(perm.shape) != 1:\n        raise ValueError(\n            msg[:-1] +\n            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(perm, 1, message=msg)]\n\n    perm_ = tf.get_static_value(perm)\n    msg = '`perm` must be a valid permutation vector.'\n    if perm_ is not None:\n      if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)):\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_equal(\n              tf.sort(perm), tf.range(tf.size(input=perm)), message=msg)\n      ]\n\n    return assertions"}
{"prompt": "def create_cookie(self, delete=None):\n        \"\"\"\n        Creates the value for ``Set-Cookie`` HTTP header.\n\n        :param bool delete:\n            If ``True`` the cookie value will be ``deleted`` and the\n            Expires value will be ``Thu, 01-Jan-1970 00:00:01 GMT``.\n\n        \"\"\"", "output": "\n        if delete:\n            self.value = ''\n            self.expires = 'Thu, 01-Jan-1970 00:00:01 GMT'\n        return '%s=%s; path=%s; domain=%s; expires=%s' % (\n            self.name,\n            self.value,\n            self.path,\n            self.domain,\n            self.expires,\n        )\n\n    def __str__(self):\n        return self.create_cookie()\n\n    def __repr__(", "solution": "\n        value = 'deleted' if delete else self._serialize(self.data)\n        split_url = parse.urlsplit(self.adapter.url)\n        domain = split_url.netloc.split(':')[0]\n\n        # Work-around for issue #11, failure of WebKit-based browsers to accept\n        # cookies set as part of a redirect response in some circumstances.\n        if '.' not in domain:\n            template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}'\n        else:\n            template = ('{name}={value}; Domain={domain}; Path={path}; '\n                        'HttpOnly{secure}{expires}')\n\n        return template.format(\n            name=self.name,\n            value=value,\n            domain=domain,\n            path=split_url.path,\n            secure='; Secure' if self.secure else '',\n            expires='; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else ''\n        )"}
{"prompt": "def derivative(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the numerical derivative of a waveform's dependent variable vector.\n\n    The method used is the `backwards differences\n    <https://en.wikipedia.org/wiki/\n    Finite_difference#Forward.2C_backward.2C_and_central_differences>`_ method\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.derivative\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"", "output": ":param indep_min: Minimum independent variable value\n    :type  indep_min: float\n\n    :param indep_max: Maximum independent variable value\n    :type  indep_max: float\n\n    :return: Dependent variable vector derivative\n    :rtype: numpy.ndarray\n    \"\"\"\n    if indep_min is None:\n        indep_min = wave.indep_min\n    if indep_max is None:\n        indep_max = wave.indep_max\n\n    indep_step = (indep_max - indep_min)", "solution": "\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    delta_indep = np.diff(ret._indep_vector)\n    delta_dep = np.diff(ret._dep_vector)\n    delta_indep = np.concatenate((np.array([delta_indep[0]]), delta_indep))\n    delta_dep = np.concatenate((np.array([delta_dep[0]]), delta_dep))\n    ret._dep_vector = np.divide(delta_dep, delta_indep)\n    ret.dep_name = \"derivative({0})\".format(ret._dep_name)\n    ret.dep_units = _build_units(ret.indep_units, ret.dep_units, \"/\")\n    return ret"}
{"prompt": "def publish_to_target(self, target_arn, message):\n        \"\"\"\n        Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str\n        \"\"\"", "output": "\n        if not target_arn:\n            raise ValueError(\"target_arn is required\")\n        if not message:\n            raise ValueError(\"message is required\")\n\n        self.client.publish(\n            TargetArn=target_arn,\n            Message=message\n        )\n\n    def publish_to_topic(self, topic_arn, message):\n        \"\"\"\n        Publish a message to a topic.\n\n        :param topic_arn: the topic arn\n        :type topic_arn: str\n        :param message: the default message you want to send", "solution": "\n\n        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )"}
{"prompt": "def has_delete_permission(self, request, obj=None):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_delete_permission, but also passes the obj parameter in.\n        \"\"\"", "output": "\n        opts = self.opts\n        codename = get_permission_codename('delete', opts)\n        return request.user.has_perm(\"%s.%s\" % (opts.app_label, codename))\n\n    def has_change_permission(self, request, obj=None):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_change_permission, but also passes the obj parameter in.\n        \"\"\"\n        opts = self.opts\n        codename = get_permission_codename('change", "solution": "\n        if settings.TREE_EDITOR_OBJECT_PERMISSIONS:\n            opts = self.opts\n            r = request.user.has_perm(opts.app_label + '.' + opts.get_delete_permission(), obj)\n        else:\n            r = True\n\n        return r and super(TreeEditor, self).has_delete_permission(request, obj)"}
{"prompt": "def _clean(self, rmConnetions=True, lockNonExternal=True):\n        \"\"\"\n        Remove all signals from this interface (used after unit is synthesized\n        and its parent is connecting its interface to this unit)\n        \"\"\"", "output": "\n        for signal in self.signals:\n            if rmConnetions:\n                if signal.isExternal:\n                    signal.remove()\n            else:\n                if signal.isExternal:\n                    signal.remove()\n                else:\n                    signal.remove()\n\n    def _connect(self, other, name=None):\n        \"\"\"\n        Connect this interface to another interface\n        \"\"\"\n        if not isinstance(other, Interface):\n            raise TypeError(\"Cannot connect to non-interface\")\n        if name is None:\n            name = self.", "solution": "\n\n        if self._interfaces:\n            for i in self._interfaces:\n                i._clean(rmConnetions=rmConnetions,\n                         lockNonExternal=lockNonExternal)\n        else:\n            self._sigInside = self._sig\n            del self._sig\n\n        if lockNonExternal and not self._isExtern:\n            self._isAccessible = False"}
{"prompt": "def bytesize(self, byteorder='@'):\n        \"\"\"Compute the byte size after serialization.\n\n        Args:\n            byteorder (str, optional): This is byte order of the serialized data. Use one\n                of the `byte order characters\n                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:\n                ``@``, ``=``, ``<``, ``>``, and ``!``.\n                Default is ``@`` -- the native order.\n\n        Returns:\n            int: Size in number of bytes after serialization.\n        \"\"\"", "output": ":\n            int: The number of bytes needed to serialize the object.\n\n        \"\"\"\n        return struct.calcsize(self._struct_format(byteorder))\n\n    def serialize(self, byteorder='@'):\n        \"\"\"Serialize the object.\n\n        Args:\n            byteorder (str, optional): This is byte order of the serialized data. Use one\n                of the `byte order characters\n                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:\n                ``@", "solution": "\n        # Use 8 bytes to store the seed integer\n        seed_size = struct.calcsize(byteorder+'q')\n        # Use 4 bytes to store the number of hash values\n        length_size = struct.calcsize(byteorder+'i')\n        # Use 4 bytes to store each hash value as we are using the lower 32 bit\n        hashvalue_size = struct.calcsize(byteorder+'I')\n        return seed_size + length_size + len(self) * hashvalue_size"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a FTP connection object\n        \"\"\"", "output": "\n        return self.conn\n\n    def get_file(self, file_path, local_path):\n        \"\"\"\n        Downloads a file from the FTP server\n        \"\"\"\n        with open(local_path, 'wb') as f:\n            self.conn.retrbinary('RETR ' + file_path, f.write)\n\n    def put_file(self, file_path, local_path):\n        \"\"\"\n        Uploads a file to the FTP server\n        \"\"\"\n        with open(local_path, 'rb') as f", "solution": "\n        if self.conn is None:\n            params = self.get_connection(self.ftp_conn_id)\n            pasv = params.extra_dejson.get(\"passive\", True)\n            self.conn = ftplib.FTP(params.host, params.login, params.password)\n            self.conn.set_pasv(pasv)\n\n        return self.conn"}
{"prompt": "def commonprefix(items):\n    \"\"\"Get common prefix for completions\n\n    Return the longest common prefix of a list of strings, but with special\n    treatment of escape characters that might precede commands in IPython,\n    such as %magic functions. Used in tab completion.\n\n    For a more general function, see os.path.commonprefix\n    \"\"\"", "output": "\n    if not items:\n        return ''\n    if not isinstance(items[0], basestring):\n        items = [str(item) for item in items]\n    common = items[0]\n    for item in items[1:]:\n        for i in range(min(len(common), len(item))):\n            if common[i] != item[i]:\n                common = common[:i]\n                break\n        else:\n            common = common[:i+1]\n    return common\n\n\ndef _ipython_completer(self", "solution": "\n    # the last item will always have the least leading % symbol\n    # min / max are first/last in alphabetical order\n    first_match  = ESCAPE_RE.match(min(items))\n    last_match  = ESCAPE_RE.match(max(items))\n    # common suffix is (common prefix of reversed items) reversed\n    if first_match and last_match:\n        prefix = os.path.commonprefix((first_match.group(0)[::-1], last_match.group(0)[::-1]))[::-1]\n    else:\n        prefix = ''\n\n    items = [s.lstrip(ESCAPE_CHARS) for s in items]\n    return prefix+os.path.commonprefix(items)"}
{"prompt": "def _graph_wrap(func, graph):\n    \"\"\"Constructs function encapsulated in the graph.\"\"\"", "output": "\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return wrapper\n\n\ndef _get_graph(name):\n    \"\"\"Returns graph by name.\"\"\"\n    if name not in _GRAPHS:\n        raise ValueError('Graph with name {} is not registered.'.format(name))\n    return _GRAPHS[name]\n\n\ndef _register_graph(name, graph):\n    \"\"\"Registers graph.\"\"\"", "solution": "\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return _wrapped"}
{"prompt": "def avail_archs(self):\n        \"\"\" Initialize the dictionary of architectures for disassembling via capstone\"\"\"", "output": "\n        self.archs = {\n            'x86': capstone.CS_ARCH_X86,\n            'x86_64': capstone.CS_ARCH_X86,\n            'arm': capstone.CS_ARCH_ARM,\n            'arm64': capstone.CS_ARCH_ARM64,\n            'mips': capstone.CS_ARCH_MIPS,\n            'mips64': capstone.CS_ARCH_MIPS,\n            'ppc", "solution": "\n\n        return {\n            ARM32:   (CS_ARCH_ARM,   CS_MODE_ARM),\n            ARM64:   (CS_ARCH_ARM64, CS_MODE_LITTLE_ENDIAN),\n            ARM_TB:  (CS_ARCH_ARM,   CS_MODE_THUMB),\n            MIPS32:  (CS_ARCH_MIPS,  CS_MODE_MIPS32),\n            MIPS64:  (CS_ARCH_MIPS,  CS_MODE_MIPS64),\n            SPARC32: (CS_ARCH_SPARC, CS_MODE_BIG_ENDIAN),\n            SPARC64: (CS_ARCH_SPARC, CS_MODE_V9),\n            SYSTEMZ: (CS_ARCH_SYSZ,  CS_MODE_BIG_ENDIAN),\n            X86_16:  (CS_ARCH_X86,   CS_MODE_16),\n            X86_32:  (CS_ARCH_X86,   CS_MODE_32),\n            X86_64:  (CS_ARCH_X86,   CS_MODE_64),\n        }"}
{"prompt": "def html_to_xhtml(html):\n    \"\"\"Convert all tags in an HTML tree to XHTML by moving them to the\n    XHTML namespace.\n    \"\"\"", "output": "\n    def _move_to_xhtml(tag):\n        if tag.namespace == 'http://www.w3.org/1999/xhtml':\n            return\n        tag.namespace = 'http://www.w3.org/1999/xhtml'\n\n    for elem in html.iter():\n        _move_to_xhtml(elem)\n\n    return html\n\n\ndef html_to_xhtml_str(html):\n    \"\"\"Convert all tags in an HTML tree to XHTML by moving them to the\n    XHTML namespace.\n   ", "solution": "\n    try:\n        html = html.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    for el in html.iter(etree.Element):\n        tag = el.tag\n        if tag[0] != '{':\n            el.tag = prefix + tag"}
{"prompt": "def p_gate_op_2(self, program):\n        \"\"\"\n        gate_op : id id_list ';'\n        \"\"\"", "output": "\n        program.append(('gate', program[-1], program[-2]))\n        del program[-2:]\n\n    def p_gate_op_3(self, program):\n        \"\"\"\n        gate_op : id id_list ',' id_list ';'\n        \"\"\"\n        program.append(('gate', program[-1], program[-2]))\n        del program[-2:]\n\n    def p_id_list(self, program):\n        \"\"\"\n        id_list : id\n        id_list : id_list ',' id\n        \"\"\"\n       ", "solution": "\n        program[0] = node.CustomUnitary([program[1], program[2]])\n        # To verify:\n        # 1. id is declared as a gate in global scope\n        # 2. everything in the id_list is declared as a bit in local scope\n        self.verify_as_gate(program[1], program[2])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def insert_and_get(self, **fields):\n        \"\"\"Creates a new record in the database and then gets\n        the entire row.\n\n        This allows specifying custom conflict behavior using .on_conflict().\n        If no special behavior was specified, this uses the normal Django create(..)\n\n        Arguments:\n            fields:\n                The fields of the row to create.\n\n        Returns:\n            The model instance representing the row that was created.\n        \"\"\"", "output": "\n        if self.on_conflict is not None:\n            return self.on_conflict.insert_and_get(**fields)\n        else:\n            return self.model.objects.create(**fields)\n\n    def insert_and_get_or_update(self, **fields):\n        \"\"\"Creates a new record in the database and then gets\n        the entire row.\n\n        This allows specifying custom conflict behavior using .on_conflict().\n        If no special behavior was specified, this uses the normal Django create(..)\n\n        Arguments", "solution": "\n\n        if not self.conflict_target and not self.conflict_action:\n            # no special action required, use the standard Django create(..)\n            return super().create(**fields)\n\n        compiler = self._build_insert_compiler([fields])\n        rows = compiler.execute_sql(return_id=False)\n\n        columns = rows[0]\n\n        # get a list of columns that are officially part of the model and preserve the fact that the attribute name\n        # might be different than the database column name\n        model_columns = {}\n        for field in self.model._meta.local_concrete_fields:\n            model_columns[field.column] = field.attname\n\n        # strip out any columns/fields returned by the db that\n        # are not present in the model\n        model_init_fields = {}\n        for column_name, column_value in columns.items():\n            try:\n                model_init_fields[model_columns[column_name]] = column_value\n            except KeyError:\n                pass\n\n        return self.model(**model_init_fields)"}
{"prompt": "def strip_email_quotes(text):\n    \"\"\"Strip leading email quotation characters ('>').\n\n    Removes any combination of leading '>' interspersed with whitespace that\n    appears *identically* in all lines of the input text.\n\n    Parameters\n    ----------\n    text : str\n\n    Examples\n    --------\n\n    Simple uses::\n\n        In [2]: strip_email_quotes('> > text')\n        Out[2]: 'text'\n\n        In [3]: strip_email_quotes('> > text\\\\n> > more')\n        Out[3]: 'text\\\\nmore'\n\n    Note how only the common prefix that appears in all lines is stripped::\n\n        In [4]: strip_email_quotes('> > text\\\\n> > more\\\\n> more...')\n        Out[4]: '> text\\\\n> more\\\\nmore...'\n\n    So if any line has no quote marks ('>') , then none are stripped from any\n    of them ::\n    \n        In [5]: strip_email_quotes('> > text\\\\n> > more\\\\nlast different')\n        Out[5]: '> > text\\\\n> > more\\\\nlast different'\n    \"\"\"", "output": "('> > text\\n> > text')\n        Out[3]: 'text\\ntext'\n\n        In [4]: strip_email_quotes('> > text\\n> > text\\n> > text')\n        Out[4]: 'text\\ntext\\ntext'\n\n    The following are all equivalent::\n\n        In [5]: strip_email_quotes('> > text\\n> > text\\n> > text')\n        Out[5]: 'text\\ntext\\ntext'\n\n        In [6]: strip_email_", "solution": "\n    lines = text.splitlines()\n    matches = set()\n    for line in lines:\n        prefix = re.match(r'^(\\s*>[ >]*)', line)\n        if prefix:\n            matches.add(prefix.group(1))\n        else:\n            break\n    else:\n        prefix = long_substr(list(matches))\n        if prefix:\n            strip = len(prefix)\n            text = '\\n'.join([ ln[strip:] for ln in lines])\n    return text"}
{"prompt": "def delete_vm_image(self, vm_image_name, delete_vhd=False):\n        \"\"\"\n        Deletes the specified VM Image from the image repository that is\n        associated with the specified subscription.\n\n        vm_image_name:\n            The name of the image.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        \"\"\"", "output": "\n        if not vm_image_name:\n            raise ValueError('vm_image_name is required.')\n\n        vm_image = self.get_vm_image(vm_image_name)\n        if not vm_image:\n            raise WindowsAzureMissingResourceException(\n                'The specified vm image does not exist.')\n\n        if vm_image.is_custom_image:\n            raise WindowsAzureError(\n                'The specified vm image is a custom image and cannot be '\n                'deleted.')\n\n        if vm_image.is", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        path = self._get_vm_image_path(vm_image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)"}
{"prompt": "def _add_column_kwargs(self, kwargs, column):\n        \"\"\"Add keyword arguments to kwargs (in-place) based on the passed in\n        `Column <sqlalchemy.schema.Column>`.\n        \"\"\"", "output": "\n        if column.primary_key:\n            kwargs['primary_key'] = True\n        if column.nullable:\n            kwargs['nullable'] = True\n        if column.default is not None:\n            kwargs['default'] = column.default\n        if column.autoincrement:\n            kwargs['autoincrement'] = True\n        if column.doc:\n            kwargs['doc'] = column.doc\n\n    def _add_column_kwargs(self, kwargs, column):\n        \"\"\"Add keyword arguments to kwargs (in-place) based", "solution": "\n        if column.nullable:\n            kwargs[\"allow_none\"] = True\n        kwargs[\"required\"] = not column.nullable and not _has_default(column)\n\n        if hasattr(column.type, \"enums\"):\n            kwargs[\"validate\"].append(validate.OneOf(choices=column.type.enums))\n\n        # Add a length validator if a max length is set on the column\n        # Skip UUID columns\n        # (see https://github.com/marshmallow-code/marshmallow-sqlalchemy/issues/54)\n        if hasattr(column.type, \"length\"):\n            try:\n                python_type = column.type.python_type\n            except (AttributeError, NotImplementedError):\n                python_type = None\n            if not python_type or not issubclass(python_type, uuid.UUID):\n                kwargs[\"validate\"].append(validate.Length(max=column.type.length))\n\n        if hasattr(column.type, \"scale\"):\n            kwargs[\"places\"] = getattr(column.type, \"scale\", None)"}
{"prompt": "def check_for_old_config(ipython_dir=None):\n    \"\"\"Check for old config files, and present a warning if they exist.\n\n    A link to the docs of the new config is included in the message.\n\n    This should mitigate confusion with the transition to the new\n    config system in 0.11.\n    \"\"\"", "output": "\n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n    old_config_files = [\n        os.path.join(ipython_dir, 'ipy_user_conf.py'),\n        os.path.join(ipython_dir, 'ipy_user_conf.pyc'),\n        os.path.join(ipython_dir, 'ipy_user_conf.pyo'),\n        os.path.join(ipython_dir, 'ipy_user_conf.pyo.", "solution": "\n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n\n    old_configs = ['ipy_user_conf.py', 'ipythonrc', 'ipython_config.py']\n    warned = False\n    for cfg in old_configs:\n        f = os.path.join(ipython_dir, cfg)\n        if os.path.exists(f):\n            if filehash(f) == old_config_md5.get(cfg, ''):\n                os.unlink(f)\n            else:\n                warnings.warn(\"Found old IPython config file %r (modified by user)\"%f)\n                warned = True\n\n    if warned:\n        warnings.warn("}
{"prompt": "def getField(self, fld_name):\n        \"\"\" Return :class:`~ekmmeters.Field` content, scaled and formatted.\n\n        Args:\n            fld_name (str): A :class:`~ekmmeters.Field` value which is on your meter.\n\n        Returns:\n            str: String value (scaled if numeric) for the field.\n        \"\"\"", "output": "\n        if fld_name in self.fields:\n            fld = self.fields[fld_name]\n            if fld.numeric:\n                return fld.scaled_value\n            else:\n                return fld.value\n        else:\n            raise ValueError(\"Field not found: %s\" % fld_name)\n\n    def getFields(self):\n        \"\"\" Return a dictionary of :class:`~ekmmeters.Field` content, scaled and formatted.\n\n        Returns:\n            dict: A dictionary of :class:`~", "solution": "\n        result = \"\"\n        if fld_name in self.m_req:\n            result = self.m_req[fld_name][MeterData.StringValue]\n        else:\n            ekm_log(\"Requested nonexistent field: \" + fld_name)\n\n        return result"}
{"prompt": "def _mk_index(self):\n        \"\"\"create the index for this set\"\"\"", "output": "\n        self._index = {}\n        for i, item in enumerate(self):\n            self._index[item] = i\n\n    def __contains__(self, item):\n        \"\"\"check if item is in this set\"\"\"\n        return item in self._index\n\n    def __iter__(self):\n        \"\"\"iterate over the items in this set\"\"\"\n        return iter(self._items)\n\n    def __len__(self):\n        \"\"\"return the number of items in this set\"\"\"\n        return len(self._items)\n\n    def __getitem", "solution": "\n        index = defaultdict(list)\n        for line_no, line in enumerate(self._stripped_lines):\n            if line:\n                index[line].append(line_no)\n        return index"}
{"prompt": "def _find_blob_start(self):\n        \"\"\"Find first blob from selection.\n        \"\"\"", "output": "\n        for i in range(self.selection_start, len(self.data)):\n            if self.data[i] == 0:\n                return i\n        return len(self.data)\n\n    def _find_blob_end(self):\n        \"\"\"Find last blob from selection.\n        \"\"\"\n        for i in range(self.selection_end, 0, -1):\n            if self.data[i] == 0:\n                return i\n        return 0\n\n    def _find_blob_start_in_selection(self):", "solution": "\n\n        # Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n\n        # Check which is the blob time offset\n        blob_time_start = self.t_start\n\n        # Check which is the blob frequency offset (in channels)\n        blob_freq_start = self.chan_start_idx\n\n        blob_start = blob_time_start * self.n_channels_in_file + blob_freq_start\n\n        return blob_start"}
{"prompt": "def update(self, ava):\n        \"\"\"\n        Implements the dict.update() method\n        \"\"\"", "output": "\n        self.update_from_dict(ava)\n\n    def update_from_dict(self, ava):\n        \"\"\"\n        Update the object from a dictionary\n        \"\"\"\n        for key, value in ava.items():\n            if key in self.keys():\n                self[key] = value\n            else:\n                self.add_attribute(key, value)\n\n    def update_from_xml(self, xml):\n        \"\"\"\n        Update the object from an XML string\n        \"\"\"\n        self.update_from_dict(xmltodict", "solution": "\n        for key, val in ava.items():\n            self[key] = val"}
{"prompt": "def multiqc(institute_id, case_name):\n    \"\"\"Load multiqc report for the case.\"\"\"", "output": "\n    from snakemake.remote.S3 import S3RemoteProvider\n    from snakemake.remote.FTP import FTPRemoteProvider\n    from snakemake.remote.HTTP import HTTPRemoteProvider\n    from snakemake.remote.SFTP import SFTPRemoteProvider\n    from snakemake.remote.SCP import SCPRemoteProvider\n    from snakemake.remote.Local import LocalRemoteProvider\n    from snakemake.remote.Dropbox import DropboxRemoteProvider\n    from snakemake.remote.GoogleCloud import", "solution": "\n    data = controllers.multiqc(store, institute_id, case_name)\n    if data['case'].get('multiqc') is None:\n        return abort(404)\n    out_dir = os.path.abspath(os.path.dirname(data['case']['multiqc']))\n    filename = os.path.basename(data['case']['multiqc'])\n    return send_from_directory(out_dir, filename)"}
{"prompt": "def get_user_token(user, purpose, minutes_valid):\n    \"\"\"Return login token info for given user.\"\"\"", "output": "\n    token = user.get_token(purpose, minutes_valid)\n    return {\n        'token': token.decode('utf-8'),\n        'minutes_valid': minutes_valid,\n        'user_id': user.id,\n        'username': user.username,\n        'email': user.email,\n        'first_name': user.first_name,\n        'last_name': user.last_name,\n        'is_admin': user.is_admin,\n        'is_active': user.is_active,\n", "solution": "\n    token = ''.join(\n        dumps([\n            user.get_username(),\n            get_auth_hash(user, purpose),\n        ]).encode('base64').split('\\n')\n    )\n    return {\n        'id': get_meteor_id(user),\n        'token': token,\n        'tokenExpires': calc_expiry_time(minutes_valid),\n    }"}
{"prompt": "def write_meta(self):\n        \"\"\"ucds, descriptions and units are written as attributes in the hdf5 file, instead of a seperate file as\n         the default :func:`Dataset.write_meta`.\n         \"\"\"", "output": "\n        for key in self.meta.keys():\n            if key == 'ucds':\n                for ucd in self.meta[key].keys():\n                    self.h5_file.attrs[ucd] = self.meta[key][ucd]\n            elif key == 'descriptions':\n                for desc in self.meta[key].keys():\n                    self.h5_file.attrs[desc] = self.meta[key][desc]\n            elif key == 'units':\n                for unit in self.meta[key].keys", "solution": "\n        with h5py.File(self.filename, \"r+\") as h5file_output:\n            h5table_root = h5file_output[self.h5table_root_name]\n            if self.description is not None:\n                h5table_root.attrs[\"description\"] = self.description\n            h5columns = h5table_root if self._version == 1 else h5table_root['columns']\n            for column_name in self.columns.keys():\n                h5dataset = None\n                if column_name in h5columns:\n                    h5dataset = h5columns[column_name]\n                else:\n                    for group in h5columns.values():\n                        if 'type' in group.attrs:\n                            if group.attrs['type'] in ['csr_matrix']: \n                                for name, column in group.items():\n                                    if name == column_name:\n                                        h5dataset = column\n                if h5dataset is None:\n                    raise ValueError('column {} not found'.format(column_name))\n                for name, values in [(\"ucd\", self.ucds), (\"unit\", self.units), (\"description\", self.descriptions)]:\n                    if column_name in values:\n                        value = ensure_string(values[column_name], cast=True)\n                        h5dataset.attrs[name] = value\n                    else:\n                        if name in h5columns.attrs:\n                            del h5dataset.attrs[name]"}
{"prompt": "def rename(self, columns=None):\n        \"\"\"\n        Change names of columns in the frame.\n\n        Dict key is an index or name of the column whose name is to be set.\n        Dict value is the new name of the column.\n\n        :param columns: dict-like transformations to apply to the column names\n        \"\"\"", "output": "\n        if columns is None:\n            return self\n        if not isinstance(columns, dict):\n            raise TypeError(\"columns must be a dict-like object\")\n        if not all(isinstance(k, (int, str)) for k in columns.keys()):\n            raise TypeError(\"columns must be a dict-like object with keys of type int or str\")\n        if not all(isinstance(v, str) for v in columns.values()):\n            raise TypeError(\"columns must be a dict-like object with values of type str\")\n        if not all(is", "solution": "\n        assert_is_type(columns, None, dict)\n        new_names = self.names\n        ncols = self.ncols\n\n        for col, name in columns.items():\n            col_index = None\n            if is_type(col, int) and (-ncols <= col < ncols):\n                col_index = (col + ncols) % ncols  # handle negative indices\n            elif is_type(col, str) and col in self.names:\n                col_index = self.names.index(col)  # lookup the name\n\n            if col_index is not None:\n                new_names[col_index] = name\n\n        return self.set_names(new_names)"}
{"prompt": "def get_root_schema_dict(record):\n    \"\"\"Return a root jsonschema for a given record\n\n    A root schema includes the $schema attribute and all sub-record\n    schemas and definitions.\n    \"\"\"", "output": "\n    schema = record.schema\n    definitions = record.definitions\n    root_schema = {\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n        \"definitions\": definitions,\n        \"type\": \"object\",\n        \"properties\": {\n            \"record\": schema\n        },\n        \"required\": [\"record\"]\n    }\n    return root_schema\n\n\ndef get_record_schema_dict(record):\n    \"\"\"Return a record jsonschema for a given record\n\n    A record schema includes", "solution": "\n    state = SchemaGeneratorState()\n    schema = get_schema_dict(record, state)\n    del state.record_schemas[record._schema_name]\n    if state.record_schemas:\n        schema['definitions'] = dict()\n        for name, sub_schema in state.record_schemas.iteritems():\n            schema['definitions'][name] = sub_schema\n    return schema"}
{"prompt": "def _check_multi_statement_line(self, node, line):\n        \"\"\"Check for lines containing multiple statements.\"\"\"", "output": "\n        if node.type == 'function':\n            return\n        if node.type == 'class':\n            return\n        if node.type == 'if':\n            return\n        if node.type == 'for':\n            return\n        if node.type == 'while':\n            return\n        if node.type == 'try':\n            return\n        if node.type == 'with':\n            return\n        if node.type == 'except':\n            return\n        if node.type == 'finally':\n            return\n        if node.type == 'def':", "solution": "\n        # Do not warn about multiple nested context managers\n        # in with statements.\n        if isinstance(node, nodes.With):\n            return\n        # For try... except... finally..., the two nodes\n        # appear to be on the same line due to how the AST is built.\n        if isinstance(node, nodes.TryExcept) and isinstance(\n            node.parent, nodes.TryFinally\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.If)\n            and not node.parent.orelse\n            and self.config.single_line_if_stmt\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.ClassDef)\n            and len(node.parent.body) == 1\n            and self.config.single_line_class_stmt\n        ):\n            return\n        self.add_message(\"multiple-statements\", node=node)\n        self._visited_lines[line] = 2"}
{"prompt": "def _validate(value, optdict, name=\"\"):\n    \"\"\"return a validated value for an option according to its type\n\n    optional argument name is only used for error message formatting\n    \"\"\"", "output": "\n    if value is None:\n        return None\n    if isinstance(value, optdict[\"type\"]):\n        return value\n    if optdict[\"type\"] == bool:\n        if value in (\"True\", \"true\", \"1\"):\n            return True\n        elif value in (\"False\", \"false\", \"0\"):\n            return False\n        else:\n            raise ValueError(\"invalid boolean value %r\" % value)\n    if optdict[\"type\"] == int:\n        try:\n            return int(value)\n        except ValueError:\n            raise", "solution": "\n    try:\n        _type = optdict[\"type\"]\n    except KeyError:\n        # FIXME\n        return value\n    return _call_validator(_type, optdict, name, value)"}
{"prompt": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"", "output": "_response': False}\n        :type extra_options: dict\n        :return: the response from the request\n        :rtype: requests.Response\n        \"\"\"\n        # Set the default headers\n        headers = headers or {}\n        headers.update({'Content-Type': 'application/json'})\n\n        # Set the default options\n        options = {'check_response': True}\n        options.update(extra_options or {})\n\n        # Set the default data\n        data = data or {}\n\n        # Set the default method\n        method = 'GET'", "solution": "\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)"}
{"prompt": "def print_annotation(self):\n        \"\"\"Print annotation \"key: value\" pairs to standard output.\"\"\"", "output": "\n        for key, value in self.items():\n            print(f\"{key}: {value}\")\n\n    def __str__(self):\n        \"\"\"Return a string representation of the annotation.\"\"\"\n        return self.print_annotation.__str__()\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the annotation.\"\"\"\n        return self.print_annotation.__repr__()\n\n    def __getitem__(self, key):\n        \"\"\"Return the value of the annotation with the given key.\"\"\"\n        return self.annotations[key]", "solution": "\n        for path, ann in self.annotation.items():\n            print(\"{}: {}\".format(path, ann['value']))"}
{"prompt": "def info(self, **kwargs):\n        \"\"\"\n        Get the detailed information about a particular credit record. This is \n        currently only supported with the new credit model found in TV. These \n        ids can be found from any TV credit response as well as the tv_credits \n        and combined_credits methods for people.\n\n        The episodes object returns a list of episodes and are generally going \n        to be guest stars. The season array will return a list of season \n        numbers.  Season credits are credits that were marked with the \n        \"add to every season\" option in the editing interface and are \n        assumed to be \"season regulars\".\n\n        Args:\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "itional\" flag.\n\n        :param credit_id: The id of the credit record.\n        :type credit_id: int\n        :returns: A dictionary containing the following information:\n\n            - **cast** (*list*): A list of dictionaries containing the following information:\n\n                - **cast_id** (*int*): The id of the cast member.\n                - **character** (*str*): The character name.\n                - **credit_id** (*str*): The id of the credit.\n                - **gender** (*int*): The gender of", "solution": "\n        path = self._get_credit_id_path('info')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def parse_options():\n    \"\"\"\n    Commandline options arguments parsing.\n\n    :return: parsed commandline arguments.\n    :rtype: optparse.Values.\n    \"\"\"", "output": "\n    parser = optparse.OptionParser()\n    parser.add_option(\"-d\", \"--debug\",\n                      action=\"store_true\", dest=\"debug\", default=False,\n                      help=\"Enable debug mode.\")\n    parser.add_option(\"-v\", \"--verbose\",\n                      action=\"store_true\", dest=\"verbose\", default=False,\n                      help=\"Enable verbose mode.\")\n    parser.add_option(\"-f\", \"--file\",\n                      action=\"store\", dest=\"file\", default=None,\n                      help", "solution": "\n\n    version = \"%%prog {version}\".format(version=__version__)\n    parser = OptionParser(version=version)\n    parser.add_option(\n        \"-s\", \"--server\", action=\"store\", dest=\"server\",\n        type=\"string\", default=\"\", metavar=\"SERVER\",\n        help=\"server name, IP address or unix socket path\"\n    )\n    parser.add_option(\n        \"-p\", \"--port\", action=\"store\", type=\"int\", dest=\"port\",\n        default=9001, metavar=\"PORT\", help=\"port number\"\n    )\n    parser.add_option(\n        \"-P\", \"--programs\", action=\"store\", dest=\"programs\", type=\"string\", default=\"\",\n        metavar=\"PROGRAMS\", help=\"comma separated programs list, or empty for all programs in supervisord response\"\n    )\n    parser.add_option(\n        \"-u\", \"--username\", action=\"store\", dest=\"username\", type=\"string\", default=\"\",\n        metavar=\"USERNAME\", help=\"supervisord user\"\n    )\n    parser.add_option(\n        \"-S\", \"--password\", action=\"store\", dest=\"password\", type=\"string\", default=\"\",\n        metavar=\"PASSWORD\", help=\"supervisord user password\"\n    )\n    parser.add_option(\n        \"-q\", \"--quiet\", metavar=\"QUIET\", action=\"store_true\", default=False, dest=\"quiet\", help=\"be quiet\"\n    )\n    parser.add_option(\n        \"--stopped-state\", action=\"store\", dest=\"stopped_state\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_OK,\n        metavar=\"STOPPED_STATE\", help=\"stopped state\"\n    )\n    parser.add_option(\n        \"--network-errors-exit-code\", action=\"store\", dest=\"network_errors_exit_code\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_UNKNOWN,\n        metavar=\"NETWORK_ERRORS_EXIT_CODE\", help=\"network errors exit code\"\n    )\n\n    options = parser.parse_args(sys.argv)[0]\n    STATE2TEMPLATE[STATE_STOPPED] = options.stopped_state  # update stopped state value from command line argument\n\n    # check mandatory command line options supplied\n    if not options.server:\n        parser.error(\"Required server address option missing\")\n    if options.username and not options.password:\n        parser.error(\"Required supervisord user password\")\n\n    return options"}
{"prompt": "def _create_hstore_unique(self, model, field, keys):\n        \"\"\"Creates a UNIQUE constraint for the specified hstore keys.\"\"\"", "output": "\n        return self._create_hstore_index(model, field, keys, True)\n\n    def _create_hstore_index(self, model, field, keys, unique=False):\n        \"\"\"Creates an index for the specified hstore keys.\"\"\"\n        if not keys:\n            return\n\n        if isinstance(keys, basestring):\n            keys = [keys]\n\n        if not isinstance(keys, (list, tuple)):\n            raise ValueError('keys must be a list or tuple')\n\n        if not isinstance(field, H", "solution": "\n\n        name = self._unique_constraint_name(\n            model._meta.db_table, field, keys)\n        columns = [\n            '(%s->\\'%s\\')' % (field.column, key)\n            for key in keys\n        ]\n        sql = self.sql_hstore_unique_create.format(\n            name=self.quote_name(name),\n            table=self.quote_name(model._meta.db_table),\n            columns=','.join(columns)\n        )\n        self.execute(sql)"}
{"prompt": "def create_trace(\n        turn_activity: Activity,\n        name: str,\n        value: object = None,\n        value_type: str = None,\n        label: str = None,\n    ) -> Activity:\n        \"\"\"Creates a trace activity based on this activity.\n\n        :param turn_activity:\n        :type turn_activity: Activity\n        :param name: The value to assign to the trace activity's <see cref=\"Activity.name\"/> property.\n        :type name: str\n        :param value: The value to assign to the trace activity's <see cref=\"Activity.value\"/> property., defaults to None\n        :param value: object, optional\n        :param value_type: The value to assign to the trace activity's <see cref=\"Activity.value_type\"/> property, defaults to None\n        :param value_type: str, optional\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property, defaults to None\n        :param label: str, optional\n        :return: The created trace activity.\n        :rtype: Activity\n        \"\"\"", "output": "the trace activity's <see cref=\"Activity.value\"/> property.\n        :type value: object\n        :param value_type: The value to assign to the trace activity's <see cref=\"Activity.valueType\"/> property.\n        :type value_type: str\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property.\n        :type label: str\n        :return: A trace activity based on this activity.\n        :rtype: Activity\n        \"\"\"\n        trace_activity = Activity(\n", "solution": "\n\n        from_property = (\n            ChannelAccount(\n                id=turn_activity.recipient.id, name=turn_activity.recipient.name\n            )\n            if turn_activity.recipient is not None\n            else ChannelAccount()\n        )\n        if value_type is None and value is not None:\n            value_type = type(value).__name__\n\n        reply = Activity(\n            type=ActivityTypes.trace,\n            timestamp=datetime.utcnow(),\n            from_property=from_property,\n            recipient=ChannelAccount(\n                id=turn_activity.from_property.id, name=turn_activity.from_property.name\n            ),\n            reply_to_id=turn_activity.id,\n            service_url=turn_activity.service_url,\n            channel_id=turn_activity.channel_id,\n            conversation=ConversationAccount(\n                is_group=turn_activity.conversation.is_group,\n                id=turn_activity.conversation.id,\n                name=turn_activity.conversation.name,\n            ),\n            name=name,\n            label=label,\n            value_type=value_type,\n            value=value,\n        )\n        return reply"}
{"prompt": "def voucher_code(request):\n    \"\"\" A view *just* for entering a voucher form. \"\"\"", "output": "\n    return render(request, 'voucher/voucher_code.html')\n\n\ndef voucher_code_result(request):\n    \"\"\" A view *just* for displaying the result of a voucher form. \"\"\"\n    return render(request, 'voucher/voucher_code_result.html')\n\n\ndef voucher_code_form(request):\n    \"\"\" A view *just* for displaying the form for a voucher form. \"\"\"\n    return render(request, 'voucher/voucher_code_", "solution": "\n\n    VOUCHERS_FORM_PREFIX = \"vouchers\"\n\n    # Handle the voucher form *before* listing products.\n    # Products can change as vouchers are entered.\n    v = _handle_voucher(request, VOUCHERS_FORM_PREFIX)\n    voucher_form, voucher_handled = v\n\n    if voucher_handled:\n        messages.success(request, \"Your voucher code was accepted.\")\n        return redirect(\"dashboard\")\n\n    data = {\n        \"voucher_form\": voucher_form,\n    }\n\n    return render(request, \"registrasion/voucher_code.html\", data)"}
{"prompt": "def create(cls, name_value, name_type):\n        \"\"\"\n            Returns a Name object, populated with the given value and type\n        \"\"\"", "output": "\n        return cls(name_value, name_type)\n\n    def __init__(self, name_value, name_type):\n        \"\"\"\n            Initializes a Name object, populated with the given value and type\n        \"\"\"\n        self.value = name_value\n        self.type = name_type\n\n    def __str__(self):\n        \"\"\"\n            Returns a string representation of the Name object\n        \"\"\"\n        return self.value\n\n    def __repr__(self):\n        \"\"\"\n            Returns a string representation of the Name object\n        \"\"\"\n", "solution": "\n        if isinstance(name_value, Name.NameValue):\n            value = name_value\n        elif isinstance(name_value, str):\n            value = cls.NameValue(name_value)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_value'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_value', type(Name.NameValue),\n                                       type(name_value)))\n\n        if isinstance(name_type, Name.NameType):\n            n_type = name_type\n        elif isinstance(name_type, Enum):\n            n_type = cls.NameType(name_type)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_type'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_type', type(Name.NameType),\n                                       type(name_type)))\n\n        return Name(name_value=value,\n                    name_type=n_type)"}
{"prompt": "def loop_gtk(kernel):\n    \"\"\"Start the kernel, coordinating with the GTK event loop\"\"\"", "output": "\n    kernel.start()\n\n    # This is a bit of a hack.  We want to be able to run the kernel\n    # in a separate thread, but we also want to be able to run the\n    # kernel in a separate process.  The problem is that the GTK\n    # event loop is not thread-safe, and the process-level event loop\n    # is not thread-safe.  So we need to run the kernel in a thread\n    # and then run the event loop in a thread.  But then we need to\n    # be able to communicate between", "solution": "\n    from .gui.gtkembed import GTKEmbed\n\n    gtk_kernel = GTKEmbed(kernel)\n    gtk_kernel.start()"}
{"prompt": "def single_request_timeout(self, value):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"", "output": "\n        self._single_request_timeout = value\n\n    @property\n    def max_retries(self):\n        \"\"\"The maximum number of retries for a single HTTP REST API request.\"\"\"\n        return self._max_retries\n\n    @max_retries.setter\n    def max_retries(self, value):\n        \"\"\"The maximum number of retries for a single HTTP REST API request.\"\"\"\n        self._max_retries = value\n\n    @property\n    def retry_delay(self):\n        \"\"\"The delay (seconds) between", "solution": "\n        check_type(value, int)\n        assert value is None or value > 0\n        self._single_request_timeout = value"}
{"prompt": "def search(self, pattern=\"*\", raw=True, search_raw=True,\n                                                        output=False):\n        \"\"\"Search the database using unix glob-style matching (wildcards\n        * and ?).\n\n        Parameters\n        ----------\n        pattern : str\n          The wildcarded pattern to match when searching\n        search_raw : bool\n          If True, search the raw input, otherwise, the parsed input\n        raw, output : bool\n          See :meth:`get_range`\n\n        Returns\n        -------\n        Tuples as :meth:`get_range`\n        \"\"\"", "output": "ple[List[str], List[str]]\n          A tuple of two lists, the first containing the matching\n          input lines, and the second containing the matching output\n          lines.\n        \"\"\"\n        if search_raw:\n            search_input = self.raw_input\n            search_output = self.raw_output\n        else:\n            search_input = self.input\n            search_output = self.output\n\n        input_matches = []\n        output_matches = []\n        for i, line in enumerate(search_input):\n            if fnmatch.fn", "solution": "\n        tosearch = \"source_raw\" if search_raw else \"source\"\n        if output:\n            tosearch = \"history.\" + tosearch\n        self.writeout_cache()\n        return self._run_sql(\"WHERE %s GLOB ?\" % tosearch, (pattern,),\n                                    raw=raw, output=output)"}
{"prompt": "async def get_bots(self, limit, offset):\n        \"\"\"Gets an object of bots on DBL\"\"\"", "output": "\n        return await self.http.get_bots(limit, offset)\n\n    async def get_bot(self, bot_id):\n        \"\"\"Gets an object of a bot on DBL\"\"\"\n        return await self.http.get_bot(bot_id)\n\n    async def get_bot_stats(self, bot_id):\n        \"\"\"Gets an object of a bot's stats on DBL\"\"\"\n        return await self.http.get_bot_stats(bot_id)\n\n    async def get_bot_vote(self", "solution": "\n        if limit > 500:\n            limit = 50\n        return await self.request('GET', '{}/bots?limit={}&offset={}'.format(self.BASE, limit, offset))"}
{"prompt": "def group_transactions(self):\n        # type: () -> List[List[Transaction]]\n        \"\"\"\n        Groups transactions in the bundle by address.\n        \"\"\"", "output": "\n        transactions = self.transactions\n        grouped_transactions = []\n        for address in transactions:\n            grouped_transactions.append([])\n            for transaction in transactions[address]:\n                grouped_transactions[-1].append(transaction)\n        return grouped_transactions\n\n    def get_transaction_hashes(self):\n        # type: () -> List[bytes]\n        \"\"\"\n        Returns the hashes of all transactions in the bundle.\n        \"\"\"\n        return [transaction.hash for transaction in self.transactions.values()]\n\n    def get", "solution": "\n        groups = []\n\n        if self:\n            last_txn = self.tail_transaction\n            current_group = [last_txn]\n            for current_txn in self.transactions[1:]:\n                # Transactions are grouped by address, so as long as the\n                # address stays consistent from one transaction to\n                # another, we are still in the same group.\n                if current_txn.address == last_txn.address:\n                    current_group.append(current_txn)\n                else:\n                    groups.append(current_group)\n                    current_group = [current_txn]\n\n                last_txn = current_txn\n\n            if current_group:\n                groups.append(current_group)\n\n        return groups"}
{"prompt": "def authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"", "output": "\n\n        :param callback_uri: The URI to redirect to after authentication.\n        :param ax_attrs: The attributes to request from the service.\n        :returns: The URL to redirect to.\n        \"\"\"\n\n        # If we don't have a callback URI, we can't authenticate.\n        if callback_uri is None:\n            raise ValueError(\"No callback URI specified.\")\n\n        # If we don't have a client ID, we can't authenticate.\n        if self.client_id is None:\n            raise ValueError(\"No", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def write_byte_data(self, addr, cmd, val):\n        \"\"\"Write a byte of data to the specified cmd register of the device.\n        \"\"\"", "output": "\n        self._write_cmd(addr, cmd, val)\n\n    def write_word_data(self, addr, cmd, val):\n        \"\"\"Write a word of data to the specified cmd register of the device.\n        \"\"\"\n        self._write_cmd(addr, cmd, val)\n\n    def write_block_data(self, addr, cmd, val):\n        \"\"\"Write a block of data to the specified cmd register of the device.\n        \"\"\"\n        self._write_cmd(addr, cmd, val)\n\n    def read_byte_data(", "solution": "\n        assert self._device is not None, 'Bus must be opened before operations are made against it!'\n        # Construct a string of data to send with the command register and byte value.\n        data = bytearray(2)\n        data[0] = cmd & 0xFF\n        data[1] = val & 0xFF\n        # Send the data to the device.\n        self._select_device(addr)\n        self._device.write(data)"}
{"prompt": "def remove_client(self, client):\n        # type: (object) -> None\n        \"\"\"Remove the client from the users of the socket.\n\n        If there are no more clients for the socket, it\n        will close automatically.\n        \"\"\"", "output": "\n        self.users.remove(client)\n        if not self.users:\n            self.close()\n\n    def close(self):\n        # type: () -> None\n        \"\"\"Close the socket.\"\"\"\n        self.socket.close()\n\n    def send(self, message):\n        # type: (str) -> None\n        \"\"\"Send a message to the socket.\"\"\"\n        self.socket.send(message)\n\n    def receive(self):\n        # type: () -> str\n        \"\"\"Receive a message from the socket.\"\"\"\n        return self", "solution": "\n\n        try:\n            self._clients.remove(id(client))\n        except ValueError:\n            pass\n\n        if len(self._clients) < 1:\n            self.close()"}
{"prompt": "def send(self, data):\n        \"\"\"\n        Send encoded instructions to Guacamole guacd server.\n        \"\"\"", "output": "\n\n        # Write data to socket\n        self.socket.sendall(data)\n\n        # Read response\n        response = self.socket.recv(4096)\n\n        # Parse response\n        return self.parse_response(response)\n\n    def parse_response(self, response):\n        \"\"\"\n        Parse a response from the Guacamole guacd server.\n        \"\"\"\n\n        # Parse response\n        opcode = ord(response[0])\n        length = ord(response[1])\n        data = response[2:2", "solution": "\n        self.logger.debug('Sending data: %s' % data)\n        self.client.sendall(data.encode())"}
{"prompt": "def strsplit(self, pattern):\n        \"\"\"\n        Split the strings in the target column on the given regular expression pattern.\n\n        :param str pattern: The split pattern.\n        :returns: H2OFrame containing columns of the split strings.\n        \"\"\"", "output": "\n        return H2OFrame._expr(expr=ExprNode(\"strsplit\", self, pattern))\n\n    def str_detect(self, pattern):\n        \"\"\"\n        Detect the given regular expression pattern in the target column.\n\n        :param str pattern: The pattern to detect.\n        :returns: H2OFrame containing columns of the detected patterns.\n        \"\"\"\n        return H2OFrame._expr(expr=ExprNode(\"str_detect\", self, pattern))\n\n    def str_replace(self, pattern, replacement):\n        \"\"\"\n        Replace", "solution": "\n        fr = H2OFrame._expr(expr=ExprNode(\"strsplit\", self, pattern))\n        fr._ex._cache.nrows = self.nrow\n        return fr"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report gathered analysis data in human readable form.\"\"\"", "output": "\n        if self.data is None:\n            raise ValueError(\"No data to report.\")\n\n        if self.data.get(\"errors\"):\n            print(\"Errors:\", file=output_file)\n            for error in self.data[\"errors\"]:\n                print(error, file=output_file)\n            print(file=output_file)\n\n        print(\"Analysis of %s\" % self.data[\"filename\"], file=output_file)\n        print(\"--------------------------------------------------------------------------------\", file=output_file)\n        print(\"\", file=output", "solution": "\n        if self.verbose > 1:\n            with pprint_nosort():\n                pprint.pprint(self.results)\n\n        if self.verbose > 0:\n            print('Runtime (per repetition): {:.2g} s'.format(\n                self.results['Runtime (per repetition) [s]']),\n                file=output_file)\n        if self.verbose > 0:\n            print('Iterations per repetition: {!s}'.format(\n                self.results['Iterations per repetition']),\n                file=output_file)\n        print('Runtime (per cacheline update): {:.2f} cy/CL'.format(\n            self.results['Runtime (per cacheline update) [cy/CL]']),\n            file=output_file)\n        print('MEM volume (per repetition): {:.0f} Byte'.format(\n            self.results['MEM volume (per repetition) [B]']),\n            file=output_file)\n        print('Performance: {:.2f} MFLOP/s'.format(self.results['Performance [MFLOP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} MLUP/s'.format(self.results['Performance [MLUP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} It/s'.format(self.results['Performance [MIt/s]']),\n              file=output_file)\n        if self.verbose > 0:\n            print('MEM bandwidth: {:.2f} MByte/s'.format(self.results['MEM BW [MByte/s]']),\n                  file=output_file)\n        print('', file=output_file)\n\n        if not self.no_phenoecm:\n            print(\"Data Transfers:\")\n            print(\"{:^8} |\".format(\"cache\"), end='')\n            for metrics in self.results['data transfers'].values():\n                for metric_name in sorted(metrics):\n                    print(\" {:^14}\".format(metric_name), end='')\n                print()\n                break\n            for cache, metrics in sorted(self.results['data transfers'].items()):\n                print(\"{!s:^8} |\".format(cache), end='')\n                for k, v in sorted(metrics.items()):\n                    print(\" {!s:^14}\".format(v), end='')\n                print()\n            print()\n\n            print('Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | '\n                  '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL'.format(\n                **{k: float(v) for k, v in self.results['ECM'].items()}),\n                file=output_file)\n            print('T_OL assumes that two loads per cycle may be retiered, which is true for '\n                  '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, '\n                  'BDW, SKL and SKX, but it also depends on AGU availability.',\n                  file=output_file)"}
{"prompt": "def wait_for_kernel(self, timeout=None):\n        \"\"\"method to wait for a kernel to be ready\"\"\"", "output": "\n        if self.kernel is None:\n            raise RuntimeError(\"No kernel\")\n        if self.kernel.is_alive():\n            return\n        if timeout is None:\n            timeout = self.timeout\n        if timeout is None:\n            timeout = 10\n        start = time.time()\n        while time.time() - start < timeout:\n            if self.kernel.is_alive():\n                return\n            time.sleep(0.1)\n        raise RuntimeError(\"Kernel not alive\")\n\n    def wait_for_idle(", "solution": "\n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n                if timeout is not None and (time.time() - tic) > timeout:\n                    return False\n        return True"}
{"prompt": "def load(self, pkcs11dll_filename=None, *init_string):\n        \"\"\"\n        load a PKCS#11 library\n\n        :type pkcs11dll_filename: string\n        :param pkcs11dll_filename: the library name.\n          If this parameter is not set then the environment variable\n          `PYKCS11LIB` is used instead\n        :returns: a :class:`PyKCS11Lib` object\n        :raises: :class:`PyKCS11Error` (-1): when the load fails\n        \"\"\"", "output": ")\n        \"\"\"\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.environ.get('PYKCS11LIB')\n        if pkcs11dll_filename is None:\n            raise PyKCS11Error('PKCS#11 library filename not specified')\n        if not os.path.exists(pkcs11dll_filename):\n            raise PyKCS11Error('PKCS#11 library not found')\n        self._lib = CDLL(pkcs11dll", "solution": "\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.getenv(\"PYKCS11LIB\")\n            if pkcs11dll_filename is None:\n                raise PyKCS11Error(-1, \"No PKCS11 library specified (set PYKCS11LIB env variable)\")\n        rv = self.lib.Load(pkcs11dll_filename)\n        if rv == 0:\n            raise PyKCS11Error(-1, pkcs11dll_filename)"}
{"prompt": "def load_publickey(type, buffer):\n    \"\"\"\n    Load a public key from a buffer.\n\n    :param type: The file type (one of :data:`FILETYPE_PEM`,\n        :data:`FILETYPE_ASN1`).\n    :param buffer: The buffer the key is stored in.\n    :type buffer: A Python string object, either unicode or bytestring.\n    :return: The PKey object.\n    :rtype: :class:`PKey`\n    \"\"\"", "output": "\n    if type == FILETYPE_PEM:\n        return PEM_read_bio_PUBKEY(buffer)\n    elif type == FILETYPE_ASN1:\n        return ASN1_read_bio_PUBKEY(buffer)\n    else:\n        raise ValueError(\"Unknown key type\")\n\n\ndef load_privatekey(type, buffer):\n    \"\"\"\n    Load a private key from a buffer.\n\n    :param type: The file type (one of :data:`FILETYPE_PEM`,\n        :data:`FILETYPE_ASN", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        evp_pkey = _lib.PEM_read_bio_PUBKEY(\n            bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        evp_pkey = _lib.d2i_PUBKEY_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if evp_pkey == _ffi.NULL:\n        _raise_current_error()\n\n    pkey = PKey.__new__(PKey)\n    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)\n    pkey._only_public = True\n    return pkey"}
{"prompt": "def words(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"", "output": "\n        return self.words_dict.keys()\n\n    def __iter__(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"\n        return self.words_dict.keys()\n\n    def __getitem__(self, word):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043b\u043e\u0432, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445\u0441\u044f \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"\n        return self.words_dict[word]\n\n    def __len__(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e", "solution": "\n        branch, word, indexes = [self.root], [], [0]\n        letters_with_children = [self._get_children_and_letters(self.root)]\n        while len(branch) > 0:\n            if self.is_final(branch[-1]):\n                yield \"\".join(word)\n            while indexes[-1] == len(letters_with_children[-1]):\n                indexes.pop()\n                letters_with_children.pop()\n                branch.pop()\n                if len(indexes) == 0:\n                    raise StopIteration()\n                word.pop()\n            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n            indexes[-1] += 1\n            indexes.append(0)\n            word.append(next_letter)\n            branch.append(next_child)\n            letters_with_children.append(self._get_children_and_letters(branch[-1]))"}
{"prompt": "def parse_forensic_report(feedback_report, sample, msg_date,\n                          nameservers=None, dns_timeout=2.0,\n                          strip_attachment_payloads=False,\n                          parallel=False):\n    \"\"\"\n    Converts a DMARC forensic report and sample to a ``OrderedDict``\n\n    Args:\n        feedback_report (str): A message's feedback report as a string\n        sample (str): The RFC 822 headers or RFC 822 message sample\n        msg_date (str): The message's date header\n        nameservers (list): A list of one or more nameservers to use\n        (Cloudflare's public DNS resolvers by default)\n        dns_timeout (float): Sets the DNS timeout in seconds\n        strip_attachment_payloads (bool): Remove attachment payloads from\n        forensic report results\n        parallel (bool): Parallel processing\n\n    Returns:\n        OrderedDict: A parsed report and sample\n    \"\"\"", "output": ".1 headers and body\n        msg_date (datetime.datetime): The date the message was sent\n        nameservers (list): A list of nameservers to use when resolving\n            domain names\n        dns_timeout (float): The timeout to use when resolving domain names\n        strip_attachment_payloads (bool): Whether or not to strip the\n            payloads of attachments\n        parallel (bool): Whether or not to use parallel processing\n\n    Returns:\n        OrderedDict: A ``OrderedDict`` of the parsed report\n    \"\"\"\n    if not", "solution": "\n    delivery_results = [\"delivered\", \"spam\", \"policy\", \"reject\", \"other\"]\n\n    try:\n        parsed_report = OrderedDict()\n        report_values = feedback_report_regex.findall(feedback_report)\n        for report_value in report_values:\n            key = report_value[0].lower().replace(\"-\", \"_\")\n            parsed_report[key] = report_value[1]\n\n        if \"arrival_date\" not in parsed_report:\n            if msg_date is None:\n                raise InvalidForensicReport(\n                    \"Forensic sample is not a valid email\")\n            parsed_report[\"arrival_date\"] = msg_date.isoformat()\n\n        if \"version\" not in parsed_report:\n            parsed_report[\"version\"] = 1\n\n        if \"user_agent\" not in parsed_report:\n            parsed_report[\"user_agent\"] = None\n\n        if \"delivery_result\" not in parsed_report:\n            parsed_report[\"delivery_result\"] = None\n        else:\n            for delivery_result in delivery_results:\n                if delivery_result in parsed_report[\"delivery_result\"].lower():\n                    parsed_report[\"delivery_result\"] = delivery_result\n                    break\n        if parsed_report[\"delivery_result\"] not in delivery_results:\n            parsed_report[\"delivery_result\"] = \"other\"\n\n        arrival_utc = human_timestamp_to_datetime(\n            parsed_report[\"arrival_date\"], to_utc=True)\n        arrival_utc = arrival_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n        parsed_report[\"arrival_date_utc\"] = arrival_utc\n\n        ip_address = parsed_report[\"source_ip\"]\n        parsed_report_source = get_ip_address_info(ip_address,\n                                                   nameservers=nameservers,\n                                                   timeout=dns_timeout,\n                                                   parallel=parallel)\n        parsed_report[\"source\"] = parsed_report_source\n        del parsed_report[\"source_ip\"]\n\n        if \"identity_alignment\" not in parsed_report:\n            parsed_report[\"authentication_mechanisms\"] = []\n        elif parsed_report[\"identity_alignment\"] == \"none\":\n            parsed_report[\"authentication_mechanisms\"] = []\n            del parsed_report[\"identity_alignment\"]\n        else:\n            auth_mechanisms = parsed_report[\"identity_alignment\"]\n            auth_mechanisms = auth_mechanisms.split(\",\")\n            parsed_report[\"authentication_mechanisms\"] = auth_mechanisms\n            del parsed_report[\"identity_alignment\"]\n\n        if \"auth_failure\" not in parsed_report:\n            parsed_report[\"auth_failure\"] = \"dmarc\"\n        auth_failure = parsed_report[\"auth_failure\"].split(\",\")\n        parsed_report[\"auth_failure\"] = auth_failure\n\n        optional_fields = [\"original_envelope_id\", \"dkim_domain\",\n                           \"original_mail_from\", \"original_rcpt_to\"]\n        for optional_field in optional_fields:\n            if optional_field not in parsed_report:\n                parsed_report[optional_field] = None\n\n        parsed_sample = parse_email(\n            sample,\n            strip_attachment_payloads=strip_attachment_payloads)\n\n        if \"reported_domain\" not in parsed_report:\n            parsed_report[\"reported_domain\"] = parsed_sample[\"from\"][\"domain\"]\n\n        sample_headers_only = False\n        number_of_attachments = len(parsed_sample[\"attachments\"])\n        if number_of_attachments < 1 and parsed_sample[\"body\"] is None:\n            sample_headers_only = True\n        if sample_headers_only and parsed_sample[\"has_defects\"]:\n            del parsed_sample[\"defects\"]\n            del parsed_sample[\"defects_categories\"]\n            del parsed_sample[\"has_defects\"]\n        parsed_report[\"sample_headers_only\"] = sample_headers_only\n        parsed_report[\"sample\"] = sample\n        parsed_report[\"parsed_sample\"] = parsed_sample\n\n        return parsed_report\n\n    except KeyError as error:\n        raise InvalidForensicReport(\"Missing value: {0}\".format(\n            error.__str__()))\n\n    except Exception as error:\n        raise InvalidForensicReport(\n            \"Unexpected error: {0}\".format(error.__str__()))"}
{"prompt": "def map_peaks_to_image(peaks, r=4, vox_dims=(2, 2, 2), dims=(91, 109, 91),\n                       header=None):\n    \"\"\" Take a set of discrete foci (i.e., 2-D array of xyz coordinates)\n    and generate a corresponding image, convolving each focus with a\n    hard sphere of radius r.\"\"\"", "output": "\n    # Create a blank image\n    if header is None:\n        image = np.zeros(dims, dtype=np.float32)\n    else:\n        image = nib.Nifti1Image(np.zeros(dims, dtype=np.float32),\n                                affine=header.get_best_affine())\n\n    # Iterate over each focus\n    for x, y, z in peaks:\n        # Convolve each focus with a hard sphere\n        image.get_data()[x - r:", "solution": "\n    data = np.zeros(dims)\n    for p in peaks:\n        valid = get_sphere(p, r, vox_dims, dims)\n        valid = valid[:, ::-1]\n        data[tuple(valid.T)] = 1\n    return nifti1.Nifti1Image(data, None, header=header)"}
{"prompt": "def iter_parse(fiql_str):\n    \"\"\"Iterate through the FIQL string. Yield a tuple containing the\n    following FIQL components for each iteration:\n\n      - preamble: Any operator or opening/closing paranthesis preceding a\n        constraint or at the very end of the FIQL string.\n      - selector: The selector portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n      - comparison: The comparison portion of a FIQL constraint or ``None``\n        if yielding the last portion of the string.\n      - argument: The argument portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n\n    For usage see :func:`parse_str_to_expression`.\n\n    Args:\n        fiql_str (string): The FIQL formatted string we want to parse.\n\n    Yields:\n        tuple: Preamble, selector, comparison, argument.\n    \"\"\"", "output": "string.\n      - value: The value portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n      - postamble: Any operator or opening/closing paranthesis following a\n        constraint or at the very end of the FIQL string.\n\n    :param fiql_str: The FIQL string to parse.\n    :type fiql_str: str\n    \"\"\"\n    # The FIQL string is parsed in three stages:\n    #\n    # 1. The FIQL string is split into a list of tokens.", "solution": "\n    while len(fiql_str):\n        constraint_match = CONSTRAINT_COMP.split(fiql_str, 1)\n        if len(constraint_match) < 2:\n            yield (constraint_match[0], None, None, None)\n            break\n        yield (\n            constraint_match[0],\n            unquote_plus(constraint_match[1]),\n            constraint_match[4],\n            unquote_plus(constraint_match[6]) \\\n                    if constraint_match[6] else None\n        )\n        fiql_str = constraint_match[8]"}
{"prompt": "def shape(self):\n        \"\"\"Returns the shape of the data.\"\"\"", "output": "\n        return self.data.shape\n\n    def __len__(self):\n        \"\"\"Returns the length of the data.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Returns the data at index idx.\"\"\"\n        return self.data[idx]\n\n    def __setitem__(self, idx, value):\n        \"\"\"Sets the data at index idx to value.\"\"\"\n        self.data[idx] = value\n\n    def __delitem__(self, idx):\n        \"\"\"Deletes the", "solution": "\n        # TODO cache\n        first = self.first().shape\n        shape = self._rdd.map(lambda x: x.shape[0]).sum()\n        return (shape,) + first[1:]"}
{"prompt": "def _wait_job_completion(self):\n        \"\"\"Wait for the cache to be empty before resizing the pool.\"\"\"", "output": "\n        while self.cache_size > 0:\n            time.sleep(1)\n\n    def _resize_pool(self, new_size):\n        \"\"\"Resize the pool to the new size.\"\"\"\n        self.pool.resize(new_size)\n        self.pool.wait_workers()\n\n    def _resize_cache(self, new_size):\n        \"\"\"Resize the cache to the new size.\"\"\"\n        self.cache.resize(new_size)\n\n    def _resize_pool_and_cache(self, new_", "solution": "\n        # Issue a warning to the user about the bad effect of this usage.\n        if len(self._pending_work_items) > 0:\n            warnings.warn(\"Trying to resize an executor with running jobs: \"\n                          \"waiting for jobs completion before resizing.\",\n                          UserWarning)\n            mp.util.debug(\"Executor {} waiting for jobs completion before\"\n                          \" resizing\".format(self.executor_id))\n        # Wait for the completion of the jobs\n        while len(self._pending_work_items) > 0:\n            time.sleep(1e-3)"}
{"prompt": "def energy(self, state=None):\n        \"\"\"Calculates the length of the route.\"\"\"", "output": "\n        if state is None:\n            state = self.state\n        return np.sum(state.position ** 2)\n\n    def energy_gradient(self, state=None):\n        \"\"\"Calculates the gradient of the energy.\"\"\"\n        if state is None:\n            state = self.state\n        return 2 * state.position\n\n    def energy_hessian(self, state=None):\n        \"\"\"Calculates the hessian of the energy.\"\"\"\n        if state is None:\n            state = self.state\n        return np.diag", "solution": "\n        state = self.state if state is None else state\n        route = state\n        e = 0\n        if self.distance_matrix:\n            for i in range(len(route)):\n                e += self.distance_matrix[\"{},{}\".format(route[i-1], route[i])]\n        else:\n            for i in range(len(route)):\n                e += distance(self.cities[route[i-1]], self.cities[route[i]])\n        return e"}
{"prompt": "def recommendations(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"", "output": "\n        return self._get(\n            'movie/{0}/recommendations'.format(self.id),\n            language=kwargs.get('language'),\n            page=kwargs.get('page')\n        )\n\n    def reviews(self, **kwargs):\n        \"\"\"\n        Get the reviews for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON", "solution": "\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def get_clinvar_id(self, submission_id):\n        \"\"\"Returns the official Clinvar submission ID for a submission object\n\n            Args:\n                submission_id(str): submission_id(str) : id of the submission\n\n            Returns:\n                clinvar_subm_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n\n        \"\"\"", "output": "\n        clinvar_subm_id = None\n        if submission_id:\n            try:\n                clinvar_subm_id = self.get_clinvar_submission_id(submission_id)\n            except Exception as e:\n                logger.error(\"Error getting clinvar submission id for submission_id: %s\" % submission_id)\n                logger.error(e)\n        return clinvar_subm_id\n\n    def get_clinvar_submission_id(self, submission_id):\n        \"\"\"Returns the official", "solution": "\n        submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n        clinvar_subm_id = submission_obj.get('clinvar_subm_id') # This key does not exist if it was not previously provided by user\n        return clinvar_subm_id"}
{"prompt": "def dict_factory(self, cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            d[col[0]] = self._json_safe(row[idx])\n        return d\n\n    def _json_safe(self, value):\n        \"\"\" Convert value to a JSON safe type.\n\n        Args:\n            value (any): value to convert.\n\n        Returns:\n            any: JSON safe value.\n        \"\"\"\n        if isinstance(value, datetime.datetime):\n            return value.isoformat()\n        elif isinstance(value, datetime.date", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp:\n                d[col[0]] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":  # or name == Field.Meter_Time:\n                continue\n            if name not in self.m_all_fields:\n                continue\n            if (str(val) != \"None\") and ((val > 0) or (val < 0)):\n                d[name] = str(val)\n        return d"}
{"prompt": "def streams(self, url, **params):\n        \"\"\"Attempts to find a plugin and extract streams from the *url*.\n\n        *params* are passed to :func:`Plugin.streams`.\n\n        Raises :exc:`NoPluginError` if no plugin is found.\n        \"\"\"", "output": "\n        plugin = self.resolve(url)\n        return plugin.streams(url, **params)\n\n    def resolve(self, url):\n        \"\"\"Attempts to find a plugin that can handle the *url*.\n\n        Raises :exc:`NoPluginError` if no plugin is found.\n        \"\"\"\n        for plugin in self.plugins:\n            if plugin.can_handle_url(url):\n                return plugin\n        raise NoPluginError('No plugin can handle %s' % url)\n\n    def __iter__(self):\n        return", "solution": "\n\n        plugin = self.resolve_url(url)\n        return plugin.streams(**params)"}
{"prompt": "def map(self, func):\n        \"\"\"\n        Process all data with given function.\n        The scheme of function should be x,y -> x,y.\n        \"\"\"", "output": "\n        for i in range(self.size):\n            self.data[i] = func(self.data[i])\n\n    def __getitem__(self, key):\n        return self.data[key]\n\n    def __setitem__(self, key, value):\n        self.data[key] = value\n\n    def __len__(self):\n        return self.size\n\n    def __iter__(self):\n        for i in range(self.size):\n            yield self.data[i]\n\n    def __repr__(self):\n", "solution": "\n        if self._train_set:\n            self._train_set = map(func, self._train_set)\n        if self._valid_set:\n            self._valid_set = map(func, self._valid_set)\n        if self._test_set:\n            self._test_set = map(func, self._test_set)"}
{"prompt": "def positions_func(inputs, pad=0):\n    \"\"\"\n    A layer filling i-th column of a 2D tensor with\n    1+ln(1+i) when it contains a meaningful symbol\n    and with 0 when it contains PAD\n    \"\"\"", "output": "\n    return K.expand_dims(K.expand_dims(K.expand_dims(K.arange(inputs.shape[1]) + 1 + K.log(1 + K.expand_dims(K.expand_dims(K.expand_dims(K.arange(inputs.shape[2]), 0), 0), 0)), 0), 0) * K.cast(K.equal(inputs, pad), K.floatx()), 0)\n\n\ndef positions_embedding(inputs,", "solution": "\n    position_inputs = kb.cumsum(kb.ones_like(inputs, dtype=\"float32\"), axis=1)\n    position_inputs *= kb.cast(kb.not_equal(inputs, pad), \"float32\")\n    return kb.log(1.0 + position_inputs)"}
{"prompt": "def _with_loc(f: W) -> W:\n    \"\"\"Wrap a reader function in a decorator to supply line and column\n    information along with relevant forms.\"\"\"", "output": "\n    def wrapper(self: 'Reader', *args: Any, **kwargs: Any) -> Any:\n        try:\n            return f(self, *args, **kwargs)\n        except Exception as e:\n            raise type(e)(str(e) + ' at line ' + str(self.line) +\n                          ', column ' + str(self.column))\n    return wrapper\n\n\nclass Reader:\n    \"\"\"A reader for the Lisp language.\"\"\"\n\n    def __init__(self, source: str) -> None:\n        \"\"\"Initialize", "solution": "\n\n    @functools.wraps(f)\n    def with_lineno_and_col(ctx):\n        meta = lmap.map(\n            {READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col}\n        )\n        v = f(ctx)\n        try:\n            return v.with_meta(meta)  # type: ignore\n        except AttributeError:\n            return v\n\n    return cast(W, with_lineno_and_col)"}
{"prompt": "def p_creg_decl(self, program):\n        \"\"\"\n           creg_decl : CREG indexed_id\n        \"\"\"", "output": "\n        self.add_creg(program[2])\n\n    def p_gate_decl(self, program):\n        \"\"\"\n           gate_decl : GATE id '(' id_list ')' gate_body\n        \"\"\"\n        self.add_gate(program[2], program[4], program[6])\n\n    def p_gate_body(self, program):\n        \"\"\"\n           gate_body : '{' gate_body_item_list '}'\n        \"\"\"\n        return program[2]\n\n    def p_gate_body_item_", "solution": "\n        program[0] = node.Creg([program[2]])\n        if program[2].name in self.external_functions:\n            raise QasmError(\"CREG names cannot be reserved words. \"\n                            + \"Received '\" + program[2].name + \"'\")\n        if program[2].index == 0:\n            raise QasmError(\"CREG size must be positive\")\n        self.update_symtab(program[0])"}
{"prompt": "def order_verification(self, institute, case, user, link, variant):\n        \"\"\"Create an event for a variant verification for a variant\n        and an event for a variant verification for a case\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n\n        Returns:\n            updated_variant(dict)\n        \"\"\"", "output": "\n        # Create an event for the variant\n        event = self.event_objs.create_event(\n            user_id=user['_id'],\n            institute_id=institute['_id'],\n            case_id=case['_id'],\n            link=link,\n            link_type='verification',\n            link_text='Verification',\n            category='verification',\n            verb='verified',\n            variant_id=variant['_id'],\n            variant=variant,\n            date=datetime.datetime.utcnow(),\n           ", "solution": "\n        LOG.info(\"Creating event for ordering validation for variant\" \\\n                    \" {0}\".format(variant['display_name']))\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {'$set': {'sanger_ordered': True}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        LOG.info(\"Creating event for ordering sanger for case\" \\\n                    \" {0}\".format(case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n        return updated_variant"}
{"prompt": "def update_product_set(\n        self,\n        product_set,\n        location=None,\n        product_set_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator`\n        \"\"\"", "output": "\n        if not product_set_id:\n            product_set_id = product_set.name.split(\"/\")[-1]\n        if not location:\n            location = product_set.name.split(\"/\")[-3]\n        if not project_id:\n            project_id = product_set.name.split(\"/\")[-5]\n        return self.update_product_set_template(\n            product_set=product_set,\n            location=location,\n            product_set_id=product_set_id,\n            update_mask=update", "solution": "\n        client = self.get_conn()\n        product_set = self.product_set_name_determiner.get_entity_with_name(\n            product_set, product_set_id, location, project_id\n        )\n        self.log.info('Updating ProductSet: %s', product_set.name)\n        response = client.update_product_set(\n            product_set=product_set, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('ProductSet updated: %s', response.name if response else '')\n        self.log.debug('ProductSet updated:\\n%s', response)\n        return MessageToDict(response)"}
{"prompt": "def get_mic(self, message, qop_req=C.GSS_C_QOP_DEFAULT):\n        \"\"\"\n        Calculates a cryptographic message integrity code (MIC) over an application message, and\n        returns that MIC in a token. This is in contrast to :meth:`wrap` which calculates a MIC\n        over a message, optionally encrypts it and returns the original message and the MIC packed\n        into a single token. The peer application can then verify the MIC to ensure the associated\n        message has not been changed in transit.\n\n        :param message: The message to calculate a MIC for\n        :type message: bytes\n        :param qop_req: The quality of protection required. It is recommended to not change this\n            from the default as most GSSAPI implementations do not support it.\n        :returns: A MIC for the message calculated using this security context's cryptographic keys\n        :rtype: bytes\n        \"\"\"", "output": "it.\n\n        :param message: The message to calculate the MIC over.\n        :type message: bytes\n        :param qop_req: The quality of protection to use when calculating the MIC.\n        :type qop_req: int\n        :return: The MIC token.\n        :rtype: bytes\n        \"\"\"\n        mic_token = self._mic(message, qop_req)\n        return mic_token\n\n    def verify_mic(self, message, mic_token, qop_req=C.GSS_C_Q", "solution": "\n        if not (self.flags & C.GSS_C_INTEG_FLAG):\n            raise GSSException(\"No integrity protection negotiated.\")\n        if not (self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)):\n            raise GSSException(\"Protection not yet ready.\")\n\n        minor_status = ffi.new('OM_uint32[1]')\n        output_token_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer[0].length = len(message)\n        c_str_message = ffi.new('char[]', message)\n        message_buffer[0].value = c_str_message\n        retval = C.gss_get_mic(\n            minor_status,\n            self._ctx[0],\n            ffi.cast('gss_qop_t', qop_req),\n            message_buffer,\n            output_token_buffer\n        )\n        try:\n            if GSS_ERROR(retval):\n                if minor_status[0] and self.mech_type:\n                    raise _exception_for_status(retval, minor_status[0], self.mech_type)\n                else:\n                    raise _exception_for_status(retval, minor_status[0])\n\n            output_token = _buf_to_str(output_token_buffer[0])\n            return output_token\n        finally:\n            if output_token_buffer[0].length != 0:\n                C.gss_release_buffer(minor_status, output_token_buffer)"}
{"prompt": "def extractHolidayDate(self, setting_holiday):\n        \"\"\" Read a single holiday date from meter buffer.\n\n        Args:\n            setting_holiday (int):  Holiday from 0-19 or in range(Extents.Holidays)\n\n        Returns:\n            tuple: Holiday tuple, elements are strings.\n\n            =============== ======================\n            Holiday         Holiday 0-19 as string\n            Day             Day 1-31 as string\n            Month           Monty 1-12 as string\n            =============== ======================\n\n        \"\"\"", "output": "string\n            =============== ======================\n\n        \"\"\"\n        holiday = self.read_int(setting_holiday)\n        day = self.read_int(holiday + 1)\n        month = self.read_int(holiday + 2)\n        return (self.holiday_names[holiday], str(day), self.month_names[month])\n\n    def extractHoliday(self, setting_holiday):\n        \"\"\" Read a single holiday from meter buffer.\n\n        Args:\n            setting_holiday", "solution": "\n        ret = namedtuple(\"result\", [\"Holiday\", \"Month\", \"Day\"])\n        setting_holiday += 1\n        ret.Holiday = str(setting_holiday)\n\n        if (setting_holiday < 1) or (setting_holiday > Extents.Holidays):\n            ekm_log(\"Out of bounds:  holiday \" + str(setting_holiday))\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n\n        idxday = \"Holiday_\" + str(setting_holiday) + \"_Day\"\n        idxmon = \"Holiday_\" + str(setting_holiday) + \"_Mon\"\n        if idxmon not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        if idxday not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        ret.Day = self.m_hldy[idxday][MeterData.StringValue]\n        ret.Month = self.m_hldy[idxmon][MeterData.StringValue]\n        return ret"}
{"prompt": "def getMaxStmIdForStm(stm):\n    \"\"\"\n    Get maximum _instId from all assigments in statement\n    \"\"\"", "output": "\n    maxStmId = 0\n    for ass in stm.getAssignments():\n        if ass.getId() > maxStmId:\n            maxStmId = ass.getId()\n    return maxStmId\n\ndef getMaxStmIdForStms(stms):\n    \"\"\"\n    Get maximum _instId from all statements\n    \"\"\"\n    maxStmId = 0\n    for stm in stms:\n        if getMaxStmIdForStm(stm) > maxStmId:\n            max", "solution": "\n    maxId = 0\n    if isinstance(stm, Assignment):\n        return stm._instId\n    elif isinstance(stm, WaitStm):\n        return maxId\n    else:\n        for _stm in stm._iter_stms():\n            maxId = max(maxId, getMaxStmIdForStm(_stm))\n        return maxId"}
{"prompt": "def delete(self, endpoint, headers):\n        \"\"\"\n        Method to delete an item or all items\n\n        headers['If-Match'] must contain the _etag identifier of the element to delete\n\n        :param endpoint: endpoint (API URL)\n        :type endpoint: str\n        :param headers: headers (example: Content-Type)\n        :type headers: dict\n        :return: response (deletion information)\n        :rtype: dict\n        \"\"\"", "output": "\n        response = requests.delete(endpoint, headers=headers)\n        return response.json()\n\n    def get_all(self, endpoint, headers):\n        \"\"\"\n        Method to get all items\n\n        :param endpoint: endpoint (API URL)\n        :type endpoint: str\n        :param headers: headers (example: Content-Type)\n        :type headers: dict\n        :return: response (all items)\n        :rtype: dict\n        \"\"\"\n        response = requests.get(endpoint, headers=headers)\n        return response.json()\n\n", "solution": "\n        response = self.get_response(method='DELETE', endpoint=endpoint, headers=headers)\n\n        logger.debug(\"delete, response: %s\", response)\n        if response.status_code != 204:  # pragma: no cover - should not happen ...\n            resp = self.decode(response=response)\n\n        resp = {\"_status\": \"OK\"}\n        return resp"}
{"prompt": "def _make_safe_pod_id(safe_dag_id, safe_task_id, safe_uuid):\n        \"\"\"\n        Kubernetes pod names must be <= 253 chars and must pass the following regex for\n        validation\n        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n\n        :param safe_dag_id: a dag_id with only alphanumeric characters\n        :param safe_task_id: a task_id with only alphanumeric characters\n        :param random_uuid: a uuid\n        :return: ``str`` valid Pod name of appropriate length\n        \"\"\"", "output": "alphanumeric characters and underscores\n        :param safe_task_id: a task_id with only alphanumeric characters and underscores\n        :param safe_uuid: a uuid with only alphanumeric characters and underscores\n        :return: a string that is a safe pod_id\n        \"\"\"\n        return '{dag_id}_{task_id}_{uuid}'.format(\n            dag_id=safe_dag_id,\n            task_id=safe_task_id,\n            uuid=safe_uuid\n        )\n\n    def _make", "solution": "\n        MAX_POD_ID_LEN = 253\n\n        safe_key = safe_dag_id + safe_task_id\n\n        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + \"-\" + safe_uuid\n\n        return safe_pod_id"}
{"prompt": "def run(self, func):\n        \"\"\"Run 'func' under os sandboxing\"\"\"", "output": "\n        if not self.sandbox:\n            return func()\n        with self.sandbox:\n            return func()\n\n    def __enter__(self):\n        \"\"\"Context manager entry point\"\"\"\n        if self.sandbox:\n            self.sandbox.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point\"\"\"\n        if self.sandbox:\n            self.sandbox.__exit__(exc_type, exc_val, exc_tb)\n", "solution": "\n        try:\n            self._copy(self)\n            if _file:\n                __builtin__.file = self._file\n            __builtin__.open = self._open\n            self._active = True\n            return func()\n        finally:\n            self._active = False\n            if _file:\n                __builtin__.file = _file\n            __builtin__.open = _open\n            self._copy(_os)"}
{"prompt": "def _isint(string):\n    \"\"\"\n    >>> _isint(\"123\")\n    True\n    >>> _isint(\"123.45\")\n    False\n    \"\"\"", "output": "\n    try:\n        int(string)\n        return True\n    except ValueError:\n        return False\n\ndef _isfloat(string):\n    \"\"\"\n    >>> _isfloat(\"123\")\n    False\n    >>> _isfloat(\"123.45\")\n    True\n    \"\"\"\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False\n\ndef _isbool(string):\n    \"\"\"\n    >>> _isbool(\"True\")\n    True\n    >>> _isbool(\"False\")\n", "solution": "\n    return type(string) is int or \\\n        (isinstance(string, _binary_type) or\n         isinstance(string, string_types)) and \\\n        _isconvertible(int, string)"}
{"prompt": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Get request payload and decode it into its\n        constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"", "output": "               defaults to KMIP 1.0.\n\n        Raises:\n            InvalidKmipEncoding: Raised if the data encoding is invalid.\n        \"\"\"\n        super(GetRequestPayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        self.unique_identifier = primitives.TextString(\n            enums.TextStringEncoding.UTF8\n        )\n        self.", "solution": "\n        super(GetRequestPayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_FORMAT_TYPE, local_stream):\n            self._key_format_type = primitives.Enumeration(\n                enum=enums.KeyFormatType,\n                tag=enums.Tags.KEY_FORMAT_TYPE\n            )\n            self._key_format_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_COMPRESSION_TYPE, local_stream):\n            self._key_compression_type = primitives.Enumeration(\n                enum=enums.KeyCompressionType,\n                tag=enums.Tags.KEY_COMPRESSION_TYPE\n            )\n            self._key_compression_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(\n                enums.Tags.KEY_WRAPPING_SPECIFICATION,\n                local_stream\n        ):\n            self._key_wrapping_specification = \\\n                objects.KeyWrappingSpecification()\n            self._key_wrapping_specification.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)"}
{"prompt": "def get_latex_maybe_optional_arg(self, pos):\n        \"\"\"\n        Parses the latex content given to the constructor (and stored in `self.s`),\n        starting at position `pos`, to attempt to parse an optional argument.\n\n        Attempts to parse an optional argument. If this is successful, we return\n        a tuple `(node, pos, len)` if success where `node` is a\n        :py:class:`LatexGroupNode`.  Otherwise, this method returns None.\n        \"\"\"", "output": "\n        # Check if we're at the start of an optional argument.\n        if self.s[pos] != '[':\n            return None\n\n        # Parse the optional argument.\n        pos += 1\n        node, pos, len = self.get_latex_group(pos)\n        if node is None:\n            return None\n\n        # Check if we're at the end of the optional argument.\n        if self.s[pos] != ']':\n            return None\n\n        # Return the node.\n        return node, pos + 1", "solution": "\n\n        tok = self.get_token(pos, brackets_are_chars=False, environments=False)\n        if (tok.tok == 'brace_open' and tok.arg == '['):\n            return self.get_latex_braced_group(pos, brace_type='[')\n\n        return None"}
{"prompt": "def _label__get(self):\n        \"\"\"\n        Get or set any <label> element associated with this element.\n        \"\"\"", "output": "\n        return self._get_or_set(Label, 'label')\n\n    @property\n    def _label(self):\n        \"\"\"\n        Get or set any <label> element associated with this element.\n        \"\"\"\n        return self._get_or_set(Label, 'label')\n\n    @property\n    def _label_text(self):\n        \"\"\"\n        Get or set the text of the <label> element associated with this element.\n        \"\"\"\n        return self._get_or_set(Label, 'label_text')\n\n    @property", "solution": "\n        id = self.get('id')\n        if not id:\n            return None\n        result = _label_xpath(self, id=id)\n        if not result:\n            return None\n        else:\n            return result[0]"}
{"prompt": "def _split_comma_separated(string):\n    \"\"\"Return a set of strings.\"\"\"", "output": "\n    return set(string.split(','))\n\n\ndef _split_comma_separated_with_spaces(string):\n    \"\"\"Return a set of strings.\"\"\"\n    return set(string.split(', '))\n\n\ndef _split_semicolon_separated(string):\n    \"\"\"Return a set of strings.\"\"\"\n    return set(string.split(';'))\n\n\ndef _split_semicolon_separated_with_spaces(string):\n    \"\"\"Return a set of strings.\"\"\"\n    return set(string.split(';", "solution": "\n    return set(text.strip() for text in string.split(',') if text.strip())"}
{"prompt": "def apply_next_patch(self, force=False, quiet=False):\n        \"\"\" Apply next patch in series file \"\"\"", "output": "\n        if not self.series_file:\n            raise Exception(\"No series file found\")\n        if not self.patch_file:\n            raise Exception(\"No patch file found\")\n        if not self.series_file_exists():\n            raise Exception(\"Series file does not exist\")\n        if not self.patch_file_exists():\n            raise Exception(\"Patch file does not exist\")\n        if not self.patch_file_is_patch():\n            raise Exception(\"Patch file is not a patch\")\n        if not self.patch_file_is_applied", "solution": "\n        self._check()\n        top = self.db.top_patch()\n        if not top:\n            patch = self.series.first_patch()\n        else:\n            patch = self.series.patch_after(top)\n\n        if not patch:\n            raise AllPatchesApplied(self.series, top)\n\n        self.applying(patch)\n\n        self._apply_patch(patch, force, quiet)\n\n        self.db.save()\n\n        self.applied(self.db.top_patch())"}
{"prompt": "def _data_sanity_checks(self, explore_iterable):\n        \"\"\"Checks if data values are  valid.\n\n        Checks if the data values are supported by the parameter and if the values are of the same\n        type as the default value.\n\n        \"\"\"", "output": "\n        if not isinstance(explore_iterable, list):\n            raise TypeError(\"Explore iterable must be a list\")\n\n        if not all(isinstance(x, str) for x in explore_iterable):\n            raise TypeError(\"Explore iterable must be a list of strings\")\n\n        if not all(x in self.data_values for x in explore_iterable):\n            raise ValueError(\"Explore iterable must contain valid data values\")\n\n        if not all(isinstance(x, type(self.data_values[", "solution": "\n        data_list = []\n\n        for val in explore_iterable:\n\n            if not self.f_supports(val):\n                raise TypeError('%s is of not supported type %s.' % (repr(val), str(type(val))))\n\n            if not self._values_of_same_type(val, self._default):\n                raise TypeError(\n                    'Data of `%s` is not of the same type as the original entry value, '\n                    'new type is %s vs old type %s.' %\n                    (self.v_full_name, str(type(val)), str(type(self._default))))\n\n            data_list.append(val)\n\n        if len(data_list) == 0:\n            raise ValueError('Cannot explore an empty list!')\n\n        return data_list"}
{"prompt": "def _cursor_position_changed(self):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"", "output": "\n        if self.is_enabled():\n            self.update_tip()\n\n    def _cursor_position_changed_delayed(self, *args):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"\n        if self.is_enabled():\n            self.update_tip_delayed()\n\n    def _cursor_position_changed_delayed_2(self, *args):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"\n        if self.is_enabled():\n            self.update_tip_delayed", "solution": "\n        cursor = self._text_edit.textCursor()\n        if cursor.position() <= self._start_position:\n            self.hide()\n        else:\n            position, commas = self._find_parenthesis(self._start_position + 1)\n            if position != -1:\n                self.hide()"}
{"prompt": "def resources(self):\n        \"\"\"\n        Returns list of unique `FragmentResource`s by order of first appearance.\n        \"\"\"", "output": "\n        return self._resources\n\n    @property\n    def fragments(self):\n        \"\"\"\n        Returns list of unique `Fragment`s by order of first appearance.\n        \"\"\"\n        return self._fragments\n\n    @property\n    def fragment_map(self):\n        \"\"\"\n        Returns a map of `FragmentResource`s to `Fragment`s.\n        \"\"\"\n        return self._fragment_map\n\n    @property\n    def fragment_resource_map(self):\n        \"\"\"\n        Returns a map of `Fragment`s to `FragmentResource`s", "solution": "\n        seen = set()\n        # seen.add always returns None, so 'not seen.add(x)' is always True,\n        # but will only be called if the value is not already in seen (because\n        # 'and' short-circuits)\n        return [x for x in self._resources if x not in seen and not seen.add(x)]"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    def wrapper(*args, **kwargs):\n        return f(*args, **kwargs)\n    return wrapper\n\n\ndef track_production_with_pos(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        return f(*args, **kwargs)\n    return wrapper\n\n\ndef track_production_with_pos_and_end(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"\n    def wrapper(*args, **kwargs):\n       ", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def s2p(self):\n        \"\"\"Return 2 proton separation energy\"\"\"", "output": "\n        return self.s2p\n\n    @property\n    def s2n(self):\n        \"\"\"Return 2 neutron separation energy\"\"\"\n        return self.s2n\n\n    @property\n    def s2d(self):\n        \"\"\"Return 2 deuteron separation energy\"\"\"\n        return self.s2d\n\n    @property\n    def s2t(self):\n        \"\"\"Return 2 triton separation energy\"\"\"\n        return self.s2t\n\n    @property\n    def s2he(self):\n       ", "solution": "\n        M_P = 7.28897050         # proton mass excess in MeV\n        f = lambda parent, daugther: -parent + daugther + 2 * M_P\n        return self.derived('s2p', (-2, 0), f)"}
{"prompt": "def clear_xcom_data(self, session=None):\n        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"", "output": "\n        session = session or settings.Session()\n        self.log.info('Clearing xcom data for %s', self)\n        session.query(XCom).filter(\n            XCom.task_id == self.task_id,\n            XCom.dag_id == self.dag_id,\n            XCom.run_id == self.run_id,\n        ).delete()\n        session.commit()\n\n    def get_xcom_dict(self, key=None):\n        \"\"\"\n        Returns the value of the XCom associated to the", "solution": "\n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()"}
{"prompt": "def as_property_description(self):\n        \"\"\"\n        Get the property description.\n\n        Returns a dictionary describing the property.\n        \"\"\"", "output": "\n        return self._property_description\n\n    def as_property_value(self):\n        \"\"\"\n        Get the property value.\n\n        Returns the value of the property.\n        \"\"\"\n        return self._property_value\n\n    def as_property_type(self):\n        \"\"\"\n        Get the property type.\n\n        Returns the type of the property.\n        \"\"\"\n        return self._property_type\n\n    def as_property_name(self):\n        \"\"\"\n        Get the property name.\n\n        Returns the name of the property.\n       ", "solution": "\n        description = deepcopy(self.metadata)\n\n        if 'links' not in description:\n            description['links'] = []\n\n        description['links'].append(\n            {\n                'rel': 'property',\n                'href': self.href_prefix + self.href,\n            }\n        )\n        return description"}
{"prompt": "def save(self, fname=None):\n        \"\"\"\n        Download the chart from the URL into a filename as a PNG\n\n        The filename defaults to the chart title (chtt) if any\n        \"\"\"", "output": "\n        if fname is None:\n            fname = self.title\n        if fname is None:\n            raise ValueError(\"No filename given\")\n        if not fname.endswith(\".png\"):\n            fname += \".png\"\n        url = self.url\n        if url is None:\n            raise ValueError(\"No URL given\")\n        if not url.startswith(\"http\"):\n            url = \"http://\" + url\n        try:\n            import urllib\n            urllib.urlretrieve(url, fname)\n        except", "solution": "\n        if not fname:\n            fname = self.getname()\n        assert fname != None, 'You must specify a filename to save to'\n        if not fname.endswith('.png'):\n            fname += '.png'\n        try:\n            urlretrieve(self.url, fname)\n        except Exception:\n            raise IOError('Problem saving %s to file'%fname)\n        return fname"}
{"prompt": "def opt(parser: Union[Parser, Sequence[Input]]) -> OptionalParser:\n    \"\"\"Optionally match a parser.\n\n    An ``OptionalParser`` attempts to match ``parser``. If it succeeds, it\n    returns a list of length one with the value returned by the parser as the\n    only element. If it fails, it returns an empty list.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"", "output": "\n    if isinstance(parser, Parser):\n        return OptionalParser(parser)\n    else:\n        return OptionalParser(LiteralParser(parser))\n\n\nclass OptionalParser(Parser):\n    \"\"\"Parser that optionally matches a parser.\n\n    Attributes:\n        parser: Parser to optionally match\n    \"\"\"\n\n    def __init__(self, parser: Parser):\n        self.parser = parser\n\n    def __repr__(self) -> str:\n        return f\"opt({self.parser!r})\"\n\n    def parse(", "solution": "\n    if isinstance(parser, str):\n        parser = lit(parser)\n    return OptionalParser(parser)"}
{"prompt": "def log(self):\n        \"\"\"\n        Print on screen and on file the percentages for each status.\n        \"\"\"", "output": "\n        print('----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------", "solution": "\n\n        if (\n            PyFunceble.CONFIGURATION[\"show_percentage\"]\n            and PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0\n        ):\n            # * We are allowed to show the percentage on screen.\n            # and\n            # * The number of tested is greater than 0.\n\n            # We initiate the output file.\n            output = (\n                PyFunceble.OUTPUT_DIRECTORY\n                + PyFunceble.OUTPUTS[\"parent_directory\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"parent\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"percentage\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"filenames\"][\"percentage\"]\n            )\n\n            # We delete the output file if it does exist.\n            File(output).delete()\n\n            # We calculate the percentage of each statuses.\n            self._calculate()\n\n            if not PyFunceble.CONFIGURATION[\"quiet\"]:\n                # The quiet mode is activated.\n\n                # We print a new line.\n                print(\"\\n\")\n\n                # We print the percentage header on file and screen.\n                Prints(None, \"Percentage\", output).header()\n\n                # We construct the different lines/data to print on screen and file.\n                lines_to_print = [\n                    [\n                        PyFunceble.STATUS[\"official\"][\"up\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"up\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"up\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"down\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"down\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"down\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"invalid\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"invalid\"])\n                        + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"invalid\"],\n                    ],\n                ]\n\n                if PyFunceble.CONFIGURATION[\"syntax\"]:\n                    # We are checking for syntax.\n\n                    # We update the denomination of the UP.\n                    lines_to_print[0][0] = PyFunceble.STATUS[\"official\"][\"valid\"]\n\n                    # And we unset the INACTIVE line.\n                    del lines_to_print[1]\n\n                for to_print in lines_to_print:\n                    # We loop throught the different line to print.\n                    # (one line for each status.)\n\n                    # And we print the current status line on file and screen.\n                    Prints(to_print, \"Percentage\", output).data()\n\n        elif PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0:\n            # * We are not allowed to show the percentage on screen.\n            # but\n            # * The number of tested is greater than 0.\n\n            # We run the calculation.\n            # Note: The following is needed, because all counter calculation are\n            # done by this class.\n            self._calculate()"}
{"prompt": "def grid(self, *args):\n        \"\"\"\n        Apply a grid to your chart\n        args are of the form::\n            <x axis step size>,\n            <y axis step size>,\n            <length of line segment>,\n            <length of blank segment>\n            <x offset>,\n            <y offset>\n        APIPARAM: chg\n        \"\"\"", "output": "\n        self.chart.grid(*args)\n        return self\n\n    def grid_color(self, *args):\n        \"\"\"\n        Set the color of the grid\n        args are of the form::\n            <color>\n        APIPARAM: chg\n        \"\"\"\n        self.chart.grid_color(*args)\n        return self\n\n    def grid_line_style(self, *args):\n        \"\"\"\n        Set the line style of the grid\n        args are of the form::\n            <line style>\n        APIPARAM: chg\n", "solution": "\n        grids =  map(str,map(float,args))\n        self['chg'] = ','.join(grids).replace('None','')\n        return self"}
{"prompt": "def delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"", "output": "dict\n        :param mongo_db: The name of the database to delete from.\n        :type mongo_db: str\n        :param kwargs: Any additional arguments to pass to the delete_one method.\n        :return: The result of the delete_one method.\n        :rtype: pymongo.results.DeleteResult\n        \"\"\"\n        return self.get_mongo_collection(mongo_collection, mongo_db).delete_one(filter_doc, **kwargs)\n\n    def delete_many(self, mongo_collection, filter_doc, mongo", "solution": "\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)"}
{"prompt": "def _check_relative_import(\n        self, modnode, importnode, importedmodnode, importedasname\n    ):\n        \"\"\"check relative import. node is either an Import or From node, modname\n        the imported module name.\n        \"\"\"", "output": "\n        if modnode.modname == importedmodnode.modname:\n            # relative import to the same module\n            return\n        if modnode.modname == \"__main__\":\n            # relative import to the main module\n            return\n        if modnode.modname == \"__init__\":\n            # relative import to an __init__ module\n            return\n        if modnode.modname == \"builtins\":\n            # relative import to the builtins module\n            return\n        if modnode.modname == \"sys\":\n            # relative import to the sys", "solution": "\n        if not self.linter.is_message_enabled(\"relative-import\"):\n            return None\n        if importedmodnode.file is None:\n            return False  # built-in module\n        if modnode is importedmodnode:\n            return False  # module importing itself\n        if modnode.absolute_import_activated() or getattr(importnode, \"level\", None):\n            return False\n        if importedmodnode.name != importedasname:\n            # this must be a relative import...\n            self.add_message(\n                \"relative-import\",\n                args=(importedasname, importedmodnode.name),\n                node=importnode,\n            )\n            return None\n        return None"}
{"prompt": "def download(self, bucket_name, object_name, filename=None):\n        \"\"\"\n        Get a file from Google Cloud Storage.\n\n        :param bucket_name: The bucket to fetch from.\n        :type bucket_name: str\n        :param object_name: The object to fetch.\n        :type object_name: str\n        :param filename: If set, a local file path where the file should be written to.\n        :type filename: str\n        \"\"\"", "output": "\n        if filename is None:\n            filename = object_name\n\n        self.logger.info('Downloading %s from %s', object_name, bucket_name)\n        self.logger.debug('Downloading %s to %s', object_name, filename)\n\n        try:\n            self.client.get_bucket(bucket_name).blob(object_name).download_to_filename(filename)\n        except Exception as e:\n            self.logger.error('Failed to download %s from %s: %s', object_name, bucket_name", "solution": "\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()"}
{"prompt": "def parse(self, s):\n        \"\"\"Parses the input string, and returns a reference to the created AST's root\"\"\"", "output": "\n        self.lexer.input(s)\n        self.lexer.lineno = 1\n        self.lexer.lexpos = 0\n        self.lexer.filename = \"<string>\"\n        return self.parser.parse(self.lexer)\n\n    def parse_file(self, fname):\n        \"\"\"Parses the input file, and returns a reference to the created AST's root\"\"\"\n        with open(fname, \"r\") as f:\n            self.lexer.input(f.read())\n        self", "solution": "\n        with self.lock:\n            try:\n                return self.parser.parse(s, lexer=self.lexer)\n            except InvalidIEMLObjectArgument as e:\n                raise CannotParse(s, str(e))\n            except CannotParse as e:\n                e.s = s\n                raise e"}
{"prompt": "def allocate_ids(self, partial_keys):\n        \"\"\"\n        Allocate IDs for incomplete keys.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list\n        \"\"\"", "output": "\n        if not partial_keys:\n            return []\n\n        # TODO: support partial_keys with incomplete keys.\n        if any(not key.is_partial for key in partial_keys):\n            raise ValueError('partial_keys must be a list of partial keys.')\n\n        # TODO: support multiple partial keys.\n        if len(partial_keys) > 1:\n            raise ValueError('partial_keys must be a list of one partial key.')\n\n        partial_key = partial_keys[0]\n\n        # TODO: support partial_key", "solution": "\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']"}
{"prompt": "def _insert_plain_text_into_buffer(self, cursor, text):\n        \"\"\" Inserts text into the input buffer using the specified cursor (which\n            must be in the input buffer), ensuring that continuation prompts are\n            inserted as necessary.\n        \"\"\"", "output": "\n        if text is None:\n            return\n\n        # If the cursor is at the end of the buffer, just insert the text.\n        if cursor.at_end():\n            cursor.insert_text(text)\n            return\n\n        # Otherwise, we need to insert the text at the cursor, and then insert\n        # continuation prompts as necessary.\n        cursor.insert_text(text)\n        self._insert_continuation_prompts(cursor)\n\n    def _insert_continuation_prompts(self, cursor):\n        \"\"\" Inserts", "solution": "\n        lines = text.splitlines(True)\n        if lines:\n            cursor.beginEditBlock()\n            cursor.insertText(lines[0])\n            for line in lines[1:]:\n                if self._continuation_prompt_html is None:\n                    cursor.insertText(self._continuation_prompt)\n                else:\n                    self._continuation_prompt = \\\n                        self._insert_html_fetching_plain_text(\n                            cursor, self._continuation_prompt_html)\n                cursor.insertText(line)\n            cursor.endEditBlock()"}
{"prompt": "def get_total_contributors(self, repo):\n        \"\"\"\n        Retrieves the number of contributors to a repo in the organization.\n        Also adds to unique contributor list.\n        \"\"\"", "output": "\n        try:\n            contributors = self.get_contributors(repo)\n            total_contributors = len(contributors)\n            self.total_contributors += total_contributors\n            self.unique_contributors.update(contributors)\n            return total_contributors\n        except Exception as e:\n            print(e)\n            return 0\n\n    def get_contributors(self, repo):\n        \"\"\"\n        Retrieves the contributors to a repo in the organization.\n        \"\"\"\n        try:\n            contributors = []\n            for", "solution": "\n        repo_contributors = 0\n        for contributor in repo.iter_contributors():\n            repo_contributors += 1\n            self.unique_contributors[contributor.id].append(repo.name)\n            self.contributors_json[repo.name].append(contributor.to_json())\n        return repo_contributors"}
{"prompt": "def _unreg_event(self, event_list, event):\n        \"\"\" Tries to remove a registered event without triggering it \"\"\"", "output": "\n        if event in event_list:\n            event_list.remove(event)\n            return True\n        return False\n\n    def _reg_event(self, event_list, event):\n        \"\"\" Tries to register an event without triggering it \"\"\"\n        if event not in event_list:\n            event_list.append(event)\n            return True\n        return False\n\n    def _trigger_event(self, event_list, event):\n        \"\"\" Triggers an event if it is registered \"\"\"\n        if event in event_list:\n            for", "solution": "\n        try:\n            self.log.debug(\"Removing event {0}({1},{2})\".format(event['function'].__name__, event['args'], event['kwargs']))\n        except AttributeError:\n            self.log.debug(\"Removing event {0}\".format(str(event)))\n\n        try:\n            event_list.remove(event)\n        except ValueError:\n            try:\n                self.log.warn(\"Unable to remove event {0}({1},{2}) , not found in list: {3}\".format(event['function'].__name__, event['args'], event['kwargs'], event_list))\n            except AttributeError:\n                self.log.debug(\"Unable to remove event {0}\".format(str(event)))\n            raise KeyError('Unable to unregister the specified event from the signals specified')"}
{"prompt": "def get_system_per_cpu_times():\n    \"\"\"Return system per-CPU times as a list of named tuples.\"\"\"", "output": "\n    with open(\"/proc/stat\", \"r\") as f:\n        data = f.read()\n    data = data.splitlines()\n    data = [x.split() for x in data]\n    data = [x for x in data if x[0] == \"cpu\"]\n    data = [x for x in data if len(x) == 10]\n    data = [x for x in data if x[2] != \"guest\"]\n    data = [x for x in data if x[2] != \"guest_nice\"]\n", "solution": "\n    ret = []\n    for cpu_t in _psutil_mswindows.get_system_cpu_times():\n        user, system, idle = cpu_t\n        item = _cputimes_ntuple(user, system, idle)\n        ret.append(item)\n    return ret"}
{"prompt": "def queue_startfile(self, cmdfile):\n        \"\"\"Arrange for file of debugger commands to get read in the\n        process-command loop.\"\"\"", "output": "\n        self.cmdqueue.append(cmdfile)\n\n    def queue_command(self, cmd):\n        \"\"\"Arrange for a string of debugger commands to get read in the\n        process-command loop.\"\"\"\n        self.cmdqueue.append(cmd)\n\n    def run(self, cmd, *args, **kwargs):\n        \"\"\"Run a command in the process-command loop.  This is a\n        convenience function for running a command in the process-command\n        loop.  It is equivalent to:\n\n        >>> from pdb import Pdb\n        >>>", "solution": "\n        expanded_cmdfile = os.path.expanduser(cmdfile)\n        is_readable = Mfile.readable(expanded_cmdfile)\n        if is_readable:\n            self.cmd_queue.append('source ' + expanded_cmdfile)\n        elif is_readable is None:\n            self.errmsg(\"source file '%s' doesn't exist\" % expanded_cmdfile)\n        else:\n            self.errmsg(\"source file '%s' is not readable\" %\n                        expanded_cmdfile)\n            pass\n        return"}
{"prompt": "def read_infile(infile: Union[Path, str], from_words=False,\n                word_column: int = WORD_COLUMN, pos_column: int = POS_COLUMN,\n                tag_column: int = TAG_COLUMN, max_sents: int = -1,\n                read_only_words: bool = False) -> List[Tuple[List, Union[List, None]]]:\n    \"\"\"Reads input file in CONLL-U format\n\n    Args:\n        infile: a path to a file\n        word_column: column containing words (default=1)\n        pos_column: column containing part-of-speech labels (default=3)\n        tag_column: column containing fine-grained tags (default=5)\n        max_sents: maximal number of sents to read\n        read_only_words: whether to read only words\n\n    Returns:\n        a list of sentences. Each item contains a word sequence and a tag sequence, which is ``None``\n        in case ``read_only_words = True``\n    \"\"\"", "output": "in CONLL-U format\n        from_words: if True, the input file is assumed to be in words format\n        word_column: the column number of the word in the input file\n        pos_column: the column number of the POS tag in the input file\n        tag_column: the column number of the tag in the input file\n        max_sents: the maximum number of sentences to read\n        read_only_words: if True, only the words are read\n\n    Returns:\n        a list of tuples of the form (words, tags)\n    \"\"\"", "solution": "\n    answer, curr_word_sent, curr_tag_sent = [], [], []\n    if from_words:\n        word_column, read_only_words = 0, True\n    with open(infile, \"r\", encoding=\"utf8\") as fin:\n        for line in fin:\n            line = line.strip()\n            if line.startswith(\"#\"):\n                continue\n            if line == \"\":\n                if len(curr_word_sent) > 0:\n                    if read_only_words:\n                        curr_tag_sent = None\n                    answer.append((curr_word_sent, curr_tag_sent))\n                curr_tag_sent, curr_word_sent = [], []\n                if len(answer) == max_sents:\n                    break\n                continue\n            splitted = line.split(\"\\t\")\n            index = splitted[0]\n            if not from_words and not index.isdigit():\n                continue\n            curr_word_sent.append(splitted[word_column])\n            if not read_only_words:\n                pos, tag = splitted[pos_column], splitted[tag_column]\n                tag = pos if tag == \"_\" else \"{},{}\".format(pos, tag)\n                curr_tag_sent.append(tag)\n        if len(curr_word_sent) > 0:\n            if read_only_words:\n                curr_tag_sent = None\n            answer.append((curr_word_sent, curr_tag_sent))\n    return answer"}
{"prompt": "def fuzzed(self):\n        \"\"\"\n        Get a printable fuzzed object\n        \"\"\"", "output": "\n        return self.fuzzed_object\n\n    def __str__(self):\n        \"\"\"\n        Get a printable fuzzed object\n        \"\"\"\n        return self.fuzzed_object\n\n    def __repr__(self):\n        \"\"\"\n        Get a printable fuzzed object\n        \"\"\"\n        return self.fuzzed_object\n\n    def __eq__(self, other):\n        \"\"\"\n        Compare two fuzzed objects\n        \"\"\"\n        if isinstance(other, Fuzzed):\n            return self.fuzzed_", "solution": "\n        try:\n            if self.config.strong_fuzz:\n                fuzzer = PJFMutators(self.config)\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                    else:\n                        return urllib.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                else:\n                    if type(self.config.json) in [list, dict]:\n                        return fuzzer.fuzz(json.dumps(self.config.json))\n                    else:\n                        return fuzzer.fuzz(self.config.json)\n            else:\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                    else:\n                        return urllib.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                else:\n                    return self.get_fuzzed(self.config.indent, self.config.utf8)\n        except Exception as e:\n            raise PJFBaseException(e.message if hasattr(e, \"message\") else str(e))"}
{"prompt": "def validate(self, request):\n        \"\"\"Validate JSON-RPC request.\n\n        :param request: RPC request object\n        :type request: dict\n\n        \"\"\"", "output": "\n        if not isinstance(request, dict):\n            raise InvalidRequest('Request must be a JSON object')\n\n        if 'jsonrpc' not in request:\n            raise InvalidRequest('Request must contain \"jsonrpc\" member')\n\n        if request['jsonrpc'] != '2.0':\n            raise InvalidRequest('Request must contain \"jsonrpc\" member with value \"2.0\"')\n\n        if 'method' not in request:\n            raise InvalidRequest('Request must contain \"method\" member')\n\n        if not isinstance(request['method", "solution": "\n\n        try:\n            validate_version(request)\n            validate_method(request)\n            validate_params(request)\n            validate_id(request)\n        except (AssertionError, KeyError) as error:\n            invalid_request(error)"}
{"prompt": "def action_notify(self, action):\n        \"\"\"\n        Notify all subscribers of an action status change.\n\n        action -- the action whose status changed\n        \"\"\"", "output": "\n        if action.status == action.STATUS_SUCCESS:\n            self.notify_success(action)\n        elif action.status == action.STATUS_FAILURE:\n            self.notify_failure(action)\n        elif action.status == action.STATUS_RUNNING:\n            self.notify_running(action)\n\n    def notify_success(self, action):\n        \"\"\"\n        Notify all subscribers of an action success.\n\n        action -- the action that succeeded\n        \"\"\"\n        for subscriber in self.subscribers:\n", "solution": "\n        message = json.dumps({\n            'messageType': 'actionStatus',\n            'data': action.as_action_description(),\n        })\n\n        for subscriber in list(self.subscribers):\n            try:\n                subscriber.write_message(message)\n            except tornado.websocket.WebSocketClosedError:\n                pass"}
{"prompt": "def update_mongo_compound_variants(self, bulk):\n        \"\"\"Update the compound information for a bulk of variants in the database\n\n            Args:\n                bulk(dict): {'_id': scout.models.Variant}\n\n        \"\"\"", "output": "\n        for variant_obj in bulk.values():\n            self.update_mongo_compound_variant(variant_obj)\n\n    def update_mongo_compound_variant(self, variant_obj):\n        \"\"\"Update the compound information for a variant in the database\n\n            Args:\n                variant_obj(scout.models.Variant)\n\n        \"\"\"\n        if not variant_obj.compound_recessive:\n            return\n\n        # Get the compound information for the variant\n        compound_info = self.get_", "solution": "\n        requests = []\n        for var_id in bulk:\n            var_obj = bulk[var_id]\n            if not var_obj.get('compounds'):\n                continue\n            # Add a request to update compounds\n            operation = pymongo.UpdateOne(\n                {'_id': var_obj['_id']},\n                {\n                    '$set': {\n                        'compounds': var_obj['compounds']\n                    }\n                })\n            requests.append(operation)\n\n        if not requests:\n            return\n\n        try:\n            self.variant_collection.bulk_write(requests, ordered=False)\n        except BulkWriteError as err:\n            LOG.warning(\"Updating compounds failed\")\n            raise err"}
{"prompt": "def module_list(path):\n    \"\"\"\n    Return the list containing the names of the modules available in the given\n    folder.\n    \"\"\"", "output": "\n    return [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n\n\ndef module_path(module):\n    \"\"\"\n    Return the path of the given module.\n    \"\"\"\n    return os.path.join(MODULES_PATH, module)\n\n\ndef module_name(module):\n    \"\"\"\n    Return the name of the given module.\n    \"\"\"\n    return os.path.basename(module)\n\n\ndef module_exists(module):\n    \"\"\"\n", "solution": "\n    # sys.path has the cwd as an empty string, but isdir/listdir need it as '.'\n    if path == '':\n        path = '.'\n\n    if os.path.isdir(path):\n        folder_list = os.listdir(path)\n    elif path.endswith('.egg'):\n        try:\n            folder_list = [f for f in zipimporter(path)._files]\n        except:\n            folder_list = []\n    else:\n        folder_list = []\n\n    if not folder_list:\n        return []\n\n    # A few local constants to be used in loops below\n    isfile = os.path.isfile\n    pjoin = os.path.join\n    basename = os.path.basename\n\n    def is_importable_file(path):\n        "}
{"prompt": "def modify_conf():\n    \"\"\"\n    pip install redbaron\n    \"\"\"", "output": "\n    from redbaron import RedBaron\n    from redbaron.exceptions import RedBaronError\n\n    def modify_conf_file(file_path):\n        try:\n            with open(file_path, \"r\") as f:\n                red = RedBaron(f.read())\n                for i in red.find_all(\"variable\"):\n                    if i.name == \"conf\":\n                        i.value = \"conf = '{}'\".format(file_path)\n                        break\n                with open(file_path, \"", "solution": "\n    import redbaron\n    import ubelt as ub\n    conf_path = 'docs/conf.py'\n\n    source = ub.readfrom(conf_path)\n    red = redbaron.RedBaron(source)\n\n    # Insert custom extensions\n    extra_extensions = [\n        '\"sphinxcontrib.napoleon\"'\n    ]\n\n    ext_node = red.find('name', value='extensions').parent\n    ext_node.value.value.extend(extra_extensions)\n\n    # Overwrite theme to read-the-docs\n    theme_node = red.find('name', value='html_theme').parent\n    theme_node.value.value = '\"sphinx_rtd_theme\"'\n\n    ub.writeto(conf_path, red.dumps())"}
{"prompt": "def show(self, *args, **kwargs):\n        \"\"\"\n        Shows the chart URL in a webbrowser\n\n        Other arguments passed to webbrowser.open\n        \"\"\"", "output": "\n        import webbrowser\n        webbrowser.open(self.url, *args, **kwargs)\n\n    def save(self, filename, *args, **kwargs):\n        \"\"\"\n        Saves the chart to a file\n\n        Other arguments passed to urllib2.urlopen\n        \"\"\"\n        import urllib2\n        urllib2.urlopen(self.url, *args, **kwargs).read()\n        with open(filename, 'w') as f:\n            f.write(urllib2.urlopen(self.url,", "solution": "\n        from webbrowser import open as webopen\n        return webopen(str(self), *args, **kwargs)"}
{"prompt": "def getView(self,name):\n        \"\"\"\n        Returns the view with name ``name``\\ .\n        \n        Raises a :py:exc:`ValueError` if the view does not exist.\n        \"\"\"", "output": "\n        return self._views[name]\n    \n    def getViews(self):\n        \"\"\"\n        Returns a list of all views in the database.\n        \"\"\"\n        return self._views.values()\n    \n    def getViewNames(self):\n        \"\"\"\n        Returns a list of all view names in the database.\n        \"\"\"\n        return self._views.keys()\n    \n    def getViewNamesAndTypes(self):\n        \"\"\"\n        Returns a list of all view names and types in the database.\n        \"\"\"\n        return [(name,view.get", "solution": "\n        if name not in self.views:\n            raise ValueError(\"Unknown world view\")\n        return self.views[name]"}
{"prompt": "def _process_execute_error(self, msg):\n        \"\"\" Process a reply for an execution request that resulted in an error.\n        \"\"\"", "output": "\n        self._process_error(msg)\n\n    def _process_execute_reply(self, msg):\n        \"\"\" Process a reply for an execution request.\n        \"\"\"\n        self._process_reply(msg)\n\n    def _process_error(self, msg):\n        \"\"\" Process an error message.\n        \"\"\"\n        self._process_reply(msg)\n\n    def _process_reply(self, msg):\n        \"\"\" Process a reply message.\n        \"\"\"\n        if msg.header.msg_type == 'status' and msg.", "solution": "\n        content = msg['content']\n        # If a SystemExit is passed along, this means exit() was called - also\n        # all the ipython %exit magic syntax of '-k' to be used to keep\n        # the kernel running\n        if content['ename']=='SystemExit':\n            keepkernel = content['evalue']=='-k' or content['evalue']=='True'\n            self._keep_kernel_on_exit = keepkernel\n            self.exit_requested.emit(self)\n        else:\n            traceback = ''.join(content['traceback'])\n            self._append_plain_text(traceback)"}
{"prompt": "def s_add(self, path, function, method=None, type_cast=None):\n        \"\"\"Function for registering a simple path.\n\n        Args:\n            path (str): Path to be matched.\n            function (function): Function to associate with this path.\n            method (str, optional): Usually used to define one of GET, POST,\n                PUT, DELETE. You may use whatever fits your situation though.\n                Defaults to None.\n            type_cast (dict, optional): Mapping between the param name and\n                one of `int`, `float` or `bool`. The value reflected by the\n                provided param name will than be casted to the given type.\n                Defaults to None.\n        \"\"\"", "output": "`, `float`, `str`, `bool`. Defaults to None.\n\n        Returns:\n            None\n        \"\"\"\n        self.add(path, function, method, type_cast)\n\n    def add(self, path, function, method=None, type_cast=None):\n        \"\"\"Function for registering a path.\n\n        Args:\n            path (str): Path to be matched.\n            function (function): Function to associate with this path.\n            method (str, optional): Usually used to define one of GET, POST,\n                PUT, DE", "solution": "\n        with self._lock:\n            try:\n                path = '^/{}'.format(path.lstrip('/'))\n                path = '{}/$'.format(path.rstrip('/'))\n                path = path.replace('<', '(?P<')\n                path = path.replace('>', '>[^/]*)')\n\n                self.add(path, function, method, type_cast)\n            except Exception:\n                pass"}
{"prompt": "def login(request, template_name='ci/login.html',\n          redirect_field_name=REDIRECT_FIELD_NAME,\n          authentication_form=AuthenticationForm):\n    \"\"\"\n    Displays the login form and handles the login action.\n    \"\"\"", "output": "\n    redirect_to = request.REQUEST.get(redirect_field_name, '')\n\n    if request.method == 'POST':\n        form = authentication_form(request, data=request.POST)\n        if form.is_valid():\n            # Ensure the user-originating redirection url is safe.\n            if not is_safe_url(url=redirect_to, host=request.get_host()):\n                redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\n            # Okay, security check complete. Log", "solution": "\n    redirect_to = request.POST.get(redirect_field_name,\n                                   request.GET.get(redirect_field_name, ''))\n\n    if request.method == \"POST\":\n        form = authentication_form(request, data=request.POST)\n        if form.is_valid():\n\n            # Ensure the user-originating redirection url is safe.\n            if not is_safe_url(url=redirect_to, host=request.get_host()):\n                redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\n            # Okay, security check complete. Get the user object from auth api.\n            user = form.get_user()\n            request.session['user_token'] = user[\"token\"]\n            request.session['user_email'] = user[\"email\"]\n            request.session['user_permissions'] = user[\"permissions\"]\n            request.session['user_id'] = user[\"id\"]\n            request.session['user_list'] = user[\"user_list\"]\n\n            if not settings.HIDE_DASHBOARDS:\n                # Set user dashboards because they are slow to change\n                dashboards = ciApi.get_user_dashboards(user[\"id\"])\n                dashboard_list = list(dashboards['results'])\n                if len(dashboard_list) > 0:\n                    request.session['user_dashboards'] = \\\n                        dashboard_list[0][\"dashboards\"]\n                    request.session['user_default_dashboard'] = \\\n                        dashboard_list[0][\"default_dashboard\"][\"id\"]\n                else:\n                    request.session['user_dashboards'] = []\n                    request.session['user_default_dashboard'] = None\n\n            # Get the user access tokens too and format for easy access\n            tokens = ciApi.get_user_service_tokens(\n                params={\"user_id\": user[\"id\"]})\n            token_list = list(tokens['results'])\n            user_tokens = {}\n            if len(token_list) > 0:\n                for token in token_list:\n                    user_tokens[token[\"service\"][\"name\"]] = {\n                        \"token\": token[\"token\"],\n                        \"url\": token[\"service\"][\"url\"] + \"/api/v1\"\n                    }\n            request.session['user_tokens'] = user_tokens\n\n            return HttpResponseRedirect(redirect_to)\n    else:\n        form = authentication_form(request)\n\n    current_site = get_current_site(request)\n\n    context = {\n        'form': form,\n        redirect_field_name: redirect_to,\n        'site': current_site,\n        'site_name': current_site.name,\n    }\n\n    return TemplateResponse(request, template_name, context)"}
{"prompt": "def get_io_write_task(self, fileobj, data, offset):\n        \"\"\"Get an IO write task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to write to\n\n        :type data: bytes\n        :param data: The data to write out\n\n        :type offset: integer\n        :param offset: The offset to write the data to in the file-like object\n\n        :returns: An IO task to be used to write data to a file-like object\n        \"\"\"", "output": "like object\n\n        :rtype: :class:`~.IOWriteTask`\n        :returns: An IO write task for the requested set of data\n        \"\"\"\n        return IOWriteTask(fileobj, data, offset)\n\n    def get_io_read_task(self, fileobj, length, offset):\n        \"\"\"Get an IO read task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj:", "solution": "\n        return IOWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n                'offset': offset,\n            }\n        )"}
{"prompt": "def __notify_listeners(self, data, stats):\n        \"\"\" notify all listeners about aggregate data and stats \"\"\"", "output": "\n        for listener in self.listeners:\n            listener.update(data, stats)\n\n    def __get_data_from_listener(self, listener):\n        \"\"\" get data from a listener \"\"\"\n        return listener.get_data()\n\n    def __get_stats_from_listener(self, listener):\n        \"\"\" get stats from a listener \"\"\"\n        return listener.get_stats()\n\n    def __get_data_from_listeners(self):\n        \"\"\" get data from all listeners \"\"\"\n        data = {}\n        for listener in self.", "solution": "\n        for listener in self.listeners:\n            listener.on_aggregated_data(data, stats)"}
{"prompt": "def utf8(unicode_str):\n    \"\"\"\n    Return a utf-8 encoded string from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"", "output": "\n    if isinstance(unicode_str, str):\n        return unicode_str\n    else:\n        return unicode_str.encode('utf-8')\n\n\ndef utf8_to_str(utf8_str):\n    \"\"\"\n    Return a str from a valid utf-8 encoded string.\n\n    :param utf8_str: UTF-8 encoded string.\n\n    :return: str\n    \"\"\"\n    if isinstance(utf8_str, str):\n        return utf8_str\n    else:\n        return utf8_str.decode", "solution": "\n    if six.PY2 and isinstance(unicode_str, __unicode__):\n        return unicode_str.encode('utf-8')\n\n    return unicode_str"}
{"prompt": "def csoftmax_for_slice(input):\n    \"\"\" It is a implementation of the constrained softmax (csoftmax) for slice.\n        Based on the paper:\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf \"Learning What's Easy: Fully Differentiable Neural Easy-First Taggers\" (page 4)\n    Args:\n        input: A list of [input tensor, cumulative attention].\n    Returns:\n        output: A list of [csoftmax results, masks]\n    \"\"\"", "output": "[output tensor, cumulative attention].\n    \"\"\"\n    input_tensor, cumulative_attention = input\n    input_tensor = tf.nn.softmax(input_tensor)\n    cumulative_attention = tf.nn.softmax(cumulative_attention)\n    output_tensor = tf.multiply(input_tensor, cumulative_attention)\n    output_tensor = tf.reduce_sum(output_tensor, axis=1)\n    output_tensor = tf.nn.softmax(output_tensor)\n    return [output_tensor", "solution": "\n\n    [ten, u] = input\n\n    shape_t = ten.shape\n    shape_u = u.shape\n\n    ten -= tf.reduce_mean(ten)\n    q = tf.exp(ten)\n    active = tf.ones_like(u, dtype=tf.int32)\n    mass = tf.constant(0, dtype=tf.float32)\n    found = tf.constant(True, dtype=tf.bool)\n\n    def loop(q_, mask, mass_, found_):\n        q_list = tf.dynamic_partition(q_, mask, 2)\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(q_)[0]), mask, 2)  # 0 element it False,\n        #  1 element if true\n\n        p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])\n        p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])\n\n        # condition verification and mask modification\n        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,\n                                                 2)  # 0 when u is bigger than p, 1 when u is less than p\n\n        split_p_new = tf.dynamic_partition(p_new, less_mask, 2)\n        split_u = tf.dynamic_partition(u, less_mask, 2)\n\n        alpha = tf.dynamic_stitch(condition_indices, [split_p_new[0], split_u[1]])\n        mass_ += tf.reduce_sum(split_u[1])\n\n        mask = mask * (tf.ones_like(less_mask) - less_mask)\n\n        found_ = tf.cond(tf.equal(tf.reduce_sum(less_mask), 0),\n                         lambda: False,\n                         lambda: True)\n\n        alpha = tf.reshape(alpha, q_.shape)\n\n        return alpha, mask, mass_, found_\n\n    (csoft, mask_, _, _) = tf.while_loop(cond=lambda _0, _1, _2, f: f,\n                                         body=loop,\n                                         loop_vars=(q, active, mass, found))\n\n    return [csoft, mask_]"}
{"prompt": "def lazy(function, *args, **kwargs):\n    \"\"\" Produces a callable so that functions can be lazily evaluated in\n    templates.\n\n    Arguments:\n\n        function (callable): The function to call at evaluation time.\n\n        args: Positional arguments, passed directly to ``function``.\n\n        kwargs: Keyword arguments, passed directly to ``function``.\n\n    Return:\n\n        callable: A callable that will evaluate a call to ``function`` with\n            the specified arguments.\n\n    \"\"\"", "output": "\n    def callable():\n        return function(*args, **kwargs)\n    return callable\n\n\ndef lazy_property(function):\n    \"\"\" Produces a property so that functions can be lazily evaluated in\n    templates.\n\n    Arguments:\n\n        function (callable): The function to call at evaluation time.\n\n    Return:\n\n        property: A property that will evaluate a call to ``function``.\n\n    \"\"\"\n    return property(lazy(function))\n\n\ndef lazy_attribute(function):\n    \"\"\" Produces an attribute so", "solution": "\n\n    NOT_EVALUATED = object()\n    retval = [NOT_EVALUATED]\n\n    def evaluate():\n        if retval[0] is NOT_EVALUATED:\n            retval[0] = function(*args, **kwargs)\n        return retval[0]\n\n    return evaluate"}
{"prompt": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"", "output": ": A structure.\n  \"\"\"\n  if is_sequence(from_structure):\n    if is_sequence(to_structure):\n      return type(from_structure)(\n          maybe_broadcast_structure(s, t) for s, t in zip(from_structure,\n                                                         to_structure))\n    else:\n      return type(from_structure)(\n          maybe_broadcast_structure(s, to_structure) for s in from_structure)\n  else:\n    if is_sequence(to_structure):\n      return type(", "solution": "\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)"}
{"prompt": "def main():\n    \"\"\"Initialize and run command line interface.\"\"\"", "output": "\n    import argparse\n    import sys\n\n    from . import __version__\n    from . import cli\n\n    parser = argparse.ArgumentParser(\n        description=\"A command line interface for the OpenAI API.\"\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"%(prog)s {version}\".format(version=__version__),\n    )\n    subparsers = parser.add_subparsers(dest=\"command\")\n\n    # Add subparsers\n    cli.add_", "solution": "\n    # Create and populate parser\n    parser = create_parser()\n\n    # Parse given arguments\n    args = parser.parse_args()\n\n    # Checking arguments\n    check_arguments(args, parser)\n\n    # BUSINESS LOGIC IS FOLLOWING\n    run(parser, args)"}
{"prompt": "def from_tuplelist(tuple_list):\n        \"\"\"\n        Populates a Layout from a list containing virtual\n        qubits---(QuantumRegister, int) tuples---, or None.\n\n        Args:\n            tuple_list (list):\n                e.g.: [qr[0], None, qr[2], qr[3]]\n        Returns:\n            Layout: the corresponding Layout object\n        Raises:\n            LayoutError: If the elements are not (Register, integer) or None\n        \"\"\"", "output": "\n        layout = Layout()\n        for element in tuple_list:\n            if element is None:\n                layout.add_register(None)\n            elif isinstance(element, tuple):\n                if len(element) != 2:\n                    raise LayoutError('Tuple must contain two elements.')\n                if not isinstance(element[0], QuantumRegister):\n                    raise LayoutError('First element of tuple must be a '\n                                      'QuantumRegister.')\n                if not isinstance(element[1], int):\n                    raise LayoutError('", "solution": "\n        out = Layout()\n        for physical, virtual in enumerate(tuple_list):\n            if virtual is None:\n                continue\n            elif Layout.is_virtual(virtual):\n                if virtual in out._v2p:\n                    raise LayoutError('Duplicate values not permitted; Layout is bijective.')\n                out[virtual] = physical\n            else:\n                raise LayoutError(\"The list should contain elements of the form\"\n                                  \" (Register, integer) or None\")\n        return out"}
{"prompt": "def reject_connection(self, reason=None):\n        \"\"\"\n        Reject a pending connection.\n        \"\"\"", "output": "\n        if self._state != self.CONNECTING:\n            raise RuntimeError(\"Cannot reject connection in state %s\" % self._state)\n        self._state = self.DISCONNECTED\n        self._close_connection(reason)\n\n    def _close_connection(self, reason=None):\n        \"\"\"\n        Close the connection.\n        \"\"\"\n        if self._state == self.DISCONNECTED:\n            return\n        self._state = self.DISCONNECTED\n        if self._connection is not None:\n            self._", "solution": "\n        assert self.pending, \"Connection is not pending.\"\n        if reason is None:\n            reason = ConnectionRefusedError()\n        self._accept_d.errback(reason)"}
{"prompt": "def signature(frame):\n    \"\"\"return suitable frame signature to key display expressions off of.\"\"\"", "output": "\n    return frame.get('signature', frame.get('name', frame.get('filename')))\n\n\ndef _get_frame_info(frame):\n    \"\"\"return frame info as a dict.\"\"\"\n    return {\n        'filename': frame.f_code.co_filename,\n        'name': frame.f_code.co_name,\n        'lineno': frame.f_lineno,\n        'signature': signature(frame),\n    }\n\n\ndef _get_frame_info_list(frames):\n    \"\"\"return a list", "solution": "\n    if not frame: return None\n    code = frame.f_code\n    return (code.co_name, code.co_filename, code.co_firstlineno)"}
{"prompt": "async def parse_tag_results(soup):\n    \"\"\"\n    Parse a page of tag or trait results. Same format.\n\n    :param soup: BS4 Class Object\n    :return: A list of tags, Nothing else really useful there\n    \"\"\"", "output": "\n    tags = []\n    for tag in soup.find_all('div', class_='tag-container'):\n        tags.append(tag.text)\n    return tags\n\n\ndef parse_trait_results(soup):\n    \"\"\"\n    Parse a page of trait results. Same format.\n\n    :param soup: BS4 Class Object\n    :return: A list of traits, Nothing else really useful there\n    \"\"\"\n    traits = []\n    for trait in soup.find_all('div', class_='trait-container'):\n       ", "solution": "\n    soup = soup.find_all('td', class_='tc3')\n    tags = []\n    for item in soup:\n        tags.append(item.a.string)\n    return tags"}
{"prompt": "def mean(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the mean for expression, possibly on a grid defined by binby.\n\n        Example:\n\n        >>> df.mean(\"x\")\n        -0.067131491264005971\n        >>> df.mean(\"(x**2+y**2)**0.5\", binby=\"E\", shape=4)\n        array([  2.43483742,   4.41840721,   8.26742458,  15.53846476])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}\n        \"\"\"", "output": "([0.00017396, 0.00020902, 0.00023811, 0.00026353])\n\n        Args:\n            expression (str): The expression to calculate the mean for.\n            binby (list of str, optional): The columns to bin by.\n            limits (list of tuple, optional): The limits to use for the bins.\n            shape (tuple of int, optional): The shape of the grid.\n            selection (bool, optional):", "solution": "\n        return self._compute_agg('mean', expression, binby, limits, shape, selection, delay, edges, progress)\n        logger.debug(\"mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r\", expression, binby, limits, shape, selection, delay)\n        expression = _ensure_strings_from_expressions(expression)\n        selection = _ensure_strings_from_expressions(selection)\n        binby = _ensure_strings_from_expressions(binby)\n\n        @delayed\n        def calculate(expression, limits):\n            task = tasks.TaskStatistic(self, binby, shape, limits, weight=expression, op=tasks.OP_ADD_WEIGHT_MOMENTS_01, selection=selection)\n            self.executor.schedule(task)\n            progressbar.add_task(task, \"mean for %s\" % expression)\n            return task\n\n        @delayed\n        def finish(*stats_args):\n            stats = np.array(stats_args)\n            counts = stats[..., 0]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                mean = stats[..., 1] / counts\n            return vaex.utils.unlistify(waslist, mean)\n        waslist, [expressions, ] = vaex.utils.listify(expression)\n        progressbar = vaex.utils.progressbars(progress)\n        limits = self.limits(binby, limits, delay=True)\n        stats = [calculate(expression, limits) for expression in expressions]\n        var = finish(*stats)\n        return self._delay(delay, var)"}
{"prompt": "def build_fake_data(num_examples=10):\n  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"", "output": "\n  # Create a fake dataset\n  fake_data = np.zeros((num_examples, 28, 28, 1), dtype=np.float32)\n  fake_labels = np.zeros((num_examples, 10), dtype=np.float32)\n  for i in range(num_examples):\n    label = i % 10\n    fake_data[i, :, :, 0] = (label + 1) * np.ones((28, 28), dtype=np.float3", "solution": "\n\n  class Dummy(object):\n    pass\n\n  num_examples = 10\n  mnist_data = Dummy()\n  mnist_data.train = Dummy()\n  mnist_data.train.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.train.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.train.num_examples = num_examples\n  mnist_data.validation = Dummy()\n  mnist_data.validation.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.validation.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.validation.num_examples = num_examples\n  return mnist_data"}
{"prompt": "def min(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the minimum for given expressions, possibly on a grid defined by binby.\n\n\n        Example:\n\n        >>> df.min(\"x\")\n        array(-128.293991)\n        >>> df.min([\"x\", \"y\"])\n        array([-128.293991 ,  -71.5523682])\n        >>> df.min(\"x\", binby=\"x\", shape=5, limits=[-10, 10])\n        array([-9.99919128, -5.99972439, -1.99991322,  2.0000093 ,  6.0004878 ])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}, the last dimension is of shape (2)\n        \"\"\"", "output": "24])\n\n        Args:\n            expression (str or list): The expression or expressions to calculate the minimum for.\n            binby (str or list): The column(s) to bin by.\n            limits (list): The limits to use for the bins.\n            shape (tuple): The shape of the output array.\n            selection (bool): If True, only the selected rows will be used.\n            delay (bool): If True, the calculation will be delayed until the ``compute`` method is called.\n            progress (bool): If True, progress messages will be printed.", "solution": "\n        return self._compute_agg('min', expression, binby, limits, shape, selection, delay, edges, progress)\n        @delayed\n        def finish(result):\n            return result[..., 0]\n        return self._delay(delay, finish(self.minmax(expression, binby=binby, limits=limits, shape=shape, selection=selection, delay=delay, progress=progress)))"}
{"prompt": "def _create_value(self, *args, **kwargs):\n        \"\"\"\n        Lowest value generator.\n\n        Separated from __call__, because it seems that python\n        cache __call__ reference on module import\n        \"\"\"", "output": "\n        return self.value(*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Highest value generator.\n\n        Separated from __call__, because it seems that python\n        cache __call__ reference on module import\n        \"\"\"\n        return self.value(*args, **kwargs)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Get attribute from value.\n\n        :param name: attribute name\n        :return: attribute value\n        \"\"\"\n        return getattr(self.value", "solution": "\n        if not len(args):\n            raise TypeError('Object instance is not provided')\n\n        if self.by_instance:\n            field_type = args[0]\n        else:\n            field_type = args[0].__class__\n\n        function = self.registry.get(field_type, self.default)\n\n        if function is None:\n            raise TypeError(\"no match %s\" % field_type)\n\n        return function(*args, **kwargs)"}
{"prompt": "def join_lines(iterator):\n    \"\"\"\n    Joins a line ending in '\\' with the previous line.\n    \"\"\"", "output": "\n    for line in iterator:\n        if line.endswith('\\\\'):\n            yield line[:-1]\n        else:\n            yield line\n\n\ndef parse_line(line):\n    \"\"\"\n    Parses a line of the form:\n        <key> <value>\n    \"\"\"\n    key, value = line.split(None, 1)\n    return key, value\n\n\ndef parse_lines(iterator):\n    \"\"\"\n    Parses a line of the form:\n        <key> <value>\n    \"\"\"\n    for line", "solution": "\n    lines = []\n    for line in iterator:\n        if not line.endswith('\\\\'):\n            if lines:\n                lines.append(line)\n                yield ''.join(lines)\n                lines = []\n            else:\n                yield line\n        else:\n            lines.append(line.strip('\\\\'))"}
{"prompt": "def handle(self):\n        \"\"\"\n        Handle the backend of the given status.\n        \"\"\"", "output": "\n        if self.status == 'success':\n            self.handle_success()\n        elif self.status == 'failure':\n            self.handle_failure()\n        elif self.status == 'error':\n            self.handle_error()\n        elif self.status == 'skipped':\n            self.handle_skipped()\n        elif self.status == 'cancelled':\n            self.handle_cancelled()\n        elif self.status == 'pending':\n            self.handle_pending()\n        elif self.status == 'running':\n           ", "solution": "\n\n        # We initiate the source we are going to parse to the Generate class.\n        source = \"URL\"\n\n        if self.catched.lower() not in PyFunceble.STATUS[\"list\"][\"invalid\"]:\n            # The parsed status is not in the list of invalid.\n\n            # We generate the status file with the catched status.\n            Generate(self.catched, source).status_file()\n        else:\n            # The parsed status is in the list of invalid.\n\n            # We generate the status file with the parsed status.\n            Generate(self.catched, \"SYNTAX\").status_file()\n\n        # We return the parsed status.\n        return self.catched"}
{"prompt": "def _segmentPath(self, path):\n        \"\"\"Return list of valid *path* segments.\"\"\"", "output": "\n        return [s for s in path.split('/') if s]\n\n    def _isValidPath(self, path):\n        \"\"\"Return True if *path* is valid.\"\"\"\n        return self._segmentPath(path) == self._segmentPath(self.path)\n\n    def _isValidSegment(self, segment):\n        \"\"\"Return True if *segment* is valid.\"\"\"\n        return segment in self.segments\n\n    def _isValidSegments(self, segments):\n        \"\"\"Return True if *segments* is valid.\"\"\"\n       ", "solution": "\n        parts = []\n        model = self._filesystemWidget.model()\n\n        # Separate root path from remainder.\n        remainder = path\n\n        while True:\n            if remainder == model.root.path:\n                break\n\n            if remainder:\n                parts.append(remainder)\n\n            head, tail = os.path.split(remainder)\n            if head == remainder:\n                break\n\n            remainder = head\n\n        parts.append(model.root.path)\n        return parts"}
{"prompt": "def p_gate_op_5(self, program):\n        \"\"\"\n        gate_op : BARRIER id_list ';'\n        \"\"\"", "output": "\n        program.append(GateOp(BARRIER, program[2]))\n\n    def p_gate_op_6(self, program):\n        \"\"\"\n        gate_op : MEASURE id_list '[' id_list ']' ';'\n        \"\"\"\n        program.append(GateOp(MEASURE, program[2], program[4]))\n\n    def p_gate_op_7(self, program):\n        \"\"\"\n        gate_op : MEASURE id_list ';'\n        \"\"\"\n        program.append(G", "solution": "\n        program[0] = node.Barrier([program[2]])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def luhn_check(card_number):\n    \"\"\" checks to make sure that the card passes a luhn mod-10 checksum \"\"\"", "output": "\n    if len(card_number) < 13:\n        return False\n    else:\n        checksum = 0\n        for i in range(len(card_number) - 1, -1, -1):\n            digit = int(card_number[i])\n            if (i % 2) == 0:\n                digit *= 2\n                if digit > 9:\n                    digit -= 9\n            checksum += digit\n        return checksum % 10 == 0\n\n\ndef get_card_type(card", "solution": "\n    sum = 0\n    num_digits = len(card_number)\n    oddeven = num_digits & 1\n\n    for count in range(0, num_digits):\n        digit = int(card_number[count])\n\n        if not ((count & 1) ^ oddeven):\n            digit *= 2\n        if digit > 9:\n            digit -= 9\n\n        sum += digit\n\n    return (sum % 10) == 0"}
{"prompt": "def _make_empty_queue_for(k, element):\n  \"\"\"Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.\n\n  For example:\n\n  ```python\n    element = tf.constant([[0., 1., 2., 3., 4.],\n                           [5., 6., 7., 8., 9.]])\n\n    # A queue capable of holding 3 elements.\n    _make_empty_queue_for(3, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: A positive scalar integer, number of elements that each queue will hold.\n    element: A `tf.Tensor`, only its shape and dtype information are relevant.\n\n  Returns:\n    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype\n    as `element`.\n  \"\"\"", "output": "0., 1., 2., 3., 4.],\n           [5., 6., 7., 8., 9.],\n           [0., 0., 0., 0., 0.]],\n          [[0., 0., 0., 0., 0.],\n           [0., 0., 0., 0., 0.],\n           [0., 0., 0., 0., 0.]],\n          [[0., 0., 0., 0., 0.],\n           [0.,", "solution": "\n  queue_shape = tf.concat(\n      [[k], distribution_util.prefer_static_shape(element)], axis=0)\n  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)"}
{"prompt": "def join(self):\n        \"\"\"Waits until all there are no processes enqueued.\"\"\"", "output": "\n        self._queue.join()\n\n    def _put(self, item):\n        \"\"\"Put an item into the queue.\"\"\"\n        self._queue.put(item)\n\n    def _get(self):\n        \"\"\"Get an item from the queue.\"\"\"\n        return self._queue.get()\n\n    def _put_nowait(self, item):\n        \"\"\"Put an item into the queue without blocking.\"\"\"\n        self._queue.put_nowait(item)\n\n    def _get_nowait(self):\n        \"\"\"Get an item from", "solution": "\n        while True:\n            with self._processes_lock:\n                if not self._processes and self._processing_semaphore.locked():\n                    return True\n            gevent.sleep(0.1)"}
{"prompt": "def phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype='l',**kwargs):\n    \"\"\"\n    Calculates phase difference between X and Y feeds given U and V (U and Q for circular basis)\n    data from a noise diode measurement on the target\n    \"\"\"", "output": "\n    # Calculate phase difference between X and Y feeds\n    # This is done by calculating the phase difference between the U and V data\n    # and then subtracting the phase difference between the U and Q data\n    # The phase difference between U and Q is calculated by taking the difference\n    # between the phase of the U and V data and then subtracting the phase of the\n    # U and I data.\n    # The phase difference between U and V is calculated by taking the difference\n    # between the phase of the U and V data and then subtracting the phase of the\n   ", "solution": "\n    #Fold noise diode data and calculate ON OFF diferences for U and V\n    if feedtype=='l':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        V_OFF,V_ON = foldcal(Vdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Vdiff = V_ON-V_OFF\n        poffset = np.arctan2(-1*Vdiff,Udiff)\n\n    if feedtype=='c':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        Q_OFF,Q_ON = foldcal(Qdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Qdiff = Q_ON-Q_OFF\n        poffset = np.arctan2(Udiff,Qdiff)\n\n    coarse_p =  convert_to_coarse(poffset,chan_per_coarse)\n\n    #Correct for problems created by discontinuity in arctan\n    #Find whether phase offsets have increasing or decreasing slope\n    y = coarse_p[:6]\n    x = np.arange(y.size)\n    m = np.polyfit(x,y,1)[0]\n\n    for i in range(coarse_p.size-3):\n        if (m>0 and coarse_p[i+1]<coarse_p[i]) or (m<0 and coarse_p[i+1]>coarse_p[i]):\n            coarse_p[i+1] = 2*coarse_p[i+2]-coarse_p[i+3]    #Move problem point near the next\n\n    return coarse_p"}
{"prompt": "def filter_on(self, filt=None, analyte=None, samples=None, subset=None, show_status=False):\n        \"\"\"\n        Turns data filters on for particular analytes and samples.\n\n        Parameters\n        ----------\n        filt : optional, str or array_like\n            Name, partial name or list of names of filters. Supports\n            partial matching. i.e. if 'cluster' is specified, all\n            filters with 'cluster' in the name are activated.\n            Defaults to all filters.\n        analyte : optional, str or array_like\n            Name or list of names of analytes. Defaults to all analytes.\n        samples : optional, array_like or None\n            Which samples to apply this filter to. If None, applies to all\n            samples.\n\n        Returns\n        -------\n        None\n        \"\"\"", "output": "te : optional, str or array_like\n            Name, partial name or list of names of analytes. Supports\n            partial matching. i.e. if 'cluster' is specified, all\n            analytes with 'cluster' in the name are activated.\n            Defaults to all analytes.\n        samples : optional, str or array_like\n            Name, partial name or list of names of samples. Supports\n            partial matching. i.e. if 'cluster' is specified, all\n            samples with 'cluster' in the name are activated.\n            Defaults to", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            try:\n                self.data[s].filt.on(analyte, filt)\n            except:\n                warnings.warn(\"filt.on failure in sample \" + s)\n\n        if show_status:\n            self.filter_status(subset=subset)\n        return"}
{"prompt": "def _add_https(self, q):\n        \"\"\"for push, pull, and other api interactions, the user can optionally\n           define a custom registry. If the registry name doesn't include http\n           or https, add it.\n \n           Parameters\n           ==========\n           q: the parsed image query (names), including the original\n        \"\"\"", "output": "\n        if not q.registry:\n            return\n        if not q.registry.startswith(\"http\"):\n            q.registry = \"https://\" + q.registry\n\n    def _add_tag(self, q):\n        \"\"\"for push, pull, and other api interactions, the user can optionally\n           define a custom tag. If the tag is not defined, add it.\n \n           Parameters\n           ==========\n           q: the parsed image query (names), including the original\n        \"\"\"\n        if not q.tag:\n           ", "solution": "\n\n        # If image uses http or https, add back\n        if not q['registry'].startswith('http'):\n\n            if q['original'].startswith('http:'):\n                q['registry'] = 'http://%s' % q['registry']\n\n            elif q['original'].startswith('https:'):\n                q['registry'] = 'https://%s' % q['registry']\n\n            # Otherwise, guess from the user's environment\n            else:\n\n                prefix = 'https://'\n\n                # The user can set an environment variable to specify nohttps\n                nohttps = os.environ.get('SREGISTRY_REGISTRY_NOHTTPS')\n                if nohttps != None:\n                    prefix = 'http://'\n                q['registry'] = '%s%s' %(prefix, q['registry'])\n\n        return q"}
{"prompt": "def clean_tempfiles():\n  \"\"\"Clean up temp files\"\"\"", "output": "\n  import glob\n  import os\n  import shutil\n  import tempfile\n\n  temp_dir = tempfile.gettempdir()\n  for f in glob.glob(os.path.join(temp_dir, '*')):\n    if os.path.isfile(f):\n      os.remove(f)\n    elif os.path.isdir(f):\n      shutil.rmtree(f)\n\n\ndef get_temp_dir():\n  \"\"\"Get the temp dir\"\"\"\n  import tempfile\n  return tempfile.get", "solution": "\n  for fn in TEMP_FILES:\n    if os.path.exists(fn):\n      os.unlink(fn)"}
{"prompt": "def get_section_by_sis_id(self, sis_section_id, params={}):\n        \"\"\"\n        Return section resource for given sis id.\n        \"\"\"", "output": "\n        return self._get_section_by_id(sis_section_id, params=params)\n\n    def get_section_by_id(self, section_id, params={}):\n        \"\"\"\n        Return section resource for given id.\n        \"\"\"\n        return self._get_section_by_id(section_id, params=params)\n\n    def _get_section_by_id(self, section_id, params={}):\n        \"\"\"\n        Return section resource for given id.\n        \"\"\"\n        return self._get_section", "solution": "\n        return self.get_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)"}
{"prompt": "async def get_tracks(self, *, limit=20, offset=0) -> List[Track]:\n        \"\"\"Get a list of the songs saved in the current Spotify user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optional[int]\n            The index of the first item to return. Default: 0\n        \"\"\"", "output": "\n        return self.http.get_tracks(limit=limit, offset=offset)\n\n    def get_track(self, track_id: str) -> Track:\n        \"\"\"Get Spotify catalog information for a single track identified by its unique Spotify ID.\n\n        Parameters\n        ----------\n        track_id : str\n            The Spotify ID for the track.\n        \"\"\"\n        return self.http.get_track(track_id)\n\n    def get_albums(self, *, limit=20, offset=0", "solution": "\n        data = await self.user.http.saved_tracks(limit=limit, offset=offset)\n\n        return [Track(self.__client, item['track']) for item in data['items']]"}
{"prompt": "def get_direct(self):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be read.\n\n        :return: A guard object that returns the buffer element.\n        \"\"\"", "output": "\n        return self.buffer.get()\n\n    def get_direct_nowait(self):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Returns None if there is no data that can be read.\n\n        :return: A guard object that returns the buffer element.\n        \"\"\"\n        return self.buffer.get_nowait()\n\n    def get_all(self):\n        \"\"\"\n        Allows direct access to the buffer elements.\n        Blocks until there is data that can be read.\n\n        :return: A guard object that returns", "solution": "\n\n        read_idx = self.__get_idx()\n\n        if read_idx is QueueClosed:\n            return QueueClosed\n\n        # Once the guard is released, read_idx will be placed into write_queue.\n        return self.Guard(self.write_queue, self.arys, lambda: read_idx)"}
{"prompt": "def _get_format_from_style(self, token, style):\n        \"\"\" Returns a QTextCharFormat for token by reading a Pygments style.\n        \"\"\"", "output": "\n        format = QTextCharFormat()\n        format.setFont(self.font)\n        format.setForeground(self.color_scheme.get_style_property(\n            style, 'color', token))\n        format.setBackground(self.color_scheme.get_style_property(\n            style, 'bgcolor', token))\n        format.setFontWeight(self.color_scheme.get_style_property(\n            style, 'bold', token))\n        format.setFontItalic(self.color_scheme.get_", "solution": "\n        result = QtGui.QTextCharFormat()\n        for key, value in style.style_for_token(token).items():\n            if value:\n                if key == 'color':\n                    result.setForeground(self._get_brush(value))\n                elif key == 'bgcolor':\n                    result.setBackground(self._get_brush(value))\n                elif key == 'bold':\n                    result.setFontWeight(QtGui.QFont.Bold)\n                elif key == 'italic':\n                    result.setFontItalic(True)\n                elif key == 'underline':\n                    result.setUnderlineStyle(\n                        QtGui.QTextCharFormat.SingleUnderline)\n                elif key == 'sans':\n                    result.setFontStyleHint(QtGui.QFont.SansSerif)\n                elif key == 'roman':\n                    result.setFontStyleHint(QtGui.QFont.Times)\n                elif key == 'mono':\n                    result.setFontStyleHint(QtGui.QFont.TypeWriter)\n        return result"}
{"prompt": "def p_closed_proposition_list(self, p):\n        \"\"\" closed_proposition_list :  closed_proposition_list SLASH SLASH closed_proposition\n                                    | closed_proposition\"\"\"", "output": "\n        if len(p) == 5:\n            p[0] = p[1] + [p[4]]\n        else:\n            p[0] = [p[1]]\n\n    def p_closed_proposition(self, p):\n        \"\"\" closed_proposition :  atom\n                                | negation\n                                | conjunction\n                                | disjunction\n                                | implication\n                                | equivalence\n                                | universal_quantification\n                                | existential_quantification\n                                | negated_existential_quantification", "solution": "\n        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[0] = p[1] + [p[4]]"}
{"prompt": "def docker(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"", "output": "\n        return self.docker_client.call(method, *args, **kwargs)\n\n    def _get_container_name(self, container_id):\n        \"\"\"Get the container name from the container id\n\n        returns a Future\n        \"\"\"\n        return self.docker('inspect_container', container_id)\n\n    def _get_container_image(self, container_id):\n        \"\"\"Get the container image from the container id\n\n        returns a Future\n        \"\"\"\n        return self.docker('inspect_container', container_id)\n\n   ", "solution": "\n        return self.executor.submit(self._docker, method, *args, **kwargs)"}
{"prompt": "def restore_context(self) -> bool:\n        \"\"\"Rollback to previous saved position.\"\"\"", "output": "\n        if self.context_stack:\n            self.context = self.context_stack.pop()\n            return True\n        return False\n\n    def save_context(self) -> bool:\n        \"\"\"Save current position.\"\"\"\n        if self.context:\n            self.context_stack.append(self.context)\n            return True\n        return False\n\n    def get_context(self) -> Optional[Context]:\n        \"\"\"Get current context.\"\"\"\n        return self.context\n\n    def get_context_stack(self) -> List[Context]:\n", "solution": "\n        self._cursor.position = self._contexts.pop()\n        return False"}
{"prompt": "def wave_interp_option(obj):\n    r\"\"\"\n    Validate if an object is a :ref:`WaveInterpOption` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by the name of the argument the\n     contract is attached to\n\n    :rtype: None\n    \"\"\"", "output": "\n    if not isinstance(obj, WaveInterpOption):\n        raise RuntimeError(\n            \"Argument `{}` is not valid\".format(\n                obj.__class__.__name__\n            )\n        )\n\n\ndef wave_interp_options(obj):\n    r\"\"\"\n    Validate if an object is a :ref:`WaveInterpOptions` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is", "solution": "\n    exdesc = pexdoc.pcontracts.get_exdesc()\n    if not isinstance(obj, str):\n        raise ValueError(exdesc)\n    if obj.upper() in [\"CONTINUOUS\", \"STAIRCASE\"]:\n        return None\n    raise ValueError(exdesc)"}
{"prompt": "def run(self, dag):\n        \"\"\"\n        Pick a layout by assigning n circuit qubits to device qubits 0, .., n-1.\n\n        Args:\n            dag (DAGCircuit): DAG to find layout for.\n\n        Raises:\n            TranspilerError: if dag wider than self.coupling_map\n        \"\"\"", "output": "\n        if len(dag.qubits) > len(self.coupling_map.get_all_qubits()):\n            raise TranspilerError(\"Layout pass requires a coupling map with at least as many qubits as the \"\n                                  \"DAG has qubits.\")\n\n        # initialize empty layout\n        layout = Layout()\n\n        # assign qubits to qubits in coupling map\n        for qubit in dag.qubits:\n            layout[qubit] = self.coupling_map.get_qubit_by_index(qubit.index", "solution": "\n        num_dag_qubits = sum([qreg.size for qreg in dag.qregs.values()])\n        if num_dag_qubits > self.coupling_map.size():\n            raise TranspilerError('Number of qubits greater than device.')\n        self.property_set['layout'] = Layout.generate_trivial_layout(*dag.qregs.values())"}
{"prompt": "def update(self,dt):\n        \"\"\"\n        Should be called regularly to move the actor.\n        \n        This method does nothing if the :py:attr:`enabled` property is set to False.\n        \n        This method is called automatically and should not be called manually.\n        \"\"\"", "output": "\n        if not self.enabled:\n            return\n        \n        # update position\n        self.position += self.velocity*dt\n        \n        # update rotation\n        self.rotation += self.angular_velocity*dt\n        \n        # update scale\n        self.scale += self.scale_velocity*dt\n        \n        # update color\n        self.color += self.color_velocity*dt\n        \n        # update alpha\n        self.alpha += self.alpha_velocity*dt\n        \n        # update alpha\n        self.alpha +=", "solution": "\n        if not self.enabled:\n            return\n        dy = self.speed * dt * self.move\n        x,y,z = self.actor._pos\n        newpos = x,dy+y,z\n        self.actor.pos = newpos"}
{"prompt": "def _start_webserver(self, authorize_url=None):\n\t\t\"\"\"\n\t\tStart the webserver that will receive the code\n\t\t\"\"\"", "output": "\n\t\tself.webserver = WebServer(self.config, authorize_url)\n\t\tself.webserver.start()\n\n\tdef _stop_webserver(self):\n\t\t\"\"\"\n\t\tStop the webserver\n\t\t\"\"\"\n\t\tself.webserver.stop()\n\n\tdef _get_authorize_url(self):\n\t\t\"\"\"\n\t\tGet the authorize url\n\t\t\"\"\"\n\t\treturn self.webserver.get_authorize_url()\n\n\tdef _get_access_token(", "solution": "\n\t\tserver_address = (SERVER_URL, SERVER_PORT)\n\t\tself.server = HTTPServer(server_address, OAuth2UtilRequestHandler)\n\t\tself.server.response_code = None\n\t\tself.server.authorize_url = authorize_url\n\t\tt = Thread(target=self.server.serve_forever)\n\t\tt.daemon = True\n\t\tt.start()"}
{"prompt": "def transcode_to_stream(input_filename, date_format=None):\n    \"\"\"\n    Read a JSON or CSV file and convert it into a JSON stream, which will\n    be saved in an anonymous temp file.\n    \"\"\"", "output": "\n    if input_filename.endswith('.json'):\n        return transcode_json_to_stream(input_filename, date_format)\n    elif input_filename.endswith('.csv'):\n        return transcode_csv_to_stream(input_filename, date_format)\n    else:\n        raise ValueError('Unknown file extension: %s' % input_filename)\n\n\ndef transcode_json_to_stream(input_filename, date_format=None):\n    \"\"\"\n    Read a JSON file and convert it into a JSON stream, which will", "solution": "\n    tmp = tempfile.TemporaryFile()\n    for entry in open_json_or_csv_somehow(input_filename,\n                                          date_format=date_format):\n        tmp.write(json.dumps(entry, ensure_ascii=False).encode('utf-8'))\n        tmp.write(b'\\n')\n    tmp.seek(0)\n    return tmp"}
{"prompt": "def visit_classdef(self, node):\n        \"\"\"visit an astroid.Class node\n\n         * set the locals_type and instance_attrs_type mappings\n         * set the implements list and build it\n         * optionally tag the node with a unique id\n        \"\"\"", "output": "\n        self.locals_type = self.locals_type.copy()\n        self.instance_attrs_type = self.instance_attrs_type.copy()\n        self.implements = []\n        self.class_name = node.name\n        if node.parent:\n            self.class_name = node.parent.name + '.' + node.name\n        if node.parent and isinstance(node.parent, astroid.Class):\n            self.implements.append(node.parent.name)\n        if node.parent and is", "solution": "\n        if hasattr(node, \"locals_type\"):\n            return\n        node.locals_type = collections.defaultdict(list)\n        if self.tag:\n            node.uid = self.generate_id()\n        # resolve ancestors\n        for baseobj in node.ancestors(recurs=False):\n            specializations = getattr(baseobj, \"specializations\", [])\n            specializations.append(node)\n            baseobj.specializations = specializations\n        # resolve instance attributes\n        node.instance_attrs_type = collections.defaultdict(list)\n        for assignattrs in node.instance_attrs.values():\n            for assignattr in assignattrs:\n                self.handle_assignattr_type(assignattr, node)\n        # resolve implemented interface\n        try:\n            node.implements = list(interfaces(node, self.inherited_interfaces))\n        except astroid.InferenceError:\n            node.implements = ()"}
{"prompt": "def _create_input_transactions(self, addy):\n        # type: (Address) -> None\n        \"\"\"\n        Creates transactions for the specified input address.\n        \"\"\"", "output": "\n        # Create a transaction for each output address.\n        for output_address in self.output_addresses:\n            # Create a transaction for each output address.\n            tx = Transaction(\n                inputs=[\n                    TxInput(\n                        address=addy,\n                        value=self.output_addresses[output_address],\n                        script=self.output_addresses[output_address].script,\n                    )\n                ],\n                outputs=[\n                    TxOutput(\n                        address=output_address,\n                        value=self", "solution": "\n        self._transactions.append(ProposedTransaction(\n            address=addy,\n            tag=self.tag,\n\n            # Spend the entire address balance; if necessary, we will\n            # add a change transaction to the bundle.\n            value=-addy.balance,\n        ))\n\n        # Signatures require additional transactions to store, due to\n        # transaction length limit.\n        # Subtract 1 to account for the transaction we just added.\n        for _ in range(addy.security_level - 1):\n            self._transactions.append(ProposedTransaction(\n                address=addy,\n                tag=self.tag,\n\n                # Note zero value; this is a meta transaction.\n                value=0,\n            ))"}
{"prompt": "def _define(self):\n        \"\"\"Calculate a subcircuit that implements this initialization\n\n        Implements a recursive initialization algorithm, including optimizations,\n        from \"Synthesis of Quantum Logic Circuits\" Shende, Bullock, Markov\n        https://arxiv.org/abs/quant-ph/0406176v5\n\n        Additionally implements some extra optimizations: remove zero rotations and\n        double cnots.\n        \"\"\"", "output": "\n        # pylint: disable=too-many-branches\n        # pylint: disable=too-many-statements\n        # pylint: disable=too-many-locals\n        # pylint: disable=too-many-nested-blocks\n        # pylint: disable=too-many-return-statements\n        # pylint: disable=too-many-arguments\n        # pylint: disable=too-many-lines\n        # pylint: disable=too-many-", "solution": "\n        # call to generate the circuit that takes the desired vector to zero\n        disentangling_circuit = self.gates_to_uncompute()\n\n        # invert the circuit to create the desired vector from zero (assuming\n        # the qubits are in the zero state)\n        initialize_instr = disentangling_circuit.to_instruction().inverse()\n\n        q = QuantumRegister(self.num_qubits, 'q')\n        initialize_circuit = QuantumCircuit(q, name='init_def')\n        for qubit in q:\n            initialize_circuit.append(Reset(), [qubit])\n        initialize_circuit.append(initialize_instr, q[:])\n\n        self.definition = initialize_circuit.data"}
{"prompt": "def generate_query(command):\n    \"\"\"Add header, checksum and footer to command data.\"\"\"", "output": "\n    command_data = command.get_data()\n    header = command.get_header()\n    footer = command.get_footer()\n    checksum = command.get_checksum()\n    return header + command_data + checksum + footer\n\n\ndef parse_query(query):\n    \"\"\"Parse query data and return command object.\"\"\"\n    header = query[:2]\n    command_data = query[2:-2]\n    checksum = query[-2:]\n    footer = query[-1]\n    command = Command(header, command_data,", "solution": "\n    data = bytearray(command)\n    c = checksum(data)\n    data.append(c >> 8)\n    data.append(c & 0xFF)\n    data.replace(b'\\xFE', b'\\xFE\\xF0')\n\n    data = bytearray.fromhex(\"FEFE\") + data + bytearray.fromhex(\"FE0D\")\n    return data"}
{"prompt": "def copy(self):\n        \"\"\"\n        :returns: datasketch.MinHash -- A copy of this MinHash by exporting its state.\n        \"\"\"", "output": "\n        return MinHash(self.seed, self.hash_type, self.hash_set)\n\n    def __eq__(self, other):\n        \"\"\"\n        :param other: datasketch.MinHash -- The other MinHash to compare with.\n        :returns: bool -- True if the two MinHashes are equal, False otherwise.\n        \"\"\"\n        if not isinstance(other, MinHash):\n            return False\n        return self.hash_set == other.hash_set\n\n    def __ne__(self, other):\n        \"\"\"\n        :param", "solution": "\n        return MinHash(seed=self.seed, hashfunc=self.hashfunc,\n                hashvalues=self.digest(),\n                permutations=self.permutations)"}
{"prompt": "def publish(\n            self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Publish runbook draft.\n\n        :param resource_group_name: Name of an Azure Resource group.\n        :type resource_group_name: str\n        :param automation_account_name: The name of the automation account.\n        :type automation_account_name: str\n        :param runbook_name: The parameters supplied to the publish runbook\n         operation.\n        :type runbook_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n        \"\"\"", "output": "the publish runbook\n         operation.\n        :type runbook_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype", "solution": "\n        raw_result = self._publish_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'location': 'str',\n                })\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _is_async(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async keyword.\"\"\"", "output": "\n    return o.meta.get(\"async\", False)\n\n\ndef _is_sync(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :sync keyword.\"\"\"\n    return o.meta.get(\"sync\", False)\n\n\ndef _is_async_or_sync(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async or :sync keyword.\"\"\"\n    return _is_async(o) or _is_sync(o)\n\n\ndef _is_async_or_sync_or_", "solution": "\n    return (  # type: ignore\n        Maybe(o.meta)\n        .map(lambda m: m.entry(SYM_ASYNC_META_KEY, None))\n        .or_else_get(False)\n    )"}
{"prompt": "def add_value(self, name, value):\n        \"\"\"\n        Add a new value to the list.\n\n        :param str name: name of the value that is being parsed\n        :param str value: value that is being parsed\n        :raises ietfparse.errors.MalformedLinkValue:\n            if *strict mode* is enabled and a validation error\n            is detected\n\n        This method implements most of the validation mentioned in\n        sections 5.3 and 5.4 of :rfc:`5988`.  The ``_rfc_values``\n        dictionary contains the appropriate values for the attributes\n        that get special handling.  If *strict mode* is enabled, then\n        only values that are acceptable will be added to ``_values``.\n\n        \"\"\"", "output": "5988_validate``\n        method is called to validate the value.  If the value is\n        valid, it is added to the list of values.\n\n        \"\"\"\n        if self._strict and not self._rfc5988_validate(name, value):\n            raise errors.MalformedLinkValue(\n                'Invalid value for {0}: {1}'.format(name, value))\n        self._values.append(value)\n\n    def _rfc5988_validate(self, name, value):\n        \"\"\"\n        Valid", "solution": "\n        try:\n            if self._rfc_values[name] is None:\n                self._rfc_values[name] = value\n            elif self.strict:\n                if name in ('media', 'type'):\n                    raise errors.MalformedLinkValue(\n                        'More than one {} parameter present'.format(name))\n                return\n        except KeyError:\n            pass\n\n        if self.strict and name in ('title', 'title*'):\n            return\n\n        self._values.append((name, value))"}
{"prompt": "def sp_search_query(query):\n    \"\"\"Translate a Mopidy search query to a Spotify search query\"\"\"", "output": "\n    if query.get('any'):\n        return '*'\n    if query.get('artist'):\n        return ' '.join(query['artist'])\n    if query.get('album'):\n        return ' '.join(query['album'])\n    if query.get('track'):\n        return ' '.join(query['track'])\n    if query.get('genre'):\n        return ' '.join(query['genre'])\n    if query.get('composer'):\n        return ' '.join(query['composer'])\n    if query.get('", "solution": "\n\n    result = []\n\n    for (field, values) in query.items():\n        field = SEARCH_FIELD_MAP.get(field, field)\n        if field is None:\n            continue\n\n        for value in values:\n            if field == 'year':\n                value = _transform_year(value)\n                if value is not None:\n                    result.append('%s:%d' % (field, value))\n            elif field == 'any':\n                result.append('\"%s\"' % value)\n            else:\n                result.append('%s:\"%s\"' % (field, value))\n\n    return ' '.join(result)"}
{"prompt": "def summary(self, fullpath=False):\n        \"\"\"Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files.\n\n        \"\"\"", "output": "\n        if fullpath:\n            return self._data\n        else:\n            return {os.path.basename(f): v for f, v in self._data.items()}\n\n    def get_coverage_data(self, fullpath=False):\n        \"\"\"Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files", "solution": "\n        summ = {}\n        if fullpath:\n            filename_fn = lambda f: f\n        else:\n            filename_fn = os.path.basename\n        for filename, lines in iitems(self.lines):\n            summ[filename_fn(filename)] = len(lines)\n        return summ"}
{"prompt": "def read_hex_integer(self) -> bool:\n    \"\"\"\n    read a hexadecimal number\n    Read the following BNF rule else return False::\n\n        readHexInteger = [\n            [ '0'..'9' | 'a'..'f' | 'A'..'F' ]+\n        ]\n    \"\"\"", "output": "\n    if self.current_char == '0':\n        self.advance()\n        if self.current_char in 'xX':\n            self.advance()\n            while self.current_char in '0123456789abcdefABCDEF':\n                self.advance()\n            return True\n        else:\n            return False\n    else:\n        return False\n\n\ndef read_octal_integer(self) -> bool:\n    \"\"\"\n    read an octal number\n    Read the following BNF rule else return False", "solution": "\n    if self.read_eof():\n        return False\n    self._stream.save_context()\n    c = self._stream.peek_char\n    if c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f'):\n        self._stream.incpos()\n        while not self.read_eof():\n            c = self._stream.peek_char\n            if not (c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f')):\n                break\n            self._stream.incpos()\n        return self._stream.validate_context()\n    return self._stream.restore_context()"}
{"prompt": "def init_app(self, app):\n        \"\"\"\n        Initializes the Flask-Allows object against the provided application\n        \"\"\"", "output": "\n        self.app = app\n        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE', 'default')\n        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_ID', 1)\n        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_NAME', 'default')\n        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_DESCRIPTION', 'default')\n        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_", "solution": "\n        if not hasattr(app, \"extensions\"):  # pragma: no cover\n            app.extensions = {}\n        app.extensions[\"allows\"] = self\n\n        @app.before_request\n        def start_context(*a, **k):\n            self.overrides.push(Override())\n            self.additional.push(Additional())\n\n        @app.after_request\n        def cleanup(response):\n            self.clear_all_overrides()\n            self.clear_all_additional()\n            return response"}
{"prompt": "def raw_dict_factory(cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types, including raw read hex strings.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            d[col[0]] = row[idx]\n        return d\n\n\ndef raw_list_factory(cursor, row):\n    \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n    Simple return of JSON safe types, including raw read hex strings.\n\n    Args:\n        cursor (sqlite cursor):  Original cursory\n        row (sqlite row tuple): Original row.\n\n    Returns:\n        list: modified row.\n    \"\"\"\n    return", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp or name == Field.Meter_Address:\n                d[name] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":\n                d[name] = str(val)\n                continue\n        return d"}
{"prompt": "def _onSelectItem(self, selection, previousSelection):\n        \"\"\"Handle selection of item in listing.\"\"\"", "output": "\n        if selection is None:\n            return\n        if selection == previousSelection:\n            return\n        if selection.isFolder:\n            self._onSelectFolder(selection)\n        else:\n            self._onSelectFile(selection)\n\n    def _onSelectFolder(self, folder):\n        \"\"\"Handle selection of folder in listing.\"\"\"\n        self.folder = folder\n        self.refresh()\n\n    def _onSelectFile(self, file):\n        \"\"\"Handle selection of file in listing.\"\"\"\n        self.file = file\n        self.refresh()\n", "solution": "\n        self._acceptButton.setEnabled(True)\n        del self._selected[:]\n        item = self._filesystemWidget.model().item(selection)\n        self._selected.append(item.path)"}
{"prompt": "def _srvc_set_config(self, trajectory):\n        \"\"\"Sets a config value to the Trajectory or changes it if the trajectory was loaded\n        a the settings no longer match\"\"\"", "output": "\n        if self.trajectory is None:\n            self.trajectory = trajectory\n        elif self.trajectory.settings != trajectory.settings:\n            self.trajectory = trajectory\n\n    def _srvc_get_config(self):\n        \"\"\"Returns the config value of the Trajectory\"\"\"\n        return self.trajectory.settings\n\n    def _srvc_get_trajectory(self):\n        \"\"\"Returns the Trajectory\"\"\"\n        return self.trajectory\n\n    def _srvc", "solution": "\n        def _set_config(name, value, comment):\n            if not trajectory.f_contains('config.'+name, shortcuts=False):\n                trajectory.f_add_config(Parameter, name, value, comment=comment)\n\n        for attr_name in HDF5StorageService.NAME_TABLE_MAPPING:\n            table_name = HDF5StorageService.NAME_TABLE_MAPPING[attr_name]\n            value = getattr(self, attr_name)\n            _set_config('hdf5.overview.' + table_name,\n                         value,\n                         comment='Whether or not to have an overview '\n                                 'table with that name')\n\n        _set_config('hdf5.purge_duplicate_comments',\n                    self._purge_duplicate_comments,\n                    comment='Whether comments of results and'\n                            ' derived parameters should only'\n                            ' be stored for the very first instance.'\n                            ' Works only if the summary tables are'\n                            ' active.')\n\n        _set_config('hdf5.results_per_run', self._results_per_run,\n                    comment='Expected number of results per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.derived_parameters_per_run',\n                    self._derived_parameters_per_run,\n                    comment='Expected number of derived parameters per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.complevel', self._complevel,\n                    comment='Compression Level (0 no compression '\n                            'to 9 highest compression)')\n\n        _set_config('hdf5.complib', self._complib,\n                    comment='Compression Algorithm')\n\n        _set_config('hdf5.encoding', self._encoding,\n                    comment='Encoding for unicode characters')\n\n        _set_config('hdf5.fletcher32', self._fletcher32,\n                    comment='Whether to use fletcher 32 checksum')\n\n        _set_config('hdf5.shuffle', self._shuffle,\n                    comment='Whether to use shuffle filtering.')\n\n        _set_config('hdf5.pandas_format', self._pandas_format,\n                    comment="}
{"prompt": "def draw(self):\n        \"\"\"\n        Draws all layers of this LayeredWidget.\n        \n        This should normally be unneccessary, since it is recommended that layers use Vertex Lists instead of OpenGL Immediate Mode.\n        \"\"\"", "output": "\n        for layer in self.layers:\n            layer.draw()\n\n    def add_layer(self, layer):\n        \"\"\"\n        Adds a layer to this LayeredWidget.\n        \n        The layer will be drawn after all other layers.\n        \"\"\"\n        self.layers.append(layer)\n\n    def remove_layer(self, layer):\n        \"\"\"\n        Removes a layer from this LayeredWidget.\n        \n        The layer will no longer be drawn.\n        \"\"\"\n        self.layers.remove(layer)\n\n    def", "solution": "\n        super(LayeredWidget,self).draw()\n        for layer,_ in self.layers:\n            layer._draw()"}
{"prompt": "def get_exitcodes_terminated_worker(processes):\n    \"\"\"Return a formated string with the exitcodes of terminated workers.\n\n    If necessary, wait (up to .25s) for the system to correctly set the\n    exitcode of one terminated worker.\n    \"\"\"", "output": "\n    exitcodes = []\n    for p in processes:\n        if p.exitcode is None:\n            p.wait(.25)\n        exitcodes.append(p.exitcode)\n    return \"exitcodes: {}\".format(exitcodes)\n\n\ndef get_exitcodes_terminated_workers(processes):\n    \"\"\"Return a formated string with the exitcodes of terminated workers.\n\n    If necessary, wait (up to .25s) for the system to correctly set the\n    exitcode of one terminated worker.\n    \"\"\"\n   ", "solution": "\n    patience = 5\n\n    # Catch the exitcode of the terminated workers. There should at least be\n    # one. If not, wait a bit for the system to correctly set the exitcode of\n    # the terminated worker.\n    exitcodes = [p.exitcode for p in list(processes.values())\n                 if p.exitcode is not None]\n    while len(exitcodes) == 0 and patience > 0:\n        patience -= 1\n        exitcodes = [p.exitcode for p in list(processes.values())\n                     if p.exitcode is not None]\n        time.sleep(.05)\n\n    return _format_exitcodes(exitcodes)"}
{"prompt": "def eintr_retry(exc_type, f, *args, **kwargs):\n    \"\"\"Calls a function.  If an error of the given exception type with\n    interrupted system call (EINTR) occurs calls the function again.\n    \"\"\"", "output": "\n    while True:\n        try:\n            return f(*args, **kwargs)\n        except exc_type, e:\n            if e.errno != errno.EINTR:\n                raise\n\ndef get_file_size(path):\n    \"\"\"Returns the size of the file in bytes.\"\"\"\n    return os.stat(path).st_size\n\ndef get_file_mtime(path):\n    \"\"\"Returns the modification time of the file.\"\"\"\n    return os.stat(path).st_mtime\n\ndef get_file_", "solution": "\n    while True:\n        try:\n            return f(*args, **kwargs)\n        except exc_type as exc:\n            if exc.errno != EINTR:\n                raise\n        else:\n            break"}
{"prompt": "def _colorify(self, data):\n        \"\"\"\n        Retun colored string.\n\n        :param data: The string to colorify.\n        :type data: str\n\n        :return: A colored string.\n        :rtype: str\n        \"\"\"", "output": "\n        return self.color + data + self.reset\n\n    def _colorify_list(self, data):\n        \"\"\"\n        Colorify a list of strings.\n\n        :param data: The list of strings to colorify.\n        :type data: list\n\n        :return: A list of colored strings.\n        :rtype: list\n        \"\"\"\n        return [self._colorify(item) for item in data]\n\n    def _colorify_dict(self, data):\n        \"\"\"\n        Colorify a dictionary of strings.\n\n        :", "solution": "\n\n        if self.template in [\"Generic\", \"Less\"]:\n            # The template is in the list of template that need the coloration.\n\n            if (\n                self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"up\"]\n                or self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"valid\"]\n            ):\n                # The status is in the list of up status.\n\n                # We print the data with a green background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.GREEN + data\n            elif self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"down\"]:\n                # The status is in the list of down status.\n\n                # We print the data with a red background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.RED + data\n            else:\n                # The status is not in the list of up and down status.\n\n                # We print the data with a cyan background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.CYAN + data\n\n        # We return the data.\n        return data"}
{"prompt": "def __eof_qubit(rho):\n    \"\"\"\n    Compute the Entanglement of Formation of a 2-qubit density matrix.\n\n    Args:\n        rho ((array_like): (4,4) array_like, input density matrix.\n\n    Returns:\n        float: The entanglement of formation.\n    \"\"\"", "output": "\n    rho = np.asarray(rho, dtype=complex)\n    if rho.shape != (4, 4):\n        raise ValueError(\"Input density matrix must be 4x4.\")\n    if not is_density_matrix(rho):\n        raise ValueError(\"Input density matrix is not a valid density matrix.\")\n    if not is_trace_preserving(rho):\n        raise ValueError(\"Input density matrix is not trace preserving.\")\n\n    # Compute the entanglement of formation\n    eof = 0.0\n    for i", "solution": "\n    c = concurrence(rho)\n    c = 0.5 + 0.5 * np.sqrt(1 - c * c)\n    return shannon_entropy([c, 1 - c])"}
{"prompt": "def compile_sympy_accesses(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"", "output": "\n        accesses = {}\n        for var in self.variables:\n            accesses[var] = []\n        for access in self.accesses:\n            if sources and access.source not in accesses:\n                continue\n            if destinations and access.destination not in accesses:\n                continue\n            accesses[access.source].append(access)\n            accesses[access.destination].append(access)\n        return accesses\n\n    def compile_sympy_accesses_by_source(self):\n        \"\"\"\n        Return a dictionary of lists of sym", "solution": "\n        sympy_accesses = defaultdict(list)\n        # Compile sympy accesses\n        for var_name in self.variables:\n            if sources:\n                for r in self.sources.get(var_name, []):\n                    if r is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))\n            if destinations:\n                for w in self.destinations.get(var_name, []):\n                    if w is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))\n\n        return sympy_accesses"}
{"prompt": "def deserialize(bstr):\n    \"\"\"\n    Attempts to deserialize a bytestring into an audiosegment.\n\n    :param bstr: The bytestring serialized via an audiosegment's serialize() method.\n    :returns: An AudioSegment object deserialized from `bstr`.\n    \"\"\"", "output": "\n    import io\n    import wave\n    import struct\n    import numpy as np\n    import soundfile as sf\n\n    # Create a wave object from the bytestring.\n    w = wave.open(io.BytesIO(bstr), 'rb')\n\n    # Get the format of the wave object.\n    nchannels, sampwidth, framerate, nframes, comptype, compname = w.getparams()\n\n    # Read the wave object into a numpy array.\n    data = np.frombuffer(w.readframes(nframes),", "solution": "\n    d = pickle.loads(bstr)\n    seg = pickle.loads(d['seg'])\n    return AudioSegment(seg, d['name'])"}
{"prompt": "def create_validator(data_struct_dict, name=None):\n    \"\"\"\n    create a Validator instance from data_struct_dict\n\n    :param data_struct_dict: a dict describe validator's fields, like the dict `to_dict()` method returned.\n    :param name: name of Validator class \n\n    :return: Validator instance\n    \"\"\"", "output": "\n    if name is None:\n        name = data_struct_dict.get('name', None)\n    if name is None:\n        raise ValueError('name is required')\n    validator_class = globals()[name]\n    return validator_class(**data_struct_dict)\n\n\ndef create_validators(data_struct_dicts, name=None):\n    \"\"\"\n    create a list of Validator instances from data_struct_dicts\n\n    :param data_struct_dicts: a list of dict describe validator's", "solution": "\n\n    if name is None:\n        name = 'FromDictValidator'\n    attrs = {}\n    for field_name, field_info in six.iteritems(data_struct_dict):\n        field_type = field_info['type']\n        if field_type == DictField.FIELD_TYPE_NAME and isinstance(field_info.get('validator'), dict):\n            field_info['validator'] = create_validator(field_info['validator'])\n        attrs[field_name] = create_field(field_info)\n    name = force_str(name)\n    return type(name, (Validator, ), attrs)"}
{"prompt": "def _store(self, lines, buffer=None, store='source'):\n        \"\"\"Store one or more lines of input.\n\n        If input lines are not newline-terminated, a newline is automatically\n        appended.\"\"\"", "output": "\n        if buffer is None:\n            buffer = self.buffer\n        if not lines:\n            return\n        if not isinstance(lines, (list, tuple)):\n            lines = [lines]\n        if not lines[-1].endswith('\\n'):\n            lines[-1] += '\\n'\n        buffer.write(''.join(lines))\n        buffer.seek(0)\n        self.input_lines.append(lines)\n        self.input_line_no += len(lines)\n        if store == 'source':\n            self.source_lines", "solution": "\n        \n        if buffer is None:\n            buffer = self._buffer\n            \n        if lines.endswith('\\n'):\n            buffer.append(lines)\n        else:\n            buffer.append(lines+'\\n')\n        setattr(self, store, self._set_source(buffer))"}
{"prompt": "def selection_redo(self, name=\"default\", executor=None):\n        \"\"\"Redo selection, for the name.\"\"\"", "output": "\n        return self._redo_selection(name, executor)\n\n    def _redo_selection(self, name=\"default\", executor=None):\n        \"\"\"Redo selection, for the name.\"\"\"\n        if executor is None:\n            executor = self.executor\n        if name not in self.selection_history:\n            return\n        if self.selection_history[name] is None:\n            return\n        self.selection_history[name] = self.selection_history[name].redo(executor)\n        self.selection_", "solution": "\n        logger.debug(\"redo\")\n        executor = executor or self.executor\n        assert self.selection_can_redo(name=name)\n        selection_history = self.selection_histories[name]\n        index = self.selection_history_indices[name]\n        next = selection_history[index + 1]\n        self.selection_history_indices[name] += 1\n        self.signal_selection_changed.emit(self)\n        logger.debug(\"redo: selection history is %r, index is %r\", selection_history, index)"}
{"prompt": "def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET):\n        \"\"\"Fetch questions from the Kitsune url.\n\n        :param category: the category of items to fetch\n        :offset: obtain questions after offset\n        :returns: a generator of questions\n        \"\"\"", "output": "\n        if category not in CATEGORIES:\n            raise ValueError(\"Invalid category: %s\" % category)\n\n        url = self.url + category\n        if offset:\n            url += \"?offset=%s\" % offset\n\n        response = self.session.get(url)\n        response.raise_for_status()\n        return self._parse_questions(response.text)\n\n    def _parse_questions(self, html):\n        \"\"\"Parse questions from the html.\n\n        :param html: the html to parse\n        :returns:", "solution": "\n        if not offset:\n            offset = DEFAULT_OFFSET\n\n        kwargs = {\"offset\": offset}\n        items = super().fetch(category, **kwargs)\n\n        return items"}
{"prompt": "def make_reports(self, stats, old_stats):\n        \"\"\"render registered reports\"\"\"", "output": "\n        for report in self.reports:\n            report.render(stats, old_stats)\n\n    def make_report(self, report_name, stats, old_stats):\n        \"\"\"render a specific report\"\"\"\n        for report in self.reports:\n            if report.name == report_name:\n                report.render(stats, old_stats)\n\n    def make_report_for_test(self, report_name, test_name, stats, old_stats):\n        \"\"\"render a specific report for a specific test\"\"\"\n        for", "solution": "\n        sect = Section(\"Report\", \"%s statements analysed.\" % (self.stats[\"statement\"]))\n        for checker in self.report_order():\n            for reportid, r_title, r_cb in self._reports[checker]:\n                if not self.report_is_enabled(reportid):\n                    continue\n                report_sect = Section(r_title)\n                try:\n                    r_cb(report_sect, stats, old_stats)\n                except EmptyReportError:\n                    continue\n                report_sect.report_id = reportid\n                sect.append(report_sect)\n        return sect"}
{"prompt": "def format_lines(statements, lines):\n    \"\"\"Nicely format a list of line numbers.\n\n    Format a list of line numbers for printing by coalescing groups of lines as\n    long as the lines represent consecutive statements.  This will coalesce\n    even if there are gaps between statements.\n\n    For example, if `statements` is [1,2,3,4,5,10,11,12,13,14] and\n    `lines` is [1,2,5,10,11,13,14] then the result will be \"1-2, 5-11, 13-14\".\n\n    \"\"\"", "output": "12,13,14], then the result will be\n    [1-3,4,10-14].\n\n    Args:\n      statements: A list of line numbers.\n      lines: A list of line numbers.\n\n    Returns:\n      A list of strings.\n    \"\"\"\n    result = []\n    i = 0\n    j = 0\n    while i < len(statements):\n        if statements[i] == lines[j]:\n            start = statements[i]\n            while i < len(statements) and", "solution": "\n    pairs = []\n    i = 0\n    j = 0\n    start = None\n    statements = sorted(statements)\n    lines = sorted(lines)\n    while i < len(statements) and j < len(lines):\n        if statements[i] == lines[j]:\n            if start == None:\n                start = lines[j]\n            end = lines[j]\n            j += 1\n        elif start:\n            pairs.append((start, end))\n            start = None\n        i += 1\n    if start:\n        pairs.append((start, end))\n    ret = ', '.join(map(nice_pair, pairs))\n    return ret"}
{"prompt": "def _check_function(self):\n        \"\"\" make some basic checks on the function to make sure it is valid\"\"\"", "output": "\n        if not self.function:\n            raise ValueError(\"No function specified\")\n        if not callable(self.function):\n            raise ValueError(\"Function is not callable\")\n        if not self.function.__name__:\n            raise ValueError(\"Function has no name\")\n\n    def _check_arguments(self):\n        \"\"\" make some basic checks on the arguments to make sure they are valid\"\"\"\n        if not self.arguments:\n            raise ValueError(\"No arguments specified\")\n        if not isinstance(self.arguments, list):\n            raise ValueError", "solution": "\n        # note, callable is valid for Python 2 and Python 3.2 onwards but\n        # not inbetween\n        if not callable(self._function):\n            raise RuntimeError(\n                \"provided function '{0}' is not callable\".\n                format(str(self._function)))\n        from inspect import getargspec\n        arg_info = getargspec(self._function)\n        if len(arg_info.args) != 1:\n            print str(arg_info)\n            raise RuntimeError(\n                \"provided function should have one argument but found \"\n                \"{0}\".format(len(arg_info.args)))"}
{"prompt": "def span(self, index):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"", "output": "\n        return self.tuples[index][1] - self.tuples[index][0]\n\n    def __getitem__(self, index):\n        \"\"\"Get the value of a tuple\n        \"\"\"\n        return self.tuples[index][0]\n\n    def __setitem__(self, index, value):\n        \"\"\"Set the value of a tuple\n        \"\"\"\n        self.tuples[index][0] = value\n\n    def __len__(self):\n        \"\"\"Get the number of tuples\n        \"\"\"\n        return len(self.tuples)", "solution": "\n        lower = self.value0+sum(1<<x for x in self.extraTable[:index])\n        upper = lower+(1<<self.extraTable[index])\n        return lower, upper-1"}
{"prompt": "def paid_invoices_by_date(request, form):\n    \"\"\" Shows the number of paid invoices containing given products or\n    categories per day. \"\"\"", "output": "\n    if form.is_valid():\n        data = form.cleaned_data\n        if data['start_date'] > data['end_date']:\n            messages.error(request, _('Start date must be before end date.'))\n            return redirect('paid_invoices_by_date')\n        if data['start_date'] == data['end_date']:\n            messages.error(request, _('Start date and end date must be different.'))\n            return redirect('paid_invoices_by_date')\n        if data", "solution": "\n\n    products = form.cleaned_data[\"product\"]\n    categories = form.cleaned_data[\"category\"]\n\n    invoices = commerce.Invoice.objects.filter(\n        (\n            Q(lineitem__product__in=products) |\n            Q(lineitem__product__category__in=categories)\n        ),\n        status=commerce.Invoice.STATUS_PAID,\n    )\n\n    # Invoices with payments will be paid at the time of their latest payment\n    payments = commerce.PaymentBase.objects.all()\n    payments = payments.filter(\n        invoice__in=invoices,\n    )\n    payments = payments.order_by(\"invoice\")\n    invoice_max_time = payments.values(\"invoice\").annotate(\n        max_time=Max(\"time\")\n    )\n\n    # Zero-value invoices will have no payments, so they're paid at issue time\n    zero_value_invoices = invoices.filter(value=0)\n\n    times = itertools.chain(\n        (line[\"max_time\"] for line in invoice_max_time),\n        (invoice.issue_time for invoice in zero_value_invoices),\n    )\n\n    by_date = collections.defaultdict(int)\n    for time in times:\n        date = datetime.datetime(\n            year=time.year, month=time.month, day=time.day\n        )\n        by_date[date] += 1\n\n    data = [(date_, count) for date_, count in sorted(by_date.items())]\n    data = [(date_.strftime(\"%Y-%m-%d\"), count) for date_, count in data]\n\n    return ListReport(\n        \"Paid Invoices By Date\",\n        [\"date\", \"count\"],\n        data,\n    )"}
{"prompt": "def get(m, k, default=None):\n    \"\"\"Return the value of k in m. Return default if k not found in m.\"\"\"", "output": "\n    return m.get(k, default)\n\n\ndef setdefault(m, k, default=None):\n    \"\"\"Set m[k] to default if k not in m; return m[k] either way.\"\"\"\n    return m.setdefault(k, default)\n\n\ndef update(m, other):\n    \"\"\"Update m from other.\n\n    For each (k, v) in other, set m[k] = v.\n    \"\"\"\n    m.update(other)\n\n\ndef pop(m, k, default=None):\n", "solution": "\n    if isinstance(m, IAssociative):\n        return m.entry(k, default=default)\n\n    try:\n        return m[k]\n    except (KeyError, IndexError, TypeError) as e:\n        logger.debug(\"Ignored %s: %s\", type(e).__name__, e)\n        return default"}
{"prompt": "def _send_coroutine():\n    \"\"\"\n    Creates a running coroutine to receive message instances and send\n    them in a futures executor.\n    \"\"\"", "output": "\n    while True:\n        message = yield\n        if message is None:\n            break\n        else:\n            message.send()\n\n\nclass Message:\n    \"\"\"\n    A message is a wrapper around a message instance.\n    \"\"\"\n\n    def __init__(self, message, future=None):\n        \"\"\"\n        Initializes a message.\n        \"\"\"\n        self.message = message\n        self.future = future\n\n    def send(self):\n        \"\"\"\n        Sends the message.\n        \"\"\"\n        if self.future is not None", "solution": "\n    with PoolExecutor() as executor:\n        while True:\n            msg = yield\n            future = executor.submit(msg.send)\n            future.add_done_callback(_exception_handler)"}
{"prompt": "def cb_list_groups(self, *args, **kwargs):\n        \"\"\"List all the check groups that pylint knows about\n\n        These should be useful to know what check groups someone can disable\n        or enable.\n        \"\"\"", "output": "\n        return self.linter.list_groups()\n\n    def cb_list_checkers(self, *args, **kwargs):\n        \"\"\"List all the checkers that pylint knows about\n\n        These should be useful to know what checkers someone can disable\n        or enable.\n        \"\"\"\n        return self.linter.list_checkers()\n\n    def cb_list_msgs(self, *args, **kwargs):\n        \"\"\"List all the messages that pylint knows about\n\n        These should be useful to know what messages someone can disable\n", "solution": "\n        for check in self.linter.get_checker_names():\n            print(check)\n        sys.exit(0)"}
{"prompt": "def add_column(self, name, data):\n        \"\"\"Add a column to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data\n        \"\"\"", "output": "\n        self.data[name] = data\n\n    def add_row(self, data):\n        \"\"\"Add a row to the DataFrame\n\n        :param data: numpy array with the data\n        \"\"\"\n        self.data = np.vstack((self.data, data))\n\n    def get_column(self, name):\n        \"\"\"Get a column from the DataFrame\n\n        :param str name: name of column\n        :return: numpy array with the data\n        \"\"\"\n        return self.data[name]\n\n    def get_row(self", "solution": "\n        # assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type)\n        # self._length = len(data)\n        # if self._length_unfiltered is None:\n        #     self._length_unfiltered = len(data)\n        #     self._length_original = len(data)\n        #     self._index_end = self._length_unfiltered\n        super(DataFrameArrays, self).add_column(name, data)\n        self._length_unfiltered = int(round(self._length_original * self._active_fraction))"}
{"prompt": "def get_objects_from_form(variant_ids, form_fields, object_type):\n    \"\"\"Extract the objects to be saved in the clinvar database collection.\n       object_type param specifies if these objects are variant or casedata objects\n\n       Args:\n        variant_ids(list): list of database variant ids\n        form_fields(dict): it's the submission form dictionary. Keys have the same names as CLINVAR_HEADER and CASEDATA_HEADER\n        object_type(str): either 'variant' or 'case_data'\n\n       Returns:\n        submission_objects(list): list of submission objects of either type 'variant' or 'casedata'\n    \"\"\"", "output": "'\n\n       Returns:\n        list: list of objects to be saved in the clinvar database collection\n    \"\"\"\n    objects = []\n    for variant_id in variant_ids:\n        object = {}\n        object['variant_id'] = variant_id\n        object['object_type'] = object_type\n        object['submission_date'] = form_fields['submission_date']\n        object['submission_id'] = form_fields['submission_id']\n        object['clinvar_id'] = form_fields['clinvar_id", "solution": "\n    submission_fields = []\n    if object_type == 'variant':\n        submission_fields = CLINVAR_HEADER\n    else: #collect casedata objects\n        submission_fields = CASEDATA_HEADER\n\n    # A list of objects (variants of casedata info) to be saved into clinvar database collection\n    submission_objects = []\n\n    # Loop over the form fields and collect the data:\n    for variant_id in variant_ids: # loop over the variants\n\n        subm_obj = {} # A new submission object for each\n\n        # Don't included casedata for a variant unless specified by user\n        if object_type == 'casedata' and 'casedata_'+variant_id not in form_fields:\n            continue\n\n        subm_obj['csv_type'] = object_type\n        subm_obj['case_id'] = form_fields.get('case_id')\n        subm_obj['category'] = form_fields.get('category@'+variant_id)\n\n        for key, values in submission_fields.items(): # loop over the form info fields\n            field_value = form_fields.get(key+'@'+variant_id)\n            if field_value and not field_value == '-':\n                if key == 'ref_seq': # split this field into\n                    refseq_raw = field_value.split('|')\n                    subm_obj['ref_seq'] = refseq_raw[0]\n                    subm_obj['hgvs'] = refseq_raw[1]\n                else:\n                    subm_obj[key] = field_value\n\n        # Create a unique ID for the database\n        # For casedata : = caseID_sampleID_variantID\n        # For variants : ID = caseID_variantID\n        if object_type == 'casedata':\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id + '_' + str(subm_obj['individual_id'])\n        else:\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id\n\n        submission_objects.append(subm_obj)\n\n    return submission_objects"}
{"prompt": "def evaluate(self, x, y=None, batch_size=32):\n        \"\"\"\n        Evaluate a model on a given dataset in distributed mode.\n\n        # Arguments\n        x: Input data. A Numpy array or RDD of Sample.\n        y: Labels. A Numpy array. Default is None if x is already RDD of Sample.\n        batch_size: Number of samples per gradient update.\n        \"\"\"", "output": "\n        if y is None:\n            x, y = x\n        if not isinstance(x, RDD):\n            x = RDD(x)\n        if not isinstance(y, RDD):\n            y = RDD(y)\n        if not x.is_distributed():\n            raise ValueError(\"x must be distributed\")\n        if not y.is_distributed():\n            raise ValueError(\"y must be distributed\")\n        if x.partitioner() != y.partitioner():\n            raise ValueError(\"x and y must have the same partition", "solution": "\n        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n            evaluation_data = to_sample_rdd(x, y)\n        elif isinstance(x, RDD) and not y:\n            evaluation_data = x\n        else:\n            raise TypeError(\"Unsupported evaluation data type: %s\" % type(x))\n        return callBigDlFunc(self.bigdl_type, \"evaluate\",\n                             self.value,\n                             evaluation_data,\n                             batch_size)"}
{"prompt": "def _loop_to_py_ast(ctx: GeneratorContext, node: Loop) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `loop*` expression.\"\"\"", "output": "\n    # TODO: This is a hack.\n    #       We should be able to get the type of the loop variable\n    #       from the type of the loop expression.\n    #       This is not possible at the moment.\n    #       We should also be able to get the type of the loop variable\n    #       from the type of the loop body.\n    #       This is not possible at the moment.\n    #       We should also be able to get the type of the loop variable\n    #       from the type of the loop condition.\n    #       This is not possible", "solution": "\n    assert node.op == NodeOp.LOOP\n\n    with ctx.new_symbol_table(\"loop\"):\n        binding_names = []\n        init_bindings: List[ast.AST] = []\n        for binding in node.bindings:\n            init_node = binding.init\n            assert init_node is not None\n            init_ast = gen_py_ast(ctx, init_node)\n            init_bindings.extend(init_ast.dependencies)\n            binding_name = genname(munge(binding.name))\n            binding_names.append(binding_name)\n            init_bindings.append(\n                ast.Assign(\n                    targets=[ast.Name(id=binding_name, ctx=ast.Store())],\n                    value=init_ast.node,\n                )\n            )\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), binding_name, LocalType.LOOP\n            )\n\n        loop_result_name = genname(\"loop\")\n        with ctx.new_recur_point(\n            node.loop_id, RecurType.LOOP, binding_names=binding_names\n        ):\n            loop_body_ast: List[ast.AST] = []\n            body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n            loop_body_ast.extend(body_ast.dependencies)\n            loop_body_ast.append(\n                ast.Assign(\n                    targets=[ast.Name(id=loop_result_name, ctx=ast.Store())],\n                    value=body_ast.node,\n                )\n            )\n            loop_body_ast.append(ast.Break())\n\n            return GeneratedPyAST(\n                node=_load_attr(loop_result_name),\n                dependencies=list(\n                    chain(\n                        [\n                            ast.Assign(\n                                targets=[\n                                    ast.Name(id=loop_result_name, ctx=ast.Store())\n                                ],\n                                value=ast.NameConstant(None),\n                            )\n                        ],\n                        init_bindings,\n                        [\n                            ast.While(\n                                test=ast.NameConstant(True),\n                                body=loop_body_ast,\n                                orelse=[],\n                            )\n                        ],\n                    )\n                ),\n            )"}
{"prompt": "def get_model(self, model_name):\n        \"\"\"\n        TODO: Need to validate model name has 2x '.' chars\n        \"\"\"", "output": "\n        model_name = model_name.replace('.', '/')\n        model_path = os.path.join(self.model_dir, model_name)\n        model = tf.saved_model.load(model_path)\n        return model\n\n    def get_model_names(self):\n        \"\"\"\n        TODO: Need to validate model name has 2x '.' chars\n        \"\"\"\n        model_names = []\n        for model_name in os.listdir(self.model_dir):\n            if os.path.isdir(os.", "solution": "\n        klass = None\n        try:\n            module_name, class_name = model_name.rsplit('.', 1)\n            mod = __import__(module_name, fromlist=[class_name])\n            klass = getattr(mod, class_name)\n        except ImportError, e:\n            self.error('Cannot find app %s %s' % (model_name, e))\n\n        return klass"}
{"prompt": "def compare(self, cost_map):\n        \"\"\"\n        Compare to previous records and return whether the given cost is a new best.\n        :return: True if the given cost is a new best\n        \"\"\"", "output": "\n        if self.best_cost is None:\n            self.best_cost = cost_map\n            return True\n        else:\n            for key, value in cost_map.items():\n                if value < self.best_cost[key]:\n                    self.best_cost = cost_map\n                    return True\n            return False\n\n    def get_best_cost(self):\n        \"\"\"\n        Return the best cost map.\n        :return: best cost map\n        \"\"\"\n        return self.best_cost\n\n    def get_best_cost", "solution": "\n        cri_val = cost_map[self._criteria]\n        if self._best_criteria is None:\n            self._best_criteria = cri_val\n            return True\n        else:\n            if self._smaller_is_better and cri_val < self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            elif not self._smaller_is_better and cri_val > self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            else:\n                return False"}
{"prompt": "def execute(self):\n        \"\"\"\n        Execute R script\n        \"\"\"", "output": "\n        self.log.info(\"Executing R script\")\n        self.log.info(\"R script: %s\", self.script)\n        self.log.info(\"R script arguments: %s\", self.arguments)\n        self.log.info(\"R script working directory: %s\", self.working_directory)\n\n        # Create R script\n        r_script = self.create_r_script()\n\n        # Execute R script\n        self.execute_r_script(r_script)\n\n        # Remove R script\n        self.remove_", "solution": "\n        rprocess = OrderedDict()\n        commands = OrderedDict([\n            (self.file, ['Rscript', self.file] + self.cmd),\n        ])\n        for cmd_name, cmd in commands.items():\n            rprocess[cmd_name] = self.run_command_under_r_root(cmd)\n        \n        return self.decode_cmd_out(completed_cmd=rprocess[self.file])"}
{"prompt": "def _duplicated_isinstance_types(node):\n        \"\"\"Get the duplicated types from the underlying isinstance calls.\n\n        :param astroid.BoolOp node: Node which should contain a bunch of isinstance calls.\n        :returns: Dictionary of the comparison objects from the isinstance calls,\n                  to duplicate values from consecutive calls.\n        :rtype: dict\n        \"\"\"", "output": "\n        duplicated_types = {}\n        for child in node.values:\n            if isinstance(child, astroid.BoolOp) and child.op == 'and':\n                duplicated_types.update(_duplicated_isinstance_types(child))\n            elif isinstance(child, astroid.Call) and child.func.attrname == 'isinstance':\n                for arg in child.args:\n                    if isinstance(arg, astroid.Const):\n                        duplicated_types[arg.value] = True\n        return duplicated_types\n", "solution": "\n        duplicated_objects = set()\n        all_types = collections.defaultdict(set)\n\n        for call in node.values:\n            if not isinstance(call, astroid.Call) or len(call.args) != 2:\n                continue\n\n            inferred = utils.safe_infer(call.func)\n            if not inferred or not utils.is_builtin_object(inferred):\n                continue\n\n            if inferred.name != \"isinstance\":\n                continue\n\n            isinstance_object = call.args[0].as_string()\n            isinstance_types = call.args[1]\n\n            if isinstance_object in all_types:\n                duplicated_objects.add(isinstance_object)\n\n            if isinstance(isinstance_types, astroid.Tuple):\n                elems = [\n                    class_type.as_string() for class_type in isinstance_types.itered()\n                ]\n            else:\n                elems = [isinstance_types.as_string()]\n            all_types[isinstance_object].update(elems)\n\n        # Remove all keys which not duplicated\n        return {\n            key: value for key, value in all_types.items() if key in duplicated_objects\n        }"}
{"prompt": "def digest(self, data, mecha=MechanismSHA1):\n        \"\"\"\n        C_DigestInit/C_Digest\n\n        :param data: the data to be digested\n        :type data:  (binary) sring or list/tuple of bytes\n        :param mecha: the digesting mechanism to be used\n          (use `MechanismSHA1` for `CKM_SHA_1`)\n        :type mecha: :class:`Mechanism`\n        :return: the computed digest\n        :rtype: list of bytes\n\n        :note: the returned value is an istance of :class:`ckbytelist`.\n          You can easly convert it to a binary string with:\n          ``bytes(ckbytelistDigest)``\n          or, for Python 2:\n          ``''.join(chr(i) for i in ckbytelistDigest)``\n\n        \"\"\"", "output": " (binary) string\n        \"\"\"\n        if not isinstance(data, (bytes, bytearray)):\n            raise TypeError(\"data must be a (binary) string\")\n        if not isinstance(mecha, Mechanism):\n            raise TypeError(\"mecha must be a Mechanism\")\n        if not mecha.is_digesting():\n            raise ValueError(\"mecha must be a digesting mechanism\")\n        if not isinstance(mecha.mecha_type, CK_MECHANISM_TYPE):\n            raise TypeError(\"mecha.mecha", "solution": "\n        digest = ckbytelist()\n        m = mecha.to_native()\n        data1 = ckbytelist(data)\n        rv = self.lib.C_DigestInit(self.session, m)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # first call get digest size\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # second call get actual digest data\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return digest"}
{"prompt": "def file_hash(self, source, cu):\n        \"\"\"Compute a hash that changes if the file needs to be re-reported.\"\"\"", "output": "\n        # TODO: This is a very simple hash. We should probably use a\n        #       more sophisticated one.\n        return source.path + str(source.mtime)\n\n    def report(self, source, cu):\n        \"\"\"Report the given compilation unit.\"\"\"\n        # TODO: This is a very simple report. We should probably use a\n        #       more sophisticated one.\n        self.report_file(source.path, cu.name, cu.line, cu.column, cu.message)\n\n    def report_file(", "solution": "\n        m = Hasher()\n        m.update(source)\n        self.coverage.data.add_to_hash(cu.filename, m)\n        return m.digest()"}
{"prompt": "def voronoi(script, hole_num=50, target_layer=None, sample_layer=None, thickness=0.5, backward=True):\n    \"\"\" Turn a model into a surface with Voronoi style holes in it\n\n    References:\n    http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html\n    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html\n\n    Requires FilterScript object\n\n    Args:\n        script: the FilterScript object to write the filter to. Does not\n            work with a script filename.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"", "output": "2.html\n    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-3.html\n    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-4.html\n    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-5.html\n    http://meshlabstuff.blogspot.com/20", "solution": "\n\n    if target_layer is None:\n        target_layer = script.current_layer()\n    if sample_layer is None:\n        # Current layer is currently not changed after poisson_disk is run\n        sampling.poisson_disk(script, sample_num=hole_num)\n        sample_layer = script.last_layer()\n\n    vert_color.voronoi(script, target_layer=target_layer, source_layer=sample_layer, backward=backward)\n    select.vert_quality(script, min_quality=0.0, max_quality=thickness)\n    if backward:\n        select.invert(script)\n    delete.selected(script)\n    smooth.laplacian(script, iterations=3)\n\n    return None"}
{"prompt": "def indices_to_global_iterator(self, indices):\n        \"\"\"\n        Transform a dictionary of indices to a global iterator integer.\n\n        Inverse of global_iterator_to_indices().\n        \"\"\"", "output": "\n        return sum([self.n_nodes[i] * indices[i] for i in range(self.dim)])\n\n    def global_iterator_to_indices(self, global_iterator):\n        \"\"\"\n        Transform a global iterator integer to a dictionary of indices.\n\n        Inverse of indices_to_global_iterator().\n        \"\"\"\n        indices = {}\n        for i in range(self.dim):\n            indices[i] = global_iterator // self.n_nodes[i]\n            global_iterator -= self.n_nodes[", "solution": "\n        global_iterator = self.subs_consts(self.global_iterator().subs(indices))\n        return global_iterator"}
{"prompt": "def _value(self, dtype=None, name=None, as_ref=False):  # pylint: disable=g-doc-args\n  \"\"\"Get the value returned by `tf.convert_to_tensor(distribution)`.\n\n  Note: this function may mutate the distribution instance state by caching\n  the concretized `Tensor` value.\n\n  Args:\n    dtype: Must return a `Tensor` with the given `dtype` if specified.\n    name: If the conversion function creates a new `Tensor`, it should use the\n      given `name` if specified.\n    as_ref: `as_ref` is true, the function must return a `Tensor` reference,\n      such as a `Variable`.\n  Returns:\n    concretized_distribution_value: `Tensor` identical to\n    `tf.convert_to_tensor(distribution)`.\n\n  #### Examples\n\n  ```python\n  tfd = tfp.distributions\n  x = tfd.Normal(0.5, 1).set_tensor_conversion(tfd.Distribution.mean)\n\n  x._value()\n  # ==> tf.convert_to_tensor(x) ==> 0.5\n\n  x._value() + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n\n  x + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n  ```\n\n  \"\"\"", "output": "     given name.\n    as_ref: If True, the result is returned as a `tf.Tensor` reference.\n\n  Returns:\n    value: A `Tensor` or `tf.Tensor` reference.\n  \"\"\"\n  if dtype is None:\n    dtype = self.dtype\n  if name is None:\n    name = self.name\n  if as_ref:\n    return self._as_ref()\n  if self._concrete_value is None:\n    self._concrete_value = tf.convert_to_tensor(\n", "solution": "\n  # pylint: disable=protected-access\n  if as_ref:\n    raise NotImplementedError(\n        'Cannot convert a `Distribution` to a reference '\n        '(e.g., `tf.Variable`).')\n  if self._concrete_value is None:\n    if self._convert_to_tensor_fn is None:\n      raise NotImplementedError(\n          'Failed to convert object of type {} to Tensor. Contents: {}. '\n          'Call `distribution.set_tensor_conversion(lambda self: ...)` to '\n          'enable `tf.convert_to_tensor` capability. For example: '\n          '`x = tfd.Normal(0,1).set_tensor_conversion(tfd.Distribution.mean)`'\n          ' results in `tf.convert_to_tensor(x)` being identical to '\n          '`x.mean()`.'.format(type(self), self))\n    with self._name_scope('value'):\n      self._concrete_value = (self._convert_to_tensor_fn(self)\n                              if callable(self._convert_to_tensor_fn)\n                              else self._convert_to_tensor_fn)\n      if not tf.is_tensor(self._concrete_value):\n        self._concrete_value = tfd._convert_to_tensor(\n            value=self._concrete_value,\n            name=name or 'concrete_value',\n            dtype=dtype,\n            dtype_hint=self.dtype)\n  return self._concrete_value"}
{"prompt": "def url_equal(first, second, ignore_scheme=False, ignore_netloc=False, ignore_path=False, ignore_params=False,\n              ignore_query=False, ignore_fragment=False):\n    \"\"\"\n    Compare two URLs and return True if they are equal, some parts of the URLs can be ignored\n    :param first: URL\n    :param second: URL\n    :param ignore_scheme: ignore the scheme\n    :param ignore_netloc: ignore the netloc\n    :param ignore_path: ignore the path\n    :param ignore_params: ignore the params\n    :param ignore_query: ignore the query string\n    :param ignore_fragment: ignore the fragment\n    :return: result of comparison\n    \"\"\"", "output": "param ignore_params: ignore the params\n    :param ignore_query: ignore the query\n    :param ignore_fragment: ignore the fragment\n    :return: True if the URLs are equal\n    \"\"\"\n    if ignore_scheme:\n        first = urlparse(first)\n        second = urlparse(second)\n        first = first._replace(scheme=None)\n        second = second._replace(scheme=None)\n    if ignore_netloc:\n        first = urlparse(first)\n        second = urlparse(second)\n        first = first._", "solution": "\n    # <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\n    firstp = urlparse(first)\n    secondp = urlparse(second)\n\n    return ((firstp.scheme == secondp.scheme or ignore_scheme) and\n            (firstp.netloc == secondp.netloc or ignore_netloc) and\n            (firstp.path == secondp.path or ignore_path) and\n            (firstp.params == secondp.params or ignore_params) and\n            (firstp.query == secondp.query or ignore_query) and\n            (firstp.fragment == secondp.fragment or ignore_fragment))"}
{"prompt": "def get_users_for_course(self, course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given course id.\n        \"\"\"", "output": "\n        return self._get_users_for_course(course_id, params)\n\n    def get_users_for_course_by_sis_id(self, sis_course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given course sis id.\n        \"\"\"\n        return self._get_users_for_course(sis_course_id, params, True)\n\n    def _get_users_for_course(self, course_id, params={}, sis_id=False):\n        \"\"\"", "solution": "\n        url = COURSES_API.format(course_id) + \"/users\"\n        data = self._get_paged_resource(url, params=params)\n        users = []\n        for datum in data:\n            users.append(CanvasUser(data=datum))\n        return users"}
{"prompt": "def make_signing_service(config, entity_id):\n    \"\"\"\n    Given configuration initiate a SigningService instance\n\n    :param config: The signing service configuration\n    :param entity_id: The entity identifier\n    :return: A SigningService instance\n    \"\"\"", "output": "\n    if config.get('signing_service_type') == 'internal':\n        return InternalSigningService(config, entity_id)\n    elif config.get('signing_service_type') == 'external':\n        return ExternalSigningService(config, entity_id)\n    else:\n        raise Exception('Unknown signing service type: {}'.format(config.get('signing_service_type')))\n\n\nclass SigningService(object):\n    \"\"\"\n    A signing service is responsible for signing and verifying signatures.\n    \"\"\"\n\n", "solution": "\n\n    _args = dict([(k, v) for k, v in config.items() if k in KJ_SPECS])\n    _kj = init_key_jar(**_args)\n\n    if config['type'] == 'internal':\n        signer = InternalSigningService(entity_id, _kj)\n    elif config['type'] == 'web':\n        _kj.issuer_keys[config['iss']] = _kj.issuer_keys['']\n        del _kj.issuer_keys['']\n        signer = WebSigningServiceClient(config['iss'], config['url'],\n                                         entity_id, _kj)\n    else:\n        raise ValueError('Unknown signer type: {}'.format(config['type']))\n\n    return signer"}
{"prompt": "def delete(filename, delete_v1=True, delete_v2=True):\n    \"\"\"Remove tags from a file.\n\n    Keyword arguments:\n\n    * delete_v1 -- delete any ID3v1 tag\n    * delete_v2 -- delete any ID3v2 tag\n    \"\"\"", "output": "\n    if not filename:\n        raise ValueError(\"Filename must be specified\")\n\n    try:\n        f = open(filename, \"rb+\")\n    except IOError:\n        raise IOError(\"Could not open file for writing\")\n\n    try:\n        f.seek(0)\n        data = f.read()\n        f.seek(0)\n        f.truncate()\n        f.write(data)\n    finally:\n        f.close()\n\n    if delete_v1:\n        delete_v1(filename", "solution": "\n\n    f = open(filename, 'rb+')\n\n    if delete_v1:\n        try:\n            f.seek(-128, 2)\n        except IOError:\n            pass\n        else:\n            if f.read(3) == b'TAG':\n                f.seek(-128, 2)\n                f.truncate()\n\n    # technically an insize=0 tag is invalid, but we delete it anyway\n    # (primarily because we used to write it)\n    if delete_v2:\n        f.seek(0, 0)\n        idata = f.read(10)\n        try:\n            id3, vmaj, vrev, flags, insize = unpack('>3sBBB4s', idata)\n        except struct.error:\n            id3, insize = b'', -1\n        insize = BitPaddedInt(insize)\n        if id3 == b'ID3' and insize >= 0:\n            delete_bytes(f, insize + 10, 0)"}
{"prompt": "def state(self):\n        \"\"\"Return internal state, useful for testing.\"\"\"", "output": "\n        return self._state\n\n    def _set_state(self, state):\n        \"\"\"Set internal state, useful for testing.\"\"\"\n        self._state = state\n\n    def _get_state(self):\n        \"\"\"Get internal state, useful for testing.\"\"\"\n        return self._state\n\n    def _set_state_from_file(self, state_file):\n        \"\"\"Set internal state from a file, useful for testing.\"\"\"\n        with open(state_file, 'r') as f:\n            self._state = json.load(f)", "solution": "\n        return {'c': self.c, 's0': self.s0, 's1': self.s1, 's2': self.s2}"}
{"prompt": "def get_mint_tree(tokens_stream):\n    \"\"\"\n    This function is wrapper to normal parsers (tag_parser, block_parser, etc.).\n    Returns mint tree.\n    \"\"\"", "output": "\n    mint_tree = []\n    while tokens_stream:\n        mint_tree.append(tag_parser(tokens_stream))\n    return mint_tree\n\n\ndef tag_parser(tokens_stream):\n    \"\"\"\n    This function parses tokens stream and returns tag.\n    \"\"\"\n    tag = tokens_stream.pop(0)\n    if tag == '(':\n        return block_parser(tokens_stream)\n    elif tag == '[':\n        return list_parser(tokens_stream)\n    elif tag == '{':", "solution": "\n    smart_stack = RecursiveStack()\n    block_parser.parse(tokens_stream, smart_stack)\n    return MintTemplate(body=smart_stack.stack)"}
{"prompt": "def from_yaml(cls, defaults, **kwargs):\n        \"\"\"Creates a new instance of a rule by merging two dictionaries.\n\n        This allows for independant configuration files to be merged\n        into the defaults.\"\"\"", "output": "\n        return cls(**dict(defaults, **kwargs))\n\n    @classmethod\n    def from_yaml_file(cls, filename, defaults=None):\n        \"\"\"Creates a new instance of a rule by merging two dictionaries.\n\n        This allows for independant configuration files to be merged\n        into the defaults.\"\"\"\n        with open(filename, 'r') as f:\n            return cls.from_yaml(defaults, **yaml.load(f))\n\n    def __init__(self, **kwargs):\n        \"\"\"Creates a new", "solution": "\n        # TODO: I hate myself for this. Fix it later mmkay?\n        if \"token\" not in defaults:\n            kwargs[\"token\"] = None\n\n        defaults = copy.deepcopy(defaults)\n        return cls(\n            defaults=defaults,\n            token=kwargs.pop(\"token\"),\n            directory=kwargs.pop(\"directory\"),\n            **kwargs\n        )"}
{"prompt": "def getPhysicalInterface(self, physicalInterfaceId, draft=False):\n        \"\"\"\n        Get a physical interface.\n        Parameters:\n          - physicalInterfaceId (string)\n          - draft (boolean)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        return self.getPhysicalInterfaces(draft=draft)[physicalInterfaceId]\n\n    def getPhysicalInterfaces(self, draft=False):\n        \"\"\"\n        Get all physical interfaces.\n        Parameters:\n          - draft (boolean)\n        Throws APIException on failure.\n        \"\"\"\n        return self.getPhysicalInterfacesByNode(None, draft=draft)\n\n    def getPhysicalInterfacesByNode(self, nodeId, draft=False):\n        \"\"\"\n        Get all physical interfaces for a node.\n       ", "solution": "\n        if draft:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"/draft\", physicalInterfaceId)\n        else:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"\", physicalInterfaceId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"physical interface retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting a physical interface\", resp)\n        return resp.json()"}
{"prompt": "def get(cls):\n        \"\"\"\n        Get the status while testing for an IP or domain.\n\n        .. note::\n            We consider that the domain or IP we are currently testing\n            is into :code:`PyFunceble.INTERN[\"to_test\"]`.\n        \"\"\"", "output": "\n\n        if PyFunceble.INTERN[\"to_test\"]:\n            if PyFunceble.INTERN[\"to_test\"] in PyFunceble.INTERN[\"ipv4_list\"]:\n                return PyFunceble.STATUS[\"ipv4\"]\n            elif PyFunceble.INTERN[\"to_test\"] in PyFunceble.INTERN[\"ipv6_list\"]:\n                return PyFunceble.STATUS[\"ipv6\"]\n            else:\n                return PyFunceble.STATUS[\"domain\"]\n        else:\n", "solution": "\n\n        if \"to_test\" in PyFunceble.INTERN and PyFunceble.INTERN[\"to_test\"]:\n            expiration_date = ExpirationDate().get()\n\n            if expiration_date is False:\n                return cls.handle(status=\"invalid\")\n\n            if expiration_date == PyFunceble.STATUS[\"official\"][\"up\"]:\n                return expiration_date, \"WHOIS\"\n\n            return cls.handle(status=\"inactive\")\n\n        raise NotImplementedError(\"We expect `INTERN['to_test']` to be set.\")"}
{"prompt": "def execute(option):\n    \"\"\"A script that melody calls with each valid set of options. This\n    script runs the required code and returns the results.\"\"\"", "output": "\n    import os\n    import sys\n    import melody\n    import melody.config\n    import melody.util\n    import melody.util.log\n    import melody.util.shell\n    import melody.util.shell.bash\n    import melody.util.shell.bash.bash_command\n    import melody.util.shell.bash.bash_command.bash_command_result\n    import melody.util.shell.bash.bash_command.bash_command_result.bash_command_result_status\n    import melody.util", "solution": "\n\n    namelist_option = []\n    makefile_option = []\n    flags = \"\"\n    for entry in option:\n        key = entry.keys()[0]\n        if key == \"Problem Size\":\n            namelist_option.append({\"SIZE\": entry[key]})\n        elif key == \"F90\":\n            makefile_option.append(entry)\n        else:\n            flags += entry[key] + \" \"\n    makefile_option.append({\"F90FLAGS\": flags})\n\n    namelist = create_input(namelist_option, \"namelist\",\n                            template_location=\"templates\")\n\n    makefile_include = create_input(makefile_option, \"Makefile.include\",\n                                    template_location=\"templates\")\n\n    benchmark_base = \"shallow\"\n\n    # save the input files in the appropriate place\n    location = benchmark_base + \"/original/namelist\"\n    my_file = open(location, 'w')\n    my_file.write(namelist)\n    my_file.flush()\n\n    location = benchmark_base + \"/common/Makefile.include\"\n    my_file = open(location, 'w')\n    my_file.write(makefile_include)\n    my_file.flush()\n\n    # compile shallow if required\n    base_path = benchmark_base + \"/original\"\n    import subprocess\n    make_process = subprocess.Popen([\"make\", \"clean\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    make_process = subprocess.Popen([\"make\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    # run shallow\n    make_process = subprocess.Popen([\"./shallow_base\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n    # _ = make_process.stderr.read()\n    stdout = make_process.stdout.read()\n\n    # determine if the results are correct. We will need to look at\n    # the results from stdout but for the moment we assume they are\n    # correct\n\n    # extract the required outputs\n    for line in stdout.split(\"\\n\"):\n        if \"Time-stepping\" in line:\n            total_time = line.split()[2]\n\n    return True, total_time"}
{"prompt": "def _read_next(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901\n    \"\"\"Read the next full form from the input stream.\"\"\"", "output": "\n    # Read the first character.\n    ch = ctx.read_char()\n\n    # If we're at the end of the stream, return EOF.\n    if ch is None:\n        return LispReaderForm(LispReaderEOF())\n\n    # If it's a whitespace character, skip it and read the next character.\n    if ch in WHITESPACE:\n        return _read_next(ctx)\n\n    # If it's a comment, skip it and read the next character.\n    if ch == COMMENT_CHAR", "solution": "\n    reader = ctx.reader\n    token = reader.peek()\n    if token == \"(\":\n        return _read_list(ctx)\n    elif token == \"[\":\n        return _read_vector(ctx)\n    elif token == \"{\":\n        return _read_map(ctx)\n    elif begin_num_chars.match(token):\n        return _read_num(ctx)\n    elif whitespace_chars.match(token):\n        reader.next_token()\n        return _read_next(ctx)\n    elif token == \":\":\n        return _read_kw(ctx)\n    elif token == '\"':\n        return _read_str(ctx)\n    elif token == \"'\":\n        return _read_quoted(ctx)\n    elif token == \"\\\\\":\n        return _read_character(ctx)\n    elif ns_name_chars.match(token):\n        return _read_sym(ctx)\n    elif token == \"#\":\n        return _read_reader_macro(ctx)\n    elif token == \"^\":\n        return _read_meta(ctx)  # type: ignore\n    elif token == \";\":\n        return _read_comment(ctx)\n    elif token == \"`\":\n        return _read_syntax_quoted(ctx)\n    elif token == \"~\":\n        return _read_unquote(ctx)\n    elif token == \"@\":\n        return _read_deref(ctx)\n    elif token == \"\":\n        return ctx.eof\n    else:\n        raise SyntaxError(\"Unexpected token '{token}'\".format(token=token))"}
{"prompt": "def __tomo_linear_inv(freqs, ops, weights=None, trace=None):\n    \"\"\"\n    Reconstruct a matrix through linear inversion.\n\n    Args:\n        freqs (list[float]): list of observed frequences.\n        ops (list[np.array]): list of corresponding projectors.\n        weights (list[float] or array_like):\n            weights to be used for weighted fitting.\n        trace (float or None): trace of returned operator.\n\n    Returns:\n        numpy.array: A numpy array of the reconstructed operator.\n    \"\"\"", "output": "reconstructed operator.\n    \"\"\"\n    if weights is None:\n        weights = np.ones(len(freqs))\n    else:\n        weights = np.asarray(weights)\n    if trace is None:\n        trace = np.sum(weights)\n    else:\n        trace = float(trace)\n    if len(freqs) != len(ops):\n        raise ValueError(\"freqs and ops must have the same length.\")\n    if len(freqs) != len(weights):\n        raise ValueError(\"", "solution": "\n    # get weights matrix\n    if weights is not None:\n        W = np.array(weights)\n        if W.ndim == 1:\n            W = np.diag(W)\n\n    # Get basis S matrix\n    S = np.array([vectorize(m).conj()\n                  for m in ops]).reshape(len(ops), ops[0].size)\n    if weights is not None:\n        S = np.dot(W, S)  # W.S\n\n    # get frequencies vec\n    v = np.array(freqs)  # |f>\n    if weights is not None:\n        v = np.dot(W, freqs)  # W.|f>\n    Sdg = S.T.conj()  # S^*.W^*\n    inv = np.linalg.pinv(np.dot(Sdg, S))  # (S^*.W^*.W.S)^-1\n\n    # linear inversion of freqs\n    ret = devectorize(np.dot(inv, np.dot(Sdg, v)))\n    # renormalize to input trace value\n    if trace is not None:\n        ret = trace * ret / np.trace(ret)\n    return ret"}
{"prompt": "def check_upload_status(self, video_id):\n        \"\"\"\n        Checks the video upload status\n        Newly uploaded videos may be in the processing state\n\n        Authentication is required\n\n        Returns:\n            True if video is available\n            otherwise a dict containes upload_state and detailed message\n            i.e. {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n        \"\"\"", "output": "\n        url = \"https://www.googleapis.com/youtube/v3/videos\"\n        params = {\n            \"part\": \"status\",\n            \"id\": video_id,\n            \"key\": self.api_key,\n        }\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        data = response.json()\n        if data[\"pageInfo\"][\"totalResults\"] == 0:\n            raise ValueError(\"Video not found\")\n        elif data[\"items\"][0][\"status\"]", "solution": "\n        # Raise ApiError if not authenticated\n        if not self.authenticated:\n            raise ApiError(_(\"Authentication is required\"))\n\n        entry = self.fetch_video(video_id)\n        upload_status = Api.yt_service.CheckUploadStatus(entry)\n\n        if upload_status is not None:\n            video_upload_state = upload_status[0]\n            detailed_message = upload_status[1]\n            return {\"upload_state\": video_upload_state, \"detailed_message\": detailed_message}\n        else:\n            return True"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        \"\"\"\n        if status.farthest is None:\n            return self\n        if self.farthest is None:\n            return status\n        if status.farthest > self.farthest:\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest == self.farthest:\n            self.expected.update(status.expected)\n        return self\n\n    def __str__(self) -> str:\n        \"\"\"Return a string representation of this status.\n\n        Returns", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def stream(self, id, task, type, follow=False, offset=0, origin=\"start\", plain=False):\n        \"\"\" This endpoint streams a task's stderr/stdout logs.\n\n            https://www.nomadproject.io/api/client.html#stream-logs\n\n            arguments:\n              - id: (str) allocation_id required\n              - task: (str) name of the task inside the allocation to stream logs from\n              - type: (str) Specifies the stream to stream. Either \"stderr|stdout\"\n              - follow: (bool) default false\n              - offset: (int) default 0\n              - origin: (str) either start|end, default \"start\"\n              - plain: (bool) Return just the plain text without framing. default False\n            returns: (str) text\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.BadRequestNomadException\n        \"\"\"", "output": ": (bool) Specifies if the connection should be kept open and\n                        logs streamed as they arrive.\n              - offset: (int) Offset is the byte offset to stream from.\n              - origin: (str) Specifies the origin of the stream. Either \"start\" or \"end\"\n              - plain: (bool) Specifies if the output should be plain text.\n\n            returns:\n              - (dict)\n        \"\"\"\n        params = {\n            \"type\": type,\n            \"follow\": follow,\n            \"offset\": offset,\n            \"origin", "solution": "\n        params = {\n            \"task\": task,\n            \"type\": type,\n            \"follow\": follow,\n            \"offset\": offset,\n            \"origin\": origin,\n            \"plain\": plain\n        }\n        return self.request(id, params=params, method=\"get\").text"}
{"prompt": "def _map_segmentation_mask_to_stft_domain(mask, times, frequencies, stft_times, stft_frequencies):\n    \"\"\"\n    Maps the given `mask`, which is in domain (`frequencies`, `times`) to the new domain (`stft_frequencies`, `stft_times`)\n    and returns the result.\n    \"\"\"", "output": "\n    # Map the mask to the STFT domain\n    mask = mask[frequencies, times]\n    mask = np.flip(mask, axis=0)\n    mask = np.flip(mask, axis=1)\n\n    # Reshape the mask to the STFT shape\n    mask = np.reshape(mask, (len(stft_frequencies), len(stft_times)))\n\n    return mask\n\n\ndef _map_stft_to_segmentation_mask_domain(stft, stft_times, stft", "solution": "\n    assert mask.shape == (frequencies.shape[0], times.shape[0]), \"Times is shape {} and frequencies is shape {}, but mask is shaped {}\".format(\n        times.shape, frequencies.shape, mask.shape\n    )\n    result = np.zeros((stft_frequencies.shape[0], stft_times.shape[0]))\n\n    if len(stft_times) > len(times):\n        all_j = [j for j in range(len(stft_times))]\n        idxs  = [int(i) for i in np.linspace(0, len(times) - 1, num=len(stft_times))]\n        all_i = [all_j[idx] for idx in idxs]\n    else:\n        all_i = [i for i in range(len(times))]\n        idxs  = [int(i) for i in np.linspace(0, len(stft_times) - 1, num=len(times))]\n        all_j = [all_i[idx] for idx in idxs]\n\n    for i, j in zip(all_i, all_j):\n        result[:, j] = np.interp(stft_frequencies, frequencies, mask[:, i])\n\n    return result"}
{"prompt": "def sync_labels(self, repo):\n        \"\"\"Creates a local map of github labels/milestones to asana tags.\"\"\"", "output": "\n        # TODO: Add a way to sync tags\n        # TODO: Add a way to sync milestones\n        # TODO: Add a way to sync issues\n        # TODO: Add a way to sync pull requests\n        # TODO: Add a way to sync comments\n        # TODO: Add a way to sync commits\n        # TODO: Add a way to sync releases\n        # TODO: Add a way to sync releases\n        # TODO: Add a way to sync releases\n        # TODO: Add a way to sync releases\n        # TODO: Add a way to sync releases\n        # TODO", "solution": "\n\n        logging.info(\"syncing new github.com labels to tags\")\n\n        # create label tag map\n        ltm = self.app.data.get(\"label-tag-map\", {})\n\n        # loop over labels, if they don't have tags, make them\n        for label in repo.get_labels():\n            tag_id = ltm.get(label.name, None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=label.name,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % label.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", label.name, tag['id'])\n                ltm[label.name] = tag['id']\n\n        # loop over milestones, if they don't have tags, make them\n        for ms in repo.get_milestones(state=\"all\"):\n            tag_id = ltm.get(_ms_label(ms.id), None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=ms.title,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % ms.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", ms.title, tag['id'])\n                ltm[_ms_label(ms.id)] = tag['id']\n\n        self.app.data['label-tag-map'] = ltm\n        return ltm"}
{"prompt": "def recurse(self, k, wait=False, wait_index=None, timeout='5m'):\n        \"\"\"Recursively get the tree below the given key\"\"\"", "output": "\n        if not wait:\n            return self.get(k, recurse=True)\n        else:\n            return self.get(k, recurse=True, wait=wait, wait_index=wait_index, timeout=timeout)\n\n    def delete(self, k, recurse=False, **kwargs):\n        \"\"\"Delete a key\"\"\"\n        if recurse:\n            return self.delete(k, recurse=True, **kwargs)\n        else:\n            return self.delete(k, **kwargs)\n\n    def", "solution": "\n        k = k.lstrip('/')\n        url = '{}/{}'.format(self.endpoint, k)\n        params = {}\n        params['recurse'] = 'true'\n        if wait:\n            params['wait'] = timeout\n            if not wait_index:\n                params['index'] = self.index(k, recursive=True)\n            else:\n                params['index'] = wait_index\n        r = requests.get(url, params=params)\n        if r.status_code == 404:\n            raise KeyDoesNotExist(\"Key \" + k + \" does not exist\")\n        if r.status_code != 200:\n            raise KVStoreError('GET returned {}'.format(r.status_code))\n        entries = {} \n        for e in r.json():\n            if e['Value']:\n                entries[e['Key']] = base64.b64decode(e['Value'])\n            else:\n                entries[e['Key']] = ''\n        return entries"}
{"prompt": "def delayed(f):\n    \"\"\"Decorator to transparantly accept delayed computation.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,\n    >>>                   shape=4, delay=True)\n    >>> @vaex.delayed\n    >>> def total_sum(sums):\n    >>>     return sums.sum()\n    >>> sum_of_sums = total_sum(delayed_sum)\n    >>> ds.execute()\n    >>> sum_of_sums.get()\n    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations\n    \"\"\"", "output": "s.get()\n    \"\"\"\n    def delayed_f(*args, **kwargs):\n        if isinstance(args[0], vaex.delayed.Delayed):\n            return args[0].map(f, *args[1:], **kwargs)\n        else:\n            return f(*args, **kwargs)\n    return delayed_f\n\n\ndef delayed_map(f, *args, **kwargs):\n    \"\"\"Map a function over a list of delayed computations.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E", "solution": "\n\n    def wrapped(*args, **kwargs):\n        # print \"calling\", f, \"with\", kwargs\n        # key_values = kwargs.items()\n        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])\n        # key_promise = [(key, promisify(value)) for key, value in key_values]\n        arg_promises = list([promisify(value) for value in args])\n        kwarg_promises = list([promise for key, promise in key_promise])\n        promises = arg_promises + kwarg_promises\n        for promise in promises:\n            def echo_error(exc, promise=promise):\n                print(\"error with \", promise, \"exception is\", exc)\n                # raise exc\n\n            def echo(value, promise=promise):\n                print(\"done with \", repr(promise), \"value is\", value)\n            # promise.then(echo, echo_error)\n\n        # print promises\n        allarguments = aplus.listPromise(*promises)\n\n        def call(_):\n            kwargs_real = {key: promise.get() for key, promise in key_promise}\n            args_real = list([promise.get() for promise in arg_promises])\n            return f(*args_real, **kwargs_real)\n\n        def error(exc):\n            print(\"error\", exc)\n            raise exc\n        return allarguments.then(call, error)\n    return wrapped"}
{"prompt": "def read(cls, path, sc=None, min_partitions=1, bigdl_type=\"float\"):\n        \"\"\"\n        Read images as Image Frame\n        if sc is defined, Read image as DistributedImageFrame from local file system or HDFS\n        if sc is null, Read image as LocalImageFrame from local file system\n        :param path path to read images\n        if sc is defined, path can be local or HDFS. Wildcard character are supported.\n        if sc is null, path is local directory/image file/image file with wildcard character\n        :param sc SparkContext\n        :param min_partitions A suggestion value of the minimal splitting number for input data.\n        :return ImageFrame\n        \"\"\"", "output": ".\n        :param sc SparkContext\n        :param min_partitions min partitions\n        :param bigdl_type bigdl_type to create the frame\n        :return: ImageFrame\n        \"\"\"\n        if sc is None:\n            jimage_frame = callBigDlFunc(bigdl_type, \"readImageFrame\", path, min_partitions)\n            return ImageFrame(jimage_frame, bigdl_type)\n        else:\n            jimage_frame = callBigDlFunc(bigdl_type, \"readDistributedImageFrame\", path,", "solution": "\n        return ImageFrame(jvalue=callBigDlFunc(bigdl_type, \"read\", path, sc, min_partitions))"}
{"prompt": "def _is_package(path):\n    \"\"\"\n    \u5224\u65ad\u4f20\u5165\u7684\u8def\u5f84\u662f\u5426\u4e3a\u4e00\u4e2a Python \u6a21\u5757\u5305\n\n    :param str path: \u5f85\u5224\u65ad\u7684\u8def\u5f84\n    :return: \u8fd4\u56de\u662f\uff0c\u5219\u4f20\u5165 path \u4e3a\u4e00\u4e2a Python \u5305\uff0c\u5426\u5219\u4e0d\u662f\n    :rtype: bool\n    \"\"\"", "output": "\n    return os.path.isdir(path) and os.path.exists(os.path.join(path, '__init__.py'))\n\n\ndef _is_module(path):\n    \"\"\"\n    \u5224\u65ad\u4f20\u5165\u7684\u8def\u5f84\u662f\u5426\u4e3a\u4e00\u4e2a Python \u6a21\u5757\n\n    :param str path: \u5f85\u5224\u65ad\u7684\u8def\u5f84\n    :return: \u8fd4\u56de\u662f\uff0c\u5219\u4f20\u5165 path \u4e3a\u4e00\u4e2a Python \u6a21\u5757\uff0c\u5426\u5219\u4e0d\u662f\n    :rtype: bool\n    \"\"\"\n", "solution": "\n    def _exists(s):\n        return os.path.exists(os.path.join(path, s))\n\n    return (\n        os.path.isdir(path) and\n        (_exists('__init__.py') or _exists('__init__.pyc'))\n    )"}
{"prompt": "def get_settings(self, client_name=None):\n    \"\"\"get all settings, either for a particular client if a name is provided,\n       or across clients.\n\n       Parameters\n       ==========\n       client_name: the client name to return settings for (optional)\n\n    \"\"\"", "output": "\n    if client_name is None:\n        return self.settings\n    else:\n        return self.settings[client_name]\n\n\ndef get_setting(self, setting_name, client_name=None):\n    \"\"\"get a particular setting, either for a particular client if a name is provided,\n       or across clients.\n\n       Parameters\n       ==========\n       setting_name: the name of the setting to return\n       client_name: the client name to return settings for (optional)\n\n    \"\"\"\n    if client_name is None:\n       ", "solution": "\n    settings = read_client_secrets()\n    if client_name is not None and client_name in settings:\n        return settings[client_name]           \n    return settings"}
{"prompt": "def _evolve(self, state, qargs=None):\n        \"\"\"Evolve a quantum state by the operator.\n\n        Args:\n            state (QuantumState): The input statevector or density matrix.\n            qargs (list): a list of QuantumState subsystem positions to apply\n                           the operator on.\n\n        Returns:\n            QuantumState: the output quantum state.\n\n        Raises:\n            QiskitError: if the operator dimension does not match the\n            specified QuantumState subsystem dimensions.\n        \"\"\"", "output": "\n        if qargs is None:\n            qargs = getattr(state, 'qargs', None)\n        if qargs is None:\n            raise QiskitError('Evolution requires QuantumState subsystem '\n                              'specification.')\n        if self._evo_type == 'unitary':\n            return self._evolve_unitary(state, qargs)\n        elif self._evo_type == 'super':\n            return self._evolve_super(state, qargs)\n        elif self._evo_type == '", "solution": "\n        state = self._format_state(state)\n        if qargs is None:\n            if state.shape[0] != self._input_dim:\n                raise QiskitError(\n                    \"Operator input dimension is not equal to state dimension.\"\n                )\n            if state.ndim == 1:\n                # Return evolved statevector\n                return np.dot(self.data, state)\n            # Return evolved density matrix\n            return np.dot(\n                np.dot(self.data, state), np.transpose(np.conj(self.data)))\n        # Subsystem evolution\n        return self._evolve_subsystem(state, qargs)"}
{"prompt": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the CreateKeyPair response payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data buffer containing encoded object\n                data, supporting a read method.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            InvalidKmipEncoding: Raised if the private key unique identifier or\n                the public key unique identifier is missing from the encoded\n                payload.\n        \"\"\"", "output": "1.0.\n\n        Raises:\n            InvalidKmipEncoding: Raised if the data in the input buffer is\n                not a valid encoding.\n        \"\"\"\n        super(CreateKeyPairResponsePayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if self.is_oversized(local_buffer):\n            raise exceptions.InvalidKmipEncoding(\n                \"", "solution": "\n        super(CreateKeyPairResponsePayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._private_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER\n            )\n            self._private_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"private key unique identifier.\"\n            )\n\n        if self.is_tag_next(\n                enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._public_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER\n            )\n            self._public_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"public key unique identifier.\"\n            )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._private_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._private_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._public_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._public_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n        self.is_oversized(local_buffer)"}
{"prompt": "def validate(self):\n        \"\"\"\n        Error check the attributes of the ActivateRequestPayload object.\n        \"\"\"", "output": "\n        self.logger.debug('validate()')\n\n        self.validate_type(self.activation_code, six.string_types)\n        self.validate_type(self.activation_id, six.string_types)\n        self.validate_type(self.activation_url, six.string_types)\n        self.validate_type(self.device_id, six.string_types)\n        self.validate_type(self.device_type, six.string_types)\n        self.validate_type(self.device_token", "solution": "\n        if self.unique_identifier is not None:\n            if not isinstance(self.unique_identifier,\n                              attributes.UniqueIdentifier):\n                msg = \"invalid unique identifier\"\n                raise TypeError(msg)\n        if self.compromise_occurrence_date is not None:\n            if not isinstance(self.compromise_occurrence_date,\n                              primitives.DateTime):\n                msg = \"invalid compromise time\"\n                raise TypeError(msg)\n        if not isinstance(self.revocation_reason, objects.RevocationReason):\n            msg = \"invalid revocation reason\"\n            raise TypeError(msg)"}
{"prompt": "def receive_ack_renewing(self, pkt):\n        \"\"\"Receive ACK in RENEWING state.\"\"\"", "output": "\n        if pkt.ack_nr != self.seq_nr:\n            self.seq_nr = pkt.ack_nr\n            self.seq_nr_ack = pkt.ack_nr\n            self.seq_nr_ack_sent = pkt.ack_nr\n            self.seq_nr_ack_sent_time = pkt.time\n            self.seq_nr_ack_sent_time_ack = pkt.time\n            self.seq_nr_ack_sent_time_ack_ack = pkt.time\n            self.", "solution": "\n        logger.debug(\"C3. Received ACK?, in RENEWING state.\")\n        if self.process_received_ack(pkt):\n            logger.debug(\"C3: T. Received ACK, in RENEWING state, \"\n                         \"raise BOUND.\")\n            raise self.BOUND()"}
{"prompt": "def _get_server_info_from_logs(self):\n        \"\"\"\n        Check server's output log, and determine its scheme / IP / port (helper method for `._launch_server()`).\n\n        This method is polled during process startup. It looks at the server output log and checks for a presence of\n        a particular string (\"INFO: Open H2O Flow in your web browser:\") which indicates that the server is\n        up-and-running. If the method detects this string, it extracts the server's scheme, ip and port and returns\n        them; otherwise it returns None.\n\n        :returns: (scheme, ip, port) tuple if the server has already started, None otherwise.\n        \"\"\"", "output": "them.\n\n        :return: scheme, ip and port of the server.\n        \"\"\"\n        # check if the server is up and running\n        if self._server_process is None:\n            raise H2OServerError(\"Server is not running.\")\n\n        # check if the server is up and running\n        if self._server_process.poll() is not None:\n            raise H2OServerError(\"Server is not running.\")\n\n        # check if the server is up and running\n        if self._server_process.stdout is None:\n            raise H2", "solution": "\n        searchstr = \"INFO: Open H2O Flow in your web browser:\"\n        with open(self._stdout, \"rt\") as f:\n            for line in f:\n                if searchstr in line:\n                    url = line[line.index(searchstr) + len(searchstr):].strip().rstrip(\"/\")\n                    parts = url.split(\":\")\n                    assert len(parts) == 3 and (parts[0] == \"http\" or parts[1] == \"https\") and parts[2].isdigit(), \\\n                        \"Unexpected URL: %s\" % url\n                    return parts[0], parts[1][2:], int(parts[2])\n        return None"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    def wrapper(self, *args, **kwargs):\n        result = f(self, *args, **kwargs)\n        if isinstance(result, list):\n            for i, r in enumerate(result):\n                r.position = i\n        else:\n            result.position = 0\n        return result\n    return wrapper\n\n\ndef track_consumption(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"\n    def wrapper(self, *args, **kwargs):\n        result = f(self, *args,", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def comparison_stats(df, els=['Mg', 'Sr', 'Ba', 'Al', 'Mn']):\n    \"\"\"\n    Compute comparison stats for test and LAtools data.\n    \n    Population-level similarity assessed by a Kolmogorov-Smirnov test.\n    \n    Individual similarity assessed by a pairwise Wilcoxon signed rank test.\n    \n    Trends in residuals assessed by regression analysis, where significance of\n    the slope and intercept is determined by t-tests (both relative to zero).\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataframe containing reference ('X/Ca_r'), test user \n        ('X/Ca_t') and LAtools ('X123') data.\n    els : list\n        list of elements (names only) to plot.\n    \n    Returns\n    -------\n    pandas.DataFrame\n    \n    \"\"\"", "output": "\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing the data to be compared.\n    els : list\n        Elements to be compared.\n    \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the comparison stats.\n    \"\"\"\n    # Check if elements are in df\n    for el in els:\n        if el not in df.columns:\n            raise ValueError(f'{el} not in df')\n    \n    # Check if df has a 'test' column\n    if 'test' not", "solution": "\n    \n    # get corresponding analyte and ratio names\n    As = []\n    Rs = []\n    analytes = [c for c in df.columns if ('_r' not in c) and ('_t' not in c)]\n    ratios = [c for c in df.columns if ('_r' in c)]\n\n    for e in els:\n        if e == 'Sr':\n            As.append('Sr88')\n        elif e == 'Mg':\n            As.append('Mg24')\n        else:\n            As.append([a for a in analytes if e in a][0])\n        Rs.append([r for r in ratios if e in r][0][:-2])\n        \n    yt_stats = []\n    yl_stats = []\n    \n    for i, (e, a) in enumerate(zip(Rs, As)):\n        if a == 'Ba138':\n            m = 1e3\n            u = '$\\mu$mol/mol'\n        else:\n            m = 1\n            u = 'mmol/mol'\n        \n        x = df.loc[:, e + '_r'].values * m\n        yt = df.loc[:, e + '_t'].values * m\n        yl = df.loc[:, a].values * m\n        \n        yt_stats.append(summary_stats(x, yt, e))\n        yl_stats.append(summary_stats(x, yl, e))\n    \n    yt_stats = pd.concat(yt_stats).T\n    yl_stats = pd.concat(yl_stats).T\n    \n    return pd.concat([yt_stats, yl_stats], keys=['Test User', 'LAtools']).T"}
{"prompt": "def get_pk_value_on_save(self, instance):\n        \"\"\"Generate ID if required.\"\"\"", "output": "\n        if instance.pk is None:\n            instance.pk = self.generate_id()\n        return instance\n\n    def generate_id(self):\n        \"\"\"Generate a random ID.\"\"\"\n        return uuid.uuid4().hex\n\n    def get_or_create(self, defaults=None, **kwargs):\n        \"\"\"Get or create an object with the given kwargs.\"\"\"\n        if defaults is None:\n            defaults = {}\n        defaults['id'] = self.generate_id()\n        return super(UUIDModel, self).get_or", "solution": "\n        value = super(AleaIdField, self).get_pk_value_on_save(instance)\n        if not value:\n            value = self.get_seeded_value(instance)\n        return value"}
{"prompt": "def _send(self, data, content_type):\n        \"\"\"\n        Sends a GCM message with the given content type\n        \"\"\"", "output": "\n        headers = {\n            'Authorization': 'key=%s' % self.api_key,\n            'Content-Type': content_type\n        }\n        response = requests.post(\n            self.url,\n            headers=headers,\n            data=data\n        )\n        return response\n\n    def send_message(self, registration_ids, data, collapse_key=None,\n                     delay_while_idle=False, time_to_live=None,\n                     restricted_package_name=None, dry_run=False,\n                ", "solution": "\n\n        headers = {\n            \"Content-Type\": content_type,\n            \"Authorization\": \"key=%s\" % (self.api_key),\n            \"Content-Length\": str(len(data))\n        }\n\n        request = Request(self.api_url, data, headers)\n        return urlopen(request).read().decode(self.encoding)"}
{"prompt": "def make_clean_visible(_html, tag_replacement_char=' '):\n    \"\"\"\n    Takes an HTML-like Unicode string as input and returns a UTF-8\n    encoded string with all tags replaced by whitespace. In particular,\n    all Unicode characters inside HTML are replaced with a single\n    whitespace character.\n\n    This does not detect comments, style, script, link.  It also does\n    do anything with HTML-escaped characters.  All of these are\n    handled by the clean_html pre-cursor step.\n\n    Pre-existing whitespace of any kind (newlines, tabs) is converted\n    to single spaces ' ', which has the same byte length (and\n    character length).\n\n    This is a simple state machine iterator without regexes\n    \"\"\"", "output": "is preserved.\n\n    :param _html: The HTML-like string to be cleaned.\n    :param tag_replacement_char: The character to replace tags with.\n    :return: The cleaned string.\n    \"\"\"\n    # Replace all tags with a single whitespace character.\n    # This is a very simple approach, but it works for our purposes.\n    # We could do something more sophisticated, but it's not worth it.\n    #\n    # We use a list of tuples to avoid having to escape the replacement\n    # character.\n", "solution": "\n    def non_tag_chars(html):\n        n = 0\n        while n < len(html):\n            angle = html.find('<', n)\n            if angle == -1:\n                yield html[n:]\n                n = len(html)\n                break\n            yield html[n:angle]\n            n = angle\n\n            while n < len(html):\n                nl = html.find('\\n', n)\n                angle = html.find('>', n)\n                if angle == -1:\n                    yield ' ' * (len(html) - n)\n                    n = len(html)\n                    break\n                elif nl == -1 or angle < nl:\n                    yield ' ' * (angle + 1 - n)\n                    n = angle + 1\n                    break\n                else:\n                    yield ' ' * (nl - n) + '\\n'\n                    n = nl + 1\n                    # do not break\n\n    if not isinstance(_html, unicode):\n        _html = unicode(_html, 'utf-8')\n\n    # Protect emails by substituting with unique key\n    _html = fix_emails(_html)\n\n    #Strip tags with previous logic\n    non_tag = ''.join(non_tag_chars(_html))\n\n    return non_tag.encode('utf-8')"}
{"prompt": "def match(self, request):\n        \"\"\"\n        Matches a given Request instance contract against the registered mocks.\n\n        If a mock passes all the matchers, its response will be returned.\n\n        Arguments:\n            request (pook.Request): Request contract to match.\n\n        Raises:\n            pook.PookNoMatches: if networking is disabled and no mock matches\n                with the given request contract.\n\n        Returns:\n            pook.Response: the mock response to be used by the interceptor.\n        \"\"\"", "output": "\n        # If networking is disabled, raise an exception\n        if not self.enabled:\n            raise PookNoMatches(\n                'No mocks found for request: {}'.format(request))\n\n        # If the request is a mock, return it\n        if request.is_mock:\n            return request.response\n\n        # If the request is a mock, return it\n        if request.is_mock:\n            return request.response\n\n        # If the request is a mock, return it\n        if request.is_mock:\n            return request.response", "solution": "\n        # Trigger engine-level request filters\n        for test in self.filters:\n            if not test(request, self):\n                return False\n\n        # Trigger engine-level request mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Store list of mock matching errors for further debugging\n        match_errors = []\n\n        # Try to match the request against registered mock definitions\n        for mock in self.mocks[:]:\n            try:\n                # Return the first matched HTTP request mock\n                matches, errors = mock.match(request.copy())\n                if len(errors):\n                    match_errors += errors\n                if matches:\n                    return mock\n            except PookExpiredMock:\n                # Remove the mock if already expired\n                self.mocks.remove(mock)\n\n        # Validate that we have a mock\n        if not self.should_use_network(request):\n            msg = 'pook error!\\n\\n'\n\n            msg += (\n                '=> Cannot match any mock for the '\n                'following request:\\n{}'.format(request)\n            )\n\n            # Compose unmatch error details, if debug mode is enabled\n            if self.debug:\n                err = '\\n\\n'.join([str(err) for err in match_errors])\n                if err:\n                    msg += '\\n\\n=> Detailed matching errors:\\n{}\\n'.format(err)\n\n            # Raise no matches exception\n            raise PookNoMatches(msg)\n\n        # Register unmatched request\n        self.unmatched_reqs.append(request)"}
{"prompt": "def discrete_best_alpha(data, alpharangemults=(0.9,1.1), n_alpha=201, approximate=True, verbose=True):\n    \"\"\"\n    Use the maximum L to determine the most likely value of alpha\n\n    *alpharangemults* [ 2-tuple ]\n        Pair of values indicating multiplicative factors above and below the\n        approximate alpha from the MLE alpha to use when determining the\n        \"exact\" alpha (by directly maximizing the likelihood function)\n    \"\"\"", "output": "\n    if approximate:\n        alpha = discrete_alpha(data, n_alpha=n_alpha, verbose=verbose)\n    else:\n        alpha = discrete_alpha(data, n_alpha=n_alpha, verbose=verbose,\n                               approximate=False)\n\n    # Get the range of alpha values to try\n    alpharange = np.linspace(alpharangemults[0]*alpha,\n                             alpharangemults[1]*alpha,\n                             n_alpha)\n\n    # Get the likelihoods", "solution": "\n\n    xmins = np.unique(data)\n    if approximate:\n        alpha_of_xmin = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n    else:\n        alpha_approx = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n        alpharanges = [(0.9*a,1.1*a) for a in alpha_approx]\n        alpha_of_xmin = [ most_likely_alpha(data,xmin,alpharange=ar,n_alpha=n_alpha) for xmin,ar in zip(xmins,alpharanges) ]\n    ksvalues = [ discrete_ksD(data, xmin, alpha) for xmin,alpha in zip(xmins,alpha_of_xmin) ]\n\n    best_index = argmin(ksvalues)\n    best_alpha = alpha_of_xmin[best_index]\n    best_xmin = xmins[best_index]\n    best_ks = ksvalues[best_index]\n    best_likelihood = discrete_likelihood(data, best_xmin, best_alpha)\n\n    if verbose:\n        print(\"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % (\n                best_alpha, best_xmin, best_ks, best_likelihood,\n                (data<best_xmin).sum(), (data>=best_xmin).sum()))\n\n    return best_alpha,best_xmin,best_ks,best_likelihood"}
{"prompt": "def _convert_date(date_string, date_format):\n    \"\"\"\n    Convert a date in a given format to epoch time. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"", "output": "\n    return int(time.mktime(datetime.datetime.strptime(date_string, date_format).timetuple()))\n\n\ndef _convert_time(time_string, time_format):\n    \"\"\"\n    Convert a time in a given format to epoch time. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"\n    return int(time.mktime(datetime.datetime.strptime(time_string, time_format).timetuple()))\n\n\ndef _convert_datetime(datetime_string, datetime_", "solution": "\n    if date_format != 'epoch':\n        return datetime.strptime(date_string, date_format).timestamp()\n    else:\n        return float(date_string)"}
{"prompt": "def create_session(token=None):\n    \"\"\"\n    Create a github3.py session connected to GitHub.com\n\n    If token is not provided, will attempt to use the GITHUB_API_TOKEN\n    environment variable if present.\n    \"\"\"", "output": "\n    if token is None:\n        token = os.environ.get('GITHUB_API_TOKEN')\n    if token is None:\n        raise ValueError('No token provided')\n    return session.Session(token)\n\n\ndef create_session_from_config(config):\n    \"\"\"\n    Create a github3.py session connected to GitHub.com\n\n    If token is not provided, will attempt to use the GITHUB_API_TOKEN\n    environment variable if present.\n    \"\"\"\n    if config.token is None:\n       ", "solution": "\n    if token is None:\n        token = os.environ.get('GITHUB_API_TOKEN', None)\n\n    gh_session = github3.login(token=token)\n\n    if gh_session is None:\n        raise RuntimeError('Invalid or missing GITHUB_API_TOKEN')\n\n    return gh_session"}
{"prompt": "def load(self, source):\n        \"\"\"\n        Opens the source file.\n        \"\"\"", "output": "\n        self.source = source\n        self.file = open(source, 'r')\n\n    def close(self):\n        \"\"\"\n        Closes the source file.\n        \"\"\"\n        self.file.close()\n\n    def read(self):\n        \"\"\"\n        Reads the source file.\n        \"\"\"\n        return self.file.read()\n\n    def write(self, data):\n        \"\"\"\n        Writes the data to the source file.\n        \"\"\"\n        self.file.write(data)\n\n    def seek(self, position):", "solution": "\n        self.source = open(self.source, 'rb')    \n        self.loaded = True"}
{"prompt": "def unregisterObserver(self, observer):\n        \"\"\" Remove an observer from the meter update() chain.\n\n        Args:\n            observer (MeterObserver): Subclassed MeterObserver.\n        \"\"\"", "output": "\n        self._observers.remove(observer)\n\n    def update(self, value):\n        \"\"\" Update the meter's value.\n\n        Args:\n            value (float): New value for the meter.\n        \"\"\"\n        self._value = value\n        for observer in self._observers:\n            observer.update(self)\n\n    def getValue(self):\n        \"\"\" Get the meter's current value.\n\n        Returns:\n            float: Current value of the meter.\n        \"\"\"\n        return self._value\n\n    def", "solution": "\n        if observer in self.m_observers:\n            self.m_observers.remove(observer)\n        pass"}
{"prompt": "def query(self, minhash, size):\n        \"\"\"\n        Giving the MinHash and size of the query set, retrieve\n        keys that references sets with containment with respect to\n        the query set greater than the threshold.\n\n        Args:\n            minhash (datasketch.MinHash): The MinHash of the query set.\n            size (int): The size (number of unique items) of the query set.\n\n        Returns:\n            `iterator` of keys.\n        \"\"\"", "output": "\n        return self._query(minhash, size)\n\n    def _query(self, minhash, size):\n        \"\"\"\n        Giving the MinHash and size of the query set, retrieve\n        keys that references sets with containment with respect to\n        the query set greater than the threshold.\n\n        Args:\n            minhash (datasketch.MinHash): The MinHash of the query set.\n            size (int): The size (number of unique items) of the query set.\n\n        Returns:\n            `iterator` of keys.\n        \"\"\"\n", "solution": "\n        for i, index in enumerate(self.indexes):\n            u = self.uppers[i]\n            if u is None:\n                continue\n            b, r = self._get_optimal_param(u, size)\n            for key in index[r]._query_b(minhash, b):\n                yield key"}
{"prompt": "def get_security_repository(self):\n        \"\"\" Security repository \"\"\"", "output": "\n        return self._security_repository\n\n    @property\n    def security_repository(self):\n        \"\"\" Security repository \"\"\"\n        return self._security_repository\n\n    @security_repository.setter\n    def security_repository(self, value):\n        \"\"\" Security repository \"\"\"\n        self._security_repository = value\n\n    @property\n    def security_repository_id(self):\n        \"\"\" Security repository id \"\"\"\n        return self._security_repository_id\n\n    @security_repository_id.setter\n    def security_repository_id(self,", "solution": "\n        from .repositories import SecurityRepository\n\n        if not self.security_repo:\n            self.security_repo = SecurityRepository(self.session)\n        return self.security_repo"}
{"prompt": "def Counter(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"", "output": "\n    return a + delta * b\n\n\ndef Counter_deriv(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"\n    return delta * b\n\n\ndef Counter_deriv2(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"\n    return 0\n\n\ndef Counter_deriv3(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"\n    return 0\n\n\ndef Counter_deriv4(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"\n    return 0", "solution": "\n    if b < a:\n        return None \n\n    return (b - a) / float(delta)"}
{"prompt": "def cli_command_restart(self, msg):\n        \"\"\"\\\n        restart the subprocess\n        i. we set our state to RESTARTING - on restarting we still send heartbeat\n        ii. we kill the subprocess\n        iii. we start again\n        iv. if its started we set our state to RUNNING, else we set it to WAITING\n\n        :param msg:\n        :return:\n        \"\"\"", "output": "\n        self.logger.info(\"restarting\")\n        self.state = self.STATE_RESTARTING\n        self.kill_subprocess()\n        self.start_subprocess()\n        if self.subprocess_is_running():\n            self.state = self.STATE_RUNNING\n        else:\n            self.state = self.STATE_WAITING\n\n    def cli_command_stop(self, msg):\n        \"\"\"\\\n        stop the subprocess\n        i. we set our state to STOPPING - on stopping we still", "solution": "\n        info = ''\n        if self.state == State.RUNNING and self.sprocess and self.sprocess.proc:\n            self.state = State.RESTARTING\n            self.sprocess.set_exit_callback(self.proc_exit_cb_restart)\n            self.sprocess.proc.kill()\n            info = 'killed'\n            # TODO: check if process is really dead etc.\n        return info"}
{"prompt": "def delete_dir(bucket_name, root_path,\n               aws_access_key_id=None, aws_secret_access_key=None,\n               aws_profile=None):\n    \"\"\"Delete all objects in the S3 bucket named ``bucket_name`` that are\n    found in the ``root_path`` directory.\n\n    Parameters\n    ----------\n    bucket_name : `str`\n        Name of an S3 bucket.\n    root_path : `str`\n        Directory in the S3 bucket that will be deleted.\n    aws_access_key_id : `str`\n        The access key for your AWS account. Also set\n        ``aws_secret_access_key``.\n    aws_secret_access_key : `str`\n        The secret key for your AWS account.\n    aws_profile : `str`, optional\n        Name of AWS profile in :file:`~/.aws/credentials`. Use this instead\n        of ``aws_access_key_id`` and ``aws_secret_access_key`` for file-based\n        credentials.\n\n    Raises\n    ------\n    ltdconveyor.s3.S3Error\n        Thrown by any unexpected faults from the S3 API.\n    \"\"\"", "output": "_access_key_id : `str`, optional\n        AWS access key ID.\n    aws_secret_access_key : `str`, optional\n        AWS secret access key.\n    aws_profile : `str`, optional\n        AWS profile name.\n\n    \"\"\"\n    s3 = get_s3_resource(aws_access_key_id, aws_secret_access_key, aws_profile)\n    bucket = s3.Bucket(bucket_name)\n    for obj in bucket.objects.filter(Prefix=root_path):\n        obj.delete", "solution": "\n    logger = logging.getLogger(__name__)\n\n    session = boto3.session.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key)\n    s3 = session.resource('s3')\n    client = s3.meta.client\n\n    # Normalize directory path for searching patch prefixes of objects\n    if not root_path.endswith('/'):\n        root_path.rstrip('/')\n\n    paginator = client.get_paginator('list_objects_v2')\n    pages = paginator.paginate(Bucket=bucket_name, Prefix=root_path)\n\n    keys = dict(Objects=[])\n    for item in pages.search('Contents'):\n        try:\n            keys['Objects'].append({'Key': item['Key']})\n        except TypeError:  # item is None; nothing to delete\n            continue\n        # Delete immediately when 1000 objects are listed\n        # the delete_objects method can only take a maximum of 1000 keys\n        if len(keys['Objects']) >= 1000:\n            try:\n                client.delete_objects(Bucket=bucket_name, Delete=keys)\n            except Exception:\n                message = 'Error deleting objects from %r' % root_path\n                logger.exception(message)\n                raise S3Error(message)\n            keys = dict(Objects=[])\n\n    # Delete remaining keys\n    if len(keys['Objects']) > 0:\n        try:\n            client.delete_objects(Bucket=bucket_name, Delete=keys)\n        except Exception:\n            message = 'Error deleting objects from %r' % root_path\n            logger.exception(message)\n            raise S3Error(message)"}
{"prompt": "def find_launcher_class(clsname, kind):\n    \"\"\"Return a launcher for a given clsname and kind.\n\n    Parameters\n    ==========\n    clsname : str\n        The full name of the launcher class, either with or without the\n        module path, or an abbreviation (MPI, SSH, SGE, PBS, LSF,\n        WindowsHPC).\n    kind : str\n        Either 'EngineSet' or 'Controller'.\n    \"\"\"", "output": "\n    if clsname in _launcher_classes:\n        return _launcher_classes[clsname]\n    if clsname in _launcher_abbreviations:\n        clsname = _launcher_abbreviations[clsname]\n        if clsname in _launcher_classes:\n            return _launcher_classes[clsname]\n    if kind == 'EngineSet':\n        if clsname in _launcher_classes:\n            return _launcher_classes[clsname]\n        if clsname in _launcher", "solution": "\n    if '.' not in clsname:\n        # not a module, presume it's the raw name in apps.launcher\n        if kind and kind not in clsname:\n            # doesn't match necessary full class name, assume it's\n            # just 'PBS' or 'MPI' prefix:\n            clsname = clsname + kind + 'Launcher'\n        clsname = 'IPython.parallel.apps.launcher.'+clsname\n    klass = import_item(clsname)\n    return klass"}
{"prompt": "def get_end_of_day(timestamp):\n    \"\"\"\n    Given a date or a datetime, return a datetime at 23:59:59 on that day\n    \"\"\"", "output": "\n    if isinstance(timestamp, datetime):\n        return datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)\n    else:\n        return datetime.combine(timestamp, time(23, 59, 59))\n\n\ndef get_start_of_day(timestamp):\n    \"\"\"\n    Given a date or a datetime, return a datetime at 00:00:00 on that day\n    \"\"\"\n    if isinstance(timestamp, datetime):\n        return datetime(timestamp.", "solution": "\n    return datetime.datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for dirpath, dirnames, filenames in os.walk(rootpath):\r\n        # Strip the rootpath from the dirpath\r\n        dirpath = dirpath[len(rootpath):].lstrip(os.path.sep)\r\n        if dirpath and dirpath not in skip:\r\n            # If we are not in the root path, add the directory to the\r\n            # modules dictionary\r\n            modules[dirpath] = []\r\n        for filename in filenames:\r\n            if filename.endswith", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def institutes(self, institute_ids=None):\n        \"\"\"Fetch all institutes.\n        \n        Args:\n            institute_ids(list(str))\n        \n        Returns:\n            res(pymongo.Cursor)\n        \"\"\"", "output": "\n        if institute_ids is None:\n            institute_ids = []\n        return self.institutes.find({\"_id\": {\"$in\": institute_ids}})\n\n    def institute(self, institute_id):\n        \"\"\"Fetch an institute.\n        \n        Args:\n            institute_id(str)\n        \n        Returns:\n            res(pymongo.Cursor)\n        \"\"\"\n        return self.institutes.find_one({\"_id\": institute_id})\n\n    def institute_", "solution": "\n        query = {}\n        if institute_ids:\n            query['_id'] = {'$in': institute_ids}\n        LOG.debug(\"Fetching all institutes\")\n        return self.institute_collection.find(query)"}
{"prompt": "def send(self, node, message):\n        \"\"\"\n        Send a message to a node. Returns False if the connection appears to be dead either before or after actually trying to send the message.\n\n        :param node: target node\n        :type node: Node\n        :param message: message\n        :param message: any\n        :returns success\n        :rtype bool\n        \"\"\"", "output": "\n        if self.is_dead():\n            return False\n        try:\n            self.send_message(node, message)\n        except Exception:\n            return False\n        return True\n\n    def send_message(self, node, message):\n        \"\"\"\n        Send a message to a node.\n\n        :param node: target node\n        :type node: Node\n        :param message: message\n        :param message: any\n        \"\"\"\n        raise NotImplementedError\n\n    def is_dead(self):\n        \"\"\"\n        Check if", "solution": "\n\n        if node not in self._connections or self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        self._connections[node].send(message)\n        if self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        return True"}
{"prompt": "def create_model(self, project_id, model):\n        \"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"", "output": "\n        operation = self.client.create_model(\n            project_id, model, retry=self.retry, timeout=self.timeout\n        )\n        operation.result(timeout=self.timeout)\n\n    def delete_model(self, project_id, model_id):\n        \"\"\"\n        Delete a Model. Blocks until finished.\n        \"\"\"\n        operation = self.client.delete_model(\n            project_id, model_id, retry=self.retry, timeout=self.timeout\n        )\n        operation.result(timeout", "solution": "\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()"}
{"prompt": "def read_msg(self):\n        \"\"\"Read one message unit. It's possible however that\n        more than one message will be set in a receive, so we will\n        have to buffer that for the next read.\n        EOFError will be raised on EOF.\n        \"\"\"", "output": "\n        if self.buffer:\n            return self.buffer.pop(0)\n        if self.closed:\n            raise EOFError\n        try:\n            msg = self.socket.recv(self.max_msg_size)\n        except socket.error as e:\n            if e.errno == errno.EAGAIN:\n                return None\n            raise\n        if not msg:\n            self.closed = True\n            raise EOFError\n        return msg\n\n    def read(self, n=-1):\n        \"\"\"Read up to n", "solution": "\n        if self.state == 'connected':\n            if 0 == len(self.buf):\n                self.buf = self.inout.recv(Mtcpfns.TCP_MAX_PACKET)\n                if 0 == (self.buf):\n                    self.state = 'disconnected'\n                    raise EOFError\n                pass\n            self.buf, data = Mtcpfns.unpack_msg(self.buf)\n            return data.decode('utf-8')\n        else:\n            raise IOError(\"read_msg called in state: %s.\" % self.state)"}
{"prompt": "def _wakeup(self):\n        \"\"\"\n        issue wakeup command to device to take out of standby mode.\n        \"\"\"", "output": "\n        self.log.debug(\"wakeup\")\n        self.write(\"wakeup\")\n\n    def _sleep(self):\n        \"\"\"\n        issue sleep command to device to put into standby mode.\n        \"\"\"\n        self.log.debug(\"sleep\")\n        self.write(\"sleep\")\n\n    def _reset(self):\n        \"\"\"\n        issue reset command to device to reset.\n        \"\"\"\n        self.log.debug(\"reset\")\n        self.write(\"reset\")\n\n    def _set_power(self, power):\n        \"\"\"", "solution": "\n        log.info(\"send: WAKEUP\")\n        for i in xrange(3):\n            self.port.write('\\n')  # wakeup device\n            ack = self.port.read(len(self.WAKE_ACK))  # read wakeup string\n            log_raw('read', ack)\n            if ack == self.WAKE_ACK:\n                return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def get_home_dir(require_writable=False):\n    \"\"\"Return the 'home' directory, as a unicode string.\n\n    * First, check for frozen env in case of py2exe\n    * Otherwise, defer to os.path.expanduser('~')\n    \n    See stdlib docs for how this is determined.\n    $HOME is first priority on *ALL* platforms.\n    \n    Parameters\n    ----------\n    \n    require_writable : bool [default: False]\n        if True:\n            guarantees the return value is a writable directory, otherwise\n            raises HomeDirError\n        if False:\n            The path is resolved, but it is not guaranteed to exist or be writable.\n    \"\"\"", "output": "value is a writable directory\n            (by checking if the directory is writable)\n\n    Returns\n    -------\n    unicode\n        the home directory\n\n    Raises\n    ------\n    IOError\n        if the home directory is not writable\n    \"\"\"\n    if getattr(sys, 'frozen', False):\n        # If the application is run as a bundle, the pyInstaller bootloader\n        # extends the sys module by a flag frozen=True and sets the app \n        # path into variable _MEIPASS'.\n        return os", "solution": "\n\n    # first, check py2exe distribution root directory for _ipython.\n    # This overrides all. Normally does not exist.\n\n    if hasattr(sys, \"frozen\"): #Is frozen by py2exe\n        if '\\\\library.zip\\\\' in IPython.__file__.lower():#libraries compressed to zip-file\n            root, rest = IPython.__file__.lower().split('library.zip')\n        else:\n            root=os.path.join(os.path.split(IPython.__file__)[0],\"../../\")\n        root=os.path.abspath(root).rstrip('\\\\')\n        if _writable_dir(os.path.join(root, '_ipython')):\n            os.environ[\"IPYKITROOT\"] = root\n        return py3compat.cast_unicode(root, fs_encoding)\n    \n    homedir = os.path.expanduser('~')\n    # Next line will make things work even when /home/ is a symlink to\n    # /usr/home as it is on FreeBSD, for example\n    homedir = os.path.realpath(homedir)\n    \n    if not _writable_dir(homedir) and os.name == 'nt':\n        # expanduser failed, use the registry to get the 'My Documents' folder.\n        try:\n            import _winreg as wreg\n            key = wreg.OpenKey(\n                wreg.HKEY_CURRENT_USER,\n                \"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n            )\n            homedir = wreg.QueryValueEx(key,'Personal')[0]\n            key.Close()\n        except:\n            pass\n    \n    if (not require_writable) or _writable_dir(homedir):\n        return py3compat.cast_unicode(homedir, fs_encoding)\n    else:\n        raise HomeDirError('%s is not a writable dir, '\n                'set $HOME environment variable to override' % homedir)"}
{"prompt": "def run_eplus(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None):\n    \"\"\"\n    Parameters\n    ----------\n    epm_or_idf_path:\n    weather_data_or_epw_path\n    simulation_dir_path\n    stdout: default sys.stdout\n    stderr: default sys.stderr\n    beat_freq: if not none, stdout will be used at least every beat_freq (in seconds)\n    \"\"\"", "output": "every beat_freq seconds\n    \"\"\"\n    if isinstance(epm_or_idf_path, str):\n        epm_or_idf_path = Path(epm_or_idf_path)\n    if isinstance(weather_data_or_epw_path, str):\n        weather_data_or_epw_path = Path(weather_data_or_epw_path)\n    if isinstance(simulation_dir_path, str):\n        simulation_dir_path = Path(simulation_dir_", "solution": "\n    # work with absolute paths\n    simulation_dir_path = os.path.abspath(simulation_dir_path)\n\n    # check dir path\n    if not os.path.isdir(simulation_dir_path):\n        raise NotADirectoryError(\"Simulation directory does not exist: '%s'.\" % simulation_dir_path)\n\n    # epm\n    if not isinstance(epm_or_idf_path, Epm):\n        # we don't copy file directly because we want to manage it's external files\n        # could be optimized (use _copy_without_read_only)\n        epm = Epm.from_idf(epm_or_idf_path)\n    else:\n        epm = epm_or_idf_path\n\n    # create idf\n    simulation_idf_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".idf\")\n    epm.to_idf(simulation_idf_path)\n\n    # weather data\n    simulation_epw_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".epw\")\n    if isinstance(weather_data_or_epw_path, WeatherData):\n        weather_data_or_epw_path.to_epw(simulation_epw_path)\n    else:\n        # no need to load: we copy directly\n        _copy_without_read_only(weather_data_or_epw_path, simulation_epw_path)\n\n    # copy epw if needed (depends on os/eplus version)\n    temp_epw_path = get_simulated_epw_path()\n    if temp_epw_path is not None:\n        _copy_without_read_only(simulation_epw_path, temp_epw_path)\n\n    # prepare command\n    eplus_relative_cmd = get_simulation_base_command()\n    eplus_cmd = os.path.join(CONF.eplus_base_dir_path, eplus_relative_cmd)\n\n    # idf\n    idf_command_style = get_simulation_input_command_style(\"idf\")\n    if idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        idf_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        idf_file_cmd = simulation_idf_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # epw\n    epw_command_style = get_simulation_input_command_style(\"epw\")\n    if epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        epw_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        epw_file_cmd = simulation_epw_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # command list\n    simulation_command_style = get_simulation_command_style()\n    if simulation_command_style == SIMULATION_COMMAND_STYLES.args:\n        cmd_l = [eplus_cmd, idf_file_cmd, epw_file_cmd]\n    elif simulation_command_style == SIMULATION_COMMAND_STYLES.kwargs:\n        cmd_l = [eplus_cmd, \"-w\", epw_file_cmd, \"-r\", idf_file_cmd]\n    else:\n        raise RuntimeError(\"should not be here\")\n\n    # launch calculation\n    run_subprocess(\n        cmd_l,\n        cwd=simulation_dir_path,\n        stdout=stdout,\n        stderr=stderr,\n        beat_freq=beat_freq\n    )\n\n    # if needed, we delete temp weather data (only on Windows, see above)\n    if (temp_epw_path is not None) and os.path.isfile(temp_epw_path):\n        os.remove(os.path.join(temp_epw_path))"}
{"prompt": "def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"", "output": "\n    return ast.Attribute(\n        value=_load_attr(name.split(\".\")[0], ctx) if \".\" in name else ctx,\n        attr=name.split(\".\")[-1],\n        ctx=ctx,\n    )\n\n\ndef _load_name(name: str, ctx: ast.AST = ast.Load()) -> ast.Name:\n    \"\"\"Generate recursive Python Name AST nodes for resolving nested names.\"\"\"\n    return ast.Name(id=name.split(\".\")[-1], ctx=ctx) if \".\" in name else", "solution": "\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)"}
{"prompt": "def _prepare_query_values(self):\n        \"\"\"Extra prep on query values by converting\n        dictionaries into :see:HStoreValue expressions.\n\n        This allows putting expressions in a dictionary.\n        The :see:HStoreValue will take care of resolving\n        the expressions inside the dictionary.\"\"\"", "output": "\n        for i, value in enumerate(self.values):\n            if isinstance(value, dict):\n                self.values[i] = HStoreValue(value)\n\n    def _prepare_query_values_for_insert(self):\n        \"\"\"Extra prep on query values by converting\n        dictionaries into :see:HStoreValue expressions.\n\n        This allows putting expressions in a dictionary.\n        The :see:HStoreValue will take care of resolving\n        the expressions inside the dictionary.\"\"\"\n        for i, value in enumerate(self.values):\n           ", "solution": "\n\n        new_query_values = []\n        for field, model, val in self.query.values:\n            if isinstance(val, dict):\n                val = HStoreValue(val)\n\n            new_query_values.append((\n                field,\n                model,\n                val\n            ))\n\n        self.query.values = new_query_values"}
{"prompt": "def get_handler(level: str, fmt: str) -> logging.Handler:\n    \"\"\"Get the default logging handler for Basilisp.\"\"\"", "output": "\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(fmt))\n    handler.setLevel(level)\n    return handler\n\n\ndef get_logger(name: str, level: str, fmt: str) -> logging.Logger:\n    \"\"\"Get the default logging handler for Basilisp.\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    logger.addHandler(get_handler(level, fmt))\n    return logger\n\n\ndef get_logger_from_config(config: dict", "solution": "\n    handler: logging.Handler = logging.NullHandler()\n    if os.getenv(\"BASILISP_USE_DEV_LOGGER\") == \"true\":\n        handler = logging.StreamHandler()\n\n    handler.setFormatter(logging.Formatter(fmt))\n    handler.setLevel(level)\n    return handler"}
{"prompt": "def _get_course_content_from_ecommerce(course_id, site_code=None):\n    \"\"\"\n    Get course information using the Ecommerce course api.\n\n    In case of error returns empty response.\n    Arguments:\n        course_id (str): course key of the course\n        site_code (str): site code\n\n    Returns:\n        course information from Ecommerce\n    \"\"\"", "output": "\n    try:\n        if site_code:\n            return get_course_content_from_ecommerce(course_id, site_code)\n        else:\n            return get_course_content_from_ecommerce(course_id)\n    except Exception:\n        return {}\n\n\ndef _get_course_content_from_edx(course_id, site_code=None):\n    \"\"\"\n    Get course information using the edX course api.\n\n    In case of error returns empty response.\n    Arguments:\n        course_id (str", "solution": "\n    api = get_ecommerce_client(site_code=site_code)\n    try:\n        api_response = api.courses(course_id).get()\n    except Exception:  # pylint: disable=broad-except\n        logger.exception(\n            'An error occurred while retrieving data for course run [%s] from the Catalog API.',\n            course_id,\n            exc_info=True\n        )\n        return {}\n\n    return {\n        'title': api_response.get('name'),\n        'verification_deadline': api_response.get('verification_deadline')\n    }"}
{"prompt": "def container_services(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-07-01: :class:`ContainerServicesOperations<azure.mgmt.containerservice.v2017_07_01.operations.ContainerServicesOperations>`\n        \"\"\"", "output": "\n        api_version = self._get_api_version('container_services')\n        if api_version == '2017-07-01':\n            from .v2017_07_01.operations import ContainerServicesOperations as OperationClass\n        else:\n            raise ValueError(\"API version {} does not have operation group 'container_services'\".format(api_version))\n        return OperationClass(self._client, self._config, Serializer(self._models_dict(api_version)), Deserializer(self._models", "solution": "\n        api_version = self._get_api_version('container_services')\n        if api_version == '2017-07-01':\n            from .v2017_07_01.operations import ContainerServicesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def doc_from_xml(document_element_name, inner_xml,\n                     xmlns='http://schemas.microsoft.com/windowsazure'):\n        \"\"\"Wraps the specified xml in an xml root element with default azure\n        namespaces\"\"\"", "output": "\n        return '<{0} xmlns=\"{1}\">{2}</{0}>'.format(document_element_name,\n                                                    xmlns, inner_xml)\n\n\ndef get_xml_element(xml_doc, element_name):\n    \"\"\"Returns the specified xml element from the xml document\"\"\"\n    return xml_doc.find(element_name)\n\n\ndef get_xml_elements(xml_doc, element_name):\n    \"\"\"Returns all the specified xml elements from the xml document\"\"\"\n    return xml_doc.findall(", "solution": "\n        xml = ''.join(['<', document_element_name,\n                      ' xmlns=\"{0}\">'.format(xmlns)])\n        xml += inner_xml\n        xml += ''.join(['</', document_element_name, '>'])\n        return xml"}
{"prompt": "def lessThan(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"", "output": "\n        return left < right\n\n    def lessThanEqual(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"\n        return left <= right\n\n    def greaterThan(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"\n        return left > right\n\n    def greaterThanEqual(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"\n        return left >= right\n\n    def equal(self, left, right):\n        \"\"\"", "solution": "\n        sourceModel = self.sourceModel()\n        if sourceModel:\n            leftItem = sourceModel.item(left)\n            rightItem = sourceModel.item(right)\n\n            if (isinstance(leftItem, Directory)\n                and not isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.AscendingOrder\n\n            elif (not isinstance(leftItem, Directory)\n                and isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.DescendingOrder\n\n        return super(FilesystemSortProxy, self).lessThan(left, right)"}
{"prompt": "def get_entry_properties_from_element(element, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        \"\"\" get properties from element tree element \"\"\"", "output": "\n        properties = {}\n        if element.tag == 'entry':\n            for child in element:\n                if child.tag == 'id' and not include_id:\n                    continue\n                if child.tag == 'id' and id_prefix_to_skip and child.text.startswith(id_prefix_to_skip):\n                    continue\n                if child.tag == 'title' and use_title_as_id:\n                    properties['id'] = child.text\n                else:\n                    properties[child.tag] = child.text\n", "solution": "\n        properties = {}\n\n        etag = element.attrib.get(_make_etree_ns_attr_name(_etree_entity_feed_namespaces['m'], 'etag'), None)\n        if etag is not None:\n            properties['etag'] = etag\n\n        updated = element.findtext('./atom:updated', '', _etree_entity_feed_namespaces)\n        if updated:\n            properties['updated'] = updated\n\n        author_name = element.findtext('./atom:author/atom:name', '', _etree_entity_feed_namespaces)\n        if author_name:\n            properties['author'] = author_name\n\n        if include_id:\n            if use_title_as_id:\n                title = element.findtext('./atom:title', '', _etree_entity_feed_namespaces)\n                if title:\n                    properties['name'] = title\n            else:\n                element_id = element.findtext('./atom:id', '', _etree_entity_feed_namespaces)\n                if element_id:\n                    properties['name'] = _get_readable_id(element_id, id_prefix_to_skip)\n\n        return properties"}
{"prompt": "def source_address(self):\n        \"\"\"Return the authorative source of the link.\"\"\"", "output": "\n        return self._source_address\n\n    @property\n    def destination_address(self):\n        \"\"\"Return the authorative destination of the link.\"\"\"\n        return self._destination_address\n\n    @property\n    def source_port(self):\n        \"\"\"Return the authorative source port of the link.\"\"\"\n        return self._source_port\n\n    @property\n    def destination_port(self):\n        \"\"\"Return the authorative destination port of the link.\"\"\"\n        return self._destination_port\n\n    @property\n    def source_interface(self", "solution": "\n        # If link is a sender, source is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_sender:\n            return self._pn_link.source.address\n        else:\n            return self._pn_link.remote_source.address"}
{"prompt": "def _serialize(xp_ast):\n    \"\"\"Generate token strings which, when joined together, form a valid\n    XPath serialization of the AST.\"\"\"", "output": "\n    if isinstance(xp_ast, str):\n        return xp_ast\n    elif isinstance(xp_ast, list):\n        return ''.join(_serialize(child) for child in xp_ast)\n    elif isinstance(xp_ast, dict):\n        return ''.join(\n            _serialize(child) for child in xp_ast.values()\n        )\n    elif isinstance(xp_ast, tuple):\n        return ''.join(\n            _serialize(child) for child in xp_ast\n        )", "solution": "\n\n    if hasattr(xp_ast, '_serialize'):\n        for tok in xp_ast._serialize():\n            yield(tok)\n    elif isinstance(xp_ast, str):\n        yield(repr(xp_ast))"}
{"prompt": "def check_for_prefix(self, bucket_name, prefix, delimiter):\n        \"\"\"\n        Checks that a prefix exists in a bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        \"\"\"", "output": "\n        if not prefix:\n            return\n\n        if delimiter:\n            prefix = prefix.rstrip(delimiter)\n\n        if prefix.endswith(delimiter):\n            prefix = prefix[:-1]\n\n        if not self.check_for_key(bucket_name, prefix):\n            raise ValueError(\"Prefix %s does not exist in bucket %s\" % (prefix, bucket_name))\n\n    def check_for_key(self, bucket_name, key):\n        \"\"\"\n        Checks that a key exists in a bucket\n\n        :", "solution": "\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist"}
{"prompt": "def group(self, name):\n        \"\"\"GROUP command.\n        \"\"\"", "output": "\n        return self.send_command(b'GROUP', name)\n\n    def id(self):\n        \"\"\"ID command.\n        \"\"\"\n        return self.send_command(b'ID')\n\n    def info(self):\n        \"\"\"INFO command.\n        \"\"\"\n        return self.send_command(b'INFO')\n\n    def lastsave(self):\n        \"\"\"LASTSAVE command.\n        \"\"\"\n        return self.send_command(b'LASTSAVE')\n\n    def monitor(self):\n        \"\"\"MONITOR", "solution": "\n        args = name\n\n        code, message = self.command(\"GROUP\", args)\n        if code != 211:\n            raise NNTPReplyError(code, message)\n\n        parts = message.split(None, 4)\n        try:\n            total = int(parts[0])\n            first = int(parts[1])\n            last  = int(parts[2])\n            group = parts[3]\n        except (IndexError, ValueError):\n            raise NNTPDataError(\"Invalid GROUP status '%s'\" % message)\n\n        return total, first, last, group"}
{"prompt": "def get(self, thing_id='0', property_name=None):\n        \"\"\"\n        Handle a GET request.\n\n        thing_id -- ID of the thing this request is for\n        property_name -- the name of the property from the URL path\n        \"\"\"", "output": "\n        if property_name is None:\n            return self.get_thing(thing_id)\n        else:\n            return self.get_property(thing_id, property_name)\n\n    def get_thing(self, thing_id):\n        \"\"\"\n        Return the thing with the given ID.\n\n        thing_id -- ID of the thing this request is for\n        \"\"\"\n        thing = self.get_thing_by_id(thing_id)\n        if thing is None:\n            return self.not_found()\n        else:\n           ", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        if thing.has_property(property_name):\n            self.set_header('Content-Type', 'application/json')\n            self.write(json.dumps({\n                property_name: thing.get_property(property_name),\n            }))\n        else:\n            self.set_status(404)"}
{"prompt": "def plexp_inv(P, xmin, alpha, guess=1.):\n    \"\"\"\n    Inverse CDF for a piecewise PDF as defined in eqn. 3.10\n    of Clauset et al.\n\n    (previous version was incorrect and lead to weird discontinuities in the\n    distribution function)\n    \"\"\"", "output": "\n    if P < 0 or P > 1:\n        raise ValueError(\"P must be in [0, 1]\")\n    if P == 0:\n        return xmin\n    if P == 1:\n        return alpha\n    if P < 1./3:\n        return xmin + (alpha - xmin) * P**(1./3)\n    if P < 2./3:\n        return alpha - (alpha - xmin) * (1 - P)**(1./3)\n    return alpha\n\n\ndef plex", "solution": "\n    def equation(x,prob):\n        return plexp_cdf(x, xmin, alpha)-prob\n    # http://stackoverflow.com/questions/19840425/scipy-optimize-faster-root-finding-over-2d-grid\n    def solver(y, x0=guess):\n        return scipy.optimize.fsolve(equation, guess, args=(y,))\n    f = np.vectorize(solver)\n    return f(P)"}
{"prompt": "def start(self, context):\n\t\t\"\"\"Construct the SQLAlchemy engine and session factory.\"\"\"", "output": "\n\t\tself.engine = sqlalchemy.create_engine(\n\t\t\tself.config.get('database', 'url'),\n\t\t\tpool_size=self.config.getint('database', 'pool_size'),\n\t\t\tmax_overflow=self.config.getint('database', 'max_overflow'),\n\t\t\tpool_recycle=self.config.getint('database', 'pool_recycle'),\n\t\t\tpool_timeout=self.config.getint('database', 'pool_timeout'),\n\t\t\techo=self", "solution": "\n\t\t\n\t\tif __debug__:\n\t\t\tlog.info(\"Connecting SQLAlchemy database layer.\", extra=dict(\n\t\t\t\t\turi = redact_uri(self.uri),\n\t\t\t\t\tconfig = self.config,\n\t\t\t\t\talias = self.alias,\n\t\t\t\t))\n\t\t\n\t\t# Construct the engine.\n\t\tengine = self.engine = create_engine(self.uri, **self.config)\n\t\t\n\t\t# Construct the session factory.\n\t\tself.Session = scoped_session(sessionmaker(bind=engine))\n\t\t\n\t\t# Test the connection.\n\t\tengine.connect().close()\n\t\t\n\t\t# Assign the engine to our database alias.\n\t\tcontext.db[self.alias] = engine"}
{"prompt": "def _matches_filepath_pattern(self, filepath):\n        \"\"\"Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns\"\"\"", "output": "\n        for pattern in self.filepath_patterns:\n            if re.match(pattern, filepath):\n                return True\n        return False\n\n    def _matches_filepath_exclude_pattern(self, filepath):\n        \"\"\"Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns\"\"\"\n        for pattern in self.filepath_exclude_patterns:\n            if re.match(pattern, filepath):\n                return True\n        return False\n\n    def _matches", "solution": "\n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False"}
{"prompt": "def iter_auth_hashes(user, purpose, minutes_valid):\n    \"\"\"\n    Generate auth tokens tied to user and specified purpose.\n\n    The hash expires at midnight on the minute of now + minutes_valid, such\n    that when minutes_valid=1 you get *at least* 1 minute to use the token.\n    \"\"\"", "output": "\n    now = datetime.datetime.utcnow()\n    expires = now + datetime.timedelta(minutes=minutes_valid)\n    expires = expires.replace(minute=0, second=0, microsecond=0)\n    if expires < now:\n        expires = expires + datetime.timedelta(days=1)\n    for i in range(10000):\n        token = hashlib.sha256(\n            str(uuid.uuid4()) + str(now) + str(i) +", "solution": "\n    now = timezone.now().replace(microsecond=0, second=0)\n    for minute in range(minutes_valid + 1):\n        yield hashlib.sha1(\n            '%s:%s:%s:%s:%s' % (\n                now - datetime.timedelta(minutes=minute),\n                user.password,\n                purpose,\n                user.pk,\n                settings.SECRET_KEY,\n            ),\n        ).hexdigest()"}
{"prompt": "def _control_key_down(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is down.\n\n        Parameters:\n        -----------\n        include_command : bool, optional (default True)\n            Whether to treat the Command key as a (mutually exclusive) synonym\n            for Control when in Mac OS.\n        \"\"\"", "output": "\n        if self.platform == 'darwin':\n            if include_command:\n                return modifiers & KeyboardModifiers.COMMAND\n            else:\n                return modifiers & KeyboardModifiers.CONTROL\n        else:\n            return modifiers & KeyboardModifiers.CONTROL\n\n    def _control_key_up(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is up.\n\n        Parameters:\n        -----------\n        include_command :", "solution": "\n        # Note that on Mac OS, ControlModifier corresponds to the Command key\n        # while MetaModifier corresponds to the Control key.\n        if sys.platform == 'darwin':\n            down = include_command and (modifiers & QtCore.Qt.ControlModifier)\n            return bool(down) ^ bool(modifiers & QtCore.Qt.MetaModifier)\n        else:\n            return bool(modifiers & QtCore.Qt.ControlModifier)"}
{"prompt": "def compat_validate_token(cls, *args, **kwargs):\n        \"\"\"Multiple algorithm-compatible token validation.\"\"\"", "output": "\n        try:\n            return super(cls, cls).validate_token(*args, **kwargs)\n        except (jwt.exceptions.InvalidSignatureError,\n                jwt.exceptions.DecodeError):\n            pass\n\n        # Try to decode with all algorithms\n        for algorithm in cls.ALGORITHMS:\n            try:\n                return super(cls, cls).decode_token(\n                    *args,\n                    algorithm=algorithm,\n                    **kwargs\n                )\n            except (jwt.exceptions.InvalidSign", "solution": "\n        data = None\n        for algorithm in SUPPORTED_DIGEST_ALGORITHMS:\n            data = cls(algorithm_name=algorithm).validate_token(\n                *args, **kwargs)\n            if not data:  # move to next algorithm\n                continue\n        return data"}
{"prompt": "def timestamp_YmdHMS(value):\n    \"\"\"Convert timestamp string to time in seconds since epoch.\n\n    Timestamps strings like '20130618120000' are able to be converted by this\n    function.\n\n    Args:\n        value: A timestamp string in the format '%Y%m%d%H%M%S'.\n\n    Returns:\n        The time in seconds since epoch as an integer.\n\n    Raises:\n        ValueError: If timestamp is invalid.\n\n    Note: The timezone is assumed to be UTC/GMT.\n    \"\"\"", "output": ":\n        This function is not used in the code, but is provided for\n        completeness.\n    \"\"\"\n    try:\n        return int(time.mktime(time.strptime(value, '%Y%m%d%H%M%S')))\n    except ValueError:\n        raise ValueError('Invalid timestamp: %s' % value)\n\n\ndef timestamp_YmdHMS_to_datetime(value):\n    \"\"\"Convert timestamp string to datetime.\n\n    Timestamps strings like '2013061812", "solution": "\n    i = int(value)\n    S = i\n    M = S//100\n    H = M//100\n    d = H//100\n    m = d//100\n    Y = m//100\n    return int(calendar.timegm((\n        Y % 10000, m % 100, d % 100, H % 100, M % 100, S % 100, 0, 0, 0)\n    ))"}
{"prompt": "def report_by_type_stats(sect, stats, _):\n    \"\"\"make a report of\n\n    * percentage of different types documented\n    * percentage of different types with a bad name\n    \"\"\"", "output": "\n    if sect.name == \"type\":\n        return\n    if sect.name == \"function\":\n        return\n    if sect.name == \"variable\":\n        return\n    if sect.name == \"constant\":\n        return\n    if sect.name == \"class\":\n        return\n    if sect.name == \"method\":\n        return\n    if sect.name == \"attribute\":\n        return\n    if sect.name == \"module\":\n        return\n    if sect.name == \"exception\":\n        return\n    if sect.name == \"keyword\":\n", "solution": "\n    # percentage of different types documented and/or with a bad name\n    nice_stats = {}\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        try:\n            total = stats[node_type]\n        except KeyError:\n            raise exceptions.EmptyReportError()\n        nice_stats[node_type] = {}\n        if total != 0:\n            try:\n                documented = total - stats[\"undocumented_\" + node_type]\n                percent = (documented * 100.0) / total\n                nice_stats[node_type][\"percent_documented\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_documented\"] = \"NC\"\n            try:\n                percent = (stats[\"badname_\" + node_type] * 100.0) / total\n                nice_stats[node_type][\"percent_badname\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_badname\"] = \"NC\"\n    lines = (\"type\", \"number\", \"old number\", \"difference\", \"%documented\", \"%badname\")\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        new = stats[node_type]\n        lines += (\n            node_type,\n            str(new),\n            \"NC\",\n            \"NC\",\n            nice_stats[node_type].get(\"percent_documented\", \"0\"),\n            nice_stats[node_type].get(\"percent_badname\", \"0\"),\n        )\n    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))"}
{"prompt": "def find_safe(ns_qualified_sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.\"\"\"", "output": "\n        if not isinstance(ns_qualified_sym, sym.Symbol):\n            raise TypeError(f\"{ns_qualified_sym} is not a Symbol\")\n        if ns_qualified_sym.ns is None:\n            raise ValueError(f\"{ns_qualified_sym} is not a qualified symbol\")\n        if ns_qualified_sym.name not in ns_qualified_sym.ns.symbols:\n            raise ValueError(f\"{ns_qualified_sym} is not bound in {ns_qualified_sym.ns", "solution": "\n        v = Var.find(ns_qualified_sym)\n        if v is None:\n            raise RuntimeException(\n                f\"Unable to resolve symbol {ns_qualified_sym} in this context\"\n            )\n        return v"}
{"prompt": "def start(self):\n        \"\"\"Start the logger background synchronization service. This allows you to not need to\n        worry about syncing with ConnectorDB - you just insert into the Logger, and the Logger\n        will by synced every syncperiod.\"\"\"", "output": "\n        self.logger.info(\"Starting Logger\")\n        self.logger.info(\"Starting Logger Sync Service\")\n        self.sync_service = threading.Thread(target=self.sync_loop)\n        self.sync_service.start()\n        self.logger.info(\"Logger Sync Service Started\")\n\n    def stop(self):\n        \"\"\"Stop the logger background synchronization service.\"\"\"\n        self.logger.info(\"Stopping Logger\")\n        self.logger.info(\"Stopping Logger Sync Service\")\n        self.sync_service.", "solution": "\n\n        with self.synclock:\n            if self.syncthread is not None:\n                logging.warn(\n                    \"Logger: Start called on a syncer that is already running\")\n                return\n\n        self.sync()  # Attempt a sync right away\n        self.__setsync()"}
{"prompt": "def make_path_relative(path, rel_to):\n    \"\"\"\n    Make a filename relative, where the filename path, and it is\n    relative to rel_to\n\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/usr/share/another-place/src/Directory')\n        '../../../something/a-file.pth'\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/home/user/src/Directory')\n        '../../../usr/share/something/a-file.pth'\n        >>> make_path_relative('/usr/share/a-file.pth', '/usr/share/')\n        'a-file.pth'\n    \"\"\"", "output": "Directory')\n        '../../../share/something/a-file.pth'\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/usr/share/something/src/Directory')\n        'src/Directory'\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/usr/share/something/src/Directory/')\n        'src/Directory'\n        >>> make_path_relative('/usr/share/something/a-file.", "solution": "\n    path_filename = os.path.basename(path)\n    path = os.path.dirname(path)\n    path = os.path.normpath(os.path.abspath(path))\n    rel_to = os.path.normpath(os.path.abspath(rel_to))\n    path_parts = path.strip(os.path.sep).split(os.path.sep)\n    rel_to_parts = rel_to.strip(os.path.sep).split(os.path.sep)\n    while path_parts and rel_to_parts and path_parts[0] == rel_to_parts[0]:\n        path_parts.pop(0)\n        rel_to_parts.pop(0)\n    full_parts = ['..'] * len(rel_to_parts) + path_parts + [path_filename]\n    if full_parts == ['']:\n        return '.' + os.path.sep\n    return os.path.sep.join(full_parts)"}
{"prompt": "def _rotations_to_disentangle(local_param):\n        \"\"\"\n        Static internal method to work out Ry and Rz rotation angles used\n        to disentangle the LSB qubit.\n        These rotations make up the block diagonal matrix U (i.e. multiplexor)\n        that disentangles the LSB.\n\n        [[Ry(theta_1).Rz(phi_1)  0   .   .   0],\n         [0         Ry(theta_2).Rz(phi_2) .  0],\n                                    .\n                                        .\n          0         0           Ry(theta_2^n).Rz(phi_2^n)]]\n        \"\"\"", "output": "        [.         .         Ry(theta_3).Rz(phi_3) .  0],\n         [.         .         .         Ry(theta_4).Rz(phi_4) .],\n         [0         .         .         .         Ry(theta_5).Rz(phi_5)]]\n\n        :param local_param: The local parameters of the circuit.\n        :type local_param: dict\n        :return: The rotation angles.\n        :rtype: list\n        \"\"\"\n        theta_1 = local_param['theta", "solution": "\n        remaining_vector = []\n        thetas = []\n        phis = []\n\n        param_len = len(local_param)\n\n        for i in range(param_len // 2):\n            # Ry and Rz rotations to move bloch vector from 0 to \"imaginary\"\n            # qubit\n            # (imagine a qubit state signified by the amplitudes at index 2*i\n            # and 2*(i+1), corresponding to the select qubits of the\n            # multiplexor being in state |i>)\n            (remains,\n             add_theta,\n             add_phi) = Initialize._bloch_angles(local_param[2 * i: 2 * (i + 1)])\n\n            remaining_vector.append(remains)\n\n            # rotations for all imaginary qubits of the full vector\n            # to move from where it is to zero, hence the negative sign\n            thetas.append(-add_theta)\n            phis.append(-add_phi)\n\n        return remaining_vector, thetas, phis"}
{"prompt": "def crcMeterRead(self, raw_read, def_buf):\n        \"\"\" Internal read CRC wrapper.\n\n        Args:\n            raw_read (str): Bytes with implicit string cast from serial read\n            def_buf (SerialBlock): Populated read buffer.\n\n        Returns:\n            bool:  True if passed CRC equals calculated CRC.\n        \"\"\"", "output": "\n        # Read the CRC from the buffer.\n        crc_read = raw_read[len(raw_read) - 2:]\n\n        # Calculate the CRC.\n        crc_calc = self.crc.calculate(def_buf.to_bytes())\n\n        # Compare the CRCs.\n        if crc_read == crc_calc:\n            return True\n        else:\n            return False\n\n    def crcMeterWrite(self, raw_write, def_buf):\n        \"\"\" Internal write C", "solution": "\n        try:\n            if len(raw_read) == 0:\n                ekm_log(\"(\" + self.m_context + \") Empty return read.\")\n                return False\n            sent_crc = self.calc_crc16(raw_read[1:-2])\n            logstr = \"(\" + self.m_context + \")CRC sent = \" + str(def_buf[\"crc16\"][MeterData.StringValue])\n            logstr += \" CRC calc = \" + sent_crc\n            ekm_log(logstr)\n            if int(def_buf[\"crc16\"][MeterData.StringValue], 16) == int(sent_crc, 16):\n                return True\n\n        # A cross simple test lines on a USB serial adapter, these occur every\n        # 1000 to 2000 reads, and they show up here as a bad unpack or\n        # a bad crc type call.  In either case, we suppress them a log will\n        # become quite large.  ekmcrc errors come through as type errors.\n        # Failures of int type conversion in 16 bit conversion occur as value\n        # errors.\n        except struct.error:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except TypeError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except ValueError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        return False"}
{"prompt": "def is_bare_exception(self, node):\n        \"\"\"\n        Checks if the node is a bare exception name from an except block.\n\n        \"\"\"", "output": "\n        if isinstance(node, ast.Name):\n            if node.id in self.exception_names:\n                return True\n        return False\n\n    def is_bare_exception_handler(self, node):\n        \"\"\"\n        Checks if the node is a bare exception handler from an except block.\n\n        \"\"\"\n        if isinstance(node, ast.ExceptHandler):\n            if self.is_bare_exception(node.type):\n                return True\n        return False\n\n    def is_bare_exception_handler_with_name(self,", "solution": "\n        return isinstance(node, Name) and node.id in self.current_except_names"}
{"prompt": "def add_gene_links(gene_obj, build=37):\n    \"\"\"Update a gene object with links\n\n    Args:\n        gene_obj(dict)\n        build(int)\n\n    Returns:\n        gene_obj(dict): gene_obj updated with many links\n    \"\"\"", "output": "\n    # Add links to gene\n    gene_obj['links'] = {\n        'self': '/genes/{}'.format(gene_obj['id']),\n        'exons': '/genes/{}/exons'.format(gene_obj['id']),\n        'transcripts': '/genes/{}/transcripts'.format(gene_obj['id']),\n        'variants': '/genes/{}/variants'.format(gene_obj['id']),\n        'similar': '/genes/{}/similar'.format(gene", "solution": "\n    try:\n        build = int(build)\n    except ValueError:\n        build = 37\n    # Add links that use the hgnc_id\n    hgnc_id = gene_obj['hgnc_id']\n\n    gene_obj['hgnc_link'] = genenames(hgnc_id)\n    gene_obj['omim_link'] = omim(hgnc_id)\n    # Add links that use ensembl_id\n    if not 'ensembl_id' in gene_obj:\n        ensembl_id = gene_obj.get('common',{}).get('ensembl_id')\n    else:\n        ensembl_id = gene_obj['ensembl_id']\n    ensembl_37_link = ensembl(ensembl_id, build=37)\n    ensembl_38_link = ensembl(ensembl_id, build=38)\n    gene_obj['ensembl_37_link'] = ensembl_37_link\n    gene_obj['ensembl_38_link'] = ensembl_38_link\n    gene_obj['ensembl_link'] = ensembl_37_link\n    if build == 38:\n        gene_obj['ensembl_link'] = ensembl_38_link\n    gene_obj['hpa_link'] = hpa(ensembl_id)\n    gene_obj['string_link'] = string(ensembl_id)\n    gene_obj['reactome_link'] = reactome(ensembl_id)\n    gene_obj['clingen_link'] = clingen(hgnc_id)\n    gene_obj['expression_atlas_link'] = expression_atlas(ensembl_id)\n    gene_obj['exac_link'] = exac(ensembl_id)\n    # Add links that use entrez_id\n    gene_obj['entrez_link'] = entrez(gene_obj.get('entrez_id'))\n    # Add links that use omim id\n    gene_obj['omim_link'] = omim(gene_obj.get('omim_id'))\n    # Add links that use hgnc_symbol\n    gene_obj['ppaint_link'] = ppaint(gene_obj['hgnc_symbol'])\n    # Add links that use vega_id\n    gene_obj['vega_link'] = vega(gene_obj.get('vega_id'))\n    # Add links that use ucsc link\n    gene_obj['ucsc_link'] = ucsc(gene_obj.get('ucsc_id'))"}
{"prompt": "def cqt_frequencies(n_bins, fmin, bins_per_octave=12, tuning=0.0):\n    \"\"\"Compute the center frequencies of Constant-Q bins.\n\n    Examples\n    --------\n    >>> # Get the CQT frequencies for 24 notes, starting at C2\n    >>> librosa.cqt_frequencies(24, fmin=librosa.note_to_hz('C2'))\n    array([  65.406,   69.296,   73.416,   77.782,   82.407,   87.307,\n             92.499,   97.999,  103.826,  110.   ,  116.541,  123.471,\n            130.813,  138.591,  146.832,  155.563,  164.814,  174.614,\n            184.997,  195.998,  207.652,  220.   ,  233.082,  246.942])\n\n    Parameters\n    ----------\n    n_bins  : int > 0 [scalar]\n        Number of constant-Q bins\n\n    fmin    : float > 0 [scalar]\n        Minimum frequency\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)`\n        Deviation from A440 tuning in fractional bins (cents)\n\n    Returns\n    -------\n    frequencies : np.ndarray [shape=(n_bins,)]\n        Center frequency for each CQT bin\n    \"\"\"", "output": ".296,   73.416,   77.782,   82.407,   87.307,\n             92.499,   97.999,  103.826,  109.855,  116.541,  123.471,\n            130.813,  138.591,  146.832,  155.566,  ", "solution": "\n\n    correction = 2.0**(float(tuning) / bins_per_octave)\n    frequencies = 2.0**(np.arange(0, n_bins, dtype=float) / bins_per_octave)\n\n    return correction * fmin * frequencies"}
{"prompt": "def export_keying_material(self, label, olen, context=None):\n        \"\"\"\n        Obtain keying material for application use.\n\n        :param: label - a disambiguating label string as described in RFC 5705\n        :param: olen - the length of the exported key material in bytes\n        :param: context - a per-association context value\n        :return: the exported key material bytes or None\n        \"\"\"", "output": "\n        raise NotImplementedError\n\n    def export_keying_material_early(self, label, olen, context=None):\n        \"\"\"\n        Obtain keying material for application use.\n\n        :param: label - a disambiguating label string as described in RFC 5705\n        :param: olen - the length of the exported key material in bytes\n        :param: context - a per-association context value\n        :return: the exported key material bytes or None\n        \"\"\"\n        raise NotImplementedError\n", "solution": "\n        outp = _no_zero_allocator(\"unsigned char[]\", olen)\n        context_buf = _ffi.NULL\n        context_len = 0\n        use_context = 0\n        if context is not None:\n            context_buf = context\n            context_len = len(context)\n            use_context = 1\n        success = _lib.SSL_export_keying_material(self._ssl, outp, olen,\n                                                  label, len(label),\n                                                  context_buf, context_len,\n                                                  use_context)\n        _openssl_assert(success == 1)\n        return _ffi.buffer(outp, olen)[:]"}
{"prompt": "def register_function(scope=None, as_property=False, name=None):\n    \"\"\"Decorator to register a new function with vaex.\n\n    Example:\n\n    >>> import vaex\n    >>> df = vaex.example()\n    >>> @vaex.register_function()\n    >>> def invert(x):\n    >>>     return 1/x\n    >>> df.x.invert()\n\n\n    >>> import numpy as np\n    >>> df = vaex.from_arrays(departure=np.arange('2015-01-01', '2015-12-05', dtype='datetime64'))\n    >>> @vaex.register_function(as_property=True, scope='dt')\n    >>> def dt_relative_day(x):\n    >>>     return vaex.functions.dt_dayofyear(x)/365.\n    >>> df.departure.dt.relative_day\n    \"\"\"", "output": "16-01-01', '2016-01-02', dtype='datetime64[D]'))\n    >>> @vaex.register_function(scope='df')\n    >>> def day_of_week(departure):\n    >>>     return departure.dayofweek()\n    >>> df.day_of_week()\n\n    >>> @vaex.register_function(scope='df')\n    >>> def day_of_week(departure):\n    >>>     return departure.dayofweek()\n    >>> df.day", "solution": "\n    prefix = ''\n    if scope:\n        prefix = scope + \"_\"\n        if scope not in scopes:\n            raise KeyError(\"unknown scope\")\n    def wrapper(f, name=name):\n        name = name or f.__name__\n        # remove possible prefix\n        if name.startswith(prefix):\n            name = name[len(prefix):]\n        full_name = prefix + name\n        if scope:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.expression.ds.func, full_name)\n                    args = (self.expression, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            if as_property:\n                setattr(scopes[scope], name, property(closure()))\n            else:\n                setattr(scopes[scope], name, closure())\n        else:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.ds.func, full_name)\n                    args = (self, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            setattr(vaex.expression.Expression, name, closure())\n        vaex.expression.expression_namespace[prefix + name] = f\n        return f  # we leave the original function as is\n    return wrapper"}
{"prompt": "def iter_hierarchy(self, ontology, size=None, sleep=None):\n        \"\"\"Iterates over parent-child relations\n\n        :param str ontology: The name of the ontology\n        :param int size: The size of each page. Defaults to 500, which is the maximum allowed by the EBI.\n        :param int sleep: The amount of time to sleep between pages. Defaults to 0 seconds.\n        :rtype: iter[tuple[str,str]]\n        \"\"\"", "output": "\n        if size is None:\n            size = 500\n        if sleep is None:\n            sleep = 0\n        for page in self.iter_pages(ontology, size=size, sleep=sleep):\n            for child in page:\n                yield (child['parent'], child['child'])\n\n    def iter_pages(self, ontology, size=None, sleep=None):\n        \"\"\"Iterates over pages of parent-child relations\n\n        :param str ontology: The name of the ontology\n        :param int size: The size of each", "solution": "\n        for term in self.iter_terms(ontology=ontology, size=size, sleep=sleep):\n            try:\n                hierarchy_children_link = term['_links'][HIERARCHICAL_CHILDREN]['href']\n            except KeyError:  # there's no children for this one\n                continue\n\n            response = requests.get(hierarchy_children_link).json()\n\n            for child_term in response['_embedded']['terms']:\n                yield term['label'], child_term['label']"}
{"prompt": "def closeAllSessions(self, slot):\n        \"\"\"\n        C_CloseAllSessions\n\n        :param slot: slot number\n        :type slot: integer\n        \"\"\"", "output": "\n        self.checkSlotAvailability(slot)\n        self.checkSessionAvailability(slot)\n        self.checkSessionStatus(slot, self.SESSION_ACTIVE)\n        self.checkSessionStatus(slot, self.SESSION_MODIFIED)\n        self.checkSessionStatus(slot, self.SESSION_READ_ONLY)\n        self.checkSessionStatus(slot, self.SESSION_READ_WRITE)\n        self.checkSessionStatus(slot, self.SESSION_RW_PUBLIC)\n       ", "solution": "\n        rv = self.lib.C_CloseAllSessions(slot)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def load_panel_app(adapter, panel_id=None, institute='cust000'):\n    \"\"\"Load PanelApp panels into scout database\n    \n    If no panel_id load all PanelApp panels \n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        panel_id(str): The panel app panel id\n    \"\"\"", "output": "\n    # Load all panel app panels\n    if panel_id is None:\n        logger.info(\"Loading all panel app panels\")\n        panel_ids = adapter.panel_app_panels()\n        logger.info(\"Found {} panel app panels\".format(len(panel_ids)))\n    else:\n        logger.info(\"Loading panel app panel {}\".format(panel_id))\n        panel_ids = [panel_id]\n\n    for panel_id in panel_ids:\n        logger.info(\"Loading panel app panel {}\".format(panel_", "solution": "\n    base_url = 'https://panelapp.genomicsengland.co.uk/WebServices/{0}/'\n    \n    hgnc_map = adapter.genes_by_alias()\n    \n    if panel_id:\n        panel_ids = [panel_id]\n\n    if not panel_id:\n        \n        LOG.info(\"Fetching all panel app panels\")\n        data = get_request(base_url.format('list_panels'))\n    \n        json_lines = json.loads(data)\n        \n        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]\n    \n    for panel_id in panel_ids:\n        panel_data = get_request(base_url.format('get_panel') + panel_id)\n        \n        parsed_panel = parse_panel_app_panel(\n            panel_info = json.loads(panel_data)['result'], \n            hgnc_map=hgnc_map,\n            institute=institute\n        )\n        parsed_panel['panel_id'] = panel_id\n        \n        if len(parsed_panel['genes']) == 0:\n            LOG.warning(\"Panel {} is missing genes. Skipping.\".format(parsed_panel['display_name']))\n            continue\n        \n        try:\n            adapter.load_panel(parsed_panel=parsed_panel)\n        except Exception as err:\n            raise err"}
{"prompt": "def reads_py(s, **kwargs):\n    \"\"\"Read a .py notebook from a string and return the NotebookNode object.\"\"\"", "output": "\n    from nbformat.reader import reads\n    return reads(s, **kwargs)\n\n\ndef reads_json(s, **kwargs):\n    \"\"\"Read a .ipynb notebook from a string and return the NotebookNode object.\"\"\"\n    from nbformat.reader import reads_json\n    return reads_json(s, **kwargs)\n\n\ndef reads_nbjson(s, **kwargs):\n    \"\"\"Read a .ipynb notebook from a string and return the NotebookNode object.\"\"\"\n    from nbformat.reader import reads_", "solution": "\n    nbf, nbm, s = parse_py(s, **kwargs)\n    if nbf == 2:\n        nb = v2.to_notebook_py(s, **kwargs)\n    elif nbf == 3:\n        nb = v3.to_notebook_py(s, **kwargs)\n    else:\n        raise NBFormatError('Unsupported PY nbformat version: %i' % nbf)\n    return nb"}
{"prompt": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"", "output": "\n    if not os.path.exists(path):\n        os.makedirs(path, mode)\n\n\ndef mkdir(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    if not os", "solution": "\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)"}
{"prompt": "def _resolve_graph(self, distribution_names=None, leaf_name='x'):\n    \"\"\"Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        appropriate `str`.)\n      leaf_name: `str` used when no maker depends on a particular\n        `model` element.\n\n    Returns:\n      graph: `tuple` of `(str tuple)` pairs representing the name of each\n        distribution (maker) and the names of its dependencies.\n\n    #### Example\n\n    ```python\n    d = tfd.JointDistributionSequential([\n                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),\n        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),\n                     tfd.Normal(loc=0, scale=2.),\n        lambda n, g: tfd.Normal(loc=n, scale=g),\n    ])\n    d._resolve_graph()\n    # ==> (\n    #       ('e', ()),\n    #       ('g', ('e',)),\n    #       ('n', ()),\n    #       ('x', ('n', 'g')),\n    #     )\n    ```\n\n    \"\"\"", "output": "leaf_name: `str` name of the leaf distribution.\n\n    Returns:\n      `tuple` of `tuple`s of dependencies.\n    \"\"\"\n    if distribution_names is None:\n      distribution_names = [\n          '{}'.format(i) for i in range(len(self.model))]\n\n    if len(distribution_names) != len(self.model):\n      raise ValueError(\n          '`distribution_names` must have the same length as `model`.')\n\n    def _resolve_graph_helper(\n        distribution_names,", "solution": "\n    # This function additionally depends on:\n    #   self._dist_fn_args\n    #   self._dist_fn_wrapped\n    # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,\n    # ignore args prefixed with `_`.\n    if distribution_names is None or any(self._dist_fn_args):\n      distribution_names = _resolve_distribution_names(\n          self._dist_fn_args, distribution_names, leaf_name)\n    if len(set(distribution_names)) != len(distribution_names):\n      raise ValueError('Distribution names must be unique: {}'.format(\n          distribution_names))\n    if len(distribution_names) != len(self._dist_fn_wrapped):\n      raise ValueError('Distribution names must be 1:1 with `rvs`.')\n    return tuple(zip(distribution_names,\n                     tuple(() if a is None else a for a in self._dist_fn_args)))"}
{"prompt": "def cli_encrypt(context, key):\n    \"\"\"\n    Encrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    This can be useful to encrypt to disk before attempting to\n    upload, allowing uploads retries and segmented encrypted objects.\n\n    See :py:mod:`swiftly.cli.encrypt` for context usage information.\n\n    See :py:class:`CLIEncrypt` for more information.\n    \"\"\"", "output": "\n    CLIEncrypt(context).encrypt(key)\n\n\ndef cli_decrypt(context, key):\n    \"\"\"\n    Decrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    See :py:mod:`swiftly.cli.decrypt` for context usage information.\n\n    See :py:class:`CLIDecrypt` for more information.\n    \"\"\"\n    CLIDecrypt(context).decrypt(key)\n\n\ndef cli_encrypt_file(context,", "solution": "\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            for chunk in aes_encrypt(key, stdin, preamble=AES256CBC):\n                stdout.write(chunk)\n            stdout.flush()"}
{"prompt": "def _confirm_exit(self):\n        \"\"\"confirm shutdown on ^C\n        \n        A second ^C, or answering 'y' within 5s will cause shutdown,\n        otherwise original SIGINT handler will be restored.\n        \n        This doesn't work on Windows.\n        \"\"\"", "output": "\n        if os.name == 'nt':\n            return\n        try:\n            sys.exit(0)\n        except SystemExit:\n            pass\n        print('^C again to exit')\n        self.set_raw_mode()\n        try:\n            rlist, _, _ = select.select([sys.stdin], [], [], 5)\n        except select.error:\n            pass\n        else:\n            if rlist:\n                answer = sys.stdin.readline().strip()\n                if answer.lower() in ('y', 'yes'):", "solution": "\n        # FIXME: remove this delay when pyzmq dependency is >= 2.1.11\n        time.sleep(0.1)\n        sys.stdout.write(\"Shutdown Notebook Server (y/[n])? \")\n        sys.stdout.flush()\n        r,w,x = select.select([sys.stdin], [], [], 5)\n        if r:\n            line = sys.stdin.readline()\n            if line.lower().startswith('y'):\n                self.log.critical(\"Shutdown confirmed\")\n                ioloop.IOLoop.instance().stop()\n                return\n        else:\n            print \"No answer for 5s:\",\n        print \"resuming operation...\"\n        # no answer, or answer is no:\n        # set it back to original SIGINT handler\n        # use IOLoop.add_callback because signal.signal must be called\n        # from main thread\n        ioloop.IOLoop.instance().add_callback(self._restore_sigint_handler)"}
{"prompt": "def send_raw(self, stream, msg_list, flags=0, copy=True, ident=None):\n        \"\"\"Send a raw message via ident path.\n\n        This method is used to send a already serialized message.\n\n        Parameters\n        ----------\n        stream : ZMQStream or Socket\n            The ZMQ stream or socket to use for sending the message.\n        msg_list : list\n            The serialized list of messages to send. This only includes the\n            [p_header,p_parent,p_content,buffer1,buffer2,...] portion of\n            the message.\n        ident : ident or list\n            A single ident or a list of idents to use in sending.\n        \"\"\"", "output": ",...] part of the\n            message.\n        flags : int\n            The flags to use for the send.\n        copy : bool\n            Whether to copy the message data before sending.\n        ident : bytes\n            The identity to use for the send.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if copy:\n            msg_list = list(msg_list)\n            for i in range(len(msg_list)):\n                msg_list[i] = msg_list[i].copy()\n\n        if ident is None:\n            stream", "solution": "\n        to_send = []\n        if isinstance(ident, bytes):\n            ident = [ident]\n        if ident is not None:\n            to_send.extend(ident)\n\n        to_send.append(DELIM)\n        to_send.append(self.sign(msg_list))\n        to_send.extend(msg_list)\n        stream.send_multipart(msg_list, flags, copy=copy)"}
{"prompt": "def resolve_expression(self, *args, **kwargs) -> HStoreColumn:\n        \"\"\"Resolves the expression into a :see:HStoreColumn expression.\"\"\"", "output": "\n        return HStoreColumn(self.expression)\n\n    def __str__(self) -> str:\n        \"\"\"Returns the string representation of the expression.\"\"\"\n        return str(self.expression)\n\n    def __repr__(self) -> str:\n        \"\"\"Returns the representation of the expression.\"\"\"\n        return f\"HStoreColumn({self.expression!r})\"\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"Returns whether the expression is equal to the other.\"\"\"\n        return isinstance(other, HStoreColumn", "solution": "\n\n        original_expression = super().resolve_expression(*args, **kwargs)\n        expression = HStoreColumn(\n            original_expression.alias,\n            original_expression.target,\n            self.key\n        )\n        return expression"}
{"prompt": "def insert_default_options():\n    \"\"\"insert default options to sys.argv\n    \"\"\"", "output": "\n    import sys\n    import os\n    import re\n    from optparse import OptionParser\n    from optparse import OptionGroup\n    from optparse import Option\n    from optparse import OptionValueError\n    from optparse import SUPPRESS_HELP\n    from optparse import SUPPRESS_USAGE\n    from optparse import AmbiguousOptionError\n    from optparse import OptionError\n    from optparse import OptionValueError\n    from optparse import OptionConflictError\n    from optparse import OptionValueError\n    from optparse import OptionValueError\n", "solution": "\n    options = get_default_options()\n    options.reverse()\n    for arg in options:\n        sys.argv.insert(1, arg)"}
{"prompt": "def get_components(self):\n        \"\"\"\n        Returns the components of this name, as a sequence of 2-tuples.\n\n        :return: The components of this name.\n        :rtype: :py:class:`list` of ``name, value`` tuples.\n        \"\"\"", "output": "\n        return self._components\n\n    def get_component(self, name):\n        \"\"\"\n        Returns the value of the component with the given name.\n\n        :param name: The name of the component to return.\n        :type name: :py:class:`str`\n        :return: The value of the component with the given name.\n        :rtype: :py:class:`str`\n        \"\"\"\n        for component in self._components:\n            if component[0] == name:\n                return component[1]\n        return None\n\n    def get", "solution": "\n        result = []\n        for i in range(_lib.X509_NAME_entry_count(self._name)):\n            ent = _lib.X509_NAME_get_entry(self._name, i)\n\n            fname = _lib.X509_NAME_ENTRY_get_object(ent)\n            fval = _lib.X509_NAME_ENTRY_get_data(ent)\n\n            nid = _lib.OBJ_obj2nid(fname)\n            name = _lib.OBJ_nid2sn(nid)\n\n            # ffi.string does not handle strings containing NULL bytes\n            # (which may have been generated by old, broken software)\n            value = _ffi.buffer(_lib.ASN1_STRING_data(fval),\n                                _lib.ASN1_STRING_length(fval))[:]\n            result.append((_ffi.string(name), value))\n\n        return result"}
{"prompt": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the pages\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"", "output": "\n        if category == 'all':\n            category = None\n        if category is None:\n            return self._fetch_all_items(**kwargs)\n        else:\n            return self._fetch_category_items(category, **kwargs)\n\n    def _fetch_all_items(self, **kwargs):\n        \"\"\"Fetch all items\n\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        for page in self._fetch_pages(**kwargs):\n            for item in page['items']:\n                yield item\n", "solution": "\n        from_date = kwargs['from_date']\n        reviews_api = kwargs['reviews_api']\n\n        mediawiki_version = self.client.get_version()\n        logger.info(\"MediaWiki version: %s\", mediawiki_version)\n\n        if reviews_api:\n            if ((mediawiki_version[0] == 1 and mediawiki_version[1] >= 27) or mediawiki_version[0] > 1):\n                fetcher = self.__fetch_1_27(from_date)\n            else:\n                logger.warning(\"Reviews API only available in MediaWiki >= 1.27\")\n                logger.warning(\"Using the Pages API instead\")\n                fetcher = self.__fetch_pre1_27(from_date)\n        else:\n            fetcher = self.__fetch_pre1_27(from_date)\n\n        for page_reviews in fetcher:\n            yield page_reviews"}
{"prompt": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"", "output": "\n    session = Session()\n    try:\n        yield session\n    finally:\n        session.close()\n\n\ndef get_session():\n    \"\"\"\n    Get a session from the current thread.\n    \"\"\"\n    session = getattr(thread_local, 'session', None)\n    if session is None:\n        session = thread_local.session = Session()\n    return session\n\n\ndef get_db_session():\n    \"\"\"\n    Get a session from the current thread.\n    \"\"\"\n    session = getattr(thread_local, 'db_session", "solution": "\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()"}
{"prompt": "def disable_report(self, reportid):\n        \"\"\"disable the report of the given id\"\"\"", "output": "\n        return self.request('disable_report', reportid=reportid)\n\n    def enable_report(self, reportid):\n        \"\"\"enable the report of the given id\"\"\"\n        return self.request('enable_report', reportid=reportid)\n\n    def get_report(self, reportid):\n        \"\"\"get the report of the given id\"\"\"\n        return self.request('get_report', reportid=reportid)\n\n    def get_report_list(self, report_type=None, report_status=None,\n                       ", "solution": "\n        reportid = reportid.upper()\n        self._reports_state[reportid] = False"}
{"prompt": "def chrono(ctx, app_id, sentence_file,\n           json_flag, sentence, doc_time, request_id):\n    # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None  # NOQA\n    \"\"\"Extract expression expressing date and time and normalize its value \"\"\"", "output": "\n    # TODO: add support for other languages\n    # TODO: add support for other time expressions\n    # TODO: add support for other date expressions\n    # TODO: add support for other timezones\n    # TODO: add support for other languages\n    # TODO: add support for other languages\n    # TODO: add support for other languages\n    # TODO: add support for other languages\n    # TODO: add support for other languages\n    # TODO: add support for other languages\n    # TODO: add support for other languages\n    # TODO: add support for other languages\n    # TODO", "solution": "\n\n    app_id = clean_app_id(app_id)\n    sentence = clean_sentence(sentence, sentence_file)\n\n    api = GoolabsAPI(app_id)\n    ret = api.chrono(\n        sentence=sentence,\n        doc_time=doc_time,\n        request_id=request_id,\n    )\n\n    if json_flag:\n        click.echo(format_json(api.response.json()))\n        return\n\n    for pair in ret['datetime_list']:\n        click.echo(u'{0}: {1}'.format(text(pair[0]), pair[1]))"}
{"prompt": "def _get_unpacking_extra_info(node, infered):\n    \"\"\"return extra information to add to the message for unpacking-non-sequence\n    and unbalanced-tuple-unpacking errors\n    \"\"\"", "output": "\n    if isinstance(node, ast.Tuple):\n        return \" (%s)\" % \", \".join(\n            \"%s=%s\" % (name, infered[name]) for name in node.elts\n        )\n    return \"\"\n\n\ndef _get_unpacking_extra_info_for_unbalanced_tuple_unpacking(node, infered):\n    \"\"\"return extra information to add to the message for unbalanced-tuple-unpacking\n    errors\n    \"\"\"\n    if isinstance(node, ast.Tuple):", "solution": "\n    more = \"\"\n    infered_module = infered.root().name\n    if node.root().name == infered_module:\n        if node.lineno == infered.lineno:\n            more = \" %s\" % infered.as_string()\n        elif infered.lineno:\n            more = \" defined at line %s\" % infered.lineno\n    elif infered.lineno:\n        more = \" defined at line %s of %s\" % (infered.lineno, infered_module)\n    return more"}
{"prompt": "def godot_options(self, info):\n        \"\"\" Handles display of the options menu. \"\"\"", "output": "\n        self.options_menu.show()\n        self.options_menu.set_title(\"Options\")\n        self.options_menu.set_description(\"\")\n        self.options_menu.set_icon(info[\"icon\"])\n        self.options_menu.set_position(info[\"position\"])\n        self.options_menu.set_size(info[\"size\"])\n        self.options_menu.set_color(info[\"color\"])\n        self.options_menu.set_background_color(info[\"background_color\"])\n        self.options_menu", "solution": "\n\n        if info.initialized:\n            self.edit_traits( parent = info.ui.control,\n                              kind   = \"livemodal\",\n                              view   = \"options_view\" )"}
{"prompt": "def getMappingsOnDeviceType(self, typeId, draft=False):\n        \"\"\"\n        Get all the mappings for a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - draft (boolean) - draft or active\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        return self._getMappings(typeId, draft)\n\n    def getMappingsOnDevice(self, deviceId, draft=False):\n        \"\"\"\n        Get all the mappings for a device.\n        Parameters:\n            - deviceId (string) - the device id\n            - draft (boolean) - draft or active\n        Throws APIException on failure.\n        \"\"\"\n        return self._getMappings(deviceId, draft)\n\n    def getMappingsOnDeviceTypeAndDevice(self, typeId, deviceId, draft=False):\n       ", "solution": "\n        if draft:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"/draft\", typeId)\n        else:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"\", typeId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"All device type mappings retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting all device type mappings\", resp)\n        return resp.json()"}
{"prompt": "def video_list(request, username=None):\n    \"\"\"\n    list of videos of a user\n    if username does not set, shows the currently logged in user\n    \"\"\"", "output": "\n    if username:\n        user = get_object_or_404(User, username=username)\n    else:\n        user = request.user\n\n    videos = Video.objects.filter(user=user)\n\n    return render(request, 'video_list.html', {'videos': videos})\n\n\ndef video_detail(request, username, video_id):\n    \"\"\"\n    show details of a video\n    \"\"\"\n    video = get_object_or_404(Video, id=video_id)\n    return render(", "solution": "\n\n    # If user is not authenticated and username is None, raise an error\n    if username is None and not request.user.is_authenticated():\n        from django.http import Http404\n        raise Http404\n\n    from django.contrib.auth.models import User\n    user = User.objects.get(username=username) if username else request.user\n\n    # loop through the videos of the user\n    videos = Video.objects.filter(user=user).all()\n    video_params = []\n    for video in videos:\n        video_params.append(_video_params(request, video.video_id))\n\n    return render_to_response(\n        \"django_youtube/videos.html\",\n        {\"video_params\": video_params},\n        context_instance=RequestContext(request)\n    )"}
{"prompt": "def weekday(year_or_num, month=None, day=None, full=False):\n    \"\"\"Simple tag - returns the weekday of the given (year, month, day) or of given (weekday_number).\n\n    Usage (in template):\n\n    {% weekday 2014 3 3 %}\n\n    Result: Mon\n\n    Return abbreviation by default. To return full name: pass full=True\n\n    {% weekday 2014 3 3 full=True %}\n\n    Result: Monday\n\n\n    When only number of weekday is given then 0 is considered as \"Monday\"\n\n    {% weekday 0 full=True %}\n\n    Result: Monday\n\n    \"\"\"", "output": "   \"\"\"\n    if month is None and day is None:\n        # weekday_number\n        weekday_number = year_or_num\n        if full:\n            return calendar.day_name[weekday_number]\n        else:\n            return calendar.day_abbr[weekday_number]\n    else:\n        # year, month, day\n        year = year_or_num\n        month = month or 1\n        day = day or 1\n        return calendar.day_name[datetime.date(year, month, day).weekday", "solution": "\n    if any([month, day]) and not all([month, day]):\n        raise TemplateSyntaxError(\"weekday accepts 1 or 3 arguments plus optional 'full' argument\")\n\n    try:\n        if all([year_or_num, month, day]):\n            weekday_num = date(*map(int, (year_or_num, month, day))).weekday()\n        else:\n            weekday_num = year_or_num\n        if full:\n            return WEEKDAYS[weekday_num]\n        else:\n            return WEEKDAYS_ABBR[weekday_num]\n    except Exception:\n        return"}
{"prompt": "def clinvar_submissions(store, user_id, institute_id):\n    \"\"\"Get all Clinvar submissions for a user and an institute\"\"\"", "output": "\n    query = \"\"\"\n        query MyQuery {\n            clinvar_submissions(user_id: \"%s\", institute_id: \"%s\") {\n                id\n                clinvar_id\n                clinvar_submission_id\n                clinvar_submission_version\n                clinvar_submission_date\n                clinvar_submission_status\n                clinvar_submission_type\n                clinvar_submission_title\n                clinvar_submission_authors\n                clinvar_submission_abstract\n                clinvar_submission_journal", "solution": "\n    submissions = list(store.clinvar_submissions(user_id, institute_id))\n    return submissions"}
{"prompt": "def get_table(self, arch, pattern, colored=False, verbose=False):\n        \"\"\"\n        This function is used in sys command (when user want to find a specific syscall)\n\n        :param Architecture for syscall table;\n        :param Searching pattern;\n        :param Flag for verbose output\n        :return Return a printable table of matched syscalls\n        \"\"\"", "output": "\n        if arch == \"x86\":\n            table = self.x86_table\n        elif arch == \"x64\":\n            table = self.x64_table\n        else:\n            raise Exception(\"Unsupported architecture\")\n\n        if colored:\n            print(colored(\"Syscall Table for %s\" % arch, \"green\"))\n        else:\n            print(\"Syscall Table for %s\" % arch)\n\n        if pattern is None:\n            print(table)\n        else:\n            for syscall in table", "solution": "\n\n        rawtable = self.search(arch, pattern)\n        if len(rawtable) == 0:\n            return None\n\n        used_hd = self.__fetch_used_headers(rawtable, verbose)\n        table   = [self.__make_colored_row(used_hd, 'yellow,bold', upper=True) if colored else used_hd]\n\n        for command in rawtable:\n            cur_tb_field = []\n            for hd in used_hd:\n                value = command[hd]\n                cur_tb_field.append(self.__make_colored_field(value, hd, verbose=verbose))\n            table.append(cur_tb_field)\n        return DoubleTable(table)"}
{"prompt": "def str_variant(store, institute_id, case_name, variant_id):\n    \"\"\"Pre-process an STR variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }\n        \"\"\"", "output": "': <variant_obj>,\n            'genes': <gene_objs>,\n            'transcripts': <transcript_objs>,\n            'hgnc_symbols': <hgnc_symbols>,\n            'clinvar_variants': <clinvar_variants>,\n            'cosmic_variants': <cosmic_variants>,\n            'transcript_consequences': <transcript_consequences>,\n            'transcript_consequence_terms': <transcript_consequence_terms>,\n            '", "solution": "\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj =  store.variant(variant_id)\n\n    # fill in information for pilup view\n    variant_case(store, case_obj, variant_obj)\n\n    variant_obj['callers'] = callers(variant_obj, category='str')\n\n    # variant_obj['str_ru']\n    # variant_obj['str_repid']\n    # variant_obj['str_ref']\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    return {\n        'institute': institute_obj,\n        'case': case_obj,\n        'variant': variant_obj,\n        'overlapping_snvs': overlapping_snvs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n    }"}
{"prompt": "def index(context, update):\n    \"\"\"Create indexes for the database\"\"\"", "output": "\n    context.execute(\"CREATE INDEX IF NOT EXISTS idx_user_id ON user (id)\")\n    context.execute(\"CREATE INDEX IF NOT EXISTS idx_user_name ON user (name)\")\n    context.execute(\"CREATE INDEX IF NOT EXISTS idx_user_email ON user (email)\")\n    context.execute(\"CREATE INDEX IF NOT EXISTS idx_user_role ON user (role)\")\n    context.execute(\"CREATE INDEX IF NOT EXISTS idx_user_active ON user (active)\")\n    context.execute(\"CREATE INDEX IF NOT EXISTS idx_user_created_", "solution": "\n    LOG.info(\"Running scout index\")\n    adapter = context.obj['adapter']\n    \n    if update:\n        adapter.update_indexes()\n    else:\n        adapter.load_indexes()"}
{"prompt": "def remove_pid_file(self):\n        \"\"\"Remove the pid file.\n\n        This should be called at shutdown by registering a callback with\n        :func:`reactor.addSystemEventTrigger`. This needs to return\n        ``None``.\n        \"\"\"", "output": "\n        if self.pid_file:\n            try:\n                os.remove(self.pid_file)\n            except OSError:\n                pass\n\n    def _get_pid_file(self):\n        \"\"\"Get the pid file.\n\n        This is a property that returns the pid file.\n        \"\"\"\n        return self._pid_file\n\n    def _set_pid_file(self, pid_file):\n        \"\"\"Set the pid file.\n\n        This is a property that sets the pid file.\n        \"\"\"\n        self._pid_file =", "solution": "\n        pid_file = os.path.join(self.profile_dir.pid_dir, self.name + u'.pid')\n        if os.path.isfile(pid_file):\n            try:\n                self.log.info(\"Removing pid file: %s\" % pid_file)\n                os.remove(pid_file)\n            except:\n                self.log.warn(\"Error removing the pid file: %s\" % pid_file)"}
{"prompt": "def getoutput_pexpect(self, cmd):\n        \"\"\"Run a command and return its stdout/stderr as a string.\n\n        Parameters\n        ----------\n        cmd : str\n          A command to be executed in the system shell.\n\n        Returns\n        -------\n        output : str\n          A string containing the combination of stdout and stderr from the\n        subprocess, in whatever order the subprocess originally wrote to its\n        file descriptors (so the order of the information in this string is the\n        correct order as would be seen if running the command in a terminal).\n        \"\"\"", "output": "in a shell).\n\n        \"\"\"\n        if self.logfile is None:\n            self.logfile = open(os.devnull, 'w')\n        self.logfile.flush()\n        self.child.sendline(cmd)\n        self.child.expect(self.prompt)\n        self.child.sendline('')\n        self.child.expect(self.prompt)\n        self.child.sendline('')\n        self.child.expect(self.prompt)\n        self.child.sendline('')\n        self.", "solution": "\n        try:\n            return pexpect.run(self.sh, args=['-c', cmd]).replace('\\r\\n', '\\n')\n        except KeyboardInterrupt:\n            print('^C', file=sys.stderr, end='')"}
{"prompt": "def get_signature(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"", "output": "\n        return self.signatures[value]\n\n    def get_signature_for_value(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"\n        return self.signatures[value]\n\n    def get_signature_for_name(self, name):\n        \"\"\"Returns the signature for the given name\"\"\"\n        return self.signatures[self.names[name]]\n\n    def get_signature_for_type(self, type):\n        \"\"\"Returns the signature for the given type\"\"\"\n        return self", "solution": "\n        value = want_bytes(value)\n        key = self.derive_key()\n        sig = self.algorithm.get_signature(key, value)\n        return base64_encode(sig)"}
{"prompt": "def generate_config_file(self):\n        \"\"\"generate default config file from Configurables\"\"\"", "output": "\n        config_file = open(self.config_file, 'w')\n        config_file.write(\"[Configurables]\\n\")\n        for key, value in self.configurables.items():\n            config_file.write(key + \"=\" + str(value) + \"\\n\")\n        config_file.close()\n\n    def load_config_file(self):\n        \"\"\"load config file to Configurables\"\"\"\n        config_file = open(self.config_file, 'r')\n        for line in config_file:\n           ", "solution": "\n        lines = [\"# Configuration file for %s.\"%self.name]\n        lines.append('')\n        lines.append('c = get_config()')\n        lines.append('')\n        for cls in self.classes:\n            lines.append(cls.class_config_section())\n        return '\\n'.join(lines)"}
{"prompt": "def _republish_displaypub(self, content, eid):\n        \"\"\"republish individual displaypub content dicts\"\"\"", "output": "\n        if not content:\n            return\n        if not isinstance(content, list):\n            content = [content]\n        for c in content:\n            if c['type'] == 'displaypub':\n                self.republish_displaypub(c, eid)\n\n    def _republish_displaypub_content(self, content, eid):\n        \"\"\"republish individual displaypub content dicts\"\"\"\n        if not content:\n            return\n        if not isinstance(content, list):\n            content = [content]\n        for c", "solution": "\n        try:\n            ip = get_ipython()\n        except NameError:\n            # displaypub is meaningless outside IPython\n            return\n        md = content['metadata'] or {}\n        md['engine'] = eid\n        ip.display_pub.publish(content['source'], content['data'], md)"}
{"prompt": "def __make_points_for_label(self, ts, data, label, prefix, gun_stats):\n        \"\"\"x\n        Make a set of points for `this` label\n\n        overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        \"\"\"", "output": "\n        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        # overall_quantiles, overall_meta, net", "solution": "\n        label_points = list()\n\n        label_points.extend(\n            (\n                # overall quantiles for label\n                self.__make_points(\n                    prefix + \"overall_quantiles\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_quantile_fields(data)\n                ),\n                # overall meta (gun status) for label\n                self.__make_points(\n                    prefix + \"overall_meta\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_overall_meta_fields(data, gun_stats)\n                ),\n                # net codes for label\n                self.__make_points(\n                    prefix + \"net_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_netcodes_fields(data)\n                ),\n                # proto codes for label\n                self.__make_points(\n                    prefix + \"proto_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_protocodes_fields(data)\n                )\n            )\n        )\n        # histograms, one row for each bin\n        if self.histograms:\n            for bin_, count in zip(data[\"interval_real\"][\"hist\"][\"bins\"],\n                                   data[\"interval_real\"][\"hist\"][\"data\"]):\n                label_points.append(\n                    self.__make_points(\n                        prefix + \"histograms\",\n                        {\"label\": label},\n                        ts,\n                        {\"bin\": bin_, \"count\": count}\n                    )\n                )\n        return label_points"}
{"prompt": "def app_url_defaults(self, f):\n        \"\"\"Same as :meth:`url_defaults` but application wide.\n        \"\"\"", "output": "\n        self.url_defaults = f\n        return f\n\n    def url_defaults(self, f):\n        \"\"\"Decorator to set the default values for the URL.\n\n        This decorator is used to set the default values for the URL.\n        The decorated function should take a single argument which is a\n        :class:`~werkzeug.routing.Map` object and return a dictionary with\n        the default values.\n\n        Example::\n\n            @app.url_defaults\n            def add_default_language_code(endpoint, values", "solution": "\n        self.record_once(lambda s: s.app.url_default_functions\n            .setdefault(None, []).append(f))\n        return f"}
{"prompt": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"", "output": "\n        self.cursor.execute(\"SELECT * FROM %s\" % table)\n        self.cursor.copy_to(tmp_file, sep=\"\\t\")\n\n    def bulk_load(self, table, tmp_file):\n        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        self.cursor.copy_from(tmp_file, table, sep=\"\\t\")\n\n    def get_table_names(self):\n        \"\"\"\n        Returns a list of table names in the database\n        \"\"\"\n        self.cursor", "solution": "\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)"}
{"prompt": "def  make_html_items( self, items ):\n        \"\"\" convert a field's content into some valid HTML \"\"\"", "output": "\n        html_items = []\n        for item in items:\n            html_items.append( self.make_html_item( item ) )\n        return html_items\n\n    def make_html_item( self, item ):\n        \"\"\" convert a field's content into some valid HTML \"\"\"\n        if isinstance( item, str ):\n            return item\n        elif isinstance( item, int ):\n            return str( item )\n        elif isinstance( item, float ):\n            return str( item )\n        elif isinstance( item, bool", "solution": "\n        lines = []\n        for item in items:\n            if item.lines:\n                lines.append( self.make_html_code( item.lines ) )\n            else:\n                lines.append( self.make_html_para( item.words ) )\n\n        return string.join( lines, '\\n' )"}
{"prompt": "def p_relate_statement_2(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase\"\"\"", "output": "\n        p[0] = RelateStatement(p[2], p[4], p[6], p[7])\n\n    def p_relate_statement_3(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase_list\"\"\"\n        p[0] = RelateStatement(p[2], p[4], p[6], p[7])\n\n    def p_relate_statement_4(self, p):\n        \"\"\"statement : RELATE instance_name TO", "solution": "\n        p[0] = RelateNode(from_variable_name=p[2],\n                          to_variable_name=p[4],\n                          rel_id=p[6],\n                          phrase=p[8])"}
{"prompt": "def load_exons(self, exons, genes=None, build='37'):\n        \"\"\"Create exon objects and insert them into the database\n        \n        Args:\n            exons(iterable(dict))\n        \"\"\"", "output": "\n        for exon in exons:\n            if exon['chrom'] not in self.chroms:\n                self.chroms[exon['chrom']] = Chrom(exon['chrom'], build=build)\n            self.chroms[exon['chrom']].add_exon(exon, genes)\n\n    def load_genes(self, genes, build='37'):\n        \"\"\"Create gene objects and insert them into the database\n        \n        Args:\n            genes(iterable(dict))\n        \"\"\"\n        for", "solution": "\n        genes = genes or self.ensembl_genes(build)\n        for exon in exons:\n            exon_obj = build_exon(exon, genes)\n            if not exon_obj:\n                continue\n            \n            res = self.exon_collection.insert_one(exon_obj)"}
{"prompt": "def enrich(self, gmt):\n        \"\"\"use local mode\n         \n        p = p-value computed using the Fisher exact test (Hypergeometric test)  \n\n        Not implemented here:\n\n            combine score = log(p)\u00b7z\n\n        see here: http://amp.pharm.mssm.edu/Enrichr/help#background&q=4\n        \n        columns contain:\n            \n            Term Overlap P-value Adjusted_P-value Genes\n\n        \"\"\"", "output": "\n        if self.verbose:\n            print(\"enrichment\")\n        if self.verbose:\n            print(\"enrichment: use local mode\")\n        if self.verbose:\n            print(\"enrichment: use local mode\")\n        if self.verbose:\n            print(\"enrichment: use local mode\")\n        if self.verbose:\n            print(\"enrichment: use local mode\")\n        if self.verbose:\n            print(\"enrichment: use local mode\")\n        if self.verbose:\n            print(\"", "solution": "\n        if isscalar(self.background):\n            if isinstance(self.background, int) or self.background.isdigit():\n                self._bg = int(self.background)\n            elif isinstance(self.background, str):\n                # self.background = set(reduce(lambda x,y: x+y, gmt.values(),[]))\n                self._bg = self.get_background()\n                self._logger.info(\"Background: found %s genes\"%(len(self._bg)))\n            else:\n                raise Exception(\"Unsupported background data type\")\n        else:\n            # handle array object: nd.array, list, tuple, set, Series\n            try:\n                it = iter(self.background)\n                self._bg = set(self.background)\n            except TypeError:\n                self._logger.error(\"Unsupported background data type\")\n        # statistical testing\n        hgtest = list(calc_pvalues(query=self._gls, gene_sets=gmt, \n                                   background=self._bg))\n        if len(hgtest) > 0:\n            terms, pvals, olsz, gsetsz, genes = hgtest\n            fdrs, rej = multiple_testing_correction(ps = pvals, \n                                                    alpha=self.cutoff,\n                                                    method='benjamini-hochberg')\n            # save to a dataframe\n            odict = OrderedDict()\n            odict['Term'] = terms\n            odict['Overlap'] = list(map(lambda h,g: \"%s/%s\"%(h, g), olsz, gsetsz))\n            odict['P-value'] = pvals\n            odict['Adjusted P-value'] = fdrs\n            # odict['Reject (FDR< %s)'%self.cutoff ] = rej\n            odict['Genes'] = [\";\".join(g) for g in genes]\n            res = pd.DataFrame(odict)\n            return res\n        return"}
{"prompt": "def populate_from_sequence(seq: list, r: ref(Edge), sr: state.StateRegister):\n    \"\"\" function that connect each other one sequence of MatchExpr. \"\"\"", "output": "\n    for i in range(len(seq) - 1):\n        r.connect(seq[i], seq[i + 1])\n\n\ndef populate_from_sequence_with_state(seq: list, r: ref(Edge), sr: state.StateRegister):\n    \"\"\" function that connect each other one sequence of MatchExpr. \"\"\"\n    for i in range(len(seq) - 1):\n        r.connect(seq[i], seq[i + 1], sr.get_state(seq[i]))\n\n\ndef populate_from_", "solution": "\n    base_state = r\n    # we need to detect the last state of the sequence\n    idxlast = len(seq) - 1\n    idx = 0\n    for m in seq:\n        # alternatives are represented by builtin list\n        if isinstance(m, list):\n            # so recursively connect all states of each alternative sequences.\n            for item in m:\n                populate_from_sequence(item, r, sr)\n        elif isinstance(m, MatchExpr):\n            # from the current state, have we a existing edge for this event?\n            eX = r().get_next_edge(m)\n            if eX is None:\n                sX = None\n                if idx != idxlast:\n                    sX = state.State(sr)\n                    sX.matchDefault(base_state().s)\n                else:\n                    # last state of sequence return to the base\n                    sX = base_state().s\n                eX = Edge(sX)\n                r().next_edge[id(sX)] = eX\n                m.attach(r().s, sX, sr)\n            r = ref(eX)\n        idx += 1"}
{"prompt": "def mr_reader(job, input_stream, loads=core.loads):\n    \"\"\" Converts a file object with json serialised pyschema records\n        to a stream of pyschema objects\n\n    Can be used as job.reader in luigi.hadoop.JobTask\n    \"\"\"", "output": "\n    for line in input_stream:\n        yield loads(line)\n\n\ndef mr_writer(job, output_stream, dumps=core.dumps):\n    \"\"\" Converts a stream of pyschema objects to a file object with\n        json serialised pyschema records\n\n    Can be used as job.writer in luigi.hadoop.JobTask\n    \"\"\"\n    for record in output_stream:\n        yield dumps(record)\n\n\ndef mr_mapper(job, input_stream, output_stream, loads=core.loads):", "solution": "\n    for line in input_stream:\n        yield loads(line),"}
{"prompt": "def add_from_raw_data(self, raw_data, data_type_id, name, description):\n        \"\"\"\n        Upload already serialized raw data as a new dataset.\n\n        Parameters\n        ----------\n        raw_data: bytes\n            Dataset contents to upload.\n        data_type_id : str\n            Serialization format of the raw data.\n            Supported formats are:\n                'PlainText'\n                'GenericCSV'\n                'GenericTSV'\n                'GenericCSVNoHeader'\n                'GenericTSVNoHeader'\n                'ARFF'\n            See the azureml.DataTypeIds class for constants.\n        name : str\n            Name for the new dataset.\n        description : str\n            Description for the new dataset.\n\n        Returns\n        -------\n        SourceDataset\n            Dataset that was just created.\n            Use open(), read_as_binary(), read_as_text() or to_dataframe() on\n            the dataset object to get its contents as a stream, bytes, str or\n            pandas DataFrame.\n        \"\"\"", "output": "TSVNoHeader'\n                'GenericJSON'\n                'GenericXML'\n                'GenericExcel'\n                'GenericExcel2007'\n                'GenericExcel2010'\n                'GenericExcel2013'\n                'GenericExcel2016'\n                'GenericExcel2019'\n                'GenericPDF'\n                'GenericRTF'\n                'GenericWord'\n                'GenericPowerPoint'\n                'GenericVisio'\n                'GenericOutlook'\n                'GenericOpenDocumentSpreadsheet'\n                'GenericOpenDocument", "solution": "\n        _not_none('raw_data', raw_data)\n        _not_none_or_empty('data_type_id', data_type_id)\n        _not_none_or_empty('name', name)\n        _not_none_or_empty('description', description)\n\n        return self._upload(raw_data, data_type_id, name, description)"}
{"prompt": "def write_bit(self, value, bit_num, address):\n        \"\"\"Writes the value given to the bit in the address specified.\n\n        :param value: The value to write.\n        :type value: int\n        :param bit_num: The bit number to write to.\n        :type bit_num: int\n        :param address: The address to write to.\n        :type address: int\n        \"\"\"", "output": "\n        self.write_byte(value, address)\n\n    def write_byte(self, value, address):\n        \"\"\"Writes the value given to the address specified.\n\n        :param value: The value to write.\n        :type value: int\n        :param address: The address to write to.\n        :type address: int\n        \"\"\"\n        self.write_bytes([value], address)\n\n    def write_bytes(self, values, address):\n        \"\"\"Writes the values given to the address specified.\n\n        :param values:", "solution": "\n        bit_mask = get_bit_mask(bit_num)\n        old_byte = self.read(address)\n         # generate the new byte\n        if value:\n            new_byte = old_byte | bit_mask\n        else:\n            new_byte = old_byte & ~bit_mask\n        self.write(new_byte, address)"}
{"prompt": "def filter(self, *filters, **kwargs):\n        \"\"\"\n        Returns this Query instance with the query args combined with\n        existing set with AND.\n\n        kwargs are simply passed to a new Filter object and combined to any\n        other filters with AND.\n\n        By default, everything is combined using AND. If you provide\n        multiple filters in a single filter call, those are ANDed\n        together. If you provide multiple filters in multiple filter\n        calls, those are ANDed together.\n\n        If you want something different, use the F class which supports\n        ``&`` (and), ``|`` (or) and ``~`` (not) operators. Then call\n        filter once with the resulting Filter instance.\n        \"\"\"", "output": "`` and ``|`` operators.\n\n        :param filters:\n            Any number of Filter objects.\n        :param kwargs:\n            Any number of keyword arguments to be passed to the Filter\n            constructor.\n        :return:\n            This Query instance.\n        \"\"\"\n        if kwargs:\n            filters = (F(**kwargs),) + filters\n\n        if not filters:\n            return self\n\n        if len(filters) == 1:\n            self._filters = filters[0]\n        else:\n            self._filters = F(filters)\n\n", "solution": "\n        f = list(filters)\n\n        if kwargs:\n            f += [Filter(**kwargs)]\n\n        return self._clone(filters=f)"}
{"prompt": "def topological_nodes(self):\n        \"\"\"\n        Yield nodes in topological order.\n\n        Returns:\n            generator(DAGNode): node in topological order\n        \"\"\"", "output": "\n        for node in self.topological_sort():\n            yield node\n\n    def topological_sort(self):\n        \"\"\"\n        Return nodes in topological order.\n\n        Returns:\n            list(DAGNode): nodes in topological order\n        \"\"\"\n        return list(self._topological_sort())\n\n    def _topological_sort(self):\n        \"\"\"\n        Return nodes in topological order.\n\n        Returns:\n            iterator(DAGNode): nodes in topological order\n        \"\"\"\n        visited = set()\n        stack = []\n        for node in", "solution": "\n        return nx.lexicographical_topological_sort(self._multi_graph,\n                                                   key=lambda x: str(x.qargs))"}
{"prompt": "def peng_power(snum):\n    r\"\"\"\n    Return engineering suffix and its floating point equivalent of a number.\n\n    :py:func:`peng.peng` lists the correspondence between suffix and floating\n    point exponent.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: named tuple in which the first item is the engineering suffix and\n            the second item is the floating point equivalent of the suffix\n            when the number is represented in engineering notation.\n\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.functions.peng_power\n\n    :raises: RuntimeError (Argument \\`snum\\` is not valid)\n\n    .. [[[end]]]\n\n    For example:\n\n        >>> import peng\n        >>> peng.peng_power(peng.peng(1235.6789E3, 3, False))\n        EngPower(suffix='M', exp=1000000.0)\n    \"\"\"", "output": "   .. versionadded:: 1.0.0\n\n    \"\"\"\n    return peng(snum)\n\n\ndef peng(snum):\n    r\"\"\"\n    Return engineering suffix and its floating point equivalent of a number.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: named tuple in which the first item is the engineering suffix and\n            the second item is the floating point equivalent of the suffix\n            when the number is represented in engineering notation.\n\n\n    .. version", "solution": "\n    suffix = \" \" if snum[-1].isdigit() else snum[-1]\n    return EngPower(suffix, _SUFFIX_POWER_DICT[suffix])"}
{"prompt": "def get_next_proto_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by NPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n\n        .. versionadded:: 0.15\n        \"\"\"", "output": "\n        return self._get_next_proto_negotiated()\n\n    def _get_next_proto_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by NPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n\n        .. versionadded:: 0.15\n        \"\"\"\n        return self._next_proto_negotiated\n\n    def set_next_proto_negotiated(self, proto):\n        \"\"\"", "solution": "\n        _warn_npn()\n        data = _ffi.new(\"unsigned char **\")\n        data_len = _ffi.new(\"unsigned int *\")\n\n        _lib.SSL_get0_next_proto_negotiated(self._ssl, data, data_len)\n\n        return _ffi.buffer(data[0], data_len[0])[:]"}
{"prompt": "def update_virtual_meta(self):\n        \"\"\"Will read back the virtual column etc, written by :func:`DataFrame.write_virtual_meta`. This will be done when opening a DataFrame.\"\"\"", "output": "\n        if self._virtual_meta is not None:\n            return\n        self._virtual_meta = {}\n        if not self._virtual_meta_path.exists():\n            return\n        with self._virtual_meta_path.open('r') as f:\n            for line in f:\n                if line.startswith('#'):\n                    continue\n                try:\n                    key, value = line.strip().split('=', 1)\n                except ValueError:\n                    continue\n                self._virtual_meta[key] = value\n\n    def write", "solution": "\n        import astropy.units\n        try:\n            path = os.path.join(self.get_private_dir(create=False), \"virtual_meta.yaml\")\n            if os.path.exists(path):\n                meta_info = vaex.utils.read_json_or_yaml(path)\n                if 'virtual_columns' not in meta_info:\n                    return\n                self.virtual_columns.update(meta_info[\"virtual_columns\"])\n                self.variables.update(meta_info[\"variables\"])\n                self.ucds.update(meta_info[\"ucds\"])\n                self.descriptions.update(meta_info[\"descriptions\"])\n                units = {key: astropy.units.Unit(value) for key, value in meta_info[\"units\"].items()}\n                self.units.update(units)\n        except:\n            logger.exception(\"non fatal error\")"}
{"prompt": "def delete(self, teamId):\n        \"\"\"Delete a team.\n\n        Args:\n            teamId(basestring): The ID of the team to be deleted.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        check_type(teamId, basestring)\n        return self.delete_path('/teams/{}'.format(teamId))\n\n    def get(self, teamId):\n        \"\"\"Get a team.\n\n        Args:\n            teamId(basestring): The ID of the team to be retrieved.\n\n        Returns:\n            Team: A Team object.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        check_", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n\n        # API request\n        self._session.delete(API_ENDPOINT + '/' + teamId)"}
{"prompt": "def per_triangle(script, sidedim=0, textdim=1024, border=2, method=1):\n    \"\"\"Trivial Per-Triangle parameterization\n\n    \"\"\"", "output": "\n    script.add_command('per_triangle')\n    script.add_arg('-sidedim', sidedim)\n    script.add_arg('-textdim', textdim)\n    script.add_arg('-border', border)\n    script.add_arg('-method', method)\n\n\ndef per_vertex(script, sidedim=0, textdim=1024, border=2, method=1):\n    \"\"\"Trivial Per-Vertex parameterization\n\n    \"\"\"\n    script.add_command('per_ver", "solution": "\n    filter_xml = ''.join([\n        '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n',\n        '    <Param name=\"sidedim\"',\n        'value=\"%d\"' % sidedim,\n        'description=\"Quads per line\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"',\n        '/>\\n',\n        '    <Param name=\"textdim\"',\n        'value=\"%d\"' % textdim,\n        'description=\"Texture Dimension (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Gives an indication on how big the texture is\"',\n        '/>\\n',\n        '    <Param name=\"border\"',\n        'value=\"%d\"' % border,\n        'description=\"Inter-Triangle border (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"',\n        '/>\\n',\n        '    <Param name=\"method\"',\n        'value=\"%d\"' % method,\n        'description=\"Method\"',\n        'enum_val0=\"Basic\"',\n        'enum_val1=\"Space-optimizing\"',\n        'enum_cardinality=\"2\"',\n        'type=\"RichEnum\"',\n        'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"'\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None"}
{"prompt": "def add_virtual_columns_cartesian_velocities_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", vx=\"vx\", vy=\"vy\", vz=\"vz\", vr=\"vr\", vlong=\"vlong\", vlat=\"vlat\", distance=None):\n        \"\"\"Concert velocities from a cartesian to a spherical coordinate system\n\n        TODO: errors\n\n        :param x: name of x column (input)\n        :param y:         y\n        :param z:         z\n        :param vx:       vx\n        :param vy:       vy\n        :param vz:       vz\n        :param vr: name of the column for the radial velocity in the r direction (output)\n        :param vlong: name of the column for the velocity component in the longitude direction  (output)\n        :param vlat: name of the column for the velocity component in the latitude direction, positive points to the north pole (output)\n        :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance\n        :return:\n        \"\"\"", "output": "vx:        vx\n        :param vy:        vy\n        :param vz:        vz\n        :param vr:        vr\n        :param vlong:     vlong\n        :param vlat:      vlat\n        :param distance:  distance from the sun\n        :return:\n        \"\"\"\n        if distance is None:\n            distance = self.distance\n        self.add_column(x, \"x\")\n        self.add_column(y, \"y\")\n        self.add_column(z, \"z\")\n        self.", "solution": "\n        # see http://www.astrosurf.com/jephem/library/li110spherCart_en.htm\n        if distance is None:\n            distance = \"sqrt({x}**2+{y}**2+{z}**2)\".format(**locals())\n        self.add_virtual_column(vr, \"({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}\".format(**locals()))\n        self.add_virtual_column(vlong, \"-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)\".format(**locals()))\n        self.add_virtual_column(vlat, \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( {distance}*sqrt({x}**2+{y}**2) )\".format(**locals()))"}
{"prompt": "def check_token(token):\n    \"\"\" Verify http header token authentification \"\"\"", "output": "\n    if token == '':\n        return False\n    else:\n        return True\n\n\ndef check_user(user):\n    \"\"\" Verify user authentification \"\"\"\n    if user == '':\n        return False\n    else:\n        return True\n\n\ndef check_password(password):\n    \"\"\" Verify user authentification \"\"\"\n    if password == '':\n        return False\n    else:\n        return True\n\n\ndef check_email(email):\n    \"\"\" Verify user email \"\"\"\n    if email == '':\n        return False\n   ", "solution": "\n    user = models.User.objects(api_key=token).first()\n    return user or None"}
{"prompt": "def validate_value(self, value):\n        \"\"\"\n        Validate new property value before setting it.\n\n        value -- New value\n        \"\"\"", "output": "\n        if value is None:\n            return\n\n        if not isinstance(value, self.value_type):\n            raise TypeError(\"Value must be of type %s\" % self.value_type)\n\n        if self.value_type is int:\n            if value < 0:\n                raise ValueError(\"Value must be positive\")\n\n        if self.value_type is float:\n            if value < 0.0:\n                raise ValueError(\"Value must be positive\")\n\n        if self.value_type is str:\n            if not isinstance", "solution": "\n        if 'readOnly' in self.metadata and self.metadata['readOnly']:\n            raise PropertyError('Read-only property')\n\n        try:\n            validate(value, self.metadata)\n        except ValidationError:\n            raise PropertyError('Invalid property value')"}
{"prompt": "def pack_metadata_statement(self, metadata, receiver='', iss='', lifetime=0,\n                                sign_alg=''):\n        \"\"\"\n        Given a MetadataStatement instance create a signed JWT.\n\n        :param metadata: Original metadata statement as a MetadataStatement\n            instance\n        :param receiver: Receiver (audience) of the JWT\n        :param iss: Issuer ID if different from default\n        :param lifetime: jWT signature life time\n        :param sign_alg: JWT signature algorithm\n        :return: A JWT\n        \"\"\"", "output": "\n        if not metadata:\n            raise ValueError('No metadata statement provided')\n\n        if not isinstance(metadata, MetadataStatement):\n            raise ValueError('Metadata statement must be a MetadataStatement '\n                             'instance')\n\n        if not receiver:\n            raise ValueError('No receiver provided')\n\n        if not iss:\n            iss = self.iss\n\n        if not lifetime:\n            lifetime = self.lifetime\n\n        if not sign_alg:\n            sign_alg = self.sign_alg\n\n        # Create the JWT header", "solution": "\n\n        return self.self_signer.sign(metadata, receiver=receiver, iss=iss,\n                                     lifetime=lifetime, sign_alg=sign_alg)"}
{"prompt": "def contact(self, id):\n        \"\"\"\n        Retrieve all details for a specific contact, including fields such as birthday and mood.\n\n        Args:\n            id (str): user identifier to lookup\n\n        Returns:\n            SkypeContact: resulting contact object\n        \"\"\"", "output": "\n        return SkypeContact(self, id)\n\n    def contacts(self):\n        \"\"\"\n        Retrieve all contacts, including fields such as birthday and mood.\n\n        Returns:\n            list: list of SkypeContact objects\n        \"\"\"\n        return [SkypeContact(self, contact) for contact in self.get_contacts()]\n\n    def get_contacts(self):\n        \"\"\"\n        Retrieve all contacts, including fields such as birthday and mood.\n\n        Returns:\n            list: list of contact identifiers\n", "solution": "\n        try:\n            json = self.skype.conn(\"POST\", \"{0}/users/batch/profiles\".format(SkypeConnection.API_USER),\n                                   json={\"usernames\": [id]}, auth=SkypeConnection.Auth.SkypeToken).json()\n            contact = SkypeContact.fromRaw(self.skype, json[0])\n            if contact.id not in self.contactIds:\n                self.contactIds.append(contact.id)\n            return self.merge(contact)\n        except SkypeApiException as e:\n            if len(e.args) >= 2 and getattr(e.args[1], \"status_code\", None) == 403:\n                # Not a contact, so no permission to retrieve information.\n                return None\n            raise"}
{"prompt": "def str_repeat(x, repeats):\n    \"\"\"Duplicate each string in a column.\n\n    :param int repeats: number of times each string sample is to be duplicated.\n    :returns: an expression containing the duplicated strings\n\n    Example:\n\n    >>> import vaex\n    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']\n    >>> df = vaex.from_arrays(text=text)\n    >>> df\n      #  text\n      0  Something\n      1  very pretty\n      2  is coming\n      3  our\n      4  way.\n\n    >>> df.text.str.repeat(3)\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: str (expression)\n    ---------------------------------\n    0        SomethingSomethingSomething\n    1  very prettyvery prettyvery pretty\n    2        is comingis comingis coming\n    3                          ourourour\n    4                       way.way.way.\n    \"\"\"", "output": "\n      2  is coming\n      3  our\n      4  way.\n    >>> df.text_repeat(2)\n      #  text\n      0  Something\n      0  Something\n      1  very pretty\n      1  very pretty\n      2  is coming\n      2  is coming\n      3  our\n      3  our\n      4  way.\n      4  way.\n    \"\"\"\n    return x.str_repeat(repeats)\n\n\ndef str_slice(x, start, stop, step=None):\n    \"\"\"Slice", "solution": "\n    sl = _to_string_sequence(x).repeat(repeats)\n    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)"}
{"prompt": "def deprecated(*args):\n    \"\"\" Deprecation warning decorator. Takes optional deprecation message,\n    otherwise will use a generic warning. \"\"\"", "output": "\n    if len(args) == 1:\n        msg = args[0]\n    else:\n        msg = \"This function is deprecated\"\n    def deprecated_decorator(func):\n        def deprecated_func(*args, **kwargs):\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return deprecated_func\n    return deprecated_decorator\n\n\ndef deprecated_class(*args):\n    \"\"\" Deprecation warning decorator for classes. Takes optional deprecation\n    message, otherwise", "solution": "\n    def wrap(func):\n        def wrapped_func(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapped_func\n\n    if len(args) == 1 and callable(args[0]):\n        msg = \"Function '%s' will be deprecated in future versions of \" \\\n            \"Neurosynth.\" % args[0].__name__\n        return wrap(args[0])\n    else:\n        msg = args[0]\n        return wrap"}
{"prompt": "def get_last_activities(self, n):\n        \"\"\"Get all activity data for the last activity\n\n        Keyword arguments:\n        \"\"\"", "output": "\n        return self.get_activities(n, 0)\n\n    def get_activities(self, n, offset):\n        \"\"\"Get all activity data for the last activity\n\n        Keyword arguments:\n        \"\"\"\n        url = self.url + \"/activities?n=\" + str(n) + \"&offset=\" + str(offset)\n        return self.get_data(url)\n\n    def get_activity(self, activity_id):\n        \"\"\"Get activity data for a specific activity\n\n        Keyword arguments:\n        \"\"\"\n        url", "solution": "\n        filenames = self.get_activity_list().iloc[-n:].filename.tolist()\n        last_activities = [self.get_activity(f) for f in filenames]\n        return last_activities"}
{"prompt": "def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):\n    \"\"\"\n    Update a user in the database\n    \"\"\"", "output": "\n    user = context.dbsession.query(User).get(user_id)\n    if user is None:\n        raise click.ClickException(\"User not found\")\n    if update_role:\n        user.role = update_role\n    if add_institute:\n        institute = context.dbsession.query(Institute).get(add_institute)\n        if institute is None:\n            raise click.ClickException(\"Institute not found\")\n        user.institutes.append(institute)\n    if remove_admin:\n        user", "solution": "\n    adapter = context.obj['adapter']\n\n    user_obj = adapter.user(user_id)\n\n    if not user_obj:\n        LOG.warning(\"User %s could not be found\", user_id)\n        context.abort()\n\n    existing_roles = set(user_obj.get('roles',[]))\n    if update_role:\n        if not update_role in user_obj['roles']:\n            existing_roles = set(user_obj['roles'])\n            existing_roles.add(update_role)\n            LOG.info(\"Adding role %s to user\", update_role)\n        else:\n            LOG.warning(\"User already have role %s\", update_role)\n\n    if remove_admin:\n        try:\n            existing_roles.remove('admin')\n            LOG.info(\"Removing admin rights from user %s\", user_id)\n        except KeyError as err:\n            LOG.info(\"User %s does not have admin rights\", user_id)\n\n    user_obj['roles'] = list(existing_roles)\n\n    existing_institutes = set(user_obj.get('institutes',[]))\n    for institute_id in add_institute:\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.warning(\"Institute %s could not be found\", institute_id)\n        else:\n            existing_institutes.add(institute_id)\n            LOG.info(\"Adding institute %s to user\", institute_id)\n\n    for institute_id in remove_institute:\n        try:\n            existing_institutes.remove(institute_id)\n            LOG.info(\"Removing institute %s from user\", institute_id)\n        except KeyError as err:\n            LOG.info(\"User does not have access to institute %s\", institute_id)\n\n    user_obj['institutes'] = list(existing_institutes)\n\n    updated_user = adapter.update_user(user_obj)"}
{"prompt": "def _update_base(self):\n        \"\"\"update the base, including the URL for GitLab and the API endpoint.\n        \"\"\"", "output": "\n        self.base = self.base_url + self.api_endpoint\n\n    def _update_headers(self):\n        \"\"\"update the headers for the requests.\n        \"\"\"\n        self.headers = {\n            'PRIVATE-TOKEN': self.token,\n            'Content-Type': 'application/json'\n        }\n\n    def _update_session(self):\n        \"\"\"update the session for the requests.\n        \"\"\"\n        self.session = requests.Session()\n        self.session.headers.update(self.headers)\n\n", "solution": "\n        self.base = self._get_and_update_setting('SREGISTRY_GITLAB_BASE',\n                                                 \"https://gitlab.com/\")\n        self.api_base = \"%s/api/v4\" % self.base.strip('/')\n        self.artifacts = self._get_and_update_setting('SREGISTRY_GITLAB_FOLDER',\n                                                      'build')\n\n        self.job = self._get_and_update_setting('SREGISTRY_GITLAB_JOB', 'build')\n\n        bot.debug('      Api: %s' % self.api_base)\n        bot.debug('Artifacts: %s' % self.artifacts)\n        bot.debug('      Job: %s' % self.job)"}
{"prompt": "def send(self, message):\n        \"\"\" Send a message object\n\n            :type message: data.OutgoingMessage\n            :param message: The message to send\n            :rtype: data.OutgoingMessage\n            :returns: The sent message with populated fields\n            :raises AssertionError: wrong provider name encountered (returned by the router, or provided to OutgoingMessage)\n            :raises MessageSendError: generic errors\n            :raises AuthError: provider authentication failed\n            :raises LimitsError: sending limits exceeded\n            :raises CreditError: not enough money on the account\n        \"\"\"", "output": "Error: provider credit exhausted\n            :raises ProviderError: provider-specific errors\n        \"\"\"\n        # TODO: add support for sending to multiple recipients\n        # TODO: add support for sending to multiple providers\n        # TODO: add support for sending to multiple providers with different recipients\n        # TODO: add support for sending to multiple providers with different messages\n        # TODO: add support for sending to multiple providers with different messages and recipients\n        # TODO: add support for sending to multiple providers with different messages and recipients\n        # TODO: add support for sending", "solution": "\n        # Which provider to use?\n        provider_name = self._default_provider  # default\n        if message.provider is not None:\n            assert message.provider in self._providers, \\\n                'Unknown provider specified in OutgoingMessage.provideer: {}'.format(provider_name)\n            provider = self.get_provider(message.provider)\n        else:\n            # Apply routing\n            if message.routing_values is not None: # Use the default provider when no routing values are given\n                # Routing values are present\n                provider_name = self.router(message, *message.routing_values) or self._default_provider\n                assert provider_name in self._providers, \\\n                    'Routing function returned an unknown provider name: {}'.format(provider_name)\n            provider = self.get_provider(provider_name)\n\n        # Set message provider name\n        message.provider = provider.name\n\n        # Send the message using the provider\n        message = provider.send(message)\n\n        # Emit the send event\n        self.onSend(message)\n\n        # Finish\n        return message"}
{"prompt": "def string_arg(ctx, obj):\n    \"\"\"\n    Handles LiteralObjects as well as computable arguments\n    \"\"\"", "output": "\n    if isinstance(obj, LiteralObject):\n        return obj.value\n    else:\n        return obj.eval(ctx)\n\n\ndef string_arg_list(ctx, obj):\n    \"\"\"\n    Handles LiteralObjects as well as computable arguments\n    \"\"\"\n    if isinstance(obj, LiteralObject):\n        return obj.value\n    else:\n        return [x.eval(ctx) for x in obj]\n\n\ndef string_arg_dict(ctx, obj):\n    \"\"\"\n    Handles LiteralObjects as well as", "solution": "\n    if hasattr(obj, 'compute'):\n        obj = next(obj.compute(ctx), False)\n    return to_string(obj)"}
{"prompt": "def on_kill(self):\n        \"\"\"\n        Cancel the submitted athena query\n        \"\"\"", "output": "\n        if self.query_id:\n            self.athena_client.stop_query(self.query_id)\n\n    def on_kill_callback(self, event):\n        \"\"\"\n        Cancel the submitted athena query\n        \"\"\"\n        self.on_kill()\n\n    def on_execute(self, event):\n        \"\"\"\n        Execute the athena query\n        \"\"\"\n        self.query_id = self.athena_client.start_query_execution(self.query)\n\n    def on_execute_callback(self,", "solution": "\n        if self.query_execution_id:\n            self.log.info('\u26b0\ufe0f\u26b0\ufe0f\u26b0\ufe0f Received a kill Signal. Time to Die')\n            self.log.info(\n                'Stopping Query with executionId - %s', self.query_execution_id\n            )\n            response = self.hook.stop_query(self.query_execution_id)\n            http_status_code = None\n            try:\n                http_status_code = response['ResponseMetadata']['HTTPStatusCode']\n            except Exception as ex:\n                self.log.error('Exception while cancelling query', ex)\n            finally:\n                if http_status_code is None or http_status_code != 200:\n                    self.log.error('Unable to request query cancel on athena. Exiting')\n                else:\n                    self.log.info(\n                        'Polling Athena for query with id %s to reach final state', self.query_execution_id\n                    )\n                    self.hook.poll_query_status(self.query_execution_id)"}
{"prompt": "def get_fernet():\n    \"\"\"\n    Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet object\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n    \"\"\"", "output": "\n    from cryptography.fernet import InvalidToken, Fernet\n\n    try:\n        return Fernet(conf.get('core', 'FERNET_KEY'))\n    except (ImportError, InvalidToken):\n        raise AirflowException(\n            \"Fernet key is missing or invalid. Please provide a valid key \"\n            \"using the 'fernet_key' CLI command or in your airflow.cfg file\"\n        )\n\n\ndef get_password_manager():\n    \"\"\"\n    Deferred load of Password Manager.\n\n    This function could fail either because", "solution": "\n    global _fernet\n    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet"}
{"prompt": "def update_board(self, query_params=None):\n        \"\"\"\n        Update this board's information. Returns a new board.\n        \"\"\"", "output": "\n        return self.client.update_board(self.id, query_params)\n\n    def delete(self, query_params=None):\n        \"\"\"\n        Delete this board. Returns a new board.\n        \"\"\"\n        return self.client.delete_board(self.id, query_params)\n\n    def get_lists(self, query_params=None):\n        \"\"\"\n        Get all lists on this board.\n        \"\"\"\n        return self.client.get_lists(self.id, query_params)\n\n    def get_list(self", "solution": "\n        board_json = self.fetch_json(\n            uri_path=self.base_uri,\n            http_method='PUT',\n            query_params=query_params or {}\n        )\n\n        return self.create_board(board_json)"}
{"prompt": "def compose_u3(theta1, phi1, lambda1, theta2, phi2, lambda2):\n        \"\"\"Return a triple theta, phi, lambda for the product.\n\n        u3(theta, phi, lambda)\n           = u3(theta1, phi1, lambda1).u3(theta2, phi2, lambda2)\n           = Rz(phi1).Ry(theta1).Rz(lambda1+phi2).Ry(theta2).Rz(lambda2)\n           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda2)\n           = u3(theta', phi1 + phi', lambda2 + lambda')\n\n        Return theta, phi, lambda.\n        \"\"\"", "output": "(phi1+lambda2).Ry(theta1+theta2).Rz(phi2)\n\n        The parameters are in radians.\n        \"\"\"\n        theta = theta1 + theta2\n        phi = phi1 + lambda2\n        lambda_ = lambda1 + phi2\n        return theta, phi, lambda_\n\n    def _u3_to_matrix(theta, phi, lambda_):\n        \"\"\"Return the 2x2 unitary matrix corresponding to the U3 gate.\n\n        The parameters are in radians", "solution": "\n        # Careful with the factor of two in yzy_to_zyz\n        thetap, phip, lambdap = Optimize1qGates.yzy_to_zyz((lambda1 + phi2), theta1, theta2)\n        (theta, phi, lamb) = (thetap, phi1 + phip, lambda2 + lambdap)\n\n        return (theta, phi, lamb)"}
{"prompt": "def filter_trim(self, start=1, end=1, filt=True):\n        \"\"\"\n        Remove points from the start and end of filter regions.\n        \n        Parameters\n        ----------\n        start, end : int\n            The number of points to remove from the start and end of\n            the specified filter.\n        filt : valid filter string or bool\n            Which filter to trim. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n        if filt is True:\n            filt = self.active_filter\n        if filt not in self.filters:\n            raise ValueError('filter not found')\n        filt = self.filters[filt]\n        filt = filt[start:-end]\n        self.filters[filt.name] = filt\n        self.active_filter = filt.name\n\n    def filter_trim_start(self, start=1, filt=True):\n        \"\"\"\n        Remove points from the start of filter regions.\n        \n       ", "solution": "\n        params = locals()\n        del(params['self'])\n            \n        f = self.filt.grab_filt(filt)\n        nf = filters.trim(f, start, end)\n        \n        self.filt.add('trimmed_filter',\n                    nf,\n                    'Trimmed Filter ({:.0f} start, {:.0f} end)'.format(start, end),\n                    params, setn=self.filt.maxset + 1)"}
{"prompt": "def verify_reg_list(self, obj, object_type):\n        \"\"\"Verify a list of registers.\"\"\"", "output": "\n        if not isinstance(obj, list):\n            raise ValueError(f\"{object_type} must be a list\")\n        for reg in obj:\n            self.verify_reg(reg, object_type)\n\n    def verify_reg(self, obj, object_type):\n        \"\"\"Verify a register.\"\"\"\n        if not isinstance(obj, dict):\n            raise ValueError(f\"{object_type} must be a dict\")\n        if \"name\" not in obj:\n            raise ValueError(f\"{object_type} must", "solution": "\n        # We expect the object to be a bitlist or an idlist, we don't care.\n        # We will iterate it and ensure everything in it is declared as a bit,\n        # and throw if not.\n        for children in obj.children:\n            self.verify_reg(children, object_type)"}
{"prompt": "def regression(self, slope=None):\n        \"\"\"regress tip values against branch values\n\n        Parameters\n        ----------\n        slope : None, optional\n            if given, the slope isn't optimized\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"", "output": "\n        if slope is None:\n            slope, intercept, r_value, p_value, std_err = stats.linregress(self.branch, self.tip)\n        else:\n            intercept = self.tip.mean() - slope * self.branch.mean()\n        return {'slope': slope, 'intercept': intercept, 'r_value': r_value, 'p_value': p_value, 'std_err': std_err}\n\n    def plot(self, ax=None, **kwargs):\n        \"\"\"plot the tree\n\n", "solution": "\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model"}
{"prompt": "def to_keep(datetimes,\n            years=0, months=0, weeks=0, days=0,\n            hours=0, minutes=0, seconds=0,\n            firstweekday=SATURDAY, now=None):\n    \"\"\"\n    Return a set of datetimes that should be kept, out of ``datetimes``.\n\n    Keeps up to ``years``, ``months``, ``weeks``, ``days``,\n    ``hours``, ``minutes``, and ``seconds`` in the past.\n\n    When keeping weeks, it prefers to keep ``firstweekday``, which\n    defaults to Saturday.\n\n    If ``now`` is None, it will base its calculations on\n    ``datetime.datetime.now()``. Datetimes after this point will always be\n    kept.\n    \"\"\"", "output": ".\n\n    If ``now`` is not given, the current time is used.\n\n    If ``firstweekday`` is given, it is used as the first day of the week\n    (default is Saturday).\n\n    >>> from datetime import datetime\n    >>> from dateutil.relativedelta import relativedelta\n    >>> from dateutil.rrule import rrule, MONTHLY\n    >>> from dateutil.rrule import WEEKLY, DAILY, HOURLY, MINUTELY\n    >>> from dateutil.rrule import SECOND", "solution": "\n    datetimes = set(datetimes)\n    return (filters.Years.filter(datetimes, number=years, now=now) |\n            filters.Months.filter(datetimes, number=months, now=now) |\n            filters.Weeks.filter(datetimes, number=weeks,\n                                 firstweekday=firstweekday, now=now) |\n            filters.Days.filter(datetimes, number=days, now=now) |\n            filters.Hours.filter(datetimes, number=hours, now=now) |\n            filters.Minutes.filter(datetimes, number=minutes, now=now) |\n            filters.Seconds.filter(datetimes, number=seconds, now=now))"}
{"prompt": "def set(self, client_id, code, request, *args, **kwargs):\n        \"\"\"Creates Grant object with the given params\n\n        :param client_id: ID of the client\n        :param code:\n        :param request: OAuthlib request object\n        \"\"\"", "output": "\n        self.client_id = client_id\n        self.code = code\n        self.redirect_uri = request.redirect_uri\n        self.scope = request.scopes\n        self.expires = datetime.utcnow() + timedelta(seconds=request.expires_in)\n        self.user = request.user\n        self.expires_in = request.expires_in\n        self.state = request.state\n        self.nonce = request.nonce\n        self.code_challenge = request.code_challenge\n", "solution": "\n        expires = datetime.utcnow() + timedelta(seconds=100)\n        grant = self.model(\n            client_id=request.client.client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scope=' '.join(request.scopes),\n            user=self.current_user(),\n            expires=expires\n        )\n        self.session.add(grant)\n\n        self.session.commit()"}
{"prompt": "def parse_hpo_diseases(hpo_lines):\n    \"\"\"Parse hpo disease phenotypes\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with mim numbers as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        if line.startswith('HP:000'):\n            mim_number = line.split(':')[1].split(' ')[0]\n            diseases[mim_number] = line.split(':')[2].strip()\n    return diseases\n\n\ndef parse_hpo_genes(hpo_lines):\n    \"\"\"Parse hpo gene phenotypes\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns", "solution": "\n    diseases = {}\n    LOG.info(\"Parsing hpo diseases...\")\n    for index, line in enumerate(hpo_lines):\n        # First line is a header\n        if index == 0:\n            continue\n        # Skip empty lines\n        if not len(line) > 3:\n            continue\n        # Parse the info\n        disease_info = parse_hpo_disease(line)\n        # Skip the line if there where no info\n        if not disease_info:\n            continue\n        disease_nr = disease_info['disease_nr']\n        hgnc_symbol = disease_info['hgnc_symbol']\n        hpo_term = disease_info['hpo_term']\n        source = disease_info['source']\n        disease_id = \"{0}:{1}\".format(source, disease_nr)\n        \n        if disease_id not in diseases:\n            diseases[disease_id] = {\n                'disease_nr': disease_nr,\n                'source': source,\n                'hgnc_symbols': set(),\n                'hpo_terms': set(),\n            }\n\n        if hgnc_symbol:\n            diseases[disease_id]['hgnc_symbols'].add(hgnc_symbol)\n        if hpo_term:\n            diseases[disease_id]['hpo_terms'].add(hpo_term)\n\n    LOG.info(\"Parsing done.\")\n    return diseases"}
{"prompt": "def _check_currency_format(self, format=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            format (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"", "output": "\n        if format is None:\n            format = self.currency_format\n        if format not in self.currency_formats:\n            raise ValueError(\n                'Invalid currency format: %s. Valid formats: %s' % (\n                    format, self.currency_formats))\n        return format\n\n    def _check_currency_symbol(self, symbol=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            symbol (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"\n        if symbol", "solution": "\n        defaults = self.settings['currency']['format']\n        if hasattr(format, '__call__'):\n            format = format()\n        if is_str(format) and re.match('%v', format):\n\n            # Create and return positive, negative and zero formats:\n            return {\n                'pos': format,\n                'neg': format.replace(\"-\", \"\").replace(\"%v\", \"-%v\"),\n                'zero': format\n            }\n        elif not format or not format['por'] or not re.match('%v',\n                                                             format['pos']):\n            self.settings['currency']['format'] = {\n                'pos': defaults,\n                'neg': defaults.replace(\"%v\", \"-%v\"),\n                'zero': defaults\n            }\n            return self.settings\n\n        return format"}
{"prompt": "def setMeterPassword(self, new_pwd, pwd=\"00000000\"):\n        \"\"\" Serial Call to set meter password.  USE WITH CAUTION.\n\n        Args:\n            new_pwd (str): 8 digit numeric password to set\n            pwd (str): Old 8 digit numeric password.\n\n        Returns:\n            bool: True on completion with ACK.\n        \"\"\"", "output": "\n        self.write(f\"{self.cmd_set_meter_password} {pwd} {new_pwd}\")\n        return self.read_ack()\n\n    def setMeterPassword(self, new_pwd, pwd=\"00000000\"):\n        \"\"\" Serial Call to set meter password.  USE WITH CAUTION.\n\n        Args:\n            new_pwd (str): 8 digit numeric password to set\n            pwd (str): Old 8 digit numeric password.\n\n        Returns:\n           ", "solution": "\n        result = False\n        self.setContext(\"setMeterPassword\")\n        try:\n            if len(new_pwd) != 8 or len(pwd) != 8:\n                self.writeCmdMsg(\"Passwords must be exactly eight characters.\")\n                self.setContext(\"\")\n                return result\n\n            if not self.request(False):\n                self.writeCmdMsg(\"Pre command read failed: check serial line.\")\n            else:\n                if not self.serialCmdPwdAuth(pwd):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_pwd = binascii.hexlify(new_pwd.zfill(8))\n                    req_str = \"015731023030323028\" + req_pwd + \"2903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success(setMeterPassword): 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"", "output": "\n        msg = self.msg\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n    def getMessageWithTime(self):\n        \"\"\"\n        Return the message for this LogRecord, with the time added.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.  If the time (or time.tzname) is not\n        available in this LogRecord, use the current time or local time zone.\n        \"\"\"\n        msg = self.getMessage()\n        if self.ex", "solution": "\n        msg = self.msg\n        if self.args:\n            msg = msg.format(*self.args)\n        return maybe_encode(msg)"}
{"prompt": "def add_observer(self, callback, ntype, sender):\n        \"\"\"Add an observer callback to this notification center.\n\n        The given callback will be called upon posting of notifications of\n        the given type/sender and will receive any additional arguments passed\n        to post_notification.\n\n        Parameters\n        ----------\n        callback : callable\n            The callable that will be called by :meth:`post_notification`\n            as ``callback(ntype, sender, *args, **kwargs)\n        ntype : hashable\n            The notification type. If None, all notifications from sender\n            will be posted.\n        sender : hashable\n            The notification sender. If None, all notifications of ntype\n            will be posted.\n        \"\"\"", "output": "the notification center is notified of\n            a notification of this type, the callback will be called.\n        sender : hashable\n            The notification sender. If the notification center is notified of\n            a notification from this sender, the callback will be called.\n\n        \"\"\"\n        if ntype not in self._observers:\n            self._observers[ntype] = {}\n        if sender not in self._observers[ntype]:\n            self._observers[ntype][sender] = []\n        self._observers[ntype][sender].", "solution": "\n        assert(callback != None)\n        self.registered_types.add(ntype)\n        self.registered_senders.add(sender)\n        self.observers.setdefault((ntype,sender), set()).add(callback)"}
{"prompt": "def cli_run():\n    \"\"\"docstring for argparse\"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='Runs the CLI')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')\n    parser.add_argument('-l', '--list', action='store_true', help='List all available commands')\n    parser.add_argument('-c', '--command', help='Run a command')\n    parser.add_argument('-p', '--param', help='Command parameter')\n    parser.add_argument('-f', '--file", "solution": "\n    parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow')\n    parser.add_argument('query', help=\"What's the problem ?\", type=str, nargs='+')\n    parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda')\n    args = parser.parse_args()\n    main(args)"}
{"prompt": "def send_email(self, to, kind, **kwargs):\n        \"\"\" Sends an e-mail to the given address.\n\n        to: The address\n        kind: the ID for an e-mail kind; it should point to a subdirectory of\n            self.template_prefix containing subject.txt and message.html, which\n            are django templates for the subject and HTML message respectively.\n\n        context: a context for rendering the e-mail.\n\n        \"\"\"", "output": "\n        subject = render_to_string(\n            '%s/%s/subject.txt' % (self.template_prefix, kind),\n            kwargs)\n        message = render_to_string(\n            '%s/%s/message.html' % (self.template_prefix, kind),\n            kwargs)\n        send_mail(subject, message, self.from_address, [to])\n\n    def send_password_reset_email(self, user, **kwargs):\n        \"\"\" Sends a password reset e-mail to the given user.\n\n", "solution": "\n\n        return __send_email__(self.template_prefix, to, kind, **kwargs)"}
{"prompt": "def list_to_str(lst: list, content: str, indent: int=1):\n    \"\"\"recurs into list for string computing \"\"\"", "output": "\n    if lst:\n        return content + \"\\n\" + \" \" * indent + str(lst[0]) + \"\\n\" + list_to_str(lst[1:], content, indent)\n    else:\n        return content\n\n\ndef list_to_str_no_indent(lst: list, content: str):\n    \"\"\"recurs into list for string computing \"\"\"\n    if lst:\n        return content + \"\\n\" + str(lst[0]) + \"\\n\" + list_to_str_no_indent(lst[1:], content)", "solution": "\n    for i in lst:\n        if isinstance(i, indentable):\n            content = i.to_str(content, indent)\n        elif isinstance(i, list):\n            content = list_to_str(i, content, indent)\n        elif isinstance(i, str):\n            content = catend(content, i, indent)\n    return content"}
{"prompt": "def add_btn_cancel(self,label_cancel):\n        \"\"\"\n        Adds a cancel button to let the user cancel whatever choice they were given.\n        \n        This widget can be triggered by setting the label ``label_cancel`` to a string.\n        \n        This widget will be positioned slightly below the main label and to the right\n        of the confirm button.\n        \"\"\"", "output": "\n        self.label_cancel = label_cancel\n        self.btn_cancel = tk.Button(self,text=label_cancel,command=self.cancel)\n        self.btn_cancel.pack(side=tk.RIGHT,anchor=tk.NW)\n        self.btn_cancel.bind(\"<Return>\",self.cancel)\n        self.btn_cancel.bind(\"<Escape>\",self.cancel)\n        self.btn_cancel.bind(\"<FocusOut>\",self.cancel)\n        self.btn_cancel.bind(\"<Button-1>\",self.cancel", "solution": "\n        # Cancel Button\n        self.wbtn_cancel = button.Button(\"btn_cancel\",self,self.window,self.peng,\n                        pos=lambda sw,sh, bw,bh: (sw/2+4,sh/2-bh/2-bh*2),\n                        size=[0,0],\n                        label=label_cancel,\n                        borderstyle=self.borderstyle\n                        )\n        self.wbtn_cancel.size = lambda sw,sh: (self.wbtn_cancel._label.font_size*8,self.wbtn_cancel._label.font_size*2)\n        self.addWidget(self.wbtn_cancel)\n        \n        def f():\n            self.doAction(\"cancel\")\n            self.exitDialog()\n        self.wbtn_cancel.addAction(\"click\",f)"}
{"prompt": "def _link_field_to_dict(field):\n        \"\"\" Utility for ripping apart github's Link header field.\n        It's kind of ugly.\n        \"\"\"", "output": "\n        parts = field.split(';')\n        url = parts[0].strip()\n        params = {}\n        for part in parts[1:]:\n            param = part.split('=')\n            params[param[0].strip()] = param[1].strip().strip('\"')\n        return url, params\n\n    def _parse_link_header(header):\n        \"\"\" Parse a link header into a dictionary of lists.\n        \"\"\"\n        links = {}\n        for field in header.split(','):\n            url, params = _link_field", "solution": "\n\n        if not field:\n            return dict()\n\n        return dict([\n            (\n                part.split('; ')[1][5:-1],\n                part.split('; ')[0][1:-1],\n            ) for part in field.split(', ')\n        ])"}
{"prompt": "def _get_protobuf_kind(kind):\n    \"\"\"Converts py_zipkin's Kind to Protobuf's Kind.\n\n    :param kind: py_zipkin's Kind.\n    :type kind: py_zipkin.Kind\n    :return: correcponding protobuf's kind value.\n    :rtype: zipkin_pb2.Span.Kind\n    \"\"\"", "output": "\n    if kind == Kind.CLIENT:\n        return zipkin_pb2.Span.CLIENT\n    elif kind == Kind.SERVER:\n        return zipkin_pb2.Span.SERVER\n    elif kind == Kind.PRODUCER:\n        return zipkin_pb2.Span.PRODUCER\n    elif kind == Kind.CONSUMER:\n        return zipkin_pb2.Span.CONSUMER\n    else:\n        raise ValueError('Unknown kind: {}'.format(kind))\n\n\ndef _get_protobuf_annotation_value", "solution": "\n    if kind == Kind.CLIENT:\n        return zipkin_pb2.Span.CLIENT\n    elif kind == Kind.SERVER:\n        return zipkin_pb2.Span.SERVER\n    elif kind == Kind.PRODUCER:\n        return zipkin_pb2.Span.PRODUCER\n    elif kind == Kind.CONSUMER:\n        return zipkin_pb2.Span.CONSUMER\n    return None"}
{"prompt": "def _assert_recur_is_tail(node: Node) -> None:  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the tail position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` node.\"\"\"", "output": "\n    if isinstance(node, DoNode):\n        assert node.is_synthetic or node.is_tail, (\n            \"recur form must be in tail position of synthetic do node\"\n        )\n    elif isinstance(node, IfNode):\n        assert (\n            node.then_node is None or _assert_recur_is_tail(node.then_node)\n        ), \"recur form must be in tail position of synthetic do node\"\n        assert (\n            node.else_node is None or _assert_recur_", "solution": "\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)"}
{"prompt": "def remove_namespaces(root):\n    \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\"", "output": "\n    for e in root.getiterator():\n        e.tag = etree.QName(e).localname\n        if e.text is None:\n            e.text = \"\"\n        if e.tail is None:\n            e.tail = \"\"\n        if e.attrib:\n            for a in list(e.attrib):\n                if a.startswith(\"{\"):\n                    del e.attrib[a]\n\n\ndef get_text(elem):\n    \"\"\"Get the text of an element, including tail text\"\"\"\n    text =", "solution": "\n    for elem in root.getiterator():\n        if not hasattr(elem.tag, 'find'):\n            continue\n\n        i = elem.tag.find('}')\n        if i >= 0:\n            elem.tag = elem.tag[i + 1:]\n\n    objectify.deannotate(root, cleanup_namespaces=True)"}
{"prompt": "def _block_tuple(iterator, dtypes, bsize=-1):\n    \"\"\"Pack rdd of tuples as tuples of arrays or scipy.sparse matrices.\"\"\"", "output": "\n    if bsize == -1:\n        bsize = len(iterator)\n    if len(dtypes) == 1:\n        return iterator.map(lambda x: tuple(x))\n    else:\n        return iterator.map(lambda x: tuple(\n            _block_array(x, dtypes[i], bsize) for i in range(len(dtypes))))\n\n\ndef _block_array(iterator, dtype, bsize=-1):\n    \"\"\"Pack rdd of arrays as arrays or scipy.sparse matrices.\"\"\"\n    if b", "solution": "\n    i = 0\n    blocked_tuple = None\n    for tuple_i in iterator:\n        if blocked_tuple is None:\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n\n        if (bsize > 0) and (i >= bsize):\n            yield tuple(_pack_accumulated(x, dtype)\n                        for x, dtype in zip(blocked_tuple, dtypes))\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n            i = 0\n\n        for x_j, x in zip(tuple_i, blocked_tuple):\n            x.append(x_j)\n        i += 1\n    if i > 0:\n        yield tuple(_pack_accumulated(x, dtype)\n                    for x, dtype in zip(blocked_tuple, dtypes))"}
{"prompt": "def ddos_custom_policies(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-11-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_11_01.operations.DdosCustomPoliciesOperations>`\n           * 2018-12-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_12_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-02-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`\n        \"\"\"", "output": "2_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-02-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-04-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_04_01.oper", "solution": "\n        api_version = self._get_api_version('ddos_custom_policies')\n        if api_version == '2018-11-01':\n            from .v2018_11_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2018-12-01':\n            from .v2018_12_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2019-02-01':\n            from .v2019_02_01.operations import DdosCustomPoliciesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def update(self):\n        \"\"\"Update repository from its remote.\n\n        Calling this method, the repository will be synchronized with\n        the remote repository using 'fetch' command for 'heads' refs.\n        Any commit stored in the local copy will be removed; refs\n        will be overwritten.\n\n        :raises RepositoryError: when an error occurs updating the\n            repository\n        \"\"\"", "output": "\n        try:\n            self.git.fetch(self.remote, self.remote_branch,\n                           prune=True, update_heads=True)\n        except GitCommandError as e:\n            raise RepositoryError(e)\n\n    def checkout(self, branch):\n        \"\"\"Checkout a branch.\n\n        :param branch: branch name\n        :type branch: str\n        :raises RepositoryError: when an error occurs checking out the\n            branch\n        \"\"\"\n        try:\n            self.git.checkout(branch)\n       ", "solution": "\n        cmd_update = ['git', 'fetch', 'origin', '+refs/heads/*:refs/heads/*', '--prune']\n        self._exec(cmd_update, cwd=self.dirpath, env=self.gitenv)\n\n        logger.debug(\"Git %s repository updated into %s\",\n                     self.uri, self.dirpath)"}
{"prompt": "def setZeroResettableKWH(self, password=\"00000000\"):\n        \"\"\" Serial call to zero resettable kWh registers.\n\n        Args:\n            password (str): Optional password.\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"", "output": "\n        return self.setZeroResettableKWH(password)\n\n    def setZeroResettableKWH(self, password=\"00000000\"):\n        \"\"\" Serial call to zero resettable kWh registers.\n\n        Args:\n            password (str): Optional password.\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"\n        return self.setZeroResettableKWH(password)\n\n    def setZeroResettableKWH(self, password=\"00000000", "solution": "\n        result = False\n        self.setContext(\"setZeroResettableKWH\")\n        try:\n            if not self.requestA():\n                self.writeCmdMsg(\"Bad read CRC on setting\")\n            else:\n                if not self.serialCmdPwdAuth(password):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_str = \"0157310230304433282903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success: 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def add_heart_failure_handler(self, handler):\n        \"\"\"add a new handler for heart failure\"\"\"", "output": "\n        self._heart_failure_handlers.append(handler)\n\n    def remove_heart_failure_handler(self, handler):\n        \"\"\"remove a handler for heart failure\"\"\"\n        self._heart_failure_handlers.remove(handler)\n\n    def add_heart_beat_handler(self, handler):\n        \"\"\"add a new handler for heart beat\"\"\"\n        self._heart_beat_handlers.append(handler)\n\n    def remove_heart_beat_handler(self, handler):\n        \"\"\"remove a handler", "solution": "\n        self.log.debug(\"heartbeat::new heart failure handler: %s\", handler)\n        self._failure_handlers.add(handler)"}
{"prompt": "def like_hashtag(self, hashtag, amount=None):\n    \"\"\" Likes last medias from hashtag \"\"\"", "output": "\n    if amount is None:\n        amount = self.max_likes_per_day\n    if self.api.login_status != 1:\n        self.api.login()\n    hashtag = hashtag.strip(\"#\")\n    self.api.get_hashtag_info(hashtag)\n    medias = self.api.get_hashtag_medias(hashtag)\n    if len(medias) == 0:\n        return False\n    for media in medias:\n        if amount == 0:", "solution": "\n    self.logger.info(\"Going to like media with hashtag #%s.\" % hashtag)\n    medias = self.get_total_hashtag_medias(hashtag, amount)\n    return self.like_medias(medias)"}
{"prompt": "def __coord_fft_hz(n, sr=22050, **_kwargs):\n    \"\"\"Get the frequencies for FFT bins\"\"\"", "output": "\n    return np.linspace(0, sr / 2, n)\n\n\ndef __coord_fft_bin(n, sr=22050, **_kwargs):\n    \"\"\"Get the bin numbers for FFT frequencies\"\"\"\n    return np.arange(n)\n\n\ndef __coord_fft_db(n, sr=22050, **_kwargs):\n    \"\"\"Get the dB values for FFT frequencies\"\"\"\n    return 20 * np.log10(n)\n\n\ndef __", "solution": "\n    n_fft = 2 * (n - 1)\n    # The following code centers the FFT bins at their frequencies\n    # and clips to the non-negative frequency range [0, nyquist]\n    basis = core.fft_frequencies(sr=sr, n_fft=n_fft)\n    fmax = basis[-1]\n    basis -= 0.5 * (basis[1] - basis[0])\n    basis = np.append(np.maximum(0, basis), [fmax])\n    return basis"}
{"prompt": "def main(ctx, log_level, keeper_hostname, username, password):\n    \"\"\"ltd is a command-line client for LSST the Docs.\n\n    Use ltd to upload new site builds, and to work with the LTD Keeper API.\n    \"\"\"", "output": "\n    import logging\n    import sys\n\n    from ltd.client import KeeperClient\n    from ltd.client import KeeperClientError\n    from ltd.client import KeeperClientTimeoutError\n    from ltd.client import KeeperClientUnauthorizedError\n    from ltd.client import KeeperClientUnknownError\n\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)s %(levelname)s %(message)s',\n        datefmt='%Y-%m", "solution": "\n    ch = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s %(levelname)8s %(name)s | %(message)s')\n    ch.setFormatter(formatter)\n\n    logger = logging.getLogger('ltdconveyor')\n    logger.addHandler(ch)\n    logger.setLevel(log_level.upper())\n\n    # Subcommands should use the click.pass_obj decorator to get this\n    # ctx.obj object as the first argument.\n    ctx.obj = {\n        'keeper_hostname': keeper_hostname,\n        'username': username,\n        'password': password,\n        'token': None\n    }"}
{"prompt": "def _fetch_from_archive(self, method, args):\n        \"\"\"Fetch data from the archive\n\n        :param method: the name of the command to execute\n        :param args: the arguments required by the command\n        \"\"\"", "output": "\n        # TODO: check if the archive is available\n        # TODO: check if the archive is up to date\n        # TODO: check if the archive is accessible\n        # TODO: check if the archive is valid\n        # TODO: check if the archive is not corrupted\n        # TODO: check if the archive is not empty\n        # TODO: check if the archive is not broken\n        # TODO: check if the archive is not malicious\n        # TODO: check if the archive is not malformed\n        # TODO: check if the archive is not malicious\n        # TODO: check", "solution": "\n        if not self.archive:\n            raise ArchiveError(cause=\"Archive not provided\")\n\n        data = self.archive.retrieve(method, args, None)\n\n        if isinstance(data, nntplib.NNTPTemporaryError):\n            raise data\n\n        return data"}
{"prompt": "def attach_dynamic_tree_items(tree_alias, src_tree_items):\n        \"\"\"Attaches dynamic sitetrees items registered with `register_dynamic_trees()`\n        to an initial (source) items list.\n\n        :param str|unicode tree_alias:\n        :param list src_tree_items:\n        :rtype: list\n        \"\"\"", "output": "\n        if tree_alias not in _dynamic_trees:\n            return src_tree_items\n\n        for item in _dynamic_trees[tree_alias]:\n            src_tree_items.append(item)\n\n        return src_tree_items\n\n    def attach_dynamic_tree_items_to_request(request, tree_alias, src_tree_items):\n        \"\"\"Attaches dynamic sitetrees items registered with `register_dynamic_trees()`\n        to an initial (source) items list.\n\n        :param HttpRequest request:\n        :param", "solution": "\n        if not _DYNAMIC_TREES:\n            return src_tree_items\n\n        # This guarantees that a dynamic source stays intact,\n        # no matter how dynamic sitetrees are attached.\n        trees = deepcopy(_DYNAMIC_TREES)\n\n        items = []\n        if not src_tree_items:\n            if _IDX_ORPHAN_TREES in trees and tree_alias in trees[_IDX_ORPHAN_TREES]:\n                for tree in trees[_IDX_ORPHAN_TREES][tree_alias]:\n                    items.extend(tree.dynamic_items)\n        else:\n\n            # TODO Seems to be underoptimized %)\n\n            # Tree item attachment by alias.\n            for static_item in list(src_tree_items):\n                items.append(static_item)\n                if not static_item.alias:\n                    continue\n\n                idx = _IDX_TPL % (tree_alias, static_item.alias)\n                if idx not in trees:\n                    continue\n\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    for dyn_item in tree.dynamic_items:\n                        if dyn_item.parent is None:\n                            dyn_item.parent = static_item\n                        # Unique IDs are required for the same trees attached\n                        # to different parents.\n                        dyn_item.id = generate_id_for(dyn_item)\n                        items.append(dyn_item)\n\n            # Tree root attachment.\n            idx = _IDX_TPL % (tree_alias, None)\n            if idx in _DYNAMIC_TREES:\n                trees = deepcopy(_DYNAMIC_TREES)\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    items.extend(tree.dynamic_items)\n\n        return items"}
{"prompt": "def snip_print(str,width = 75,print_full = 0,header = ''):\n    \"\"\"Print a string snipping the midsection to fit in width.\n\n    print_full: mode control:\n      - 0: only snip long strings\n      - 1: send to page() directly.\n      - 2: snip long strings and ask for full length viewing with page()\n    Return 1 if snipping was necessary, 0 otherwise.\"\"\"", "output": "\n    if len(str) <= width:\n        if print_full:\n            page(str)\n        else:\n            print(str)\n        return 0\n    else:\n        if print_full:\n            page(str)\n        else:\n            print(str[:width/2]+'...'+str[-width/2:])\n        return 1\n\ndef snip_print_list(lst,width = 75,print_full = 0,header = ''):\n    \"\"\"Print a list of strings snipping the midsection to fit", "solution": "\n\n    if print_full == 1:\n        page(header+str)\n        return 0\n\n    print header,\n    if len(str) < width:\n        print str\n        snip = 0\n    else:\n        whalf = int((width -5)/2)\n        print str[:whalf] + ' <...> ' + str[-whalf:]\n        snip = 1\n    if snip and print_full == 2:\n        if raw_input(header+' Snipped. View (y/n)? [N]').lower() == 'y':\n            page(str)\n    return snip"}
{"prompt": "def _choi_to_chi(data, input_dim, output_dim):\n    \"\"\"Transform Choi representation to the Chi representation.\"\"\"", "output": "\n    if input_dim != output_dim:\n        raise ValueError(\"Input and output dimensions must be equal.\")\n    if input_dim != 2:\n        raise ValueError(\"Input and output dimensions must be 2.\")\n\n    data = np.array(data)\n    if data.shape != (input_dim, input_dim, output_dim, output_dim):\n        raise ValueError(\"Invalid shape of the Choi representation.\")\n\n    data = np.reshape(data, (input_dim, input_dim, output_dim, output_dim))", "solution": "\n    num_qubits = int(np.log2(input_dim))\n    return _transform_to_pauli(data, num_qubits)"}
{"prompt": "def rst_msg(self, text, opts={}):\n        \"\"\"Convert ReStructuredText and run through msg()\"\"\"", "output": "\n        return self.msg(text, opts, 'rst')\n\n    def rst_msg_block(self, text, opts={}):\n        \"\"\"Convert ReStructuredText and run through msg_block()\"\"\"\n        return self.msg_block(text, opts, 'rst')\n\n    def rst_msg_block_list(self, text, opts={}):\n        \"\"\"Convert ReStructuredText and run through msg_block_list()\"\"\"\n        return self.msg_block_list(text, opts, 'rst')\n", "solution": "\n        text = Mformat.rst_text(text,\n                                'plain' == self.debugger.settings['highlight'],\n                                self.debugger.settings['width'])\n        return self.msg(text)"}
{"prompt": "def measure_topology(fbasename=None, log=None, ml_version=ml_version):\n    \"\"\"Measures mesh topology\n\n    Args:\n        fbasename (str): input filename.\n        log (str): filename to log output\n\n    Returns:\n        dict: dictionary with the following keys:\n            vert_num (int): number of vertices\n            edge_num (int): number of edges\n            face_num (int): number of faces\n            unref_vert_num (int): number or unreferenced vertices\n            boundry_edge_num (int): number of boundary edges\n            part_num (int): number of parts (components) in the mesh.\n            manifold (bool): True if mesh is two-manifold, otherwise false.\n            non_manifold_edge (int): number of non_manifold edges.\n            non_manifold_vert (int): number of non-manifold verices\n            genus (int or str): genus of the mesh, either a number or\n                'undefined' if the mesh is non-manifold.\n            holes (int or str): number of holes in the mesh, either a number\n                or 'undefined' if the mesh is non-manifold.\n\n    \"\"\"", "output": "_vert_num (int): number of boundary vertices\n            unref_edge_num (int): number of unreferenced edges\n            unref_face_num (int): number of unreferenced faces\n            unref_edge_num (int): number of unreferenced edges\n            unref_face_num (int): number of unreferenced faces\n            unref_vert_num (int): number of unreferenced vertices\n            unref_edge_num (int): number of unreferenced edges\n            unref_face_num (int): number of unrefer", "solution": "\n    ml_script1_file = 'TEMP3D_measure_topology.mlx'\n    ml_script1 = mlx.FilterScript(file_in=fbasename, ml_version=ml_version)\n    compute.measure_topology(ml_script1)\n    ml_script1.save_to_file(ml_script1_file)\n    ml_script1.run_script(log=log, script_file=ml_script1_file)\n    topology = ml_script1.topology\n    return topology"}
{"prompt": "def import_submodule(mod, subname, fullname):\n    \"\"\"m = import_submodule(mod, subname, fullname)\"\"\"", "output": "\n    import importlib\n    return importlib.import_module('.' + subname, mod.__name__)\n\n\ndef import_submodules(mod, subnames, fullname):\n    \"\"\"m = import_submodules(mod, subnames, fullname)\"\"\"\n    import importlib\n    for subname in subnames:\n        importlib.import_module('.' + subname, mod.__name__)\n\n\ndef import_submodules_from_file(mod, subnames, fullname):\n    \"\"\"m = import_submodules_from_file", "solution": "\n    # Require:\n    # if mod == None: subname == fullname\n    # else: mod.__name__ + \".\" + subname == fullname\n\n    global found_now\n    if fullname in found_now and fullname in sys.modules:\n        m = sys.modules[fullname]\n    else:\n        print 'Reloading', fullname\n        found_now[fullname] = 1\n        oldm = sys.modules.get(fullname, None)\n\n        if mod is None:\n            path = None\n        elif hasattr(mod, '__path__'):\n            path = mod.__path__\n        else:\n            return None\n\n        try:\n            # This appears to be necessary on Python 3, because imp.find_module()\n            # tries to import standard libraries (like io) itself, and we don't\n            # want them to be processed by our deep_import_hook.\n            with replace_import_hook(original_import):\n                fp, filename, stuff = imp.find_module(subname, path)\n        except ImportError:\n            return None\n\n        try:\n            m = imp.load_module(fullname, fp, filename, stuff)\n        except:\n            # load_module probably removed name from modules because of\n            # the error.  Put back the original module object.\n            if oldm:\n                sys.modules[fullname] = oldm\n            raise\n        finally:\n            if fp: fp.close()\n\n        add_submodule(mod, m, fullname, subname)\n\n    return m"}
{"prompt": "def velocity_from_bundle(self, bundle):\n        \"\"\"[DEPRECATED] Return velocity, given the `coefficient_bundle()` return value.\"\"\"", "output": "\n        warnings.warn(\n            \"velocity_from_bundle() is deprecated. Use velocity_from_bundle_coefficients() instead.\",\n            DeprecationWarning,\n        )\n        return self.velocity_from_bundle_coefficients(bundle)\n\n    def velocity_from_bundle_coefficients(self, bundle):\n        \"\"\"Return velocity, given the `coefficient_bundle()` return value.\"\"\"\n        return np.dot(self.basis, bundle)\n\n    def velocity_from_bundle_coefficients_derivative(", "solution": "\n\n        coefficients, days_per_set, T, twot1 = bundle\n        coefficient_count = coefficients.shape[2]\n\n        # Chebyshev derivative:\n\n        dT = np.empty_like(T)\n        dT[0] = 0.0\n        dT[1] = 1.0\n        dT[2] = twot1 + twot1\n        for i in range(3, coefficient_count):\n            dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]\n        dT *= 2.0\n        dT /= days_per_set\n\n        return (dT.T * coefficients).sum(axis=2)"}
{"prompt": "def update(self):\n        \"\"\"Update cameras and motion settings with latest from API.\"\"\"", "output": "\n        self._update_cameras()\n        self._update_motion_settings()\n\n    def _update_cameras(self):\n        \"\"\"Update cameras with latest from API.\"\"\"\n        self.cameras = []\n        for camera in self._api.cameras:\n            if camera.id in self._api.camera_settings:\n                self.cameras.append(\n                    Camera(\n                        camera,\n                        self._api.camera_settings[camera.id],\n                        self._api.camera_pres", "solution": "\n        cameras = self._api.camera_list()\n        self._cameras_by_id = {v.camera_id: v for i, v in enumerate(cameras)}\n\n        motion_settings = []\n        for camera_id in self._cameras_by_id.keys():\n            motion_setting = self._api.camera_event_motion_enum(camera_id)\n            motion_settings.append(motion_setting)\n\n        self._motion_settings_by_id = {\n            v.camera_id: v for i, v in enumerate(motion_settings)}"}
{"prompt": "def get_refs(self, location):\n        \"\"\"Return map of named refs (branches or tags) to commit hashes.\"\"\"", "output": "\n        refs = {}\n        for line in self.git(\"for-each-ref\", \"--format=%(refname:short) %(objectname)\", \"refs/heads\", \"refs/tags\"):\n            ref, commit = line.split()\n            refs[ref] = commit\n        return refs\n\n    def get_tags(self):\n        \"\"\"Return map of tag names to commit hashes.\"\"\"\n        return self.get_refs(\"refs/tags\")\n\n    def get_branches(self):\n        \"\"\"Return map of branch names", "solution": "\n        output = call_subprocess([self.cmd, 'show-ref'],\n                                 show_stdout=False, cwd=location)\n        rv = {}\n        for line in output.strip().splitlines():\n            commit, ref = line.split(' ', 1)\n            ref = ref.strip()\n            ref_name = None\n            if ref.startswith('refs/remotes/'):\n                ref_name = ref[len('refs/remotes/'):]\n            elif ref.startswith('refs/heads/'):\n                ref_name = ref[len('refs/heads/'):]\n            elif ref.startswith('refs/tags/'):\n                ref_name = ref[len('refs/tags/'):]\n            if ref_name is not None:\n                rv[ref_name] = commit.strip()\n        return rv"}
{"prompt": "def main():\n    \"\"\"The main function. Instantiates a GameState object and then\n    enters a REPL-like main loop, waiting for input, updating the state\n    based on the input, then outputting the new state.\"\"\"", "output": "\n    state = GameState()\n    while True:\n        state.print_state()\n        input_ = input(\"> \")\n        if input_ == \"quit\":\n            break\n        elif input_ == \"help\":\n            print(\"Commands:\")\n            print(\"  help - print this message\")\n            print(\"  quit - quit the game\")\n            print(\"  move <direction> - move in the given direction\")\n            print(\"  look - look at the current location\")\n            print(\"  inventory - list the items you are carrying\")\n            print(\"", "solution": "\n\n    state = GameState()\n    print(state)\n    while state.running:\n        input = get_single_char()\n\n        state, should_advance = state.handle_input(input)\n        if should_advance:\n            state = state.advance_robots()\n            state = state.check_game_end()\n\n        print(state)\n\n    print(state.message)"}
{"prompt": "def delete_policy_name(self, name):\n        \"\"\"**Description**\n            Delete the policy with the given name.\n\n        **Arguments**\n            - name: the name of the policy to delete\n\n        **Success Return Value**\n            The JSON object representing the now-deleted policy.\n\n        **Example**\n            `examples/delete_policy.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_policy.py>`_\n\n        \"\"\"", "output": "\n        return self.delete_policy(name=name)\n\n    def delete_policy(self, name=None, id=None):\n        \"\"\"**Description**\n            Delete the policy with the given name or id.\n\n        **Arguments**\n            - name: the name of the policy to delete\n            - id: the id of the policy to delete\n\n        **Success Return Value**\n            The JSON object representing the now-deleted policy.\n\n        **Example**\n            `examples/delete_policy.py <https://github.com/draios/python-", "solution": "\n        res = requests.get(self.url + '/api/policies', headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        # Find the policy with the given name and delete it\n        for policy in res.json()[\"policies\"]:\n            if policy[\"name\"] == name:\n                return self.delete_policy_id(policy[\"id\"])\n\n        return [False, \"No policy with name {}\".format(name)]"}
{"prompt": "def share_vm_image(self, vm_image_name, permission):\n        \"\"\"\n        Share an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to be able to call this.\n\n        vm_image_name:\n            The name of the virtual machine image to share\n        permission:\n            The sharing permission: public, msdn, or private\n        \"\"\"", "output": "\n        self.vm_image_service.share_vm_image(vm_image_name, permission)\n\n    def unshare_vm_image(self, vm_image_name):\n        \"\"\"\n        Unshare an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to be able to call this.\n\n        vm_image_name:\n            The name of the virtual machine image to unshare\n        \"\"\"\n        self.vm_image_service.unshare_vm_image", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('permission', permission)\n\n        path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n        query = '&permission=' + permission\n        path = path + '?' + query.lstrip('&')\n\n        return self._perform_put(\n            path, None, as_async=True, x_ms_version='2015-04-01'\n        )"}
{"prompt": "def get_stats(self, username='', password='', organization='llnl', force=True):\n        \"\"\"\n        Retrieves the traffic for the users of the given organization.\n        Requires organization admin credentials token to access the data.\n        \"\"\"", "output": "\n        if not force and self.stats:\n            return self.stats\n        self.stats = {}\n        if not username or not password:\n            return self.stats\n        try:\n            response = requests.get(\n                'https://api.github.com/orgs/{}/people'.format(organization),\n                auth=(username, password),\n                headers={'Accept': 'application/vnd.github.v3+json'})\n            response.raise_for_status()\n            for user in response.json():\n                self.stats[user['", "solution": "\n        date = str(datetime.date.today())\n        referrers_file_path =  ('../github_stats_output/referrers.csv')\n        views_file_path =  ('../github_stats_output/views.csv')\n        clones_file_path =  ('../github_stats_output/clones.csv')\n        if force or not os.path.isfile(file_path):\n            my_github.login(username, password)\n            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1\n            print 'Rate Limit: ' + str(calls_beginning)\n            my_github.get_org(organization)\n            my_github.get_traffic()\n            views_row_count = my_github.check_data_redundancy(file_path=views_file_path,\n                dict_to_check=self.views)\n            clones_row_count = my_github.check_data_redundancy(file_path=clones_file_path,\n                dict_to_check=self.clones)\n            my_github.write_to_file(referrers_file_path=referrers_file_path,\n                views_file_path=views_file_path,\n                clones_file_path=clones_file_path,\n                views_row_count=views_row_count,\n                clones_row_count=clones_row_count)\n            my_github.write_json(dict_to_write=self.referrers_json,\n                path_ending_type='traffic_popular_referrers')\n            my_github.write_json(dict_to_write=self.views_json,\n                path_ending_type='traffic_views')\n            my_github.write_json(dict_to_write=self.clones_json,\n                path_ending_type='traffic_clones')\n            my_github.write_json(dict_to_write=self.releases_json,\n                path_ending_type='releases')\n            calls_remaining = self.logged_in_gh.ratelimit_remaining\n            calls_used = calls_beginning - calls_remaining\n            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\\nUsed '\n                + str(calls_used) + ' API calls.')"}
{"prompt": "def save_setting(self, setting_name, value):\n        \"\"\"Saves the setting value into the database.\"\"\"", "output": "\n        self.settings[setting_name] = value\n        self.save_settings()\n\n    def save_settings(self):\n        \"\"\"Saves all the settings into the database.\"\"\"\n        self.settings.save()\n\n    def get_setting(self, setting_name):\n        \"\"\"Returns the setting value from the database.\"\"\"\n        return self.settings[setting_name]\n\n    def get_settings(self):\n        \"\"\"Returns all the settings from the database.\"\"\"\n        return self.settings\n\n    def get_setting_names(", "solution": "\n        setting = self.get_setting(setting_name)\n        if setting is None:\n            setting = models.DashboardWidgetSettings.objects.create(\n                widget_name=self.get_name(),\n                setting_name=setting_name,\n                value=value)\n        setting.value = value\n        setting.save()\n        return setting"}
{"prompt": "def get_extended_key_usage_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the extendedKeyUsage\n    extension.\n    \"\"\"", "output": "\n    ext = certificate.extensions.get_extension_for_oid(\n        x509.oid.ExtensionOID.EXTENDED_KEY_USAGE\n    )\n    if ext is None:\n        return None\n    return ext.value\n\n\ndef get_key_usage_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the keyUsage extension.\n    \"\"\"\n    ext = certificate.extensions.get_extension_for_oid(\n        x509.oid.ExtensionO", "solution": "\n    try:\n        return certificate.extensions.get_extension_for_oid(\n            x509.oid.ExtensionOID.EXTENDED_KEY_USAGE\n        ).value\n    except x509.ExtensionNotFound:\n        return None"}
{"prompt": "def join_html_attrs(attrs):\n    \"\"\"Joins the map structure into HTML attributes.\n\n    The return value is a 2-tuple ``(template, ordered_values)``. It should be\n    passed into :class:`markupsafe.Markup` to prevent XSS attacked.\n\n    e.g.::\n\n        >>> join_html_attrs({'href': '/', 'data-active': 'true'})\n        ('data-active=\"{0}\" href=\"{1}\"', ['true', '/'])\n    \"\"\"", "output": "\n    template = ''\n    values = []\n    for key, value in attrs.items():\n        if value is None:\n            continue\n        template += ' {0}=\"{1}\"'.format(key, value)\n        values.append(value)\n    return template, values\n\n\ndef join_html_classes(classes):\n    \"\"\"Joins the list structure into HTML classes.\n\n    The return value is a 2-tuple ``(template, ordered_values)``. It should be\n    passed into :class:`markupsafe.Markup` to", "solution": "\n    attrs = collections.OrderedDict(freeze_dict(attrs or {}))\n    template = ' '.join('%s=\"{%d}\"' % (k, i) for i, k in enumerate(attrs))\n    return template, list(attrs.values())"}
{"prompt": "def restore_sys_module_state(self):\n        \"\"\"Restore the state of the sys module.\"\"\"", "output": "\n        sys.modules['sys'] = self.sys_module\n\n    def restore_builtins_module_state(self):\n        \"\"\"Restore the state of the builtins module.\"\"\"\n        sys.modules['builtins'] = self.builtins_module\n\n    def restore_os_module_state(self):\n        \"\"\"Restore the state of the os module.\"\"\"\n        sys.modules['os'] = self.os_module\n\n    def restore_os_path_module_state(self):\n        \"\"\"Restore the state of the os.path", "solution": "\n        try:\n            for k, v in self._orig_sys_module_state.iteritems():\n                setattr(sys, k, v)\n        except AttributeError:\n            pass\n        # Reset what what done in self.init_sys_modules\n        if self._orig_sys_modules_main_mod is not None:\n            sys.modules[self._orig_sys_modules_main_name] = self._orig_sys_modules_main_mod"}
{"prompt": "def dequantize_flow(dx, dy, max_val=0.02, denorm=True):\n    \"\"\"Recover from quantized flow.\n\n    Args:\n        dx (ndarray): Quantized dx.\n        dy (ndarray): Quantized dy.\n        max_val (float): Maximum value used when quantizing.\n        denorm (bool): Whether to multiply flow values with width/height.\n\n    Returns:\n        ndarray: Dequantized flow.\n    \"\"\"", "output": "\n    dx = dx.astype(np.float32)\n    dy = dy.astype(np.float32)\n    dx = dx * max_val\n    dy = dy * max_val\n    if denorm:\n        dx = dx * dx.shape[1]\n        dy = dy * dy.shape[0]\n    return dx, dy\n\n\ndef quantize_flow(flow, max_val=0.02, denorm=True):\n    \"\"\"Quantize flow.\n\n    Args:\n        flow (ndarray", "solution": "\n    assert dx.shape == dy.shape\n    assert dx.ndim == 2 or (dx.ndim == 3 and dx.shape[-1] == 1)\n\n    dx, dy = [dequantize(d, -max_val, max_val, 255) for d in [dx, dy]]\n\n    if denorm:\n        dx *= dx.shape[1]\n        dy *= dx.shape[0]\n    flow = np.dstack((dx, dy))\n    return flow"}
{"prompt": "def addCases(self, tupesValStmnts):\n        \"\"\"\n        Add multiple case statements from iterable of tuleles\n        (caseVal, statements)\n        \"\"\"", "output": "\n        for caseVal, statements in tupesValStmnts:\n            self.addCase(caseVal, statements)\n\n    def addCase(self, caseVal, statements):\n        \"\"\"\n        Add a case statement\n        \"\"\"\n        if not isinstance(statements, list):\n            statements = [statements]\n        self.cases.append((caseVal, statements))\n\n    def getCase(self, caseVal):\n        \"\"\"\n        Get a case statement\n        \"\"\"\n        for caseVal_, statements in self.cases:\n            if case", "solution": "\n        s = self\n        for val, statements in tupesValStmnts:\n            s = s.Case(val, statements)\n        return s"}
{"prompt": "def create_endpoint(port=None, service_name=None, host=None, use_defaults=True):\n    \"\"\"Creates a new Endpoint object.\n\n    :param port: TCP/UDP port. Defaults to 0.\n    :type port: int\n    :param service_name: service name as a str. Defaults to 'unknown'.\n    :type service_name: str\n    :param host: ipv4 or ipv6 address of the host. Defaults to the\n    current host ip.\n    :type host: str\n    :param use_defaults: whether to use defaults.\n    :type use_defaults: bool\n    :returns: zipkin Endpoint object\n    \"\"\"", "output": "\n    :param use_defaults: if True, uses the default values for the\n    parameters. Defaults to True.\n    :type use_defaults: bool\n    :return: Endpoint object\n    :rtype: Endpoint\n    \"\"\"\n    if use_defaults:\n        if port is None:\n            port = 0\n        if service_name is None:\n            service_name = 'unknown'\n        if host is None:\n            host = get_host_ip()\n\n    return Endpoint(port, service_name, host)\n", "solution": "\n    if use_defaults:\n        if port is None:\n            port = 0\n        if service_name is None:\n            service_name = 'unknown'\n        if host is None:\n            try:\n                host = socket.gethostbyname(socket.gethostname())\n            except socket.gaierror:\n                host = '127.0.0.1'\n\n    ipv4 = None\n    ipv6 = None\n\n    if host:\n        # Check ipv4 or ipv6.\n        try:\n            socket.inet_pton(socket.AF_INET, host)\n            ipv4 = host\n        except socket.error:\n            # If it's not an ipv4 address, maybe it's ipv6.\n            try:\n                socket.inet_pton(socket.AF_INET6, host)\n                ipv6 = host\n            except socket.error:\n                # If it's neither ipv4 or ipv6, leave both ip addresses unset.\n                pass\n\n    return Endpoint(\n        ipv4=ipv4,\n        ipv6=ipv6,\n        port=port,\n        service_name=service_name,\n    )"}
{"prompt": "def main():\n    \"\"\"\n    Parse argv for options and arguments, and start schema generation.\n    \"\"\"", "output": "\n    import argparse\n    import sys\n\n    parser = argparse.ArgumentParser(description='Generate a schema from a set of files.')\n    parser.add_argument('-o', '--output', help='Output file.', default='schema.json')\n    parser.add_argument('-d', '--directory', help='Directory to search for files.', default='.')\n    parser.add_argument('-e', '--extension', help='File extension to search for.', default='.py')\n    parser.add_argument('-v',", "solution": "\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path...]\",\n                                   version=xtuml.version.complete_string,\n                                   formatter=optparse.TitledHelpFormatter())\n                                   \n    parser.set_description(__doc__.strip())\n    \n    parser.add_option(\"-c\", \"--component\", dest=\"component\", metavar=\"NAME\",\n                      help=\"export sql schema for the component named NAME\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-d\", \"--derived-attributes\", dest=\"derived\",\n                      help=\"include derived attributes in the schema\",\n                      action=\"store_true\", default=False)\n    \n    parser.add_option(\"-o\", \"--output\", dest='output', metavar=\"PATH\",\n                      help=\"save sql schema to PATH (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity', action=\"count\", \n                      help=\"increase debug logging level\", default=2)\n\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or opts.output is None:\n        parser.print_help()\n        sys.exit(1)\n\n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n\n    loader = ooaofooa.Loader()\n    for filename in args:\n        loader.filename_input(filename)\n\n    c = loader.build_component(opts.component, opts.derived)\n    xtuml.persist_database(c, opts.output)"}
{"prompt": "def disconnect(self, mol):\n        \"\"\"Break covalent bonds between metals and organic atoms under certain conditions.\n\n        The algorithm works as follows:\n\n        - Disconnect N, O, F from any metal.\n        - Disconnect other non-metals from transition metals + Al (but not Hg, Ga, Ge, In, Sn, As, Tl, Pb, Bi, Po).\n        - For every bond broken, adjust the charges of the begin and end atoms accordingly.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: The molecule with metals disconnected.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"", "output": "\n        :type mol: :class:`~chempy.molecule.Molecule`\n        :return: The disconnected molecule.\n        :rtype: :class:`~chempy.molecule.Molecule`\n        \"\"\"\n\n        # Disconnect N, O, F from any metal\n        for atom in mol.atoms:\n            if atom.element.number == 7 or atom.element.number == 8 or atom.element.number == 9:\n                if atom.bonds:\n                    for bond", "solution": "\n        log.debug('Running MetalDisconnector')\n        # Remove bonds that match SMARTS\n        for smarts in [self._metal_nof, self._metal_non]:\n            pairs = mol.GetSubstructMatches(smarts)\n            rwmol = Chem.RWMol(mol)\n            orders = []\n            for i, j in pairs:\n                # TODO: Could get the valence contributions of the bond instead of GetBondTypeAsDouble?\n                orders.append(int(mol.GetBondBetweenAtoms(i, j).GetBondTypeAsDouble()))\n                rwmol.RemoveBond(i, j)\n            # Adjust neighbouring charges accordingly\n            mol = rwmol.GetMol()\n            for n, (i, j) in enumerate(pairs):\n                chg = orders[n]\n                atom1 = mol.GetAtomWithIdx(i)\n                atom1.SetFormalCharge(atom1.GetFormalCharge() + chg)\n                atom2 = mol.GetAtomWithIdx(j)\n                atom2.SetFormalCharge(atom2.GetFormalCharge() - chg)\n                log.info('Removed covalent bond between %s and %s', atom1.GetSymbol(), atom2.GetSymbol())\n        Chem.SanitizeMol(mol)\n        return mol"}
{"prompt": "def accept_alert(self, text=None, wait=None):\n        \"\"\"\n        Execute the wrapped code, accepting an alert.\n\n        Args:\n            text (str | RegexObject, optional): Text to match against the text in the modal.\n            wait (int | float, optional): Maximum time to wait for the modal to appear after\n                executing the wrapped code.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been found.\n        \"\"\"", "output": "\n        self.execute_code(wait=wait)\n        self.accept_modal(text=text)\n\n    def dismiss_alert(self, text=None, wait=None):\n        \"\"\"\n        Execute the wrapped code, dismissing an alert.\n\n        Args:\n            text (str | RegexObject, optional): Text to match against the text in the modal.\n            wait (int | float, optional): Maximum time to wait for the modal to appear after\n                executing the wrapped code.\n\n        Raises:\n            ModalNotFound: If a", "solution": "\n\n        wait = wait or capybara.default_max_wait_time\n        with self.driver.accept_modal(\"alert\", text=text, wait=wait):\n            yield"}
{"prompt": "def build_type(s_dt):\n    \"\"\"\n    Build a partial xsd tree out of a S_DT and its sub types S_CDT, S_EDT, S_SDT and S_UDT.\n    \"\"\"", "output": "\n    if s_dt.s_cdt:\n        s_cdt = s_dt.s_cdt\n        s_cdt.s_dt = s_dt\n        s_cdt.s_dt_id = s_dt.s_dt_id\n        s_cdt.s_dt_name = s_dt.s_dt_name\n        s_cdt.s_dt_type = s_dt.s_dt_type\n        s_cdt.s_dt_version = s_dt.s_dt_", "solution": "\n    s_cdt = nav_one(s_dt).S_CDT[17]()\n    if s_cdt:\n        return build_core_type(s_cdt)\n    \n    s_edt = nav_one(s_dt).S_EDT[17]()\n    if s_edt:\n        return build_enum_type(s_edt)\n    \n    s_udt = nav_one(s_dt).S_UDT[17]()\n    if s_udt:\n        return build_user_type(s_udt)"}
{"prompt": "def _get_obj_count_difference(objs1, objs2):\n    \"\"\"Returns count difference in two collections of Python objects.\"\"\"", "output": "\n    return len(objs1) - len(objs2)\n\n\ndef _get_obj_count_difference_str(objs1, objs2):\n    \"\"\"Returns count difference in two collections of Python objects.\"\"\"\n    return \"{} objects in the first collection, {} objects in the second collection\".format(\n        len(objs1), len(objs2)\n    )\n\n\ndef _get_obj_count_difference_str_with_diff(objs1, objs2):\n    \"\"\"Returns count difference in", "solution": "\n    clean_obj_list1 = _process_in_memory_objects(objs1)\n    clean_obj_list2 = _process_in_memory_objects(objs2)\n    obj_count_1 = _get_object_count_by_type(clean_obj_list1)\n    obj_count_2 = _get_object_count_by_type(clean_obj_list2)\n    return obj_count_1 - obj_count_2"}
{"prompt": "def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"", "output": "\n        return self._api_options\n\n    @api_options.setter\n    def api_options(self, value):\n        \"\"\"\n        current spelling settings\n        :param value: api options as number\n        \"\"\"\n        self._api_options = value\n\n    @property\n    def api_options_string(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as string\n        \"\"\"\n        return self._api_options_string\n\n    @api_options_string.setter\n    def api_options_string(", "solution": "\n        options = 0\n        if self._ignore_uppercase:\n            options |= 1\n        if self._ignore_digits:\n            options |= 2\n        if self._ignore_urls:\n            options |= 4\n        if self._find_repeat_words:\n            options |= 8\n        if self._ignore_latin:\n            options |= 16\n        if self._flag_latin:\n            options |= 128\n        if self._by_words:\n            options |= 256\n        if self._ignore_capitalization:\n            options |= 512\n        if self._ignore_roman_numerals:\n            options |= 2048\n        return options"}
{"prompt": "def get_all_intervals(self):\n        \"\"\"Returns the true list of intervals including the empty intervals.\"\"\"", "output": "\n        intervals = []\n        for i in range(self.size):\n            intervals.append(self.get_interval(i))\n        return intervals\n\n    def get_interval(self, index):\n        \"\"\"Returns the interval at the given index.\"\"\"\n        if index < 0 or index >= self.size:\n            raise IndexError(\"Index out of range.\")\n        return self.intervals[index]\n\n    def get_interval_at_position(self, position):\n        \"\"\"Returns the interval at the given position.\"\"\"\n        if position <", "solution": "\n        ints = sorted(self.get_intervals(True))\n        if self.tier_type == 'IntervalTier':\n            if not ints:\n                ints.append((self.xmin, self.xmax, ''))\n            else:\n                if ints[0][0] > self.xmin:\n                    ints.insert(0, (self.xmin, ints[0][0], ''))\n                if ints[-1][1] < self.xmax:\n                    ints.append((ints[-1][1], self.xmax, ''))\n                p = ints[-1]\n                for index, i in reversed(list(enumerate(ints[:-1], 1))):\n                    if p[0] - i[1] != 0:\n                        ints.insert(index, (i[1], p[0], ''))\n                    p = i\n        return ints"}
{"prompt": "def info_update(self, obj_id, data):\n\t\t\"\"\"Update metadata with of a specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type.\"\"\"", "output": "\n\t\treturn self.request('PUT', '/%s/%s' % (self.object_type, obj_id), data)\n\n\tdef info_delete(self, obj_id):\n\t\t\"\"\"Delete metadata of a specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type.\"\"\"\n\t\treturn self.request('DELETE', '/%s/%s' % (self.object_type", "solution": "\n\t\treturn self(obj_id, method='put', data=data, auth_header=True)"}
{"prompt": "def hide_error_message(self):\n        \"\"\"\n        This function hides the error message when all values are correct.\n        \"\"\"", "output": "\n        self.error_message.setVisible(False)\n\n    def show_error_message(self):\n        \"\"\"\n        This function shows the error message when all values are incorrect.\n        \"\"\"\n        self.error_message.setVisible(True)\n\n    def clear_fields(self):\n        \"\"\"\n        This function clears all the fields.\n        \"\"\"\n        self.name_field.clear()\n        self.email_field.clear()\n        self.password_field.clear()\n        self.confirm_password_field.clear()\n\n", "solution": "\n        self.ui.error_label.setScaledContents(False)  # Warning image hiden.\n        self.ui.error_text_label.hide()"}
{"prompt": "def conditional_entropy(X, Y, base=2):\n    \"\"\"Calculates the conditional entropy, H(X|Y), in the given base\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    Y: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    base: integer (default: 2)\n        The base in which to calculate conditional entropy\n\n    Returns\n    ----------\n    conditional_entropy: float\n        The conditional entropy calculated according to the equation H(X|Y) = H(X,Y) - H(Y)\n\n    \"\"\"", "output": ": float\n        The conditional entropy of X given Y\n    \"\"\"\n    # Calculate the joint entropy\n    joint_entropy = entropy(X, Y, base=base)\n\n    # Calculate the entropy of Y\n    entropy_y = entropy(Y, base=base)\n\n    # Calculate the conditional entropy\n    conditional_entropy = joint_entropy - entropy_y\n\n    return conditional_entropy\n\n\ndef mutual_information(X, Y, base=2):\n    \"\"\"Calculates the mutual information, I(X;Y),", "solution": "\n    return joint_entropy(X, Y, base=base) - entropy(Y, base=base)"}
{"prompt": "def update_policy(self, id, policy):\n        \"\"\" Create policy.\n\n            https://www.nomadproject.io/api/acl-policies.html\n\n            arguments:\n                - name\n                - policy\n            returns: request.Response\n\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.URLNotFoundNomadException\n        \"\"\"", "output": "\n        params = {\n            'policy': policy,\n        }\n        return self._put(f\"{self.endpoint}/{id}\", params=params)\n\n    def delete_policy(self, id):\n        \"\"\" Delete policy.\n\n            https://www.nomadproject.io/api/acl-policies.html\n\n            arguments:\n                - id\n            returns: request.Response\n\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.URLNotFound", "solution": "\n        return self.request(\"policy\", id, json=policy, method=\"post\")"}
{"prompt": "def mark(self, value=1):\n        \"\"\"Record an event with the derive.\n\n        :param value: counter value to record\n        \"\"\"", "output": "\n        self._check_state()\n        self._client.record_gauge(self._name, value, tags=self._tags)\n\n    def set_function(self, function):\n        \"\"\"Set a function to be called when the gauge is marked.\n\n        :param function: function to be called when the gauge is marked\n        \"\"\"\n        self._check_state()\n        self._function = function\n\n    def _check_state(self):\n        \"\"\"Check if the gauge is in a valid state.\n\n        :raises: :class:`Runtime", "solution": "\n        last = self.last.get_and_set(value)\n        if last <= value:\n            value = value - last\n        super(Derive, self).mark(value)"}
{"prompt": "def __fetch_pull_requests(self, from_date, to_date):\n        \"\"\"Fetch the pull requests\"\"\"", "output": "\n        pull_requests = []\n        for pull_request in self.client.pull_requests.list(\n                state='all',\n                sort='created',\n                direction='asc',\n                created_after=from_date,\n                created_before=to_date):\n            pull_requests.append(pull_request)\n        return pull_requests\n\n    def __fetch_commits(self, from_date, to_date):\n        \"\"\"Fetch the commits\"\"\"\n        commits = []\n        for commit in self.client.commits.list(\n", "solution": "\n\n        raw_pulls = self.client.pulls(from_date=from_date)\n        for raw_pull in raw_pulls:\n            pull = json.loads(raw_pull)\n\n            if str_to_datetime(pull['updated_at']) > to_date:\n                return\n\n            self.__init_extra_pull_fields(pull)\n            for field in TARGET_PULL_FIELDS:\n\n                if not pull[field]:\n                    continue\n\n                if field == 'user':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'merged_by':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'review_comments':\n                    pull[field + '_data'] = self.__get_pull_review_comments(pull['number'])\n                elif field == 'requested_reviewers':\n                    pull[field + '_data'] = self.__get_pull_requested_reviewers(pull['number'])\n                elif field == 'commits':\n                    pull[field + '_data'] = self.__get_pull_commits(pull['number'])\n\n            yield pull"}
{"prompt": "def connect(src, *destinations, exclude: set=None, fit=False):\n    \"\"\"\n    Connect src (signals/interfaces/values) to all destinations\n\n    :param exclude: interfaces on any level on src or destinations\n        which should be excluded from connection process\n    :param fit: auto fit source width to destination width\n    \"\"\"", "output": "\n    if exclude is None:\n        exclude = set()\n\n    if isinstance(src, Signal):\n        src = [src]\n\n    for destination in destinations:\n        if isinstance(destination, Signal):\n            destination = [destination]\n\n        for dest in destination:\n            if dest in exclude:\n                continue\n\n            if isinstance(dest, Signal):\n                if fit:\n                    src[0].fit_to(dest)\n                src[0].connect(dest)\n            elif isinstance(dest, Interface):\n", "solution": "\n    assignemnts = []\n\n    if isinstance(src, HObjList):\n        for dst in destinations:\n            assert len(src) == len(dst), (src, dst)\n        _destinations = [iter(d) for d in destinations]\n        for _src in src:\n            dsts = [next(d) for d in _destinations]\n            assignemnts.append(connect(_src, *dsts, exclude=exclude, fit=fit))\n    else:\n        for dst in destinations:\n            assignemnts.append(_connect(src, dst, exclude, fit))\n\n    return assignemnts"}
{"prompt": "def addLogicalInterfaceToDeviceType(self, typeId, logicalInterfaceId):\n        \"\"\"\n        Adds a logical interface to a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - logicalInterfaceId (string) - the id returned by the platform on creation of the logical interface\n            - description (string) - optional (not used)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        url = self.url + \"/device/type/\" + typeId + \"/logicalInterface/\" + logicalInterfaceId\n        self.logger.debug(\"url: %s\", url)\n        self.logger.debug(\"payload: %s\", logicalInterfaceId)\n        r = requests.put(url, data=logicalInterfaceId, headers=self.header)\n        self.logger.debug(\"r.text: %s\", r.text)\n        if r.status_code == 200:\n            return True\n        else:\n            raise APIException(\"", "solution": "\n        req = ApiClient.allDeviceTypeLogicalInterfacesUrl % (self.host, \"/draft\", typeId)\n        body = {\"id\" : logicalInterfaceId}\n#       body = {\"name\" : \"required but not used!!!\", \"id\" : logicalInterfaceId, \"schemaId\" : schemaId}\n#       if description:\n#           body[\"description\"] = description\n        resp = requests.post(req, auth=self.credentials, headers={\"Content-Type\":\"application/json\"}, data=json.dumps(body),\n                        verify=self.verify)\n        if resp.status_code == 201:\n            self.logger.debug(\"Logical interface added to a device type\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error adding logical interface to a device type\", resp)\n        return resp.json()"}
{"prompt": "def repository(self, owner, repository):\n        \"\"\"Fetch information about a repository.\"\"\"", "output": "\n        return self.get(\n            '/repos/{owner}/{repository}'.format(\n                owner=owner, repository=repository\n            )\n        )\n\n    def repository_branches(self, owner, repository):\n        \"\"\"Fetch branches for a repository.\"\"\"\n        return self.get(\n            '/repos/{owner}/{repository}/branches'.format(\n                owner=owner, repository=repository\n            )\n        )\n\n    def repository_branch(self, owner, repository, branch):\n        \"\"\"Fetch a branch for a repository.\"\"\"\n       ", "solution": "\n\n        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)\n\n        logger.debug(\"DockerHub client requests: %s\", url)\n\n        response = self.fetch(url)\n\n        return response.text"}
{"prompt": "def target_address(self):\n        \"\"\"Return the authorative target of the link.\"\"\"", "output": "\n        return self.target\n\n    @property\n    def target_type(self):\n        \"\"\"Return the type of the target.\"\"\"\n        return self.target.type\n\n    @property\n    def target_name(self):\n        \"\"\"Return the name of the target.\"\"\"\n        return self.target.name\n\n    @property\n    def target_address(self):\n        \"\"\"Return the authorative target of the link.\"\"\"\n        return self.target.address\n\n    @property\n    def target_address_type(self):\n        \"\"\"Return the", "solution": "\n        # If link is a receiver, target is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_receiver:\n            return self._pn_link.target.address\n        else:\n            return self._pn_link.remote_target.address"}
{"prompt": "def recv_unsub(self, id_=None):\n        \"\"\"DDP unsub handler.\"\"\"", "output": "\n        if id_ is None:\n            id_ = self.id\n        self.send_unsub(id_)\n        self.subscriptions.pop(id_)\n\n    def send_unsub(self, id_):\n        \"\"\"DDP unsub handler.\"\"\"\n        self.send_message(\n            msg_type=\"unsub\",\n            msg_id=id_,\n        )\n\n    def send_ping(self):\n        \"\"\"DDP ping handler.\"\"\"\n        self.send_message(\n            msg_type=\"ping\",\n       ", "solution": "\n        if id_:\n            self.api.unsub(id_)\n        else:\n            self.reply('nosub')"}
{"prompt": "def score(infile, outfile, classifier, xgb_autotune, apply_weights, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test):\n    \"\"\"\n    Conduct semi-supervised learning and error-rate estimation for MS1, MS2 and transition-level data. \n    \"\"\"", "output": "lfdr_method, lfdr_smooth_df, lfdr_smooth_log_pi0, lfdr_pi0_lambda, lfdr_pi0_method, lfdr_pi0_smooth_df, lfdr_pi0_smooth_log_pi0, lfdr_ss_initial_fdr, lfdr_ss_iteration_fdr, lfdr_ss_num_iter, lfdr_ss_main_score, lfdr_group_id, lfdr_param", "solution": "\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    # Prepare XGBoost-specific parameters\n    xgb_hyperparams = {'autotune': xgb_autotune, 'autotune_num_rounds': 10, 'num_boost_round': 100, 'early_stopping_rounds': 10, 'test_size': 0.33}\n\n    xgb_params = {'eta': 0.3, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': 1, 'alpha': 0, 'scale_pos_weight': 1, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    xgb_params_space = {'eta': hp.uniform('eta', 0.0, 0.3), 'gamma': hp.uniform('gamma', 0.0, 0.5), 'max_depth': hp.quniform('max_depth', 2, 8, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': hp.uniform('lambda', 0.0, 1.0), 'alpha': hp.uniform('alpha', 0.0, 1.0), 'scale_pos_weight': 1.0, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    if not apply_weights:\n        PyProphetLearner(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test).run()\n    else:\n        PyProphetWeightApplier(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test, apply_weights).run()"}
{"prompt": "def cleanup(controller, engines):\n    \"\"\"Cleanup routine to shut down all subprocesses we opened.\"\"\"", "output": "\n    for engine in engines:\n        engine.cleanup()\n    controller.cleanup()\n\n\ndef main():\n    \"\"\"Main routine.\"\"\"\n    # Create the controller.\n    controller = Controller()\n\n    # Create the engines.\n    engines = []\n    engines.append(Engine(controller, \"engine1\", \"engine1.py\"))\n    engines.append(Engine(controller, \"engine2\", \"engine2.py\"))\n\n    # Start the engines.\n    for engine in engines:\n        engine.start()\n\n    # Wait for the", "solution": "\n    import signal, time\n    \n    print('Starting cleanup')\n    print('Stopping engines...')\n    for e in engines:\n        e.send_signal(signal.SIGINT)\n    print('Stopping controller...')\n    # so it can shut down its queues\n    controller.send_signal(signal.SIGINT)\n    time.sleep(0.1)\n    print('Killing controller...')\n    controller.kill()\n    print('Cleanup done')"}
{"prompt": "def reset(self):\n        \"\"\"Reset the input buffer and associated state.\"\"\"", "output": "\n        self.buffer = []\n        self.buffer_index = 0\n        self.buffer_size = 0\n        self.buffer_offset = 0\n        self.buffer_length = 0\n        self.buffer_length_limit = 0\n        self.buffer_length_limit_set = False\n        self.buffer_length_limit_exceeded = False\n        self.buffer_length_limit_exceeded_count = 0\n        self.buffer_length_limit_exceeded_count_limit = 0\n        self", "solution": "\n        super(IPythonInputSplitter, self).reset()\n        self._buffer_raw[:] = []\n        self.source_raw = ''\n        self.cell_magic_parts = []\n        self.processing_cell_magic = False"}
{"prompt": "def for_type_by_name(type_module, type_name, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the module and name of a type\n    rather than the type object itself.\n    \"\"\"", "output": "\n    type_module = importlib.import_module(type_module)\n    type_obj = getattr(type_module, type_name)\n    return for_type(type_obj, func)\n\n\ndef for_type(type_obj, func):\n    \"\"\"\n    Add a pretty printer for a type.\n    \"\"\"\n    if not isinstance(type_obj, type):\n        raise TypeError(\"type_obj must be a type\")\n    if not callable(func):\n        raise TypeError(\"func must be callable\")\n    if not has", "solution": "\n    key = (type_module, type_name)\n    oldfunc = _deferred_type_pprinters.get(key, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _deferred_type_pprinters[key] = func\n    return oldfunc"}
{"prompt": "def domain(self, domain=None, last_domain=None):\n        \"\"\"\n        Manage the case that we want to test only a domain.\n\n        :param domain: The domain or IP to test.\n        :type domain: str\n\n        :param last_domain:\n            The last domain to test if we are testing a file.\n        :type last_domain: str\n\n        :param return_status: Tell us if we need to return the status.\n        :type return_status: bool\n        \"\"\"", "output": "\n        if domain is None:\n            domain = self.domain\n\n        if last_domain is None:\n            last_domain = self.last_domain\n\n        if domain is None:\n            return\n\n        if last_domain is None:\n            last_domain = domain\n\n        if self.domain_type == \"file\":\n            self.domain = domain\n            self.last_domain = last_domain\n            self.domain_type = \"domain\"\n            self.domain_file = None\n            self.domain_file_path = None\n            self", "solution": "\n\n        # We print the header.\n        self._print_header()\n\n        if domain:\n            # A domain is given.\n\n            # We format and set the domain we are testing and treating.\n            PyFunceble.INTERN[\"to_test\"] = self._format_domain(domain)\n        else:\n            # A domain is not given.\n\n            # We set the domain we are testing and treating to None.\n            PyFunceble.INTERN[\"to_test\"] = None\n\n        if PyFunceble.INTERN[\"to_test\"]:\n            # The domain is given (Not None).\n\n            if PyFunceble.CONFIGURATION[\"syntax\"]:\n                # The syntax mode is activated.\n\n                # We get the status from Syntax.\n                status = self.syntax_status.get()\n            else:\n                # We test and get the status of the domain.\n                status, _ = self.status.get()\n\n            # We run the file decision logic.\n            self._file_decision(PyFunceble.INTERN[\"to_test\"], last_domain, status)\n\n            if PyFunceble.CONFIGURATION[\"simple\"]:\n                # The simple mode is activated.\n\n                # We print the domain and the status.\n                print(PyFunceble.INTERN[\"to_test\"], status)\n\n            # We return the tested domain and its status.\n            return PyFunceble.INTERN[\"to_test\"], status\n\n        # We return None, there is nothing to test.\n        return None"}
{"prompt": "def varimp(self, use_pandas=False):\n        \"\"\"\n        Pretty print the variable importances, or return them in a list/pandas DataFrame.\n\n        :param bool use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n\n        :returns: A dictionary of lists or Pandas DataFrame instances.\n        \"\"\"", "output": "\n        if use_pandas:\n            return pd.DataFrame(self.varimp, index=self.feature_names)\n        else:\n            return self.varimp\n\n    def get_booster(self):\n        \"\"\"\n        Get the underlying XGBoost Booster object.\n\n        :returns: The underlying XGBoost Booster object.\n        \"\"\"\n        return self.model\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the XGBoost model.\n\n        :param params: The parameters", "solution": "\n        return {model.model_id: model.varimp(use_pandas) for model in self.models}"}
{"prompt": "def statementsAreSame(statements: List[HdlStatement]) -> bool:\n    \"\"\"\n    :return: True if all statements are same\n    \"\"\"", "output": "\n    if not statements:\n        return True\n    return all(s == statements[0] for s in statements[1:])\n\n\ndef statementsAreSameWithoutTypes(statements: List[HdlStatement]) -> bool:\n    \"\"\"\n    :return: True if all statements are same without types\n    \"\"\"\n    if not statements:\n        return True\n    return all(s.with_types(None) == statements[0].with_types(None)\n               for s in statements[1:])\n\n\ndef statementsAreSameWithoutTypesAnd", "solution": "\n    iterator = iter(statements)\n    try:\n        first = next(iterator)\n    except StopIteration:\n        return True\n\n    return all(first.isSame(rest) for rest in iterator)"}
{"prompt": "def get_meta_netnode():\n    \"\"\"\n    Get the netnode used to store settings metadata in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"", "output": "\n    return idc.get_meta_netnode()\n\n\ndef get_meta_netnode_by_id(id):\n    \"\"\"\n    Get the netnode used to store settings metadata in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"\n    return idc.get_meta_netnode_by_id(id)\n\n\ndef get_meta_netnode_by_name(name):\n    \"\"\"\n    Get the netnode used to store settings metadata in the current IDB.\n", "solution": "\n    node_name = \"$ {org:s}.{application:s}\".format(\n        org=IDA_SETTINGS_ORGANIZATION,\n        application=IDA_SETTINGS_APPLICATION)\n    return netnode.Netnode(node_name)"}
{"prompt": "def setPin(self, old_pin, new_pin):\n        \"\"\"\n        C_SetPIN\n\n        :param old_pin: old PIN\n        :param new_pin: new PIN\n        \"\"\"", "output": "\n        self.send_command(C_SetPIN, old_pin, new_pin)\n\n    def setPin2(self, old_pin, new_pin):\n        \"\"\"\n        C_SetPIN2\n\n        :param old_pin: old PIN\n        :param new_pin: new PIN\n        \"\"\"\n        self.send_command(C_SetPIN2, old_pin, new_pin)\n\n    def getVersion(self):\n        \"\"\"\n        C_GetVersion\n        \"\"\"\n        return self.send", "solution": "\n        old_pin1 = ckbytelist(old_pin)\n        new_pin1 = ckbytelist(new_pin)\n        rv = self.lib.C_SetPIN(self.session, old_pin1, new_pin1)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def divide(df, new_column, column_1, column_2):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"", "output": "\n    df[new_column] = df[column_1] / df[column_2]\n    return df\n\n\ndef divide_by_constant(df, new_column, column, constant):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"\n    df[new_column] = df[column] / constant\n    return df\n\n\ndef multiply(df, new_column, column_1, column_2):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"\n", "solution": "\n    return _basic_math_operation(df, new_column, column_1, column_2, op='truediv')"}
{"prompt": "def _setup_template_file(self, template_file_path):\n        \"\"\" Setup self.template\n\n        Parameters\n        ----------\n        template_file_path: str\n            Document template file path.\n        \"\"\"", "output": "\n        self.template = DocumentTemplate(template_file_path)\n\n    def _setup_template_string(self, template_string):\n        \"\"\" Setup self.template\n\n        Parameters\n        ----------\n        template_string: str\n            Document template string.\n        \"\"\"\n        self.template = DocumentTemplate(template_string)\n\n    def _setup_template_dict(self, template_dict):\n        \"\"\" Setup self.template\n\n        Parameters\n        ----------\n        template_dict: dict\n            Document template dict.\n", "solution": "\n        try:\n            template_file = template_file_path\n            template_env = get_environment_for(template_file_path)\n            template = template_env.get_template(os.path.basename(template_file))\n        except:\n            raise\n        else:\n            self._template_file = template_file\n            self._template_env = template_env\n            self.template = template"}
{"prompt": "def pxconfig(self, line):\n        \"\"\"configure default targets/blocking for %px magics\"\"\"", "output": "\n        if line.strip() == '':\n            self.px_config = {}\n            return\n        try:\n            key, value = line.split('=')\n        except ValueError:\n            self.px_config = {}\n            return\n        self.px_config[key.strip()] = value.strip()\n\n    def px(self, line):\n        \"\"\"%px <target> [<blocking>]\n\n        Execute a pexpect command.\n\n        If <target> is not specified, the default target is used.\n       ", "solution": "\n        args = magic_arguments.parse_argstring(self.pxconfig, line)\n        if args.targets:\n            self.view.targets = self._eval_target_str(args.targets)\n        if args.block is not None:\n            self.view.block = args.block\n        if args.set_verbose is not None:\n            self.verbose = args.set_verbose"}
{"prompt": "def l(*members, meta=None) -> List:\n    \"\"\"Creates a new list from members.\"\"\"", "output": "\n    return List(members, meta)\n\n\ndef l_(*members, meta=None) -> List:\n    \"\"\"Creates a new list from members.\"\"\"\n    return List(members, meta)\n\n\ndef l_from_dict(d: dict, meta=None) -> List:\n    \"\"\"Creates a new list from a dictionary.\"\"\"\n    return List(d.items(), meta)\n\n\ndef l_from_list(l: list, meta=None) -> List:\n    \"\"\"Creates a new list from a list.\"\"\"\n    return", "solution": "\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )"}
{"prompt": "def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n    \"\"\"wrapper\"\"\"", "output": "\n    if re.match(r'https?://www.fc2.com/', url):\n        return fc2_download(url, output_dir, merge, info_only, **kwargs)\n    elif re.match(r'https?://www.pixiv.net/', url):\n        return pixiv_download(url, output_dir, merge, info_only, **kwargs)\n    elif re.match(r'https?://www.nicovideo.jp/', url):\n        return nico_download(url, output_dir", "solution": "\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/tw/content/20151021bTVKnbEw'\n    hostname = urlparse(url).hostname\n    if not ('fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname):\n        return False\n    upid = match1(url, r'.+/content/(\\w+)')\n\n    fc2video_download_by_upid(upid, output_dir, merge, info_only)"}
{"prompt": "def tobin(deci_num, len=32):\n    \"\"\"\n    Given a decimal number, returns a string bitfield of length = len\n    Example: given deci_num = 1 and len = 10, it return 0000000001\n    \"\"\"", "output": "\n    bin_num = bin(deci_num)[2:]\n    return bin_num.zfill(len)\n\n\ndef tobin_list(deci_nums, len=32):\n    \"\"\"\n    Given a list of decimal numbers, returns a list of bitfields of length = len\n    Example: given deci_nums = [1, 2, 3] and len = 10, it return [0000000001, 0000000010, 00000", "solution": "\n    bitstr = \"\".join(map(lambda y: str((deci_num >> y) & 1), range(len - 1, -1, -1)))\n    return bitstr"}
{"prompt": "def export_mt_variants(variants, sample_id):\n    \"\"\"Export mitochondrial variants for a case to create a MT excel report\n\n    Args:\n        variants(list): all MT variants for a case, sorted by position\n        sample_id(str) : the id of a sample within the case\n\n    Returns:\n        document_lines(list): list of lines to include in the document\n    \"\"\"", "output": "\n    document_lines = []\n    document_lines.append(\n        \"### Mitochondrial variants for sample {0}\".format(sample_id))\n    document_lines.append(\"| Variant | Allele |\")\n    document_lines.append(\"| ------- | ------ |\")\n    for variant in variants:\n        document_lines.append(\n            \"| {0} | {1} |\".format(variant.get_variant_id(),\n                                   variant.get_variant_id().split(\"_\")[1]))\n", "solution": "\n    document_lines = []\n    for variant in variants:\n        line = []\n        position = variant.get('position')\n        change = '>'.join([variant.get('reference'),variant.get('alternative')])\n        line.append(position)\n        line.append(change)\n        line.append(str(position)+change)\n        genes = []\n        prot_effect = []\n        for gene in variant.get('genes'):\n            genes.append(gene.get('hgnc_symbol',''))\n            for transcript in gene.get('transcripts'):\n                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n        line.append(','.join(prot_effect))\n        line.append(','.join(genes))\n        ref_ad = ''\n        alt_ad = ''\n        for sample in variant['samples']:\n            if sample.get('sample_id') == sample_id:\n                ref_ad = sample['allele_depths'][0]\n                alt_ad = sample['allele_depths'][1]\n        line.append(ref_ad)\n        line.append(alt_ad)\n        document_lines.append(line)\n    return document_lines"}
{"prompt": "def write_sky_params_to_file(self):\n        \"\"\"Writes the params to file that skytool_Free needs to generate the sky radiance distribution.\"\"\"", "output": "\n        # Write the params to file\n        with open(self.sky_params_file, 'w') as f:\n            f.write(str(self.sky_params))\n\n    def write_sky_params_to_file(self):\n        \"\"\"Writes the params to file that skytool_Free needs to generate the sky radiance distribution.\"\"\"\n        # Write the params to file\n        with open(self.sky_params_file, 'w') as f:\n            f.write(str(self.sky_params))\n\n    def write", "solution": "\n\n        inp_file = self.sky_file + '_params.txt'\n        lg.info('Writing Inputs to file : ' + inp_file)\n\n        f = open(inp_file, 'w')\n\n        f.write('verbose= ' + str(self.verbose) + '\\n')\n        f.write('band_count= ' + str(self.num_bands) + '\\n')\n        f.write('band_centres_data= ')\n        f.write(\",\".join([str(wave) for wave in self.wavelengths]) + '\\n')\n        f.write('partition= ' + self.partition + '\\n')\n        f.write('vn= ' + str(self.vn) + '\\n')\n        f.write('hn= ' + str(self.hn) + '\\n')\n        f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.write('theta_points= ')\n        f.write(\",\".join([str(theta) for theta in self.theta_points]) + '\\n')\n        f.write('type= ' + self.sky_type + '\\n')\n        f.write('azimuth= ' + str(self.sky_azimuth) + '\\n')\n        f.write('zenith= ' + str(self.sky_zenith) + '\\n')\n        f.write('sky_save_fp= ' + inp_file.strip('_params.txt') + '\\n')\n        f.write('sky_image_save_fp= ' + self.sky_file + '.ppm' + '\\n')\n        f.write('sky_image_size= 256' + '\\n')\n        if self.sky_type == 'hlideal':\n            f.write('C= ' + str(self.sky_c) + '\\n')\n            f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.flush()\n        f.close()"}
{"prompt": "def defrag(filt, threshold=3, mode='include'):\n    \"\"\"\n    'Defragment' a filter.\n\n    Parameters\n    ----------\n    filt : boolean array\n        A filter\n    threshold : int\n        Consecutive values equal to or below this threshold\n        length are considered fragments, and will be removed.\n    mode : str\n        Wheter to change False fragments to True ('include')\n        or True fragments to False ('exclude')\n\n    Returns\n    -------\n    defragmented filter : boolean array\n    \"\"\"", "output": "\n    if mode not in ['include', 'exclude']:\n        raise ValueError(\"mode must be 'include' or 'exclude'\")\n\n    # Find the fragments\n    fragments = np.where(np.diff(filt) != 1)[0]\n    fragments = np.insert(fragments, 0, 0)\n    fragments = np.append(fragments, len(filt))\n\n    # Remove fragments shorter than threshold\n    fragments = fragments[fragments[1:] - fragments[:-1] >= threshold]\n\n   ", "solution": "\n    if bool_2_indices(filt) is None:\n        return filt\n\n    if mode == 'include':\n        inds = bool_2_indices(~filt) + 1\n        rep = True\n    if mode == 'exclude':\n        inds = bool_2_indices(filt) + 1\n        rep = False\n\n    rem = (np.diff(inds) <= threshold)[:, 0]\n\n    cfilt = filt.copy()\n    if any(rem):\n        for lo, hi in inds[rem]:\n            cfilt[lo:hi] = rep\n\n    return cfilt"}
{"prompt": "def _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku):\n    \"\"\"Build and return Sailthru purchase item object\"\"\"", "output": "\n    item = {\n        'course_id': course_id,\n        'course_url': course_url,\n        'cost_in_cents': cost_in_cents,\n        'mode': mode,\n        'course_data': course_data,\n        'sku': sku,\n    }\n    return item\n\n\ndef _build_purchase_items(course_id, course_url, cost_in_cents, mode, course_data, sku):\n    \"\"\"Build and return list of Sailthru purchase item objects", "solution": "\n\n    # build item description\n    item = {\n        'id': \"{}-{}\".format(course_id, mode),\n        'url': course_url,\n        'price': cost_in_cents,\n        'qty': 1,\n    }\n\n    # get title from course info if we don't already have it from Sailthru\n    if 'title' in course_data:\n        item['title'] = course_data['title']\n    else:\n        # can't find, just invent title\n        item['title'] = 'Course {} mode: {}'.format(course_id, mode)\n\n    if 'tags' in course_data:\n        item['tags'] = course_data['tags']\n\n    # add vars to item\n    item['vars'] = dict(course_data.get('vars', {}), mode=mode, course_run_id=course_id)\n\n    item['vars']['purchase_sku'] = sku\n\n    return item"}
{"prompt": "def _kl_normal_normal(n_a, n_b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n\n  Args:\n    n_a: instance of a Normal distribution object.\n    n_b: instance of a Normal distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_normal_normal\".\n\n  Returns:\n    Batchwise KL(n_a || n_b)\n  \"\"\"", "output": "n_b)\n  \"\"\"\n  with tf.name_scope(name, \"kl_normal_normal\", [n_a.loc, n_a.scale, n_b.loc,\n                                                n_b.scale]):\n    # TODO(b/68008390): Replace with tf.distribution.kl_divergence when it's\n    # fixed.\n    # return tf.distributions.kl_divergence(n_a, n_b)\n    # TODO(b/6800839", "solution": "\n  with tf.name_scope(name or \"kl_normal_normal\"):\n    one = tf.constant(1, dtype=n_a.dtype)\n    two = tf.constant(2, dtype=n_a.dtype)\n    half = tf.constant(0.5, dtype=n_a.dtype)\n    s_a_squared = tf.square(n_a.scale)\n    s_b_squared = tf.square(n_b.scale)\n    ratio = s_a_squared / s_b_squared\n    return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *\n            (ratio - one - tf.math.log(ratio)))"}
{"prompt": "def scan(self, *key_ranges, **kwargs):\n        \"\"\"Scan for FCs in the given id ranges.\n\n        :param key_ranges:\n          ``key_ranges`` should be a list of pairs of ranges. The first\n          value is the lower bound id and the second value is the\n          upper bound id. Use ``()`` in either position to leave it\n          unbounded. If no ``key_ranges`` are given, then all FCs in\n          the store are returned.\n        :param [str] feature_names:\n          A list of feature names to retrieve. When ``None``, all\n          features are retrieved. Wildcards are allowed.\n        :rtype: Iterable of ``(content_id, FC)``\n        \"\"\"", "output": "A list of feature names to return. If not given, all\n          features are returned.\n        :param [str] feature_types:\n          A list of feature types to return. If not given, all\n          features are returned.\n        :param [str] feature_ids:\n          A list of feature ids to return. If not given, all\n          features are returned.\n        :param [str] feature_tags:\n          A list of feature tags to return. If not given, all\n          features are returned.\n        :param [str] feature_tags_not:", "solution": "\n        for hit in self._scan(*key_ranges, **kwargs):\n            yield did(hit['_id']), self.fc_from_dict(hit['_source']['fc'])"}
{"prompt": "def encode_span(self, span):\n        \"\"\"Encodes a single span to protobuf.\"\"\"", "output": "\n        span_pb = self.span_pb_cls()\n        span_pb.span_id = span.span_id\n        span_pb.parent_span_id = span.parent_span_id\n        span_pb.name = span.name\n        span_pb.kind = span.kind\n        span_pb.start_time.FromDatetime(span.start_time)\n        span_pb.end_time.FromDatetime(span.end_time)\n        span_pb.attributes.attributes.extend(\n            [self._encode", "solution": "\n        if not protobuf.installed():\n            raise ZipkinError(\n                'protobuf encoding requires installing the protobuf\\'s extra '\n                'requirements. Use py-zipkin[protobuf] in your requirements.txt.'\n            )\n\n        pb_span = protobuf.create_protobuf_span(span)\n        return protobuf.encode_pb_list([pb_span])"}
{"prompt": "def verilogTypeOfSig(signalItem):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"", "output": "\n    if signalItem.is_register:\n        return \"reg\"\n    else:\n        return \"wire\"\n\n\ndef verilogTypeOfSig(signalItem):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"\n    if signalItem.is_register:\n        return \"reg\"\n    else:\n        return \"wire\"\n\n\ndef verilogTypeOfSig(signalItem):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"\n    if signalItem.is_register:\n        return \"reg\"\n    else:", "solution": "\n    driver_cnt = len(signalItem.drivers)\n    if signalItem._const or driver_cnt > 1 or\\\n       arr_any(signalItem.drivers, _isEventDependentDriver):\n        return SIGNAL_TYPE.REG\n    else:\n        if driver_cnt == 1:\n            d = signalItem.drivers[0]\n            if not isinstance(d, (Assignment, PortItem)):\n                return SIGNAL_TYPE.REG\n\n        return SIGNAL_TYPE.WIRE"}
{"prompt": "def check_for_bucket(self, bucket_name):\n        \"\"\"\n        Check if bucket_name exists.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"", "output": "\n        try:\n            self.s3_client.head_bucket(Bucket=bucket_name)\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                return False\n            else:\n                raise\n        return True\n\n    def create_bucket(self, bucket_name):\n        \"\"\"\n        Create a bucket.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        self.s3_client.create_bucket(B", "solution": "\n        try:\n            self.get_conn().head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False"}
{"prompt": "def get_event_hub(self, hub_name):\n        \"\"\"\n        Retrieves an existing event hub.\n\n        hub_name:\n            Name of the event hub.\n        \"\"\"", "output": "\n        return self.event_hubs.get(hub_name)\n\n    def get_event_hubs(self):\n        \"\"\"\n        Retrieves all event hubs.\n        \"\"\"\n        return self.event_hubs\n\n    def get_event_hub_names(self):\n        \"\"\"\n        Retrieves all event hub names.\n        \"\"\"\n        return self.event_hubs.keys()\n\n    def get_event_hub_names_and_types(self):\n        \"\"\"\n        Retrieves all event hub names and types", "solution": "\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + _str(hub_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)"}
{"prompt": "def get_plot_data(self):\n        \"\"\" Generates the JSON report to plot the gene boxes\n\n        Following the convention of the reports platform, this method returns\n        a list of JSON/dict objects with the information about each entry in\n        the abricate file. The information contained in this JSON is::\n\n            {contig_id: <str>,\n             seqRange: [<int>, <int>],\n             gene: <str>,\n             accession: <str>,\n             coverage: <float>,\n             identity: <float>\n             }\n\n        Note that the `seqRange` entry contains the position in the\n        corresponding contig, not the absolute position in the whole assembly.\n\n        Returns\n        -------\n        json_dic : list\n            List of JSON/dict objects with the report data.\n        \"\"\"", "output": "       Returns\n        -------\n        list\n            A list of JSON objects with the information about each entry in\n            the abricate file.\n        \"\"\"\n        data = []\n        for contig in self.contigs:\n            for gene in contig.genes:\n                data.append({'contig_id': contig.id,\n                             'seqRange': [gene.start, gene.end],\n                             'gene': gene.name,\n                             'accession': gene.accession,\n                             'coverage': gene.cover", "solution": "\n\n        json_dic = {\"plotData\": []}\n        sample_dic = {}\n        sample_assembly_map = {}\n\n        for entry in self.storage.values():\n\n            sample_id = re.match(\"(.*)_abr\", entry[\"log_file\"]).groups()[0]\n            if sample_id not in sample_dic:\n                sample_dic[sample_id] = {}\n\n            # Get contig ID using the same regex as in `assembly_report.py`\n            # template\n            contig_id = self._get_contig_id(entry[\"reference\"])\n            # Get database\n            database = entry[\"database\"]\n            if database not in sample_dic[sample_id]:\n                sample_dic[sample_id][database] = []\n\n            # Update the sample-assembly correspondence dict\n            if sample_id not in sample_assembly_map:\n                sample_assembly_map[sample_id] = entry[\"infile\"]\n\n            sample_dic[sample_id][database].append(\n                {\"contig\": contig_id,\n                 \"seqRange\": entry[\"seq_range\"],\n                 \"gene\": entry[\"gene\"].replace(\"'\", \"\"),\n                 \"accession\": entry[\"accession\"],\n                 \"coverage\": entry[\"coverage\"],\n                 \"identity\": entry[\"identity\"],\n                 },\n            )\n\n        for sample, data in sample_dic.items():\n            json_dic[\"plotData\"].append(\n                {\n                    \"sample\": sample,\n                    \"data\": {\"abricateXrange\": data},\n                    \"assemblyFile\": sample_assembly_map[sample]\n                }\n            )\n\n        return json_dic"}
{"prompt": "def usi(self):\n        \"\"\"\n        Gets an USI string for the move.\n        For example a move from 7A to 8A would be `7a8a` or `7a8a+` if it is\n        a promotion.\n        \"\"\"", "output": "\n        if self.promotion:\n            return \"%s%s%s\" % (self.from_square, self.to_square, self.promotion)\n        else:\n            return \"%s%s\" % (self.from_square, self.to_square)\n\n    def __str__(self):\n        \"\"\"\n        Gets a string representation of the move.\n        \"\"\"\n        if self.promotion:\n            return \"%s%s%s\" % (self.from_square, self.to_square, self.promotion)\n       ", "solution": "\n        if self:\n            if self.drop_piece_type:\n                return '{0}*{1}'.format(PIECE_SYMBOLS[self.drop_piece_type].upper(), SQUARE_NAMES[self.to_square])\n            else:\n                return SQUARE_NAMES[self.from_square] + SQUARE_NAMES[self.to_square] + \\\n                       ('+' if self.promotion else '')\n        else:\n            return '0000'"}
{"prompt": "def load_institute(adapter, internal_id, display_name, sanger_recipients=None):\n    \"\"\"Load a institute into the database\n\n        Args:\n            adapter(MongoAdapter)\n            internal_id(str)\n            display_name(str)\n            sanger_recipients(list(email))\n    \"\"\"", "output": "\n    institute = adapter.institute(internal_id=internal_id)\n    if not institute:\n        institute = adapter.institute(display_name=display_name)\n    if not institute:\n        institute = adapter.institute(internal_id=internal_id, display_name=display_name)\n    if not institute:\n        institute = adapter.institute(internal_id=internal_id, display_name=display_name,\n                                      sanger_recipients=sanger_recipients)\n", "solution": "\n\n    institute_obj = build_institute(\n        internal_id=internal_id,\n        display_name=display_name,\n        sanger_recipients=sanger_recipients\n    )\n    log.info(\"Loading institute {0} with display name {1}\" \\\n             \" into database\".format(internal_id, display_name))\n\n    adapter.add_institute(institute_obj)"}
{"prompt": "def compute_nc(X, G):\n    \"\"\"Computes the novelty curve from the self-similarity matrix X and\n        the gaussian kernel G.\"\"\"", "output": "\n    n = X.shape[0]\n    nc = np.zeros(n)\n    for i in range(n):\n        nc[i] = np.sum(G[:, i])\n    return nc\n\n\ndef compute_nc_from_ssm(X, sigma=1):\n    \"\"\"Computes the novelty curve from the self-similarity matrix X and\n        the gaussian kernel G.\"\"\"\n    n = X.shape[0]\n    G = np.exp(-X / sigma)\n    return compute_", "solution": "\n    N = X.shape[0]\n    M = G.shape[0]\n    nc = np.zeros(N)\n\n    for i in range(M // 2, N - M // 2 + 1):\n        nc[i] = np.sum(X[i - M // 2:i + M // 2, i - M // 2:i + M // 2] * G)\n\n    # Normalize\n    nc += nc.min()\n    nc /= nc.max()\n    return nc"}
{"prompt": "def _get_boot_time():\n    \"\"\"Return system boot time (epoch in seconds)\"\"\"", "output": "\n    return int(open('/proc/uptime').read().split()[0])\n\n\ndef _get_cpu_times():\n    \"\"\"Return system cpu times (user, system, idle, iowait, irq, softirq, steal, guest, guest_nice)\"\"\"\n    with open('/proc/stat') as f:\n        data = f.readline()\n    data = data.split()\n    return (int(data[1]), int(data[2]), int(data[3]), int(data[4]), int(data[5]),", "solution": "\n    f = open('/proc/stat', 'r')\n    try:\n        for line in f:\n            if line.startswith('btime'):\n                return float(line.strip().split()[1])\n        raise RuntimeError(\"line not found\")\n    finally:\n        f.close()"}
{"prompt": "def get_plugin_of_type(self, plugin_class):\n        \"\"\"\n        Retrieve a plugin of desired class, KeyError raised otherwise\n        \"\"\"", "output": "\n        for plugin in self.plugins:\n            if isinstance(plugin, plugin_class):\n                return plugin\n        raise KeyError(\"No plugin of type {} found\".format(plugin_class))\n\n    def get_plugin_by_name(self, plugin_name):\n        \"\"\"\n        Retrieve a plugin by name, KeyError raised otherwise\n        \"\"\"\n        for plugin in self.plugins:\n            if plugin.name == plugin_name:\n                return plugin\n        raise KeyError(\"No plugin with name {} found\".format(plugin_name))\n\n   ", "solution": "\n        logger.debug(\"Searching for plugin: %s\", plugin_class)\n        matches = [plugin for plugin in self.plugins.values() if isinstance(plugin, plugin_class)]\n        if matches:\n            if len(matches) > 1:\n                logger.debug(\n                    \"More then one plugin of type %s found. Using first one.\",\n                    plugin_class)\n            return matches[-1]\n        else:\n            raise KeyError(\"Requested plugin type not found: %s\" % plugin_class)"}
{"prompt": "def calculate_linear_predictor(model_matrix, model_coefficients, offset=None,\n                               name=None):\n  \"\"\"Computes `model_matrix @ model_coefficients + offset`.\"\"\"", "output": "\n  if offset is None:\n    return model_matrix @ model_coefficients\n  else:\n    return model_matrix @ model_coefficients + offset\n\n\ndef calculate_linear_predictor_and_residuals(model_matrix, model_coefficients,\n                                             offset=None, name=None):\n  \"\"\"Computes `model_matrix @ model_coefficients + offset` and residuals.\"\"\"\n  if offset is None:\n    linear_predictor = model_matrix @ model_coefficients\n  else:\n   ", "solution": "\n  with tf.compat.v1.name_scope(name, 'calculate_linear_predictor',\n                               [model_matrix, model_coefficients, offset]):\n    predicted_linear_response = tf.linalg.matvec(model_matrix,\n                                                 model_coefficients)\n    if offset is not None:\n      predicted_linear_response += offset\n    return predicted_linear_response"}
{"prompt": "def render_hero_slider(context):\n    \"\"\"\n    Renders the hero slider.\n\n    \"\"\"", "output": "\n    return context.render_template(\n        \"includes/partials/hero-slider.html\",\n        hero_slides=context.site.hero_slides,\n    )\n\n\ndef render_footer(context):\n    \"\"\"\n    Renders the footer.\n\n    \"\"\"\n    return context.render_template(\n        \"includes/partials/footer.html\",\n        site=context.site,\n    )\n\n\ndef render_footer_copyright(context):\n    \"\"\"\n    Renders the footer copyright.\n\n   ", "solution": "\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    return {\n        'slider_items': qs,\n    }"}
{"prompt": "def get_color(self, color, intensity=0):\n        \"\"\" Returns a QColor for a given color code, or None if one cannot be\n            constructed.\n        \"\"\"", "output": "\n        if color is None:\n            return None\n        if isinstance(color, str):\n            color = color.lower()\n            if color in self.colors:\n                color = self.colors[color]\n            else:\n                return None\n        if isinstance(color, tuple):\n            color = QColor(*color)\n        elif isinstance(color, QColor):\n            pass\n        else:\n            return None\n        if intensity:\n            color.setAlpha(intensity)\n        return color\n\n    def get_color_name(self", "solution": "\n        if color is None:\n            return None\n\n        # Adjust for intensity, if possible.\n        if color < 8 and intensity > 0:\n            color += 8\n\n        constructor = self.color_map.get(color, None)\n        if isinstance(constructor, basestring):\n            # If this is an X11 color name, we just hope there is a close SVG\n            # color name. We could use QColor's static method\n            # 'setAllowX11ColorNames()', but this is global and only available\n            # on X11. It seems cleaner to aim for uniformity of behavior.\n            return QtGui.QColor(constructor)\n\n        elif isinstance(constructor, (tuple, list)):\n            return QtGui.QColor(*constructor)\n\n        return None"}
{"prompt": "def update_z(self, z, indices=None):\n        \"\"\"\n        Update partial or entire z.\n\n        Args:\n            z (numpy.ndarray or list): to-be-updated z\n            indices (numpy.ndarray or list or optional): to-be-updated qubit indices\n\n        Returns:\n            Pauli: self\n\n        Raises:\n            QiskitError: when updating whole z, the number of qubits must be the same.\n        \"\"\"", "output": "\n        if indices is None:\n            if isinstance(z, list):\n                z = np.array(z)\n            if isinstance(z, np.ndarray):\n                if z.size == 1:\n                    z = np.array([z])\n                if z.size != self.num_qubits:\n                    raise QiskitError(\n                        \"When updating whole z, the number of qubits must be the same.\"\n                    )\n                self._z = z\n            else:\n                raise QiskitError(\"z must", "solution": "\n        z = _make_np_bool(z)\n        if indices is None:\n            if len(self._z) != len(z):\n                raise QiskitError(\"During updating whole z, you can not \"\n                                  \"change the number of qubits.\")\n            self._z = z\n        else:\n            if not isinstance(indices, list) and not isinstance(indices, np.ndarray):\n                indices = [indices]\n            for p, idx in enumerate(indices):\n                self._z[idx] = z[p]\n\n        return self"}
{"prompt": "def is_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if completely-positive trace-preserving.\"\"\"", "output": "\n        return self.is_cptp(atol, rtol)\n\n    def is_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if completely-positive trace-preserving.\"\"\"\n        if atol is None:\n            atol = self._atol\n        if rtol is None:\n            rtol = self._rtol\n        if self.dim < 2:\n            return True\n        if self.is_zero_trace():\n            return True\n        if not self.is_square:\n           ", "solution": "\n        if self._data[1] is not None:\n            return False\n        if atol is None:\n            atol = self._atol\n        if rtol is None:\n            rtol = self._rtol\n        accum = 0j\n        for op in self._data[0]:\n            accum += np.dot(np.transpose(np.conj(op)), op)\n        return is_identity_matrix(accum, rtol=rtol, atol=atol)"}
{"prompt": "def delete_clinvar_object(self, object_id, object_type, submission_id):\n        \"\"\"Remove a variant object from clinvar database and update the relative submission object\n\n            Args:\n                object_id(str) : the id of an object to remove from clinvar_collection database collection (a variant of a case)\n                object_type(str) : either 'variant_data' or 'case_data'. It's a key in the clinvar_submission object.\n                submission_id(str): the _id key of a clinvar submission\n\n            Returns:\n                updated_submission(obj): an updated clinvar submission\n        \"\"\"", "output": "_submission object\n\n            Returns:\n                None\n        \"\"\"\n        # get the clinvar_submission object\n        clinvar_submission = self.clinvar_collection.find_one({'_id': ObjectId(submission_id)})\n\n        # remove the object from the clinvar_collection database collection\n        self.clinvar_collection.delete_one({'_id': ObjectId(object_id)})\n\n        # remove the object from the clinvar_submission object\n        clinvar_submission[object_type].remove(object_id", "solution": "\n\n        LOG.info(\"Deleting clinvar object %s (%s)\", object_id, object_type)\n\n        # If it's a variant object to be removed:\n        #   remove reference to it in the submission object 'variant_data' list field\n        #   remove the variant object from clinvar collection\n        #   remove casedata object from clinvar collection\n        #   remove reference to it in the submission object 'caset_data' list field\n\n        # if it's a casedata object to be removed:\n        #   remove reference to it in the submission object 'caset_data' list field\n        #   remove casedata object from clinvar collection\n\n        result = ''\n\n        if object_type == 'variant_data':\n            # pull out a variant from submission object\n            self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'variant_data': object_id} })\n\n            variant_object = self.clinvar_collection.find_one( {'_id': object_id} )\n            linking_id = variant_object.get(\"linking_id\") #it's the original ID of the variant in scout, it's linking clinvar variants and casedata objects together\n\n            # remove any object with that linking_id from clinvar_collection. This removes variant and casedata\n            result = self.clinvar_collection.delete_many( {'linking_id': linking_id } )\n\n        else: # remove case_data but keep variant in submission\n            # delete the object itself from clinvar_collection\n             result = self.clinvar_collection.delete_one( {'_id': object_id } )\n\n        # in any case remove reference to it in the submission object 'caset_data' list field\n        self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'case_data': object_id} })\n\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id':submission_id}, { '$set' : {'updated_at': datetime.now()} }, return_document=pymongo.ReturnDocument.AFTER )\n\n        return updated_submission"}
{"prompt": "def _loadFromArray(self, dtype: HdlType, bitAddr: int) -> int:\n        \"\"\"\n        Parse HArray type to this transaction template instance\n\n        :return: address of it's end\n        \"\"\"", "output": "\n        if not isinstance(dtype, HArray):\n            raise TypeError(\n                f\"{self.__class__.__name__} can't load from {dtype.__class__.__name__} type\"\n            )\n        if not isinstance(bitAddr, int):\n            raise TypeError(\n                f\"{self.__class__.__name__} can't load from {type(bitAddr).__name__} type\"\n            )\n        if not isinstance(dtype.baseType, HdlType):\n            raise TypeError(\n                f", "solution": "\n        self.itemCnt = evalParam(dtype.size).val\n        self.children = TransTmpl(\n            dtype.elmType, 0, parent=self, origin=self.origin)\n        return bitAddr + self.itemCnt * self.children.bitAddrEnd"}
{"prompt": "def select(selector, obj):\n    \"\"\"Appy selector to obj and return matching nodes.\n\n    If only one node is found, return it, otherwise return a list of matches.\n    Returns False on syntax error. None if no results found.\n    \"\"\"", "output": "\n    if not isinstance(obj, (list, tuple)):\n        obj = [obj]\n    if not isinstance(selector, (list, tuple)):\n        selector = [selector]\n    matches = []\n    for o in obj:\n        for s in selector:\n            if isinstance(s, str):\n                try:\n                    matches.append(o.select(s))\n                except:\n                    return False\n            elif isinstance(s, dict):\n                try:\n                    matches.append(o.select(**s))\n                except", "solution": "\n\n    parser = Parser(obj)\n    try:\n        return parser.parse(selector)\n    except SelectorSyntaxError as e:\n        log.exception(e)\n        return False"}
{"prompt": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"", "output": "\n    if ':' in host:\n        host, port = host.split(':')\n        port = int(port)\n    else:\n        host, port = host, 80\n    return host, port\n\n\ndef parse_url(url):\n    \"\"\"Parses a URL into a tuple of host, port, path.\n    \"\"\"\n    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)\n    host, port = parse_host(netloc)\n    return host, port, path\n\n\ndef parse_", "solution": "\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)"}
{"prompt": "def receive_nak_rebinding(self, pkt):\n        \"\"\"Receive NAK in REBINDING state.\"\"\"", "output": "\n        # TODO: Implement\n        pass\n\n    def receive_nak_renewing(self, pkt):\n        \"\"\"Receive NAK in RENEWING state.\"\"\"\n        # TODO: Implement\n        pass\n\n    def receive_nak_releasing(self, pkt):\n        \"\"\"Receive NAK in RELEASING state.\"\"\"\n        # TODO: Implement\n        pass\n\n    def receive_nak_requesting(self, pkt):\n        \"\"\"Receive NAK in REQUESTING state.\"\"\"\n        # TODO: Im", "solution": "\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()"}
{"prompt": "def diseases(context):\n    \"\"\"Show all diseases in the database\"\"\"", "output": "\n    diseases = context.db.diseases.find()\n    return {\"diseases\": diseases}\n\n\ndef disease(context, disease_id):\n    \"\"\"Show a specific disease in the database\"\"\"\n    disease = context.db.diseases.find_one({\"_id\": ObjectId(disease_id)})\n    return {\"disease\": disease}\n\n\ndef disease_by_name(context, disease_name):\n    \"\"\"Show a specific disease in the database\"\"\"\n    disease = context.db.diseases", "solution": "\n    LOG.info(\"Running scout view diseases\")\n    adapter = context.obj['adapter']\n\n    disease_objs = adapter.disease_terms()\n\n    nr_diseases = disease_objs.count()\n    if nr_diseases == 0:\n        click.echo(\"No diseases found\")\n    else:\n        click.echo(\"Disease\")\n        for disease_obj in adapter.disease_terms():\n            click.echo(\"{0}\".format(disease_obj['_id']))\n        LOG.info(\"{0} diseases found\".format(nr_diseases))"}
{"prompt": "def infix_to_postfix(nodes, *, recurse_types=None):\n  \"\"\"Convert a list of nodes in infix order to a list of nodes in postfix order.\n\n  E.G. with normal algebraic precedence, 3 + 4 * 5 -> 3 4 5 * +\n  \"\"\"", "output": "\n  if recurse_types is None:\n    recurse_types = (ast.BinOp, ast.UnaryOp)\n  stack = []\n  for node in nodes:\n    if isinstance(node, recurse_types):\n      stack.append(node)\n    elif isinstance(node, ast.Num):\n      while stack and isinstance(stack[-1], recurse_types):\n        stack.pop()\n      yield node\n    elif isinstance(node, ast.Name):\n      while stack and isinstance(stack[-", "solution": "\n  output = []\n  operators = []\n\n  for node in nodes:\n    if isinstance(node, OperatorNode):\n      # Drain out all operators whose precedence is gte the node's...\n      cmp_operator = node.operator\n      while operators:\n        current_operator = operators[-1].operator\n        if current_operator.precedence > cmp_operator.precedence or \\\n           current_operator.precedence == cmp_operator.precedence and current_operator.association == Association.left:\n          output.append(operators.pop())\n        else:\n          break\n      operators.append(node)\n    else:\n      if recurse_types is not None and node.node_type in recurse_types:\n        output.extend(infix_to_postfix(node.children, recurse_types=recurse_types))\n      else:\n        output.append(node)\n\n  return output + list(reversed(operators))"}
{"prompt": "def get(cls, key, default=None):\n        \"\"\"\n        Resembles the :meth:`dict.get` method.\n\n        :returns:\n            A configuration dictionary for specified provider.\n\n        \"\"\"", "output": "\n        if key not in cls._providers:\n            return default\n        return cls._providers[key]\n\n    @classmethod\n    def get_all(cls):\n        \"\"\"\n        Returns a copy of the configuration dictionary.\n\n        :returns:\n            A copy of the configuration dictionary.\n\n        \"\"\"\n        return cls._providers.copy()\n\n    @classmethod\n    def get_providers(cls):\n        \"\"\"\n        Returns a list of the providers.\n\n        :returns:\n            A list of the providers", "solution": "\n\n        # Query datastore.\n        result = cls.query(cls.provider_name == key).get()\n\n        if result:\n            result_dict = result.to_dict()\n\n            # Use NDBOpenIDStore by default\n            result_dict['store'] = NDBOpenIDStore\n\n            # Convert coma-separated values to list. Currently only scope is\n            # csv.\n            for i in ('scope', ):\n                prop = result_dict.get(i)\n                if prop:\n                    result_dict[i] = [s.strip() for s in prop.split(',')]\n                else:\n                    result_dict[i] = None\n\n            return result_dict\n        else:\n            return default"}
{"prompt": "def dump(obj, file, reducers=None, protocol=None):\n    \"\"\"Replacement for pickle.dump() using _LokyPickler.\"\"\"", "output": "\n    if reducers is None:\n        reducers = _default_reducers\n    if protocol is None:\n        protocol = pickle.HIGHEST_PROTOCOL\n    pickler = _LokyPickler(file, protocol=protocol, reducers=reducers)\n    pickler.dump(obj)\n\n\ndef dumps(obj, reducers=None, protocol=None):\n    \"\"\"Replacement for pickle.dumps() using _LokyPickler.\"\"\"\n    if reducers is None:\n        reducers = _", "solution": "\n    global _LokyPickler\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)"}
{"prompt": "def rule(cls, rulename=None, erase=False):\n    \"\"\"Attach a method to a parsing class and register it as a parser rule.\n\n       The method is registered with its name unless rulename is provided.\n    \"\"\"", "output": "\n    def decorator(func):\n        if rulename is None:\n            rulename = func.__name__\n        func.rulename = rulename\n        func.erase = erase\n        cls.rules[rulename] = func\n        return func\n    return decorator\n\ndef rule_erase(cls, rulename=None):\n    \"\"\"Attach a method to a parsing class and register it as a parser rule.\n\n       The method is registered with its name unless rulename is provided.\n    \"\"\"\n    def decor", "solution": "\n    if not hasattr(cls, '_rules'):\n        raise TypeError(\n            \"%s didn't seems to be a BasicParser subsclasse\" % cls.__name__)\n    class_hook_list = cls._hooks\n    class_rule_list = cls._rules\n\n    def wrapper(f):\n        nonlocal rulename\n        add_method(cls)(f)\n        if rulename is None:\n            rulename = f.__name__\n        if not erase and (rulename in class_hook_list or rulename in class_rule_list):\n            raise TypeError(\"%s is already define has rule or hook\" % rulename)\n        if '.' not in rulename:\n            rulename = cls.__module__ + '.' + cls.__name__ + '.' + rulename\n        set_one(class_rule_list, rulename, f)\n        return f\n    return wrapper"}
{"prompt": "def download_url(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"", "output": "\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n           ", "solution": "\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == 'https':\n                url = url.replace('https:', 'http:')\n                print('Failed download. Trying https -> http instead.'\n                      ' Downloading ' + url + ' to ' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )"}
{"prompt": "def _in_buffer(self, position=None):\n        \"\"\" Returns whether the current cursor (or, if specified, a position) is\n            inside the editing region.\n        \"\"\"", "output": "\n        if position is None:\n            position = self.get_cursor_position()\n        return self.edit_start <= position <= self.edit_end\n\n    def _get_edit_region(self):\n        \"\"\" Returns the current editing region.\n        \"\"\"\n        return self.edit_start, self.edit_end\n\n    def _set_edit_region(self, start, end):\n        \"\"\" Sets the current editing region.\n        \"\"\"\n        self.edit_start = start\n        self.edit_end = end\n\n    edit_region", "solution": "\n        cursor = self._control.textCursor()\n        if position is None:\n            position = cursor.position()\n        else:\n            cursor.setPosition(position)\n        line = cursor.blockNumber()\n        prompt_line = self._get_prompt_cursor().blockNumber()\n        if line == prompt_line:\n            return position >= self._prompt_pos\n        elif line > prompt_line:\n            cursor.movePosition(QtGui.QTextCursor.StartOfBlock)\n            prompt_pos = cursor.position() + len(self._continuation_prompt)\n            return position >= prompt_pos\n        return False"}
{"prompt": "def split_type(self, typename):\n        \"\"\"\n        Given a potentially complex type, split it into its base type and specializers\n        \"\"\"", "output": "\n        if typename.startswith('['):\n            return self.split_type(typename[1:-1])\n        elif typename.startswith('*'):\n            return self.split_type(typename[1:])\n        elif typename.startswith('const '):\n            return self.split_type(typename[6:])\n        elif typename.startswith('enum '):\n            return typename[5:], []\n        elif typename.startswith('struct '):\n            return typename[7:], []\n        elif typename.starts", "solution": "\n\n        name = self._canonicalize_type(typename)\n        if '(' not in name:\n            return name, False, []\n\n        base, sub = name.split('(')\n        if len(sub) == 0 or sub[-1] != ')':\n            raise ArgumentError(\"syntax error in complex type, no matching ) found\", passed_type=typename, basetype=base, subtype_string=sub)\n\n        sub = sub[:-1]\n\n        subs = sub.split(',')\n        return base, True, subs"}
{"prompt": "def get_response_code(url, timeout=10):\n    \"\"\"\n    Visit the URL and return the HTTP response code in 'int'\n    \"\"\"", "output": "\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response.status_code\n    except requests.exceptions.RequestException:\n        return None\n\n\ndef get_response_content(url, timeout=10):\n    \"\"\"\n    Visit the URL and return the HTTP response content in 'str'\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response.content\n    except requests.exceptions.RequestException:\n        return None\n\n\ndef get_response_json", "solution": "\n    try:    \n        req = urllib2.urlopen(url, timeout=timeout)\n    except HTTPError, e:\n        return e.getcode()\n    except Exception, _:\n        fail(\"Couldn't reach the URL '%s'\" % url)\n    else:\n        return req.getcode()"}
{"prompt": "def asAMP(cls):\n        \"\"\"\n        Returns the exception's name in an AMP Command friendly format.\n\n        For example, given a class named ``ExampleExceptionClass``, returns\n        ``\"EXAMPLE_EXCEPTION_CLASS\"``.\n        \"\"\"", "output": "\n        return cls.__name__.upper().replace('_', ' ')\n\n    @classmethod\n    def asString(cls):\n        \"\"\"\n        Returns the exception's name in a string friendly format.\n\n        For example, given a class named ``ExampleExceptionClass``, returns\n        ``\"Example Exception Class\"``.\n        \"\"\"\n        return cls.__name__.replace('_', ' ')\n\n    @classmethod\n    def asTitle(cls):\n        \"\"\"\n        Returns the exception's name in a title friendly format.\n\n        For example, given", "solution": "\n        parts = groupByUpperCase(cls.__name__)\n        return cls, \"_\".join(part.upper() for part in parts)"}
{"prompt": "def to_grayscale(img):\n    \"\"\"Convert PIL image to numpy grayscale array and numpy alpha array.\n\n    Args:\n      img (PIL.Image): PIL Image object.\n\n    Returns:\n      (gray, alpha): both numpy arrays.\n    \"\"\"", "output": "\n    gray = np.array(img.convert('L'))\n    alpha = np.array(img.convert('RGBA'))[:, :, 3]\n    return gray, alpha\n\n\ndef to_rgb(img):\n    \"\"\"Convert PIL image to numpy rgb array and numpy alpha array.\n\n    Args:\n      img (PIL.Image): PIL Image object.\n\n    Returns:\n      (rgb, alpha): both numpy arrays.\n    \"\"\"\n    rgb = np.array(img.convert('RGB'))\n    alpha =", "solution": "\n    gray = numpy.asarray(ImageOps.grayscale(img)).astype(numpy.float)\n\n    imbands = img.getbands()\n    alpha = None\n    if 'A' in imbands:\n        alpha = numpy.asarray(img.split()[-1]).astype(numpy.float)\n\n    return gray, alpha"}
{"prompt": "def _handle_options(self, options):\n        \"\"\"\n        Handle the data from the options.\n\n        :param options: The list of options from the rule.\n        :type options: list\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"", "output": "\n        domains = []\n        for option in options:\n            if option.startswith(\"domain:\"):\n                domain = option.replace(\"domain:\", \"\")\n                domains.append(domain)\n        return domains\n\n    def _get_domains(self, options):\n        \"\"\"\n        Get the domains from the options.\n\n        :param options: The list of options from the rule.\n        :type options: list\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"\n        domains = []\n        for", "solution": "\n\n        # We initiate a variable which will save our result\n        result = []\n\n        # We initiate the regex which will be used to extract the domain listed\n        # under the option domain=\n        regex_domain_option = r\"domain=(.*)\"\n\n        for option in options:\n            # We loop through the list of option.\n            try:\n                # We try to extract the list of domains from the currently read\n                # option.\n                domains = Regex(\n                    option, regex_domain_option, return_data=True, rematch=True, group=0\n                ).match()[-1]\n\n                if domains:\n                    # We could extract something.\n\n                    if self.aggressive:  # pragma: no cover\n                        result.extend(\n                            [\n                                x\n                                for x in domains.split(\"|\")\n                                if x and not x.startswith(\"~\")\n                            ]\n                        )\n                    else:\n                        # We return True.\n                        return True\n            except TypeError:\n                pass\n\n        # We return the result.\n        return result"}
{"prompt": "def to_fmt(self) -> fmt.indentable:\n    \"\"\"\n    Return an Fmt representation for pretty-printing\n    \"\"\"", "output": "\n    return fmt.indentable(\n        fmt.line(\"{} {}\".format(self.name, self.type)),\n        fmt.line(\"{} {}\".format(\"default\", self.default)),\n        fmt.line(\"{} {}\".format(\"description\", self.description)),\n        fmt.line(\"{} {}\".format(\"required\", self.required)),\n        fmt.line(\"{} {}\".format(\"enum\", self.enum)),\n        fmt.line(\"{} {}\".format(\"minimum\", self.minimum)),\n        fmt.line(\"{} {}\".format(\"maximum\",", "solution": "\n    qual = \"scope\"\n    txt = fmt.sep(\" \", [qual])\n    name = self.show_name()\n    if name != \"\":\n        txt.lsdata.append(name)\n    if len(self._hsig) > 0 or len(self.mapTypeTranslate) > 0:\n        lsb = []\n        if len(self.mapTypeTranslate) > 0:\n            lsb.append(\"translate:\\n\")\n            lsb.append(fmt.end(\"\\n\", self.mapTypeTranslate.to_fmt()))\n        for k in sorted(self._hsig.keys()):\n            s = self._hsig[k]\n            lsb.append(fmt.end(\"\\n\", [s.to_fmt()]))\n        block = fmt.block(\":\\n\", \"\", fmt.tab(lsb))\n        txt.lsdata.append(block)\n    return txt"}
{"prompt": "def when_children_replaced ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for children being replaced on a\n            specified object.\n        \"\"\"", "output": "\n        if remove:\n            self._remove_listener ( object, listener, 'children_replaced' )\n        else:\n            self._add_listener ( object, listener, 'children_replaced' )\n\n    def when_children_added ( self, object, listener ):\n        \"\"\" Sets up a listener for children being added to a specified object.\n        \"\"\"\n        self._add_listener ( object, listener, 'children_added' )\n\n    def when_children_added ( self, object, listener, remove ):\n        \"\"\" Sets", "solution": "\n        object.on_trait_change( listener, \"subgraphs\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"clusters\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"nodes\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"edges\", remove = remove,\n                                dispatch = \"fast_ui\" )"}
{"prompt": "def tokenize_line(line):\n    \"\"\"\n    Tokenize a line:\n    * split tokens on whitespace\n    * treat quoted strings as a single token\n    * drop comments\n    * handle escaped spaces and comment delimiters\n    \"\"\"", "output": "\n    tokens = []\n    token = ''\n    in_quotes = False\n    for c in line:\n        if c == ' ' and not in_quotes:\n            if token:\n                tokens.append(token)\n                token = ''\n        elif c == '\"':\n            in_quotes = not in_quotes\n        elif c == '#':\n            break\n        elif c == '\\\\':\n            token += c\n            token += next(line)\n        else:\n            token += c\n    if token:\n        tokens.append(token", "solution": "\n    ret = []\n    escape = False\n    quote = False\n    tokbuf = \"\"\n    ll = list(line)\n    while len(ll) > 0:\n        c = ll.pop(0)\n        if c.isspace():\n            if not quote and not escape:\n                # end of token\n                if len(tokbuf) > 0:\n                    ret.append(tokbuf)\n\n                tokbuf = \"\"\n            elif quote:\n                # in quotes\n                tokbuf += c\n            elif escape:\n                # escaped space\n                tokbuf += c\n                escape = False\n            else:\n                tokbuf = \"\"\n\n            continue\n\n        if c == '\\\\':\n            escape = True\n            continue\n        elif c == '\"':\n            if not escape:\n                if quote:\n                    # end of quote\n                    ret.append(tokbuf)\n                    tokbuf = \"\"\n                    quote = False\n                    continue\n                else:\n                    # beginning of quote\n                    quote = True\n                    continue\n        elif c == ';':\n            if not escape:\n                # comment \n                ret.append(tokbuf)\n                tokbuf = \"\"\n                break\n            \n        # normal character\n        tokbuf += c\n        escape = False\n\n    if len(tokbuf.strip(\" \").strip(\"\\n\")) > 0:\n        ret.append(tokbuf)\n\n    return ret"}
{"prompt": "def tasks(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-09-01: :class:`TasksOperations<azure.mgmt.containerregistry.v2018_09_01.operations.TasksOperations>`\n        \"\"\"", "output": "\n        api_version = self._get_api_version('tasks')\n        if api_version == '2018-09-01':\n            from .v2018_09_01.operations import TasksOperations as OperationClass\n        else:\n            raise ValueError(\"API version {} does not have operation group 'tasks'\".format(api_version))\n        return OperationClass(self._client, self._config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api", "solution": "\n        api_version = self._get_api_version('tasks')\n        if api_version == '2018-09-01':\n            from .v2018_09_01.operations import TasksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def f_supports(self, data):\n        \"\"\"Checks if input data is supported by the parameter.\"\"\"", "output": "\n        return True\n\n    def f_validate(self, data):\n        \"\"\"Validates input data.\"\"\"\n        return True\n\n    def f_convert(self, data):\n        \"\"\"Converts input data to the parameter type.\"\"\"\n        return data\n\n    def f_get_default(self):\n        \"\"\"Returns default value for the parameter.\"\"\"\n        return self.default\n\n    def f_get_value(self):\n        \"\"\"Returns value of the parameter.\"\"\"\n        return self.value\n\n    def f_set_value(", "solution": "\n        dtype = type(data)\n        if dtype is tuple or dtype is list and len(data) == 0:\n            return True  #  ArrayParameter does support empty tuples\n        elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:\n                return True  #  ArrayParameter supports empty numpy arrays\n        else:\n            return super(ArrayParameter, self).f_supports(data)"}
{"prompt": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"", "output": "of shape `batch_shape`.\n  \"\"\"\n  # TODO(allenl): Add support for broadcasting over the batch dimension.\n  # TODO(allenl): Add support for broadcasting over the time dimension.\n  with tf.name_scope('moments_of_masked_time_series'):\n    # TODO(allenl): Use tf.nn.moments when it supports masking.\n    # TODO(allenl): Use tf.nn.compute_covariance when it supports masking.\n    # TODO(allenl): Use tf.nn.normal", "solution": "\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance"}
{"prompt": "def flush(self, full=False, all=False, force=False):\n        \"\"\"\n        Call ``.save()`` on all modified entities in the session. Use when you\n        want to flush changes to Redis, but don't want to lose your local\n        session cache.\n\n        See the ``.commit()`` method for arguments and their meanings.\n        \"\"\"", "output": "\n        if full:\n            self._flush_full(force=force)\n        elif all:\n            self._flush_all(force=force)\n        else:\n            self._flush_modified(force=force)\n\n    def _flush_full(self, force=False):\n        \"\"\"\n        Flush all entities in the session.\n        \"\"\"\n        self._flush_modified(force=force)\n        self._flush_deleted(force=force)\n\n    def _flush_all(self, force=False):\n        \"\"\"\n       ", "solution": "\n        self._init()\n\n        return self.save(*self.known.values(), full=full, all=all, force=force)"}
{"prompt": "def format_price_commas(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) with commas\n    \"\"\"", "output": "\n    return f\"\u00a3{round(price, 2):,.2f}\"\n\n\ndef format_price_no_commas(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) without commas\n    \"\"\"\n    return f\"\u00a3{round(price, 2):.2f}\"\n\n\ndef format_price_no_pounds(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) without pounds sign\n    \"\"\"\n   ", "solution": "\n    if price is None:\n        return None\n    if price >= 0:\n        return jinja2.Markup('&pound;{:,.2f}'.format(price))\n    else:\n        return jinja2.Markup('-&pound;{:,.2f}'.format(-price))"}
{"prompt": "def _distribute_jobs(self):\n        \"\"\"\n        Distributes jobs from the active job set to any waiting get_job\n        callbacks.\n        \"\"\"", "output": "\n        if self.active_jobs:\n            for callback in self.waiting_get_jobs:\n                callback(self.active_jobs.pop())\n            self.waiting_get_jobs = []\n\n    def _get_job(self):\n        \"\"\"\n        Returns the next job in the active job set.\n        \"\"\"\n        if self.active_jobs:\n            job = self.active_jobs.pop()\n            self._distribute_jobs()\n            return job\n        else:\n            self.waiting_", "solution": "\n\n        while (self._active_js.job_available()\n                and len(self._ready_callbacks) > 0):\n            job = self._active_js.get_job()\n            self._job_sources[job] = self._active_js\n            callback = self._ready_callbacks.popleft()\n            callback(job)"}
{"prompt": "def finalize(self, success_or_fail, message=''):\n        \"\"\"\n        :param success_or_fail: string that is 'success' or 'fail'\n        :param message:\n        \"\"\"", "output": "\n        if success_or_fail == 'success':\n            self.success = True\n            self.message = message\n        elif success_or_fail == 'fail':\n            self.success = False\n            self.message = message\n        else:\n            raise Exception('success_or_fail must be \"success\" or \"fail\"')\n\n    def __str__(self):\n        return 'success: %s, message: %s' % (self.success, self.message)\n\n\nclass Command(object):\n    \"\"\"\n    Base class for all commands.", "solution": "\n        self.logit.debug('String OutputPorts: %s' % self.__string_output_ports)\n        if self.__string_output_ports:\n            with open(os.path.join(self.output_path, 'ports.json'), 'w') as opf:\n                json.dump(self.__string_output_ports, opf, indent=4)\n            self.logit.debug('Ports.json written to %s' % os.path.join(self.output_path, 'ports.json'))\n\n        with open(os.path.join(self.base_path, 'status.json'), 'w') as sf:\n            json.dump({'status': success_or_fail, 'reason': message}, sf, indent=4)"}
{"prompt": "def get_users_for_sis_course_id(self, sis_course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given sis course id.\n        \"\"\"", "output": "\n        return self._get_users_for_sis_id(sis_course_id, params)\n\n    def get_users_for_sis_section_id(self, sis_section_id, params={}):\n        \"\"\"\n        Returns a list of users for the given sis section id.\n        \"\"\"\n        return self._get_users_for_sis_id(sis_section_id, params)\n\n    def get_users_for_sis_account_id(self, sis_account_id, params", "solution": "\n        return self.get_users_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)"}
{"prompt": "def sync_folder(self, path, bucket):\n        \"\"\"Syncs a local directory with an S3 bucket.\n     \n        Currently does not delete files from S3 that are not in the local directory.\n\n        path: The path to the directory to sync to S3\n        bucket: The name of the bucket on S3\n        \"\"\"", "output": "\n        # Get the list of files in the directory\n        files = os.listdir(path)\n\n        # Get the list of files in the bucket\n        bucket_files = self.list_bucket(bucket)\n\n        # Get the list of files in the bucket that are not in the directory\n        bucket_files_not_in_dir = [file for file in bucket_files if file not in files]\n\n        # Upload the files in the directory that are not in the bucket\n        for file in files:\n            if file not in bucket_files:\n               ", "solution": "\n        bucket = self.conn.get_bucket(bucket)\n        local_files = self._get_local_files(path)\n        s3_files = self._get_s3_files(bucket)\n        for filename, hash in local_files.iteritems():\n            s3_key = s3_files[filename]\n            if s3_key is None:\n                s3_key = Key(bucket)\n                s3_key.key = filename\n                s3_key.etag = '\"!\"'\n            \n            if s3_key.etag[1:-1] != hash[0]:\n                s3_key.set_contents_from_filename(join(path, filename), md5=hash)"}
{"prompt": "def new(self, *args, **kwargs):\n        \"\"\"\n        Create and return a new instance.\n        \"\"\"", "output": "\n        return self.get_class()(*args, **kwargs)\n\n    def get_class(self):\n        \"\"\"\n        Return the class to be instantiated.\n        \"\"\"\n        return self.cls\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Create and return a new instance.\n        \"\"\"\n        return self.new(*args, **kwargs)\n\n    def __getattr__(self, name):\n        \"\"\"\n        Delegate access to attributes to the instance.\n        \"\"\"\n        return getattr(self.new(),", "solution": "\n        inst = self.clazz()\n        self.storage.append(inst)\n        \n        # set all attributes with an initial default value\n        referential_attributes = dict()\n        for name, ty in self.attributes:\n            if name not in self.referential_attributes:\n                value = self.default_value(ty)\n                setattr(inst, name, value)\n            \n        # set all positional arguments\n        for attr, value in zip(self.attributes, args):\n            name, ty = attr\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n            \n        # set all named arguments\n        for name, value in kwargs.items():\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n        \n        if not referential_attributes:\n            return inst\n        \n        # batch relate referential attributes \n        for link in self.links.values():\n            if set(link.key_map.values()) - set(referential_attributes.keys()):\n                continue\n             \n            kwargs = dict()\n            for key, value in link.key_map.items():\n                kwargs[key] = referential_attributes[value]\n            \n            if not kwargs:\n                continue\n            \n            for other_inst in link.to_metaclass.query(kwargs):\n                relate(other_inst, inst, link.rel_id, link.phrase)\n        \n        for name, value in referential_attributes.items():\n            if getattr(inst, name) != value:\n                logger.warning('unable to assign %s to %s', name, inst)\n                \n        return inst"}
{"prompt": "def plotBrightLimitInV(gBright, pdf=False, png=False):\n  \"\"\"\n  Plot the bright limit of Gaia in V as a function of (V-I).\n\n  Parameters\n  ----------\n\n  gBright - The bright limit of Gaia in G\n  \"\"\"", "output": "\n\n  # Plot the bright limit of Gaia in V as a function of (V-I)\n  fig = plt.figure(figsize=(10, 10))\n  ax = fig.add_subplot(111)\n  ax.plot(gBright, 'k-')\n  ax.set_xlabel('V-I')\n  ax.set_ylabel('Gaia bright limit in V')\n  ax.set_xlim(0.5, 2.0)\n  ax.set_ylim(0.", "solution": "\n  vmini=np.linspace(0.0,6.0,1001)\n  gminv=gminvFromVmini(vmini)\n  vBright=gBright-gminv\n\n  fig=plt.figure(figsize=(10,6.5))\n  plt.plot(vmini,vBright,'b-')\n  plt.xlabel('$(V-I)$')\n  plt.ylabel('Bright limit of Gaia in $V$')\n  plt.xlim(0,6)\n  plt.ylim(5,11)\n  plt.grid(which='both')\n  plt.title(\"Bright limit in $G$: {0}\".format(gBright))\n\n  if (pdf):\n    plt.savefig('VBandBrightLimit.pdf')\n  elif (png):\n    plt.savefig('VBandBrightLimit.png')\n  else:\n    plt.show()"}
{"prompt": "def _all_get_or_create_table(self, where, tablename, description, expectedrows=None):\n        \"\"\"Creates a new table, or if the table already exists, returns it.\"\"\"", "output": "\n        if self.table_exists(tablename):\n            return self.get_table(tablename)\n        else:\n            return self.create_table(tablename, description, where, expectedrows)\n\n    def _all_get_or_create_view(self, where, viewname, description, expectedrows=None):\n        \"\"\"Creates a new view, or if the view already exists, returns it.\"\"\"\n        if self.view_exists(viewname):\n            return self.get_view(viewname)\n        else:\n", "solution": "\n        where_node = self._hdf5file.get_node(where)\n\n        if not tablename in where_node:\n            if not expectedrows is None:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              expectedrows=expectedrows,\n                                              filters=self._all_get_filters())\n            else:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              filters=self._all_get_filters())\n        else:\n            table = where_node._f_get_child(tablename)\n\n        return table"}
{"prompt": "def _align(self, axes, key_shape=None):\n        \"\"\"\n        Align local bolt array so that axes for iteration are in the keys.\n\n        This operation is applied before most functional operators.\n        It ensures that the specified axes are valid, and might transpose/reshape\n        the underlying array so that the functional operators can be applied\n        over the correct records.\n\n        Parameters\n        ----------\n        axes: tuple[int]\n            One or more axes that will be iterated over by a functional operator\n\n        Returns\n        -------\n        BoltArrayLocal\n        \"\"\"", "output": "------\n        BoltArray\n            The same array, but with the axes aligned\n        \"\"\"\n        if key_shape is None:\n            key_shape = self.key_shape\n        if len(axes) == 0:\n            return self\n        if len(axes) == 1:\n            if axes[0] == 0:\n                return self\n            if axes[0] == 1:\n                return self.T\n            if axes[0] == 2:\n                return self.T.T\n            if axes[0] == ", "solution": "\n\n        # ensure that the key axes are valid for an ndarray of this shape\n        inshape(self.shape, axes)\n\n        # compute the set of dimensions/axes that will be used to reshape\n        remaining = [dim for dim in range(len(self.shape)) if dim not in axes]\n        key_shape = key_shape if key_shape else [self.shape[axis] for axis in axes]\n        remaining_shape = [self.shape[axis] for axis in remaining]\n        linearized_shape = [prod(key_shape)] + remaining_shape\n\n        # compute the transpose permutation\n        transpose_order = axes + remaining\n\n        # transpose the array so that the keys being mapped over come first, then linearize keys\n        reshaped = self.transpose(*transpose_order).reshape(*linearized_shape)\n\n        return reshaped"}
{"prompt": "def info(self):\n        \"\"\" Returns a description of the trait.\"\"\"", "output": "\n        return \"A trait that is a list of strings.\"\n\n    def validate(self, obj, value):\n        \"\"\" Validates that the value is a list of strings.\"\"\"\n        if not isinstance(value, list):\n            raise TraitError(\"A list of strings is required.\")\n        for item in value:\n            if not isinstance(item, basestring):\n                raise TraitError(\"A list of strings is required.\")\n\n\nclass ListOfInts(TraitType):\n    \"\"\" A trait whose value is a list of integers.\"\"\"\n\n", "solution": "\n        result = 'any of ' + repr(self.values)\n        if self._allow_none:\n            return result + ' or None'\n        return result"}
{"prompt": "def update_affinity_group(self, affinity_group_name, label,\n                              description=None):\n        \"\"\"\n        Updates the label and/or the description for an affinity group for the\n        specified subscription.\n\n        affinity_group_name:\n            The name of the affinity group.\n        label:\n            A name for the affinity group. The name can be up to 100 characters\n            in length.\n        description:\n            A description for the affinity group. The description can be up to\n            1024 characters in length.\n        \"\"\"", "output": "4 characters in length.\n        \"\"\"\n        return self.mgmt_client.update_affinity_group(\n            affinity_group_name=affinity_group_name,\n            label=label,\n            description=description)\n\n    def list_affinity_groups(self):\n        \"\"\"\n        Lists the affinity groups for the specified subscription.\n        \"\"\"\n        return self.mgmt_client.list_affinity_groups()\n\n    def delete_affinity_group(self, affinity_group_name):\n        \"\"\"\n", "solution": "\n        _validate_not_none('affinity_group_name', affinity_group_name)\n        _validate_not_none('label', label)\n        return self._perform_put(\n            '/' + self.subscription_id + '/affinitygroups/' +\n            _str(affinity_group_name),\n            _XmlSerializer.update_affinity_group_to_xml(label, description))"}
{"prompt": "def HWProcess(cls, proc, ctx):\n        \"\"\"\n        Serialize HWProcess instance\n\n        :param scope: name scope to prevent name collisions\n        \"\"\"", "output": "\n        if proc.is_a(HWProcess):\n            return cls.HWProcess(proc, ctx)\n        elif proc.is_a(HWThread):\n            return cls.HWThread(proc, ctx)\n        elif proc.is_a(HWBlock):\n            return cls.HWBlock(proc, ctx)\n        elif proc.is_a(HWModule):\n            return cls.HWModule(proc, ctx)\n        elif proc.is_a(HWInterface):\n            return cls.H", "solution": "\n        body = proc.statements\n        childCtx = ctx.withIndent()\n        statemets = [cls.asHdl(s, childCtx) for s in body]\n        proc.name = ctx.scope.checkedName(proc.name, proc)\n\n        return cls.methodTmpl.render(\n            indent=getIndent(ctx.indent),\n            name=proc.name,\n            statements=statemets\n        )"}
{"prompt": "def _get_tol(tol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.\"\"\"", "output": "\n  if tol is None:\n    return tf.constant(0, dtype=dtype)\n  if validate_args:\n    return tf.convert_to_tensor(tol, dtype=dtype)\n  return tf.convert_to_tensor(tol, dtype=dtype, name='tol')\n\n\ndef _get_atol(atol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 0 if `atol` is None, validation optional.\"\"\"\n  if atol is None:\n    return tf.constant(", "solution": "\n  if tol is None:\n    return tf.convert_to_tensor(value=0, dtype=dtype)\n\n  tol = tf.convert_to_tensor(value=tol, dtype=dtype)\n  if validate_args:\n    tol = distribution_util.with_dependencies([\n        assert_util.assert_non_negative(\n            tol, message=\"Argument 'tol' must be non-negative\")\n    ], tol)\n  return tol"}
{"prompt": "def get_item_objects(self, query_params=None):\n        \"\"\"\n        Get the items for this checklist. Returns a list of ChecklistItem objects.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        return self._get_objects(\n            self.get_item_url(),\n            query_params,\n            ChecklistItem,\n            self.get_item_objects_from_json,\n        )\n\n    def get_item_objects_from_json(self, json_items):\n        \"\"\"\n        Get the items for this checklist from a JSON response. Returns a list of ChecklistItem objects.\n        \"\"\"\n        return [ChecklistItem(self, item) for item in", "solution": "\n        card = self.get_card()\n        checklistitems_list = []\n        for checklistitem_json in self.get_items(query_params):\n            checklistitems_list.append(self.create_checklist_item(card.id, self.id, checklistitem_json))\n\n        return checklistitems_list"}
{"prompt": "def onTWriteCallback__init(self, sim):\n        \"\"\"\n        Process for injecting of this callback loop into simulator\n        \"\"\"", "output": "\n        self.sim = sim\n        self.sim.register_callback(self.onTWriteCallback)\n\n    def onTWriteCallback(self, t, data):\n        \"\"\"\n        Callback function for TWrite\n        \"\"\"\n        if self.sim.get_state() == 'RUNNING':\n            self.sim.set_state('STOPPED')\n            self.sim.set_state('RUNNING')\n\n    def onTWriteCallback__exit(self, sim):\n        \"\"\"\n        Process for removing of this callback loop", "solution": "\n        yield from self.onTWriteCallback(sim)\n        self.intf.t._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)\n        self.intf.o._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)"}
{"prompt": "def GetParam(tag, param, default=__SENTINEL):\r\n    \"\"\" Convenience function for accessing tag parameters\"\"\"", "output": "\r\n    if default is __SENTINEL:\r\n        default = None\r\n    return tag.get(param, default)\r\n\r\n\r\ndef GetParamAsBool(tag, param, default=__SENTINEL):\r\n    \"\"\" Convenience function for accessing tag parameters\"\"\" \r\n    if default is __SENTINEL:\r\n        default = False\r\n    return tag.get(param, default)\r\n\r\n\r\ndef GetParamAsInt(tag, param, default=__SENTINEL):\r\n    \"\"\" Convenience function", "solution": "\r\n    if tag.HasParam(param):\r\n        return tag.GetParam(param)\r\n    else:\r\n        if default == __SENTINEL:\r\n            raise KeyError\r\n        else:\r\n            return default"}
{"prompt": "def _connection_failed(self, error=\"Error not specified!\"):\n        \"\"\"Clean up after connection failure detected.\"\"\"", "output": "\n        self.connection_failed = True\n        self.connection_failed_error = error\n        self.connection_failed_time = time.time()\n        self.connection_failed_count += 1\n        self.connection_failed_count_time = time.time()\n        self.connection_failed_count_total += 1\n        self.connection_failed_count_total_time = time.time()\n        self.connection_failed_count_total_time_start = self.connection_failed_count_total_time\n        self.connection_failed", "solution": "\n        if not self._error:\n            LOG.error(\"Connection failed: %s\", str(error))\n            self._error = error"}
{"prompt": "def sequence_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use out-of-sequence message detection for messages\n        protected by :meth:`get_mic` and :meth:`wrap`. False if OOS detection cannot be used.\n        \"\"\"", "output": "\n        return self._sequence_detection_negotiated\n\n    @property\n    def sequence_detection_supported(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use out-of-sequence message detection for messages\n        protected by :meth:`get_mic` and :meth:`wrap`. False if OOS detection cannot be used.\n        \"\"\"\n        return self._sequence_detection_supported\n\n    @property\n    def sequence_detection", "solution": "\n        return (\n            self.flags & C.GSS_C_SEQUENCE_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def domain_to_fqdn(domain, proto=None):\n    \"\"\" returns a fully qualified app domain name \"\"\"", "output": "\n    if not domain:\n        return None\n    if not proto:\n        proto = 'http'\n    return '%s://%s' % (proto, domain)\n\n\ndef fqdn_to_domain(fqdn):\n    \"\"\" returns a domain name from a fully qualified app domain name \"\"\"\n    if not fqdn:\n        return None\n    return fqdn.split('://')[1]\n\n\ndef get_domain_from_request(request):\n    \"\"\" returns the domain from a request \"\"\"\n    return fqdn_to_domain", "solution": "\n    from .generic import get_site_proto\n    proto = proto or get_site_proto()\n    fdqn = '{proto}://{domain}'.format(proto=proto, domain=domain)\n    return fdqn"}
{"prompt": "def default(self, o):\n        \"\"\"Implement this method in a subclass such that it returns a\n        serializable object for ``o``, or calls the base implementation (to\n        raise a ``TypeError``).\n\n        For example, to support arbitrary iterators, you could implement\n        default like this::\n\n            def default(self, o):\n                try:\n                    iterable = iter(o)\n                except TypeError:\n                    pass\n                else:\n                    return list(iterable)\n                return JSONEncoder.default(self, o)\n        \"\"\"", "output": ")\n        \"\"\"\n        raise TypeError(repr(o) + \" is not JSON serializable\")\n\n    def encode(self, o):\n        \"\"\"Return a JSON representation of a Python data structure.\n\n        >>> from json import JSONEncoder\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n        '{\"foo\": [\"bar\", \"baz\"]}'\n\n        \"\"\"\n        # This is for extreme backward compatibility.\n        if isinstance(o, basestring):\n            return o\n        # This should be a safe fall-", "solution": "\n        if isinstance(o, datetime):\n            return http_date(o)\n        if isinstance(o, uuid.UUID):\n            return str(o)\n        if hasattr(o, '__html__'):\n            return text_type(o.__html__())\n        return _json.JSONEncoder.default(self, o)"}
{"prompt": "def merge(\n            self, reservation_order_id, sources=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Merges two `Reservation`s.\n\n        Merge the specified `Reservation`s into a new `Reservation`. The two\n        `Reservation`s being merged must have same properties.\n\n        :param reservation_order_id: Order Id of the reservation\n        :type reservation_order_id: str\n        :param sources: Format of the resource id should be\n         /providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}\n        :type sources: list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns list or\n         ClientRawResponse<list> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`\n        \"\"\"", "output": "be\n         /Providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}\n        :type sources: list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPol", "solution": "\n        raw_result = self._merge_initial(\n            reservation_order_id=reservation_order_id,\n            sources=sources,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[ReservationResponse]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _get_concatenation(extractors, text, *, ignore_whitespace=True):\n  \"\"\"Returns a concatenation ParseNode whose children are the nodes returned by each of the\n  methods in the extractors enumerable.\n\n  If ignore_whitespace is True, whitespace will be ignored and then attached to the child it\n  preceeded.\n  \"\"\"", "output": "\n  children = []\n  for extractor in extractors:\n    node = extractor(text)\n    if node is None:\n      continue\n    if ignore_whitespace:\n      children.extend(node.children)\n    else:\n      children.append(node)\n  return ParseNode(children)\n\n\ndef _get_concatenation_with_whitespace(extractors, text):\n  return _get_concatenation(extractors, text, ignore_whitespace=False)\n\n\ndef _get_conc", "solution": "\n  ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n\n  extractor, *remaining = extractors\n\n  child = _call_extractor(extractor, use_text)\n  child.add_ignored(ignored_ws)\n\n  # TODO: Should I set node.position = -len(text) for the case that ignored whitespace will cause\n  #       the first child's position to not be the whitespace, and therefore the concatenation's\n  #       position will be the first non-whitespace? I think not, but I'm adding this note in\n  #       case that causes an issue I'm not seeing at the moment.\n  node = ParseNode(ParseNodeType.concatenation, children=[child])\n\n  if remaining:\n    # child.consumed will include ignored whitespace, so we base the text we pass on on text rather\n    # than use_text.\n    return node.merged(_get_concatenation(remaining,\n                                          text[child.consumed:],\n                                          ignore_whitespace=ignore_whitespace))\n  else:\n    return node"}
{"prompt": "def add_virtual_columns_rotation(self, x, y, xnew, ynew, angle_degrees, propagate_uncertainties=False):\n        \"\"\"Rotation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:\n        \"\"\"", "output": "\n        angle_radians = angle_degrees * np.pi / 180.\n        xnew_expr = 'cos({})*{} - sin({})*{}'.format(angle_radians, x, angle_radians, y)\n        ynew_expr = 'sin({})*{} + cos({})*{}'.format(angle_radians, x, angle_radians, y)\n        self.add_column(xnew, xnew_expr)\n        self.add_column(ynew, ynew_expr)\n       ", "solution": "\n        x = _ensure_string_from_expression(x)\n        y = _ensure_string_from_expression(y)\n        theta = np.radians(angle_degrees)\n        matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n        m = matrix_name = x + \"_\" + y + \"_rot\"\n        for i in range(2):\n            for j in range(2):\n                self.set_variable(matrix_name + \"_%d%d\" % (i, j), matrix[i, j].item())\n        self[xnew] = self._expr(\"{m}_00 * {x} + {m}_01 * {y}\".format(**locals()))\n        self[ynew] = self._expr(\"{m}_10 * {x} + {m}_11 * {y}\".format(**locals()))\n        if propagate_uncertainties:\n            self.propagate_uncertainties([self[xnew], self[ynew]])"}
{"prompt": "def _call(self, resource, params):\n        \"\"\"Call to get a resource.\n\n        :param method: resource to get\n        :param params: dict with the HTTP parameters needed to get\n            the given resource\n        \"\"\"", "output": "\n        return self._call_method(resource, params)\n\n    def _call_method(self, resource, params):\n        \"\"\"Call to get a resource.\n\n        :param method: resource to get\n        :param params: dict with the HTTP parameters needed to get\n            the given resource\n        \"\"\"\n        url = self.url + resource\n        if params:\n            url += '?' + urlencode(params)\n        response = self.session.get(url)\n        return response\n\n    def _call_post(self, resource, params):\n       ", "solution": "\n        url = self.URL % {'base': self.base_url, 'resource': resource}\n\n        if self.api_token:\n            params[self.PKEY] = self.api_token\n\n        logger.debug(\"Redmine client requests: %s params: %s\",\n                     resource, str(params))\n\n        r = self.fetch(url, payload=params, verify=False)\n\n        return r.text"}
{"prompt": "def maintained_selection():\n    \"\"\"Maintain selection during context\n\n    Example:\n        >>> with maintained_selection():\n        ...     # Modify selection\n        ...     cmds.select('node', replace=True)\n        >>> # Selection restored\n\n    \"\"\"", "output": "\n    selection = cmds.ls(sl=True)\n    try:\n        yield\n    finally:\n        cmds.select(selection, replace=True)\n\n\ndef get_selection():\n    \"\"\"Get current selection\n\n    Returns:\n        list: List of selected nodes\n\n    \"\"\"\n    return cmds.ls(sl=True)\n\n\ndef get_selection_transforms():\n    \"\"\"Get current selection\n\n    Returns:\n        list: List of selected nodes\n\n    \"\"\"\n    return cmds.ls(sl=True, type='", "solution": "\n\n    previous_selection = cmds.ls(selection=True)\n    try:\n        yield\n    finally:\n        if previous_selection:\n            cmds.select(previous_selection,\n                        replace=True,\n                        noExpand=True)\n        else:\n            cmds.select(deselect=True,\n                        noExpand=True)"}
{"prompt": "def get_marginal_distribution(self, index_points=None):\n    \"\"\"Compute the marginal of this GP over function values at `index_points`.\n\n    Args:\n      index_points: `float` `Tensor` representing finite (batch of) vector(s) of\n        points in the index set over which the GP is defined. Shape has the form\n        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e` is the number\n        (size) of index points in each batch. Ultimately this distribution\n        corresponds to a `e`-dimensional multivariate normal. The batch shape\n        must be broadcastable with `kernel.batch_shape` and any batch dims\n        yielded by `mean_fn`.\n\n    Returns:\n      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,\n        according to whether `index_points` consists of one or many index\n        points, respectively.\n    \"\"\"", "output": "ndims` and `e` is the number\n        (size) of points. The batch shape must be broadcastable with `kernel.batch_shape`\n        and any batch dims yielded by `index_points`.\n\n    Returns:\n      marginal_distribution: a `tfp.distributions.Distribution` instance.\n    \"\"\"\n    if index_points is None:\n      index_points = self.index_points\n    return self.marginal_distribution(index_points)\n\n  def _make_marginal_distribution(self, index_", "solution": "\n    with self._name_scope('get_marginal_distribution'):\n      # TODO(cgs): consider caching the result here, keyed on `index_points`.\n      index_points = self._get_index_points(index_points)\n      covariance = self._compute_covariance(index_points)\n      loc = self._mean_fn(index_points)\n      # If we're sure the number of index points is 1, we can just construct a\n      # scalar Normal. This has computational benefits and supports things like\n      # CDF that aren't otherwise straightforward to provide.\n      if self._is_univariate_marginal(index_points):\n        scale = tf.sqrt(covariance)\n        # `loc` has a trailing 1 in the shape; squeeze it.\n        loc = tf.squeeze(loc, axis=-1)\n        return normal.Normal(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')\n      else:\n        scale = tf.linalg.LinearOperatorLowerTriangular(\n            tf.linalg.cholesky(_add_diagonal_shift(covariance, self.jitter)),\n            is_non_singular=True,\n            name='GaussianProcessScaleLinearOperator')\n        return mvn_linear_operator.MultivariateNormalLinearOperator(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')"}
{"prompt": "def _reset(self, index, total, percentage_step, length):\n        \"\"\"Resets to the progressbar to start a new one\"\"\"", "output": "\n        self.index = index\n        self.total = total\n        self.percentage_step = percentage_step\n        self.length = length\n        self.percentage = 0\n        self.percentage_done = 0\n        self.percentage_left = 0\n        self.percentage_left_done = 0\n        self.percentage_left_left = 0\n        self.percentage_left_left_done = 0\n        self.percentage_left_left_left = 0\n        self.percentage_left", "solution": "\n        self._start_time = datetime.datetime.now()\n        self._start_index = index\n        self._current_index = index\n        self._percentage_step = percentage_step\n        self._total = float(total)\n        self._total_minus_one = total - 1\n        self._length = length\n        self._norm_factor = total * percentage_step / 100.0\n        self._current_interval = int((index + 1.0) / self._norm_factor)"}
{"prompt": "def _swap_ops_from_edge(edge, layout):\n    \"\"\"Generate list of ops to implement a SWAP gate along a coupling edge.\"\"\"", "output": "\n    if edge[0] == edge[1]:\n        return []\n\n    if layout[edge[0]] == layout[edge[1]]:\n        return [\n            {\n                'name': 'cx',\n                'targets': [edge[0], edge[1]],\n            },\n            {\n                'name': 'cx',\n                'targets': [edge[1], edge[0]],\n            },\n        ]\n\n    if layout[edge[0]] == layout[edge[1] - 1]:\n        return [\n            {\n", "solution": "\n\n    device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q')\n    qreg_edge = [(device_qreg, i) for i in edge]\n\n    # TODO shouldn't be making other nodes not by the DAG!!\n    return [\n        DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'})\n    ]"}
{"prompt": "def frames(self, key=None, timeoutSecs=60, **kwargs):\n    if not (key is None or isinstance(key, (basestring, Key))):\n        raise Exception(\"frames: key should be string or Key type %s %s\" % (type(key), key))\n\n    params_dict = {\n        'find_compatible_models': 0,\n        'row_offset': 0, # is offset working yet?\n        'row_count': 5,\n    }\n    \"\"\"\n    Return a single Frame or all of the Frames in the h2o cluster.  The\n    frames are contained in a list called \"frames\" at the top level of the\n    result.  Currently the list is unordered.\n    TODO:\n    When find_compatible_models is implemented then the top level \n    dict will also contain a \"models\" list.\n    \"\"\"", "output": "list of Frames.\n\n    :param key: (optional) The key of the Frame to be returned.\n    :param timeoutSecs: (optional) How long to wait for the Frame to be ready.\n    :param kwargs: (optional) Additional arguments to pass to the Frame.\n    :return: A single Frame or list of Frames.\n    \"\"\"\n    if key is None:\n        key = Key.make_frames(kwargs.get('frame_id'))\n\n    if isinstance(key, basestring):\n        key = Key.make(", "solution": "\n    h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'frames', False)\n    \n    # key can be type Key? (from h2o_xl) str(key) should return\n    if key:\n        if isinstance(key, Key):\n            keyStr = key.frame\n        else:\n            keyStr = key\n        result = self.do_json_request('3/Frames.json/' + keyStr, timeout=timeoutSecs, params=params_dict)\n    else:\n        result = self.do_json_request('3/Frames.json', timeout=timeoutSecs, params=params_dict)\n    return result"}
{"prompt": "def queryGitHubFromFile(self, filePath, gitvars={}, verbosity=0, **kwargs):\n        \"\"\"Submit a GitHub GraphQL query from a file.\n\n        Can only be used with GraphQL queries.\n        For REST queries, see the 'queryGitHub' method.\n\n        Args:\n            filePath (str): A relative or absolute path to a file containing\n                a GraphQL query.\n                File may use comments and multi-line formatting.\n                .. _GitHub GraphQL Explorer:\n                   https://developer.github.com/v4/explorer/\n            gitvars (Optional[Dict]): All query variables.\n                Defaults to empty.\n                GraphQL Only.\n            verbosity (Optional[int]): Changes output verbosity levels.\n                If < 0, all extra printouts are suppressed.\n                If == 0, normal print statements are displayed.\n                If > 0, additional status print statements are displayed.\n                Defaults to 0.\n            **kwargs: Keyword arguments for the 'queryGitHub' method.\n\n        Returns:\n            Dict: A JSON style dictionary.\n\n        \"\"\"", "output": "v4/explorer/\n            gitvars (dict): A dictionary of GitHub variables.\n                See the GitHub GraphQL Explorer for more information.\n            verbosity (int): The verbosity level.\n\n        Returns:\n            dict: A dictionary of the query response.\n        \"\"\"\n        query = self.readFile(filePath)\n        return self.queryGitHub(query, gitvars, verbosity, **kwargs)\n\n    def queryGitHub(self, query, gitvars={}, verbosity=0, **kwargs):\n        \"\"\"Submit a GitHub", "solution": "\n        gitquery = self._readGQL(filePath, verbose=(verbosity >= 0))\n        return self.queryGitHub(gitquery, gitvars=gitvars, verbosity=verbosity, **kwargs)"}
{"prompt": "def get_revision(self, location):\n        \"\"\"\n        Return the maximum revision for all files under a given location\n        \"\"\"", "output": "\n        if not self.is_repository_root(location):\n            raise NotImplementedError\n\n        # Get the list of files in the repository\n        files = self.get_list(location)\n\n        # Get the maximum revision for each file\n        max_revision = 0\n        for file in files:\n            revision = self.get_revision(file)\n            if revision > max_revision:\n                max_revision = revision\n\n        return max_revision\n\n    def get_revision(self, location):\n        \"\"\"", "solution": "\n        # Note: taken from setuptools.command.egg_info\n        revision = 0\n\n        for base, dirs, files in os.walk(location):\n            if self.dirname not in dirs:\n                dirs[:] = []\n                continue    # no sense walking uncontrolled subdirs\n            dirs.remove(self.dirname)\n            entries_fn = os.path.join(base, self.dirname, 'entries')\n            if not os.path.exists(entries_fn):\n                # FIXME: should we warn?\n                continue\n\n            dirurl, localrev = self._get_svn_url_rev(base)\n\n            if base == location:\n                base_url = dirurl + '/'   # save the root url\n            elif not dirurl or not dirurl.startswith(base_url):\n                dirs[:] = []\n                continue    # not part of the same svn tree, skip it\n            revision = max(revision, localrev)\n        return revision"}
{"prompt": "def scientific_notation_elements(self, value, locale):\n        \"\"\" Returns normalized scientific notation components of a value.\"\"\"", "output": "\n        if value is None:\n            return None, None, None\n        if locale is None:\n            locale = self.locale\n        if locale is None:\n            locale = 'en'\n        try:\n            value = locale.format_string(value, type='n')\n        except:\n            pass\n        match = re.match(r'^([+-]?\\d+(?:\\.\\d*)?)([eE])([+-]?\\d+)$', value)\n        if match is None:\n            return None, None, None\n", "solution": "\n        # Normalize value to only have one lead digit.\n        exp = value.adjusted()\n        value = value * get_decimal_quantum(exp)\n        assert value.adjusted() == 0\n\n        # Shift exponent and value by the minimum number of leading digits\n        # imposed by the rendering pattern. And always make that number\n        # greater or equal to 1.\n        lead_shift = max([1, min(self.int_prec)]) - 1\n        exp = exp - lead_shift\n        value = value * get_decimal_quantum(-lead_shift)\n\n        # Get exponent sign symbol.\n        exp_sign = ''\n        if exp < 0:\n            exp_sign = babel.numbers.get_minus_sign_symbol(locale)\n        elif self.exp_plus:\n            exp_sign = babel.numbers.get_plus_sign_symbol(locale)\n\n        # Normalize exponent value now that we have the sign.\n        exp = abs(exp)\n\n        return value, exp, exp_sign"}
{"prompt": "def difflag1(self):\n        \"\"\"\n        Conduct a diff-1 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the previous-row element in the same frame.\n        \"\"\"", "output": "\n        return H2OFrame._expr(ExprNode(\"diff1\", self))\n\n    def difflag2(self):\n        \"\"\"\n        Conduct a diff-2 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the previous-row element in the same frame.\n        \"\"\"\n        return H2OFrame._expr(ExprNode(\"diff2\", self))\n\n    def difflag3(self):\n        \"\"\"\n        Conduct a diff-3 transform on a", "solution": "\n        if self.ncols > 1:\n            raise H2OValueError(\"Only single-column frames supported\")\n        if self.types[self.columns[0]] not in {\"real\", \"int\", \"bool\"}:\n            raise H2OValueError(\"Numeric column expected\")\n        fr = H2OFrame._expr(expr=ExprNode(\"difflag1\", self), cache=self._ex._cache)\n        return fr"}
{"prompt": "async def _run_spider_hook(self, hook_func):\n        \"\"\"\n        Run hook before/after spider start crawling\n        :param hook_func: aws function\n        :return:\n        \"\"\"", "output": "\n        if hook_func:\n            hook_func(self.spider)\n\n    def _run_spider_start_requests(self):\n        \"\"\"\n        Run spider start requests\n        :return:\n        \"\"\"\n        for request in self.spider.start_requests():\n            self.crawler.engine.crawl(request, spider=self.spider)\n\n    def _run_spider_close(self):\n        \"\"\"\n        Run spider close\n        :return:\n        \"\"\"\n        self.spider.", "solution": "\n        if callable(hook_func):\n            try:\n                aws_hook_func = hook_func(weakref.proxy(self))\n                if isawaitable(aws_hook_func):\n                    await aws_hook_func\n            except Exception as e:\n                self.logger.error(f'<Hook {hook_func.__name__}: {e}')"}
{"prompt": "def save(self, name=None, output='png', dirc=None):\n        \"\"\"Saves Bloch sphere to file of type ``format`` in directory ``dirc``.\n        Args:\n            name (str):\n                Name of saved image. Must include path and format as well.\n                i.e. '/Users/Paul/Desktop/bloch.png'\n                This overrides the 'format' and 'dirc' arguments.\n            output (str):\n                Format of output image.\n            dirc (str):\n                Directory for output images. Defaults to current working directory.\n        \"\"\"", "output": "\n        \"\"\"\n        if name is None:\n            if dirc is None:\n                dirc = os.getcwd()\n            name = os.path.join(dirc, 'bloch.' + output)\n        plt.savefig(name, format=output, dpi=300)\n\n    def _get_axes(self):\n        \"\"\"Returns the axes of the Bloch sphere.\n        \"\"\"\n        return self._axes\n\n    def _get_points(self):\n        \"\"\"Returns the points of the Bloch sphere.", "solution": "\n\n        self.render()\n        if dirc:\n            if not os.path.isdir(os.getcwd() + \"/\" + str(dirc)):\n                os.makedirs(os.getcwd() + \"/\" + str(dirc))\n        if name is None:\n            if dirc:\n                self.fig.savefig(os.getcwd() + \"/\" + str(dirc) + '/bloch_' +\n                                 str(self.savenum) + '.' + output)\n            else:\n                self.fig.savefig(os.getcwd() + '/bloch_' + str(self.savenum) +\n                                 '.' + output)\n        else:\n            self.fig.savefig(name)\n        self.savenum += 1\n        if self.fig:\n            plt.close(self.fig)"}
{"prompt": "def rejester_run(work_unit):\n     \"\"\"get a rejester.WorkUnit with KBA s3 path, fetch it, and save\n     some counts about it.\n     \"\"\"", "output": "\n     # get the work unit\n     work_unit = work_unit.get()\n     # get the s3 path\n     s3_path = work_unit.get_s3_path()\n     # get the s3 bucket\n     s3_bucket = work_unit.get_s3_bucket()\n     # get the s3 key\n     s3_key = work_unit.get_s3_key()\n     # get the s3 object\n     s3_object = s3_bucket.get_key(s3_key)\n    ", "solution": "\n     #fname = 'verify-chunks-%d-%d' % (os.getpid(), time.time())\n     fname = work_unit.key.strip().split('/')[-1]\n     \n     output_dir_path = work_unit.data.get('output_dir_path', '/mnt')\n     u = uuid.uuid3(uuid.UUID(int=0), work_unit.key.strip())\n     path1 = u.hex[0]\n     path2 = u.hex[1]\n     fpath = os.path.join(output_dir_path, path1, path2, fname)\n     if not os.path.exists(os.path.dirname(fpath)):\n          os.makedirs(os.path.dirname(fpath))\n\n     output = gzip.open(fpath + '-out.gz', 'wb')\n\n     expected_si_count = int(fname.split('-')[1])\n\n     max_tries = 20\n     tries = 0\n     while tries < max_tries:\n          try:\n               exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, stream_ids = \\\n                   attempt_fetch(work_unit, fpath)\n               if si_count != expected_si_count:\n                    print 'retrying because si_count = %d != %d expected_si_count' % (si_count, expected_si_count)\n                    sys.stdout.flush()\n                    tries += 1\n                    continue\n               else:\n                    print 'succeeded in reading si_count = %d' % (si_count,)\n                    sys.stdout.flush()\n               output.write( '%s\\t%d\\t%d\\t%d\\t%d\\t%s\\t%s\\n' % (\n                         exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, \n                         work_unit.key.strip(), ','.join(['%s|%s' % tup for tup in stream_ids])) )\n               break\n          except Exception, exc:\n               print 'broken?'\n               print traceback.format_exc(exc)\n               sys.stdout.flush()\n               tries += 1\n               output.write(traceback.format_exc(exc))\n\n     output.close()"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        parser = argparse.ArgumentParser(\n            description=\"A CLI for the Cisco DNA Center API\",\n            prog=\"dna_center\",\n            add_help=True,\n            formatter_class=argparse.RawTextHelpFormatter,\n        )\n        parser.add_argument(\n            \"--version\",\n            action=\"version\",\n            version=\"%(prog)s (version {version})\".format(version=__version__),\n        )\n        parser.add_argument(\n            \"--host\",\n            help=\"The hostname", "solution": "\n\n        AlarmModify.get_arguments(self)\n        self._alarm_id = self.args.alarm_id if self.args.alarm_id is not None else None\n        self.get_api_parameters()"}
{"prompt": "def html_tags_for(self, asset_type, *args, **kwargs):\n        \"\"\"Return html tags for urls of asset_type\n        \"\"\"", "output": "\n        if asset_type not in self.asset_types:\n            raise ValueError('asset_type must be one of %s' % self.asset_types)\n        return self.html_tags_for_asset_type(asset_type, *args, **kwargs)\n\n    def html_tags_for_asset_type(self, asset_type, *args, **kwargs):\n        \"\"\"Return html tags for urls of asset_type\n        \"\"\"\n        raise NotImplementedError\n\n    def html_tags_for_asset_type_with_id", "solution": "\n        html = []\n        for ref in self.depends:\n            html.append(self._ref(ref).html_tags_for(asset_type, *args, **kwargs))\n        if asset_type in self.typed_bundles:\n            html.append(render_asset_html_tags(asset_type, self.urls_for_self(asset_type, *args, **kwargs)))\n        return \"\\n\".join(html)"}
{"prompt": "def read_key(self, key, bucket_name=None):\n        \"\"\"\n        Reads a key from S3\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"", "output": "\n        if bucket_name is None:\n            bucket_name = self.bucket_name\n\n        try:\n            response = self.client.get_object(Bucket=bucket_name, Key=key)\n            return response['Body'].read()\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'NoSuchKey':\n                raise KeyError(f'Key {key} does not exist')\n            else:\n                raise e\n\n    def write_key(self, key, data, bucket_name=None):", "solution": "\n\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')"}
{"prompt": "def _map_type_to_dict(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding dictionary in root.\n\n        \"\"\"", "output": "\n        return self._root[type_name]\n\n    def _map_type_to_list(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding list in root.\n\n        \"\"\"\n        return self._root[type_name]\n\n    def _map_type_to_list_of_dicts(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding list of dictionaries in root.", "solution": "\n        root = self._root_instance\n\n        if type_name == RESULT:\n            return root._results\n        elif type_name == PARAMETER:\n            return root._parameters\n        elif type_name == DERIVED_PARAMETER:\n            return root._derived_parameters\n        elif type_name == CONFIG:\n            return root._config\n        elif type_name == LEAF:\n            return root._other_leaves\n        else:\n            raise RuntimeError('You shall not pass!')"}
{"prompt": "def create(self, public=False, **kwargs):\n        \"\"\"Creates the device. Attempts to create private devices by default,\n        but if public is set to true, creates public devices.\n\n        You can also set other default properties by passing in the relevant information.\n        For example, setting a device with the given nickname and description::\n\n            dev.create(nickname=\"mydevice\", description=\"This is an example\")\n\n        Furthermore, ConnectorDB supports creation of a device's streams immediately,\n        which can considerably speed up device setup::\n\n            dev.create(streams={\n                \"stream1\": {\"schema\": '{\\\"type\\\":\\\"number\\\"}'}\n            })\n\n        Note that the schema must be encoded as a string when creating in this format.\n        \"\"\"", "output": "\n\n            dev.create(streams=[\n                {\n                    \"name\": \"temperature\",\n                    \"type\": \"float\",\n                    \"description\": \"Temperature in degrees celsius\"\n                },\n                {\n                    \"name\": \"humidity\",\n                    \"type\": \"float\",\n                    \"description\": \"Humidity in percent\"\n                }\n            ])\n\n        :param public: Whether to create a public device or not.\n        :type public: bool\n        :param kwargs: Any other properties to", "solution": "\n        kwargs[\"public\"] = public\n        self.metadata = self.db.create(self.path, kwargs).json()"}
{"prompt": "def _check_init(self, node):\n        \"\"\"check that the __init__ method call super or ancestors'__init__\n        method\n        \"\"\"", "output": "\n        if node.name == '__init__':\n            if not self._check_super_init(node):\n                self.add_message('super-init-not-called', node=node)\n\n    def _check_super_init(self, node):\n        \"\"\"check that the __init__ method call super or ancestors'__init__\n        method\n        \"\"\"\n        if node.args.args:\n            for arg in node.args.args:\n                if arg.name == 'self':\n                    break\n            else:\n                return False", "solution": "\n        if not self.linter.is_message_enabled(\n            \"super-init-not-called\"\n        ) and not self.linter.is_message_enabled(\"non-parent-init-called\"):\n            return\n        klass_node = node.parent.frame()\n        to_call = _ancestors_to_call(klass_node)\n        not_called_yet = dict(to_call)\n        for stmt in node.nodes_of_class(astroid.Call):\n            expr = stmt.func\n            if not isinstance(expr, astroid.Attribute) or expr.attrname != \"__init__\":\n                continue\n            # skip the test if using super\n            if (\n                isinstance(expr.expr, astroid.Call)\n                and isinstance(expr.expr.func, astroid.Name)\n                and expr.expr.func.name == \"super\"\n            ):\n                return\n            try:\n                for klass in expr.expr.infer():\n                    if klass is astroid.Uninferable:\n                        continue\n                    # The infered klass can be super(), which was\n                    # assigned to a variable and the `__init__`\n                    # was called later.\n                    #\n                    # base = super()\n                    # base.__init__(...)\n\n                    if (\n                        isinstance(klass, astroid.Instance)\n                        and isinstance(klass._proxied, astroid.ClassDef)\n                        and is_builtin_object(klass._proxied)\n                        and klass._proxied.name == \"super\"\n                    ):\n                        return\n                    if isinstance(klass, objects.Super):\n                        return\n                    try:\n                        del not_called_yet[klass]\n                    except KeyError:\n                        if klass not in to_call:\n                            self.add_message(\n                                \"non-parent-init-called\", node=expr, args=klass.name\n                            )\n            except astroid.InferenceError:\n                continue\n        for klass, method in not_called_yet.items():\n            cls = node_frame_class(method)\n            if klass.name == \"object\" or (cls and cls.name == \"object\"):\n                continue\n            self.add_message(\"super-init-not-called\", args=klass.name, node=node)"}
{"prompt": "def analyse(self, traj, network, current_subrun, subrun_list, network_dict):\n        \"\"\"Calculates average Fano Factor of a network.\n\n        :param traj:\n\n            Trajectory container\n\n            Expects:\n\n            `results.monitors.spikes_e`: Data from SpikeMonitor for excitatory neurons\n\n            Adds:\n\n            `results.statistics.mean_fano_factor`: Average Fano Factor\n\n        :param network:\n\n            The BRIAN network\n\n        :param current_subrun:\n\n            BrianParameter\n\n        :param subrun_list:\n\n            Upcoming subruns, analysis is only performed if subruns is empty,\n            aka the final subrun has finished.\n\n        :param network_dict:\n\n            Dictionary of items shared among componetns\n\n        \"\"\"", "output": "network\n\n        :param current_subrun:\n\n            The current subrun\n\n        :param subrun_list:\n\n            List of subruns\n\n        :param network_dict:\n\n            Dictionary of network parameters\n\n        :return:\n\n            None\n\n        \"\"\"\n\n        # Get spike times\n        spikes_e = traj.get(\"results.monitors.spikes_e\")\n\n        # Get spike times for each subrun\n        spikes_e_subrun = []\n        for subrun in subrun_", "solution": "\n        #Check if we finished all subruns\n        if len(subrun_list)==0:\n            spikes_e = traj.results.monitors.spikes_e\n\n            time_window = traj.parameters.analysis.statistics.time_window\n            start_time = traj.parameters.simulation.durations.initial_run\n            end_time = start_time+traj.parameters.simulation.durations.measurement_run\n            neuron_ids = traj.parameters.analysis.statistics.neuron_ids\n\n            mean_ff = self._compute_mean_fano_factor(\n                neuron_ids, spikes_e, time_window, start_time, end_time)\n\n            traj.f_add_result('statistics.mean_fano_factor', mean_ff, comment='Average Fano '\n                                                                      'Factor over all '\n                                                                      'exc neurons')\n\n            print('R_ee: %f, Mean FF: %f' % (traj.R_ee, mean_ff))"}
{"prompt": "def _get_properties(config):\n    \"\"\"Returns a tuple of property classes and names.\n\n    Property classes are fully qualified, such as 'abc.abstractproperty' and\n    property names are the actual names, such as 'abstract_property'.\n    \"\"\"", "output": "\n    properties = []\n    for name, value in config.items():\n        if name.startswith('property_'):\n            properties.append((value, name[9:]))\n    return properties\n\n\ndef _get_property_classes(config):\n    \"\"\"Returns a tuple of property classes.\n\n    Property classes are fully qualified, such as 'abc.abstractproperty' and\n    property names are the actual names, such as 'abstract_property'.\n    \"\"\"\n    properties = []\n    for name, value in config.items():\n        if name.startswith", "solution": "\n    property_classes = {BUILTIN_PROPERTY}\n    property_names = set()  # Not returning 'property', it has its own check.\n    if config is not None:\n        property_classes.update(config.property_classes)\n        property_names.update(\n            (prop.rsplit(\".\", 1)[-1] for prop in config.property_classes)\n        )\n    return property_classes, property_names"}
{"prompt": "def _call(self, utterances_batch: list, utterances_ids: Optional[list]=None) -> list:\n        \"\"\"\n        Processes batch of utterances and returns corresponding responses batch.\n\n        Each call of Agent passes incoming utterances batch through skills filter,\n        agent skills, skills processor. Batch of dialog IDs can be provided, in\n        other case utterances indexes in incoming batch are used as dialog IDs.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        \"\"\"", "output": "to utterances.\n\n        Returns:\n            Batch of responses.\n        \"\"\"\n        if utterances_ids is None:\n            utterances_ids = [i for i in range(len(utterances_batch))]\n\n        # filter utterances by skills\n        utterances_batch, utterances_ids = self.filter_utterances(utterances_batch, utterances_ids)\n\n        # process utterances by skills\n        utterances_batch, utterances_ids = self.process_utterances(utterances_batch, utterances_ids)\n\n", "solution": "\n        batch_size = len(utterances_batch)\n        ids = utterances_ids or list(range(batch_size))\n        batch_history = [self.history[utt_id] for utt_id in ids]\n        responses = []\n\n        filtered = self.skills_filter(utterances_batch, batch_history)\n\n        for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)):\n            skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter]\n\n            if skill_i_utt_indexes:\n                skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes]\n                skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes]\n                res = [(None, 0.)] * batch_size\n                predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids)\n\n                for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence):\n                    res[i] = (predicted, confidence)\n\n                responses.append(res)\n\n        responses = self.skills_processor(utterances_batch, batch_history, *responses)\n\n        return responses"}
{"prompt": "def _add_label_from_dict(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from a dictionary.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        query_params['id'] = self.id\n        query_params['label'] = self.label\n        query_params['color'] = self.color\n        query_params['name'] = self.name\n        query_params['value'] = self.value\n        query_params['description'] = self.description\n        query_params['due'] = self.due\n        query_params['dueComplete'] = self.dueComplete\n        query_params['idBoard'] = self.idBoard", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri + '/labels',\n            http_method='POST',\n            query_params=query_params or {}\n        )"}
{"prompt": "def diff_lorenz(value_array, sigma, beta, rho):\n    \"\"\"The Lorenz attractor differential equation\n\n    :param value_array: 3d array containing the x,y, and z component values.\n    :param sigma: Constant attractor parameter\n    :param beta: FConstant attractor parameter\n    :param rho: Constant attractor parameter\n\n    :return: 3d array of the Lorenz system evaluated at `value_array`\n\n    \"\"\"", "output": "\n    x, y, z = value_array\n    dx = sigma * (y - x)\n    dy = x * (rho - z) - y\n    dz = x * y - beta * z\n    return np.array([dx, dy, dz])\n\n\ndef lorenz_system(x0, sigma, beta, rho, t_max, dt, n_steps):\n    \"\"\"The Lorenz attractor differential equation\n\n    :param x0: 3d array containing the x,y, and z component values.\n    :param", "solution": "\n    diff_array = np.zeros(3)\n    diff_array[0] = sigma * (value_array[1]-value_array[0])\n    diff_array[1] = value_array[0] * (rho - value_array[2]) - value_array[1]\n    diff_array[2] = value_array[0] * value_array[1] - beta * value_array[2]\n\n    return diff_array"}
{"prompt": "def _onDeviceCommand(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for gateway command messages, parses source device from topic string and\n        passes the information on to the registered device command callback\n        \"\"\"", "output": "\n        topic = pahoMessage.topic\n        payload = pahoMessage.payload\n        device = topic.split(\"/\")[1]\n        if device in self.devices:\n            self.devices[device].onCommand(client, userdata, pahoMessage)\n        else:\n            print(\"Received command for unknown device: \" + device)\n\n    def _onDeviceStatus(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for gateway status messages, parses source device from topic string and\n        passes the information on to", "solution": "\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received gateway command '%s'\" % (command.command))\n            if self.deviceCommandCallback:\n                self.deviceCommandCallback(command)"}
{"prompt": "def wait_next_block_factory(app, timeout=None):\n    \"\"\"Creates a `wait_next_block` function, that\n    will wait `timeout` seconds (`None` = indefinitely)\n    for a new block to appear.\n\n    :param app: the app-instance the function should work for\n    :param timeout: timeout in seconds\n    \"\"\"", "output": "\n    def wait_next_block(timeout=timeout):\n        \"\"\"Waits for a new block to appear.\n\n        :param timeout: timeout in seconds\n        \"\"\"\n        if timeout is None:\n            timeout = 10000000000000000000000000000000000000000000000000000000000000000000000000000", "solution": "\n\n    chain = app.services.chain\n\n    # setup new block callbacks and events\n    new_block_evt = gevent.event.Event()\n\n    def _on_new_block(app):\n        log.DEV('new block mined')\n        new_block_evt.set()\n    chain.on_new_head_cbs.append(_on_new_block)\n\n    def wait_next_block():\n        bn = chain.chain.head.number\n        chain.consensus_manager.log('waiting for new block', block=bn)\n        new_block_evt.wait(timeout)\n        new_block_evt.clear()\n        if chain.chain.head.number > bn:\n            chain.consensus_manager.log('new block event', block=chain.chain.head.number)\n        elif chain.chain.head.number == bn:\n            chain.consensus_manager.log('wait_next_block timed out', block=bn)\n\n    return wait_next_block"}
{"prompt": "def _set_affiliation(self, v, load=False):\n        \"\"\"\n    Setter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_affiliation is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_affiliation() directly.\n    \"\"\"", "output": "\n        if not load:\n            self._set_affiliation(v)\n        else:\n            self._affiliation = v\n\n    def _get_affiliation(self):\n        \"\"\"\n    Getter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _get_affiliation is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling", "solution": "\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=RestrictedClassType(\n                    base_type=unicode,\n                    restriction_type=\"dict_key\",\n                    restriction_arg={\n                        u\"napalm-star-wars:EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"napalm-star-wars:REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                    },\n                ),\n                is_leaf=True,\n                yang_name=\"affiliation\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                namespace=\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                defining_module=\"napalm-star-wars\",\n                yang_type=\"identityref\",\n                is_config=True,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": "}
{"prompt": "def run(self, cmd, start_opts=None, globals_=None, locals_=None):\n        \"\"\" Run debugger on string `cmd' using builtin function eval\n        and if that builtin exec.  Arguments `globals_' and `locals_'\n        are the dictionaries to use for local and global variables. By\n        default, the value of globals is globals(), the current global\n        variables. If `locals_' is not given, it becomes a copy of\n        `globals_'.\n\n        Debugger.core.start settings are passed via optional\n        dictionary `start_opts'. Overall debugger settings are in\n        Debugger.settings which changed after an instance is created\n        . Also see `run_eval' if what you want to run is an\n        run_eval'able expression have that result returned and\n        `run_call' if you want to debug function run_call.\n        \"\"\"", "output": ".run() is called to run the command.\n        \"\"\"\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_.copy()\n        self.runcmd(cmd, start_opts, globals_, locals_)\n\n    def runcmd(self, cmd, start_opts=None, globals_=None, locals_=None):\n        \"\"\" Run debugger on string `cmd' using builtin function eval\n        and if that builtin exec.  Arguments `glob", "solution": "\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_\n        if not isinstance(cmd, types.CodeType):\n            self.eval_string = cmd\n            cmd = cmd+'\\n'\n            pass\n        retval = None\n        self.core.start(start_opts)\n        try:\n            retval = eval(cmd, globals_, locals_)\n        except SyntaxError:\n            try:\n                exec(cmd, globals_, locals_)\n            except DebuggerQuit:\n                pass\n            except DebuggerQuit:\n                pass\n            pass\n        except DebuggerQuit:\n            pass\n        finally:\n            self.core.stop()\n        return retval"}
{"prompt": "def make_links_absolute(self, base_url=None, resolve_base_href=True,\n                            handle_failures=None):\n        \"\"\"\n        Make all links in the document absolute, given the\n        ``base_url`` for the document (the full URL where the document\n        came from), or if no ``base_url`` is given, then the ``.base_url``\n        of the document.\n\n        If ``resolve_base_href`` is true, then any ``<base href>``\n        tags in the document are used *and* removed from the document.\n        If it is false then any such tag is ignored.\n\n        If ``handle_failures`` is None (default), a failure to process\n        a URL will abort the processing.  If set to 'ignore', errors\n        are ignored.  If set to 'discard', failing URLs will be removed.\n        \"\"\"", "output": "* the ``base_url`` is\n        updated to the new ``href`` value.\n\n        If ``handle_failures`` is true, then any links that fail to\n        resolve are logged to the ``self.log`` list.\n\n        If ``handle_failures`` is false, then any links that fail to\n        resolve are ignored.\n\n        If ``handle_failures`` is None, then any links that fail to\n        resolve are logged to the ``self.log`` list, but the\n        ``self.log`` list is not cleared before the links are\n", "solution": "\n        if base_url is None:\n            base_url = self.base_url\n            if base_url is None:\n                raise TypeError(\n                    \"No base_url given, and the document has no base_url\")\n        if resolve_base_href:\n            self.resolve_base_href()\n\n        if handle_failures == 'ignore':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return href\n        elif handle_failures == 'discard':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return None\n        elif handle_failures is None:\n            def link_repl(href):\n                return urljoin(base_url, href)\n        else:\n            raise ValueError(\n                \"unexpected value for handle_failures: %r\" % handle_failures)\n\n        self.rewrite_links(link_repl)"}
{"prompt": "def signin_card(card: SigninCard) -> Attachment:\n        \"\"\"\n        Returns an attachment for a signin card. For channels that don't natively support signin cards an alternative\n        message will be rendered. Will raise a TypeError if 'card' argument is not a SigninCard.\n        :param card:\n        :return:\n        \"\"\"", "output": "\n        if not isinstance(card, SigninCard):\n            raise TypeError(\"'card' must be of type 'SigninCard'\")\n\n        return Attachment(\n            content_type=CardConstants.content_type,\n            content=card.to_dict(),\n        )\n\n    @staticmethod\n    def thumbnail_card(card: ThumbnailCard) -> Attachment:\n        \"\"\"\n        Returns an attachment for a thumbnail card. For channels that don't natively support thumbnail cards an\n        alternative message will be rendered. Will raise a TypeError if", "solution": "\n        if not isinstance(card, SigninCard):\n            raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of an SigninCard, '\n                            'unable to prepare attachment.')\n\n        return Attachment(content_type=CardFactory.content_types.signin_card,\n                          content=card)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Clear GDoc Spreadsheet by sending empty csv file.\n        \"\"\"", "output": "\n        self.send_csv_file(self.empty_csv_file)\n\n    def send_csv_file(self, csv_file):\n        \"\"\"\n        Send csv file to GDoc Spreadsheet.\n        \"\"\"\n        self.logger.info(\"Sending csv file to GDoc Spreadsheet\")\n        self.logger.info(f\"CSV file: {csv_file}\")\n        self.logger.info(f\"GDoc Spreadsheet: {self.spreadsheet_id}\")\n        self.logger.info(f\"GDoc Spreadsheet sheet: {", "solution": "\n        empty_file_path = os.path.join(self.temp_path, 'empty.csv')\n        try:\n            empty_file = open(empty_file_path, 'w')\n            empty_file.write(',')\n            empty_file.close()\n        except IOError as e:\n            raise PODocsError(e)\n\n        self._upload_file_to_gdoc(empty_file_path, content_type='text/csv')\n\n        os.remove(empty_file_path)"}
{"prompt": "def add_item_to_basket(self, item, variant=VARIANT.MEDIUM, quantity=1):\n        \"\"\"\n        Add an item to the current basket.\n\n        :param Item item: Item from menu.\n        :param int variant: Item SKU id. Ignored if the item is a side.\n        :param int quantity: The quantity of item to be added.\n        :return: A response having added an item to the current basket.\n        :rtype: requests.Response\n        \"\"\"", "output": "\n        return self.post(\n            'basket/add_item',\n            data={\n                'item_id': item.id,\n                'variant': variant,\n                'quantity': quantity\n            }\n        )\n\n    def remove_item_from_basket(self, item, variant=VARIANT.MEDIUM, quantity=1):\n        \"\"\"\n        Remove an item from the current basket.\n\n        :param Item item: Item from menu.\n        :param int variant: Item SKU id. Ignored if the item is a side.", "solution": "\n        item_type = item.type\n\n        if item_type == 'Pizza':\n            return self.add_pizza_to_basket(item, variant, quantity)\n        elif item_type == 'Side':\n            return self.add_side_to_basket(item, quantity)\n        return None"}
{"prompt": "def validate_expression(self, expression):\n        \"\"\"Validate an expression (may throw Exceptions)\"\"\"", "output": "\n        if expression is None:\n            raise ValueError(\"expression is None\")\n        if not isinstance(expression, str):\n            raise ValueError(\"expression is not a string\")\n        if not expression:\n            raise ValueError(\"expression is empty\")\n        if not self.is_valid_expression(expression):\n            raise ValueError(\"expression is invalid\")\n\n    def is_valid_expression(self, expression):\n        \"\"\"Check if an expression is valid\"\"\"\n        if expression is None:\n            return False\n        if not isinstance(expression, str):\n", "solution": "\n        # return self.evaluate(expression, 0, 2)\n        vars = set(self.get_column_names()) | set(self.variables.keys())\n        funcs = set(expression_namespace.keys())\n        return vaex.expresso.validate_expression(expression, vars, funcs)"}
{"prompt": "def plot(self, fig=None):\n        \"\"\"Plot visualization of model prediction.\"\"\"", "output": "\n        if fig is None:\n            fig = plt.figure()\n\n        ax = fig.add_subplot(111)\n        ax.set_title(\"Model Prediction\")\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"y\")\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n        ax.grid(True)\n\n        x = np.linspace(-1, 1, 100)\n        y = self.predict", "solution": "\n        if not fig:\n            fig = plt.gcf()\n\n        fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)\n        ax = fig.add_subplot(1, 1, 1)\n\n        sorted_overlapping_ports = sorted(\n            [(p, self.results['port cycles'][p]) for p in self.machine['overlapping ports']],\n            key=lambda x: x[1])\n\n        yticks_labels = []\n        yticks = []\n        xticks_labels = []\n        xticks = []\n\n        # Plot configuration\n        height = 0.9\n\n        i = 0\n        # T_OL\n        colors = ([(254. / 255, 177. / 255., 178. / 255.)] +\n                  [(255. / 255., 255. / 255., 255. / 255.)] * (len(sorted_overlapping_ports) - 1))\n        for p, c in sorted_overlapping_ports:\n            ax.barh(i, c, height, align='center', color=colors.pop(),\n                    edgecolor=(0.5, 0.5, 0.5), linestyle='dashed')\n            if i == len(sorted_overlapping_ports) - 1:\n                ax.text(c / 2.0, i, '$T_\\mathrm{OL}$', ha='center', va='center')\n            yticks_labels.append(p)\n            yticks.append(i)\n            i += 1\n        xticks.append(sorted_overlapping_ports[-1][1])\n        xticks_labels.append('{:.1f}'.format(sorted_overlapping_ports[-1][1]))\n\n        # T_nOL + memory transfers\n        y = 0\n        colors = [(187. / 255., 255 / 255., 188. / 255.)] * (len(self.results['cycles'])) + \\\n                 [(119. / 255, 194. / 255., 255. / 255.)]\n        for k, v in [('nOL', self.results['T_nOL'])] + self.results['cycles']:\n            ax.barh(i, v, height, y, align='center', color=colors.pop())\n            ax.text(y + v / 2.0, i, '$T_\\mathrm{' + k + '}$', ha='center', va='center')\n            xticks.append(y + v)\n            xticks_labels.append('{:.1f}'.format(y + v))\n            y += v\n        yticks_labels.append('LD')\n        yticks.append(i)\n\n        ax.tick_params(axis='y', which='both', left='off', right='off')\n        ax.tick_params(axis='x', which='both', top='off')\n        ax.set_xlabel('t [cy]')\n        ax.set_ylabel('execution port')\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticks_labels)\n        ax.set_xticks(xticks)\n        ax.set_xticklabels(xticks_labels, rotation='vertical')\n        ax.xaxis.grid(alpha=0.7, linestyle='--')\n        fig.savefig(self._args.ecm_plot)"}
{"prompt": "def reduceCnf(cnf):\n    \"\"\"\n    I just found a remarkably large bug in my SAT solver and found an\n    interesting solution.\n    Remove all b | -b\n    (-b | b) & (b | -a) & (-b | a) & (a | -a)\n    becomes\n    (b | -a) & (-b | a)\n\n    Remove all (-e) & (-e)\n    (-e | a) & (-e | a) & (-e | a) & (-e | a)\n    becomes\n    (-e | a)\n    (-b | b | c) becomes nothing, not (c)\n    \"\"\"", "output": "becomes\n    (-e | a)\n    \"\"\"\n    newCnf = []\n    for clause in cnf:\n        newClause = []\n        for literal in clause:\n            if literal[0] == '-':\n                literal = literal[1:]\n            if literal not in newClause:\n                newClause.append(literal)\n        newCnf.append(newClause)\n    return newCnf\n\ndef reduceCnf2(cnf):\n    \"\"\"\n    Remove all b | -b\n    (-b", "solution": "\n    output = Cnf()\n    for x in cnf.dis:\n        dont_add = False\n        for y in x:\n            for z in x:\n                if z == -y:\n                    dont_add = True\n                    break\n            if dont_add: break\n        if dont_add: continue\n        # TODO: Is this necessary anymore? Probably not. Do statistical analysis.\n        if x not in output.dis:\n            output.dis |= frozenset([x])\n    return output"}
{"prompt": "def edit(self,index=None):\n        \"\"\"Edit a block.\n\n        If no number is given, use the last block executed.\n\n        This edits the in-memory copy of the demo, it does NOT modify the\n        original source file.  If you want to do that, simply open the file in\n        an editor and use reload() when you make changes to the file.  This\n        method is meant to let you change a block during a demonstration for\n        explanatory purposes, without damaging your original script.\"\"\"", "output": "\n        if index is None:\n            index = self.last_block\n        if index is None:\n            raise ValueError(\"No block to edit\")\n        if index < 0 or index >= len(self.blocks):\n            raise ValueError(\"Block index out of range\")\n        self.blocks[index].edit()\n\n    def reload(self):\n        \"\"\"Reload the demo from the original source file.\n\n        This is useful if you have made changes to the demo source file.\n        It will reload the original source file, and replace the in-", "solution": "\n\n        index = self._get_index(index)\n        if index is None:\n            return\n        # decrease the index by one (unless we're at the very beginning), so\n        # that the default demo.edit() call opens up the sblock we've last run\n        if index>0:\n            index -= 1\n\n        filename = self.shell.mktempfile(self.src_blocks[index])\n        self.shell.hooks.editor(filename,1)\n        new_block = file_read(filename)\n        # update the source and colored block\n        self.src_blocks[index] = new_block\n        self.src_blocks_colored[index] = self.ip_colorize(new_block)\n        self.block_index = index\n        # call to run with the newly edited index\n        self()"}
{"prompt": "def enum(*sequential, **named):\n    \"\"\"\n    Build an enum statement\n    \"\"\"", "output": "\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    return type('Enum', (), enums)\n\n\nclass Enum(object):\n    \"\"\"\n    Enum class\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.args = args\n        self.kwargs = kwargs\n\n    def __repr__(self):\n        return 'Enum({}, {})'.format(self.args, self.kwargs)\n\n    def __str__(self):\n        return 'Enum", "solution": "\n    #: build enums from parameter\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    enums['map'] = copy.copy(enums)\n    #: build reverse mapping\n    enums['rmap'] = {}\n    for key, value in enums.items():\n        if type(value) is int:\n            enums['rmap'][value] = key\n    return type('Enum', (), enums)"}
{"prompt": "def check_part(state, name, part_msg, missing_msg=None, expand_msg=None):\n    \"\"\"Return child state with name part as its ast tree\"\"\"", "output": "\n    if name not in state.parts:\n        if missing_msg is None:\n            missing_msg = \"missing part '%s'\" % name\n        raise ValueError(missing_msg)\n    part = state.parts[name]\n    if part.ast is None:\n        if expand_msg is None:\n            expand_msg = \"expand part '%s'\" % name\n        raise ValueError(expand_msg)\n    if part.ast.msg != part_msg:\n        raise ValueError(\"part '%s' has wrong message '%s'\" % (name,", "solution": "\n\n    if missing_msg is None:\n        missing_msg = \"Are you sure you defined the {{part}}? \"\n    if expand_msg is None:\n        expand_msg = \"Did you correctly specify the {{part}}? \"\n\n    if not part_msg:\n        part_msg = name\n    append_message = {\"msg\": expand_msg, \"kwargs\": {\"part\": part_msg}}\n\n    has_part(state, name, missing_msg, append_message[\"kwargs\"])\n\n    stu_part = state.student_parts[name]\n    sol_part = state.solution_parts[name]\n\n    assert_ast(state, sol_part, append_message[\"kwargs\"])\n\n    return part_to_child(stu_part, sol_part, append_message, state)"}
{"prompt": "def list_datasets(self, get_global_public):\n        \"\"\"\n        Lists datasets in resources. Setting 'get_global_public' to 'True'\n        will retrieve all public datasets in cloud. 'False' will get user's\n        public datasets.\n\n        Arguments:\n            get_global_public (bool): True if user wants all public datasets in\n                                      cloud. False if user wants only their\n                                      public datasets.\n\n        Returns:\n            dict: Returns datasets in JSON format\n\n        \"\"\"", "output": "\n        if get_global_public:\n            url = self.url + '/datasets'\n        else:\n            url = self.url + '/datasets/public'\n\n        response = requests.get(url, headers=self.headers)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception(response.text)\n\n    def get_dataset(self, dataset_id):\n        \"\"\"\n        Gets dataset by id.\n\n        Arguments:\n            dataset_id (str", "solution": "\n        appending = \"\"\n        if get_global_public:\n            appending = \"public\"\n        url = self.url() + \"/resource/{}dataset/\".format(appending)\n        req = self.remote_utils.get_url(url)\n\n        if req.status_code is not 200:\n            raise RemoteDataNotFoundError('Could not find {}'.format(req.text))\n        else:\n            return req.json()"}
{"prompt": "def is_known_type(self, type_name):\n        \"\"\"Check if type is known to the type system.\n\n        Returns:\n            bool: True if the type is a known instantiated simple type, False otherwise\n        \"\"\"", "output": "\n        return type_name in self.simple_types\n\n    def get_type_name(self, type_name):\n        \"\"\"Get the type name of a type.\n\n        Args:\n            type_name (str): The type name to get the type name of.\n\n        Returns:\n            str: The type name of the type.\n        \"\"\"\n        if type_name in self.simple_types:\n            return type_name\n        else:\n            return self.simple_types[type_name]\n\n    def get_type_info(", "solution": "\n\n        type_name = str(type_name)\n        if type_name in self.known_types:\n            return True\n\n        return False"}
{"prompt": "async def fetch_nearby(lat: float, long: float, limit: int = 10) -> Optional[List[Dict]]:\n    \"\"\"\n    Gets wikipedia articles near a given set of coordinates.\n    :raise ApiError: When there was an error connecting to the API.\n\n    todo cache\n    \"\"\"", "output": "\n    params = {\n        'action': 'query',\n        'list': 'geosearch',\n        'gsradius': '1000',\n        'gscoord': f'{lat}|{long}',\n        'gslimit': limit,\n        'format': 'json',\n        'gsprop': 'title|extracts',\n        'gsdir': 'NW',\n        'gsformat': 'json',\n        'gsnamespace': '0',\n        'gscontinue': ''\n    }\n\n    response = requests.get(WI", "solution": "\n    request_url = f\"https://en.wikipedia.org/w/api.php?action=query\" \\\n                  f\"&list=geosearch\" \\\n                  f\"&gscoord={lat}%7C{long}\" \\\n                  f\"&gsradius=10000\" \\\n                  f\"&gslimit={limit}\" \\\n                  f\"&format=json\"\n\n    async with ClientSession() as session:\n        try:\n            async with session.get(request_url) as request:\n                if request.status == 404:\n                    return None\n                data = (await request.json())[\"query\"][\"geosearch\"]\n\n        except ClientConnectionError as con_err:\n            logger.debug(f\"Could not connect to {con_err.host}\")\n            raise ApiError(f\"Could not connect to {con_err.host}\")\n        except JSONDecodeError as dec_err:\n            logger.error(f\"Could not decode data: {dec_err}\")\n            raise ApiError(f\"Could not decode data: {dec_err}\")\n        except KeyError:\n            return None\n        else:\n            for location in data:\n                location.pop(\"ns\")\n                location.pop(\"primary\")\n            return data"}
{"prompt": "def create_symmetric_key(self, algorithm, length):\n        \"\"\"\n        Create a symmetric key.\n\n        Args:\n            algorithm(CryptographicAlgorithm): An enumeration specifying the\n                algorithm for which the created key will be compliant.\n            length(int): The length of the key to be created. This value must\n                be compliant with the constraints of the provided algorithm.\n\n        Returns:\n            dict: A dictionary containing the key data, with the following\n                key/value fields:\n                * value - the bytes of the key\n                * format - a KeyFormatType enumeration for the bytes format\n\n        Raises:\n            InvalidField: Raised when the algorithm is unsupported or the\n                length is incompatible with the algorithm.\n            CryptographicFailure: Raised when the key generation process\n                fails.\n\n        Example:\n            >>> engine = CryptographyEngine()\n            >>> key = engine.create_symmetric_key(\n            ...     CryptographicAlgorithm.AES, 256)\n        \"\"\"", "output": "\n                * algorithm - the algorithm for which the key is compliant\n                * length - the length of the key\n        \"\"\"\n        raise NotImplementedError()\n\n    def create_asymmetric_key_pair(self, algorithm, length):\n        \"\"\"\n        Create an asymmetric key pair.\n\n        Args:\n            algorithm(CryptographicAlgorithm): An enumeration specifying the\n                algorithm for which the created key will be compliant.\n            length(int): The length of the key to be created. This value must\n                be compliant", "solution": "\n        if algorithm not in self._symmetric_key_algorithms.keys():\n            raise exceptions.InvalidField(\n                \"The cryptographic algorithm {0} is not a supported symmetric \"\n                \"key algorithm.\".format(algorithm)\n            )\n\n        cryptography_algorithm = self._symmetric_key_algorithms.get(algorithm)\n\n        if length not in cryptography_algorithm.key_sizes:\n            raise exceptions.InvalidField(\n                \"The cryptographic length ({0}) is not valid for \"\n                \"the cryptographic algorithm ({1}).\".format(\n                    length, algorithm.name\n                )\n            )\n\n        self.logger.info(\n            \"Generating a {0} symmetric key with length: {1}\".format(\n                algorithm.name, length\n            )\n        )\n\n        key_bytes = os.urandom(length // 8)\n        try:\n            cryptography_algorithm(key_bytes)\n        except Exception as e:\n            self.logger.exception(e)\n            raise exceptions.CryptographicFailure(\n                \"Invalid bytes for the provided cryptographic algorithm.\")\n\n        return {'value': key_bytes, 'format': enums.KeyFormatType.RAW}"}
{"prompt": "def _get_organisations(self):\n        \"\"\"\n        :returns: list of organisations, sorted alphabetically\n        :rtype: list(list(str))\n        \"\"\"", "output": "\n        organisations = []\n        for org in self.organisations:\n            organisations.append(org.get_name())\n        organisations.sort()\n        return organisations\n\n    def _get_organisations_with_counts(self):\n        \"\"\"\n        :returns: list of organisations, sorted alphabetically\n        :rtype: list(list(str))\n        \"\"\"\n        organisations = []\n        for org in self.organisations:\n            organisations.append([org.get_name(), org.get_count()])", "solution": "\n        organisations = []\n        for child in self.vcard.getChildren():\n            if child.name == \"ORG\":\n                organisations.append(child.value)\n        return sorted(organisations)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Completely resets the database. This means that all information in\n        the local cache and on disc will be erased.\n        \"\"\"", "output": "\n        self.db.clear()\n\n    def get(self, key, default=None):\n        \"\"\"\n        Returns the value for the given key. If the key is not present,\n        the default value is returned.\n\n        :param key: The key to look up.\n        :param default: The default value to return if the key is not present.\n        :return: The value for the given key.\n        \"\"\"\n        return self.db.get(key, default)\n\n    def set(self, key, value):\n        \"\"\"\n        Sets", "solution": "\n        if not os.path.isdir(self.fdir):\n            os.makedirs(self.fdir, exist_ok=True)\n            return\n\n        for f in os.listdir(self.fdir):\n            del self[f]"}
{"prompt": "def reset_selective(self, regex=None):\n        \"\"\"Clear selective variables from internal namespaces based on a\n        specified regular expression.\n\n        Parameters\n        ----------\n        regex : string or compiled pattern, optional\n            A regular expression pattern that will be used in searching\n            variable names in the users namespaces.\n        \"\"\"", "output": "\n        if regex is None:\n            regex = '.*'\n        elif not isinstance(regex, (str, bytes)):\n            regex = regex.pattern\n        names = [name for name in self.user_ns if re.match(regex, name)]\n        for name in names:\n            del self.user_ns[name]\n\n    def reset_all(self):\n        \"\"\"Clear all variables from internal namespaces.\"\"\"\n        self.reset_user()\n        self.reset_user_global()\n        self.reset_user_ns()\n       ", "solution": "\n        if regex is not None:\n            try:\n                m = re.compile(regex)\n            except TypeError:\n                raise TypeError('regex must be a string or compiled pattern')\n            # Search for keys in each namespace that match the given regex\n            # If a match is found, delete the key/value pair.\n            for ns in self.all_ns_refs:\n                for var in ns:\n                    if m.search(var):\n                        del ns[var]"}
{"prompt": "def load_from_path(path):\n    \"\"\"\n    Load a spec from a given path, discovering specs if a directory is given.\n\n    \"\"\"", "output": "\n    if os.path.isdir(path):\n        specs = []\n        for root, dirs, files in os.walk(path):\n            for name in files:\n                if name.endswith('.spec'):\n                    specs.append(os.path.join(root, name))\n        if not specs:\n            raise ValueError('No specs found in %s' % path)\n        return specs\n    elif os.path.isfile(path):\n        return [path]\n    else:\n        raise ValueError('%s", "solution": "\n\n    if os.path.isdir(path):\n        paths = discover(path)\n    else:\n        paths = [path]\n\n    for path in paths:\n        name = os.path.basename(os.path.splitext(path)[0])\n        imp.load_source(name, path)"}
{"prompt": "def _has_role(self, role_name_or_list):\n        \"\"\"\n        Whether the user has this role name\n        \"\"\"", "output": "\n        if isinstance(role_name_or_list, list):\n            return any(self.has_role(role_name) for role_name in role_name_or_list)\n        else:\n            return self.has_role(role_name_or_list)\n\n    @property\n    def is_admin(self):\n        \"\"\"\n        Whether the user is an administrator\n        \"\"\"\n        return self._has_role('admin')\n\n    @property\n    def is_active(self):\n        \"\"\"\n        Whether the user is active", "solution": "\n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])"}
{"prompt": "def submit(self, executor, task, tag=None):\n        \"\"\"Submits a task to a provided executor\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to submit to the executor\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: A tag to associate to the submitted task\n\n        :rtype: concurrent.futures.Future\n        :returns: A future representing the submitted task\n        \"\"\"", "output": ":rtype: s3transfer.futures.Future\n        :return: A future representing the submitted task\n        \"\"\"\n        future = executor.submit(task.callable, *task.args, **task.kwargs)\n        future.add_done_callback(self._done_callback)\n        if tag is not None:\n            future.add_tag(tag)\n        return future\n\n    def _done_callback(self, future):\n        \"\"\"Callback for when a future is done\n\n        :type future: s3transfer.futures", "solution": "\n        logger.debug(\n            \"Submitting task %s to executor %s for transfer request: %s.\" % (\n                task, executor, self.transfer_id)\n        )\n        future = executor.submit(task, tag=tag)\n        # Add this created future to the list of associated future just\n        # in case it is needed during cleanups.\n        self.add_associated_future(future)\n        future.add_done_callback(\n            FunctionContainer(self.remove_associated_future, future))\n        return future"}
{"prompt": "def _get_streams(self):\n        \"\"\"\n        Find the streams for OlympicChannel\n        :return:\n        \"\"\"", "output": "\n        streams = HLSStream.parse_variant_playlist(self.session,\n                                                   self.url)\n        return streams\n\n\nclass OlympicChannel(Plugin):\n    @classmethod\n    def can_handle_url(cls, url):\n        return _url_re.match(url)\n\n    def _get_streams(self):\n        match = _url_re.match(self.url)\n        channel = match.group(\"channel\")\n        if channel == \"olympic\":\n            return _OlympicStream.parse", "solution": "\n        match = self._url_re.match(self.url)\n        type_of_stream = match.group('type')\n        lang = re.search(r\"/../\", self.url).group(0)\n\n        if type_of_stream == 'tv':\n            path = re.search(r\"tv/.*-\\d/$\", self.url).group(0)\n\n            return self._get_live_streams(lang, path)\n        elif type_of_stream == 'playback':\n            path = re.search(r\"/playback/.*/$\", self.url).group(0)\n            return self._get_vod_streams()"}
{"prompt": "def compute_ffmc2d(X):\n    \"\"\"Computes the 2D-Fourier Magnitude Coefficients.\"\"\"", "output": "\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2D array.\")\n    X = np.fft.fft2(X)\n    X = np.abs(X)\n    return X\n\n\ndef compute_fftmc2d(X):\n    \"\"\"Computes the 2D-Fourier Magnitude Coefficients.\"\"\"\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueError(\"", "solution": "\n    # 2d-fft\n    fft2 = scipy.fftpack.fft2(X)\n\n    # Magnitude\n    fft2m = magnitude(fft2)\n\n    # FFTshift and flatten\n    fftshift = scipy.fftpack.fftshift(fft2m).flatten()\n\n    #cmap = plt.cm.get_cmap('hot')\n    #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation=\"nearest\",\n    #    aspect=\"auto\", cmap=cmap)\n    #plt.show()\n\n    # Take out redundant components\n    return fftshift[:fftshift.shape[0] // 2 + 1]"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        for line in lines:\n            line = self.prefilter_line(line, continue_prompt)\n            if line is not None:\n                yield line\n\n    def prefilter_line(self, line, continue_prompt=False):\n        \"\"\"Prefilter a single input line of text.\n\n        This is the main entry point for prefiltering a single line of input.\n        This is called by :meth:`prefilter_lines` for each line of input.\n\n        This covers cases where there is a single line of", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def get_service_certificate(self, service_name, thumbalgorithm, thumbprint):\n        \"\"\"\n        Returns the public data for the specified X.509 certificate associated\n        with a hosted service.\n\n        service_name:\n            Name of the hosted service.\n        thumbalgorithm:\n            The algorithm for the certificate's thumbprint.\n        thumbprint:\n            The hexadecimal representation of the thumbprint.\n        \"\"\"", "output": "\n        return self.get_certificate(service_name, thumbalgorithm, thumbprint)\n\n    def get_certificate(self, service_name, thumbalgorithm, thumbprint):\n        \"\"\"\n        Returns the public data for the specified X.509 certificate.\n\n        service_name:\n            Name of the hosted service.\n        thumbalgorithm:\n            The algorithm for the certificate's thumbprint.\n        thumbprint:\n            The hexadecimal representation of the thumbprint.\n        \"\"\"\n        return self.get_resource(\n            'certificates", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('thumbalgorithm', thumbalgorithm)\n        _validate_not_none('thumbprint', thumbprint)\n        return self._perform_get(\n            '/' + self.subscription_id + '/services/hostedservices/' +\n            _str(service_name) + '/certificates/' +\n            _str(thumbalgorithm) + '-' + _str(thumbprint) + '',\n            Certificate)"}
{"prompt": "def get_default_args(func):\n    \"\"\"\n    returns a dictionary of arg_name:default_values for the input function\n    \"\"\"", "output": "\n    argspec = inspect.getfullargspec(func)\n    defaults = argspec.defaults\n    if defaults is None:\n        return {}\n    else:\n        return dict(zip(argspec.args[-len(defaults):], defaults))\n\n\ndef get_default_args_from_func(func):\n    \"\"\"\n    returns a dictionary of arg_name:default_values for the input function\n    \"\"\"\n    argspec = inspect.getfullargspec(func)\n    defaults = argspec.defaults\n    if defaults is None:\n", "solution": "\n    args, _, _, defaults, *rest = inspect.getfullargspec(func)\n    return dict(zip(reversed(args), reversed(defaults)))"}
{"prompt": "def index():\n    \"\"\"Display a list of all user institutes.\"\"\"", "output": "\n    institutes = Institute.query.all()\n    return render_template('institutes/index.html', institutes=institutes)\n\n\n@bp.route('/new', methods=['GET', 'POST'])\n@login_required\ndef new():\n    \"\"\"Create a new institute.\"\"\"\n    form = InstituteForm()\n    if form.validate_on_submit():\n        institute = Institute(name=form.name.data,\n                              address=form.address.data,\n                              city=form.city.data,\n                             ", "solution": "\n    institute_objs = user_institutes(store, current_user)\n    institutes_count = ((institute_obj, store.cases(collaborator=institute_obj['_id']).count())\n                        for institute_obj in institute_objs if institute_obj)\n    return dict(institutes=institutes_count)"}
{"prompt": "def _plotting(self, rank_metric, results, graph_num, outdir, \n                  format, figsize, pheno_pos='', pheno_neg=''):\n        \"\"\" Plotting API.\n            :param rank_metric: sorted pd.Series with rankings values.\n            :param results: self.results\n            :param data: preprocessed expression table\n\n        \"\"\"", "output": "\n        # plotting\n        if self.plotting:\n            # plotting\n            if self.plotting == 'default':\n                # plotting\n                fig, ax = plt.subplots(figsize=figsize)\n                ax.plot(rank_metric, color='black', linewidth=2)\n                ax.set_xlabel('Genes')\n                ax.set_ylabel('Ranking')\n                ax.set_title('Ranking')\n                fig.tight_layout()\n                fig.savefig(os.path.join", "solution": "\n        \n        # no values need to be returned\n        if self._outdir is None: return\n        #Plotting\n        top_term = self.res2d.index[:graph_num]\n        # multi-threading\n        pool = Pool(self._processes)\n        for gs in top_term:\n            hit = results.get(gs)['hits_indices']\n            NES = 'nes' if self.module != 'ssgsea' else 'es'\n            term = gs.replace('/','_').replace(\":\",\"_\")\n            outfile = '{0}/{1}.{2}.{3}'.format(self.outdir, term, self.module, self.format)\n            # gseaplot(rank_metric=rank_metric, term=term, hits_indices=hit,\n            #           nes=results.get(gs)[NES], pval=results.get(gs)['pval'], \n            #           fdr=results.get(gs)['fdr'], RES=results.get(gs)['RES'],\n            #           pheno_pos=pheno_pos, pheno_neg=pheno_neg, figsize=figsize,\n            #           ofname=outfile)\n            pool.apply_async(gseaplot, args=(rank_metric, term, hit, results.get(gs)[NES],\n                                              results.get(gs)['pval'],results.get(gs)['fdr'],\n                                              results.get(gs)['RES'],\n                                              pheno_pos, pheno_neg, \n                                              figsize, 'seismic', outfile))\n            if self.module == 'gsea':\n                outfile2 = \"{0}/{1}.heatmap.{2}\".format(self.outdir, term, self.format)\n                # heatmap(df=self.heatmat.iloc[hit, :], title=term, ofname=outfile2, \n                #         z_score=0, figsize=(self._width, len(hit)/2))\n                pool.apply_async(heatmap, args=(self.heatmat.iloc[hit, :], 0, term, \n                                               (self._width, len(hit)/2+2), 'RdBu_r',\n                                                True, True, outfile2))\n        pool.close()\n        pool.join()"}
{"prompt": "def get(self, requestId):\n        \"\"\"\n        Gets details of a device management request.\n        It accepts requestId (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"", "output": "\n        try:\n            response = self.api_client.get(\n                self.api_path + '/' + requestId,\n                None,\n                self.transaction,\n                self.configuration_attributes\n            )\n            return self.return_obj(response, requestId)\n        except APIException as e:\n            raise e\n\n    def list(self, **kwargs):\n        \"\"\"\n        Lists all device management requests.\n        It accepts limit (integer) and offset (integer) as input parameters\n        In case of failure it throws APIException\n        \"\"\"", "solution": "\n        url = MgmtRequests.mgmtSingleRequest % (requestId)\n        r = self._apiClient.get(url)\n\n        if r.status_code == 200:\n            return r.json()\n        else:\n            raise ApiException(r)"}
{"prompt": "def load(self, reload=False):\n        \"\"\"\n        Load the config and defaults from files.\n        \"\"\"", "output": "\n        if not reload and self.loaded:\n            return\n\n        self.loaded = True\n\n        # Load the config file\n        if self.config_file is not None:\n            self.load_config_file(self.config_file)\n\n        # Load the defaults file\n        if self.defaults_file is not None:\n            self.load_defaults_file(self.defaults_file)\n\n        # Load the environment variables\n        self.load_env_vars()\n\n        # Load the command line options\n        self.load", "solution": "\n        if reload or not self._loaded:\n            # load defaults\n            if self._defaults_file and type(self._defaults_file) == str:\n                self._defaults_file = File(self._defaults_file, parent=self._parent)\n            defaults = {}\n            if self._defaults_file:\n                defaults = yaml.safe_load(self._defaults_file.read().replace('\\t', '    '))\n\n            # load data\n            data = {}\n            if self.exists:\n                data = yaml.safe_load(self.read().replace('\\t', '    '))\n\n            # initialise with the loaded data\n            self._defaults = defaults\n            self._data = copy.deepcopy(self._defaults)\n            self.update(data=data)\n\n            # if specified, apply environment variables\n            if self._apply_env:\n                self.update(ConfigEnv(self._env_prefix))\n\n            self._loaded = True\n\n        return self"}
{"prompt": "def walk_files(mgr):\n    \"\"\"\n    Iterate over all files visible to ``mgr``.\n    \"\"\"", "output": "\n    for root, dirs, files in mgr.walk():\n        for f in files:\n            yield os.path.join(root, f)\n\n\ndef walk_dirs(mgr):\n    \"\"\"\n    Iterate over all directories visible to ``mgr``.\n    \"\"\"\n    for root, dirs, files in mgr.walk():\n        for d in dirs:\n            yield os.path.join(root, d)\n\n\ndef walk_dirs_and_files(mgr):\n    \"\"\"\n    Iterate over all", "solution": "\n    for dir_, subdirs, files in walk_files(mgr):\n        for file_ in files:\n            yield file_"}
{"prompt": "def data_types(self):\n        \"\"\"Return a list of data types.\"\"\"", "output": "\n        return self._data_types\n\n    @property\n    def data_type_names(self):\n        \"\"\"Return a list of data type names.\"\"\"\n        return [d.name for d in self.data_types]\n\n    @property\n    def data_type_ids(self):\n        \"\"\"Return a list of data type ids.\"\"\"\n        return [d.id for d in self.data_types]\n\n    @property\n    def data_type_names_and_ids(self):\n        \"\"\"Return a list of data type names and ids", "solution": "\n        data = self.gencloud.project_data(self.id)\n        return sorted(set(d.type for d in data))"}
{"prompt": "def ProductsForm(category, products):\n    \"\"\" Produces an appropriate _ProductsForm subclass for the given render\n    type. \"\"\"", "output": "\n    if category.render_type == 'select':\n        return SelectProductsForm(category, products)\n    elif category.render_type == 'checkbox':\n        return CheckboxProductsForm(category, products)\n    elif category.render_type == 'radio':\n        return RadioProductsForm(category, products)\n    else:\n        raise ValueError(\"Unknown render type: %s\" % category.render_type)\n\n\nclass _ProductsForm(Form):\n    \"\"\" Base class for all product forms. \"\"\"\n    def __init__(self, category, products):\n", "solution": "\n\n    # Each Category.RENDER_TYPE value has a subclass here.\n    cat = inventory.Category\n    RENDER_TYPES = {\n        cat.RENDER_TYPE_QUANTITY: _QuantityBoxProductsForm,\n        cat.RENDER_TYPE_RADIO: _RadioButtonProductsForm,\n        cat.RENDER_TYPE_ITEM_QUANTITY: _ItemQuantityProductsForm,\n        cat.RENDER_TYPE_CHECKBOX: _CheckboxProductsForm,\n    }\n\n    # Produce a subclass of _ProductsForm which we can alter the base_fields on\n    class ProductsForm(RENDER_TYPES[category.render_type]):\n        pass\n\n    products = list(products)\n    products.sort(key=lambda prod: prod.order)\n\n    ProductsForm.set_fields(category, products)\n\n    if category.render_type == inventory.Category.RENDER_TYPE_ITEM_QUANTITY:\n        ProductsForm = forms.formset_factory(\n            ProductsForm,\n            formset=_ItemQuantityProductsFormSet,\n        )\n\n    return ProductsForm"}
{"prompt": "def chop(array, epsilon=1e-10):\n    \"\"\"\n    Truncate small values of a complex array.\n\n    Args:\n        array (array_like): array to truncte small values.\n        epsilon (float): threshold.\n\n    Returns:\n        np.array: A new operator with small values set to zero.\n    \"\"\"", "output": "\n    return np.array([complex(x.real, 0) if abs(x.imag) < epsilon else x for x in array])\n\n\ndef chop_real(array, epsilon=1e-10):\n    \"\"\"\n    Truncate small values of a real array.\n\n    Args:\n        array (array_like): array to truncte small values.\n        epsilon (float): threshold.\n\n    Returns:\n        np.array: A new operator with small values set to zero.\n    \"\"\"\n    return np", "solution": "\n    ret = np.array(array)\n\n    if np.isrealobj(ret):\n        ret[abs(ret) < epsilon] = 0.0\n    else:\n        ret.real[abs(ret.real) < epsilon] = 0.0\n        ret.imag[abs(ret.imag) < epsilon] = 0.0\n    return ret"}
{"prompt": "def make_clean_html(raw, stream_item=None, encoding=None):\n    \"\"\"Get a clean text representation of presumed HTML.\n\n    Treat `raw` as though it is HTML, even if we have no idea what it\n    really is, and attempt to get a properly formatted HTML document\n    with all HTML-escaped characters converted to their unicode.\n\n    This is called below by the `clean_html` transform stage, which\n    interprets MIME-type.  If `character_encoding` is not provided,\n    and `stream_item` is provided, then this falles back to\n    :attr:`streamcorpus.StreamItem.body.encoding`.\n\n    :param str raw: raw text to clean up\n    :param stream_item: optional stream item with encoding metadata\n    :type stream_item: :class:`streamcorpus.StreamItem`\n    :returns: UTF-8-encoded byte string of cleaned HTML text\n    :returntype: :class:`str`\n\n    \"\"\"", "output": "item` is not None, then we attempt to guess the\n    encoding from the `Content-Type` header.\n\n    \"\"\"\n    if stream_item is not None:\n        if encoding is None:\n            encoding = stream_item.get_encoding()\n    if encoding is None:\n        encoding = 'utf-8'\n    try:\n        raw = raw.decode(encoding)\n    except UnicodeDecodeError:\n        pass\n    try:\n        raw = raw.encode('utf-8')\n    except UnicodeDecodeError:\n        pass\n    return", "solution": "\n    # Fix emails by protecting the <,> from HTML\n    raw = fix_emails(raw)\n    raw_decoded = nice_decode(raw, stream_item=stream_item, encoding=encoding)\n    if raw_decoded is None:\n        # give up on decoding it... maybe this should use force_unicode\n        raw_decoded = raw\n\n    # default attempt uses vanilla lxml.html\n    try:\n        root = lxml.html.document_fromstring(raw_decoded)\n    except ValueError, exc:\n        if 'with encoding declaration' in str(exc):\n            root = lxml.html.document_fromstring(raw)\n        else:\n            raise\n\n    # While we have the document parsed as a DOM, let's strip attributes.\n    # (The HTML cleaner seems to only support whitelisting attributes.\n    # As of now, we just want to blacklist a few.)\n    lxml.etree.strip_attributes(root, 'class', 'id')\n\n    # if that worked, then we will be able to generate a\n    # valid HTML string\n    fixed_html = lxml.html.tostring(root, encoding=unicode)\n\n    # remove any ^M characters\n    fixed_html = string.replace(fixed_html, '\\r', ' ')\n\n    # We drop utf8 characters that are above 0xFFFF as\n    # Lingpipe seems to be doing the wrong thing with them.\n    fixed_html = drop_invalid_and_upper_utf8_chars(fixed_html)\n\n    # construct a Cleaner that removes any ``<script>`` tags,\n    # Javascript, like an ``onclick`` attribute, comments, style\n    # tags or attributes, ``<link>`` tags\n    cleaner = lxml.html.clean.Cleaner(\n        scripts=True, javascript=True,\n        comments=True,\n        # do not remove <html> <head> <title> etc\n        page_structure=False,\n        remove_tags=['base'],\n        style=True, links=True)\n\n    # now get the really sanitized HTML\n    _clean_html = cleaner.clean_html(fixed_html)\n\n    # generate pretty HTML in utf-8\n    _clean_html = lxml.html.tostring(\n        lxml.html.document_fromstring(_clean_html),\n        method='html', encoding='utf-8',\n        pretty_print=True,\n        # include_meta_content_type=True\n        )\n\n    return uniform_html(_clean_html)"}
{"prompt": "def func(self):\n        \"\"\"Alternative naming, you can use `node.func.name` instead of `node.f_func`\"\"\"", "output": "\n        return self.f_func\n\n    def __str__(self):\n        return self.f_func\n\n    def __repr__(self):\n        return self.f_func\n\n    def __eq__(self, other):\n        return self.f_func == other.f_func\n\n    def __hash__(self):\n        return hash(self.f_func)\n\n    def __lt__(self, other):\n        return self.f_func < other.f_func\n\n    def __le__(self, other):\n        return self", "solution": "\n        if self._func is None:\n            self._func = NNTreeNodeFunc(self)\n        return self._func"}
{"prompt": "def load_java_messages_to_ignore():\n    \"\"\"\n    Load in pickle file that contains dict structure with bad java messages to ignore per unit test\n    or for all cases.  The ignored bad java info is stored in g_ok_java_messages dict.\n\n    :return:\n    \"\"\"", "output": "\n    global g_ok_java_messages\n    try:\n        with open(os.path.join(os.path.dirname(__file__), 'java_messages_to_ignore.pickle'), 'rb') as f:\n            g_ok_java_messages = pickle.load(f)\n    except Exception as e:\n        print(\"Error loading java messages to ignore: \" + str(e))\n        g_ok_java_messages = {}\n\n\ndef load_java_messages_to_ignore_for_unit_test(unit_test_", "solution": "\n    global g_ok_java_messages\n    global g_java_message_pickle_filename\n\n    if os.path.isfile(g_java_message_pickle_filename):\n        with open(g_java_message_pickle_filename,'rb') as tfile:\n            g_ok_java_messages = pickle.load(tfile)\n    else:\n        g_ok_java_messages[\"general\"] = []"}
{"prompt": "def rowCount(self, parent):\n        \"\"\"Return number of children *parent* index has.\"\"\"", "output": "\n        return len(self.data[parent])\n\n    def columnCount(self, parent):\n        \"\"\"Return number of columns *parent* index has.\"\"\"\n        return len(self.data[0])\n\n    def data(self, index, role):\n        \"\"\"Return data for *index* and *role*.\"\"\"\n        if not index.isValid():\n            return None\n        if role != Qt.DisplayRole:\n            return None\n        return self.data[index.row()][index.column()]\n\n    def headerData(self, section", "solution": "\n        if parent.column() > 0:\n            return 0\n\n        if parent.isValid():\n            item = parent.internalPointer()\n        else:\n            item = self.root\n\n        return len(item.children)"}
{"prompt": "def sh(cmd, escape=True):\n    \"\"\" Executes the given command.\n    returns a 2-tuple with returncode (integer) and OUTPUT (string)\n    \"\"\"", "output": "\n    if escape:\n        cmd = \" \".join(map(pipes.quote, cmd))\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    out, _ = p.communicate()\n    return p.returncode, out\n\n\ndef sh_stdout(cmd, escape=True):\n    \"\"\" Executes the given command.\n    returns the output (string)\n    \"\"\"\n    if escape:\n        cmd = \" \".join(map(pipes.", "solution": "\n\n    if escape:\n        cmd = quote(cmd)\n\n    process = Popen(cmd, stdout=PIPE, stderr=STDOUT, shell=True)\n    output, unused_err = process.communicate()\n    retcode = process.poll()\n\n    return (retcode, output)"}
{"prompt": "def abort(self, jobs=None, targets=None, block=None):\n        \"\"\"Abort specific jobs from the execution queues of target(s).\n\n        This is a mechanism to prevent jobs that have already been submitted\n        from executing.\n\n        Parameters\n        ----------\n\n        jobs : msg_id, list of msg_ids, or AsyncResult\n            The jobs to be aborted\n            \n            If unspecified/None: abort all outstanding jobs.\n\n        \"\"\"", "output": "\n        if jobs is None:\n            jobs = []\n        elif isinstance(jobs, AsyncResult):\n            jobs = [jobs]\n        elif isinstance(jobs, list):\n            if not all(isinstance(job, AsyncResult) for job in jobs):\n                raise TypeError(\"jobs must be a list of AsyncResults\")\n        elif isinstance(jobs, str):\n            jobs = [jobs]\n        else:\n            raise TypeError(\"jobs must be a msg_id, list of msg_ids, or AsyncResult\")\n\n        if", "solution": "\n        block = self.block if block is None else block\n        jobs = jobs if jobs is not None else list(self.outstanding)\n        targets = self._build_targets(targets)[0]\n        \n        msg_ids = []\n        if isinstance(jobs, (basestring,AsyncResult)):\n            jobs = [jobs]\n        bad_ids = filter(lambda obj: not isinstance(obj, (basestring, AsyncResult)), jobs)\n        if bad_ids:\n            raise TypeError(\"Invalid msg_id type %r, expected str or AsyncResult\"%bad_ids[0])\n        for j in jobs:\n            if isinstance(j, AsyncResult):\n                msg_ids.extend(j.msg_ids)\n            else:\n                msg_ids.append(j)\n        content = dict(msg_ids=msg_ids)\n        for t in targets:\n            self.session.send(self._control_socket, 'abort_request',\n                    content=content, ident=t)\n        error = False\n        if block:\n            self._flush_ignored_control()\n            for i in range(len(targets)):\n                idents,msg = self.session.recv(self._control_socket,0)\n                if self.debug:\n                    pprint(msg)\n                if msg['content']['status'] != 'ok':\n                    error = self._unwrap_exception(msg['content'])\n        else:\n            self._ignored_control_replies += len(targets)\n        if error:\n            raise error"}
{"prompt": "def format_execution_time(self, start=None, end=None):\n        \"\"\"\n        Format the calculated time into a human readable format.\n\n        :param start: A starting time.\n        :type start: int|str\n\n        :param stop: A ending time.\n        :type stop: int|str\n\n        :return: A human readable date.\n        :rtype: str\n        \"\"\"", "output": "\n        if start is None:\n            start = self.start\n        if end is None:\n            end = self.end\n\n        if isinstance(start, str):\n            start = int(start)\n        if isinstance(end, str):\n            end = int(end)\n\n        if start is None or end is None:\n            return None\n\n        if start > end:\n            start, end = end, start\n\n        seconds = end - start\n        days = seconds // 86400\n        seconds -= days * 864", "solution": "\n\n        # We return the formatted execution time.\n        return \":\".join(list(self._calculate(start, end).values()))"}
{"prompt": "def _all_load_skeleton(self, traj_node, hdf5_group):\n        \"\"\"Reloads skeleton data of a tree node\"\"\"", "output": "\n        # Load the skeleton\n        skeleton_group = hdf5_group['skeleton']\n        self.skeleton = Skeleton(skeleton_group)\n        # Load the skeleton tree\n        self.skeleton_tree = SkeletonTree(skeleton_group)\n        # Load the skeleton tree\n        self.skeleton_tree = SkeletonTree(skeleton_group)\n        # Load the skeleton tree\n        self.skeleton_tree = SkeletonTree(skeleton_group)\n        # Load the skeleton tree", "solution": "\n        if traj_node.v_annotations.f_is_empty():\n            self._ann_load_annotations(traj_node, hdf5_group)\n        if traj_node.v_comment == '':\n            comment = self._all_get_from_attrs(hdf5_group, HDF5StorageService.COMMENT)\n            if comment is None:\n                comment = ''\n            traj_node.v_comment = comment"}
{"prompt": "def overlapping(self, variant_obj):\n        \"\"\"Return overlapping variants.\n\n        Look at the genes that a variant overlaps to.\n        Then return all variants that overlap these genes.\n\n        If variant_obj is sv it will return the overlapping snvs and oposite\n        There is a problem when SVs are huge since there are to many overlapping variants.\n\n        Args:\n            variant_obj(dict)\n\n        Returns:\n            variants(iterable(dict))\n        \"\"\"", "output": "\n        variants = []\n        if variant_obj[\"type\"] == \"sv\":\n            # get all overlapping variants\n            variants = self.get_overlapping_variants(variant_obj)\n            # get all overlapping snvs\n            variants += self.get_overlapping_snvs(variant_obj)\n            # get all overlapping oposites\n            variants += self.get_overlapping_oposites(variant_obj)\n        else:\n            # get all overlapping variants\n            variants = self.get_over", "solution": "\n        #This is the category of the variants that we want to collect\n        category = 'snv' if variant_obj['category'] == 'sv' else 'sv'\n\n        query = {\n            '$and': [\n                {'case_id': variant_obj['case_id']},\n                {'category': category},\n                {'hgnc_ids' : { '$in' : variant_obj['hgnc_ids']}}\n            ]\n        }\n\n        sort_key = [('rank_score', pymongo.DESCENDING)]\n        # We collect the 30 most severe overlapping variants\n        variants = self.variant_collection.find(query).sort(sort_key).limit(30)\n\n        return variants"}
{"prompt": "def validate(self):\n        \"\"\"\n        Verify that the contents of the SecretData object are valid.\n\n        Raises:\n            TypeError: if the types of any SecretData attributes are invalid.\n        \"\"\"", "output": "\n        if not isinstance(self.name, str):\n            raise TypeError(\"SecretData.name must be a string.\")\n        if not isinstance(self.description, str):\n            raise TypeError(\"SecretData.description must be a string.\")\n        if not isinstance(self.secret_type, str):\n            raise TypeError(\"SecretData.secret_type must be a string.\")\n        if not isinstance(self.secret_data, dict):\n            raise TypeError(\"SecretData.secret_data must be a dictionary.\")\n        if not isinstance(self.secret_data", "solution": "\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"secret value must be bytes\")\n        elif not isinstance(self.data_type, enums.SecretDataType):\n            raise TypeError(\"secret data type must be a SecretDataType \"\n                            \"enumeration\")\n\n        mask_count = len(self.cryptographic_usage_masks)\n        for i in range(mask_count):\n            mask = self.cryptographic_usage_masks[i]\n            if not isinstance(mask, enums.CryptographicUsageMask):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\n                    \"secret data mask {0} must be a CryptographicUsageMask \"\n                    \"enumeration\".format(position))\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"secret data name {0} must be a string\".format(\n                    position))"}
{"prompt": "def to_matrix(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"", "output": "\n        return np.array([[1, 0, 0, 0],\n                         [0, 1, 0, 0],\n                         [0, 0, 1, 0],\n                         [0, 0, 0, 1]], dtype=complex)\n\n    def to_matrix(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"\n        return np.array([[1, 0, 0, 0],\n                         [0, 1, 0, 0],", "solution": "\n        theta, phi, lam = self.params\n        return numpy.array(\n            [[\n                numpy.cos(theta / 2),\n                -numpy.exp(1j * lam) * numpy.sin(theta / 2)\n            ],\n             [\n                 numpy.exp(1j * phi) * numpy.sin(theta / 2),\n                 numpy.exp(1j * (phi + lam)) * numpy.cos(theta / 2)\n             ]],\n            dtype=complex)"}
{"prompt": "def _merge_with_other_stm(self, other: \"IfContainer\") -> None:\n        \"\"\"\n        Merge other statement to this statement\n        \"\"\"", "output": "\n        self.statements.extend(other.statements)\n        self.else_statements.extend(other.else_statements)\n\n    def _merge_with_other_stm(self, other: \"WhileContainer\") -> None:\n        \"\"\"\n        Merge other statement to this statement\n        \"\"\"\n        self.statements.extend(other.statements)\n\n    def _merge_with_other_stm(self, other: \"ForContainer\") -> None:\n        \"\"\"\n        Merge other statement to this statement\n        \"\"\"\n", "solution": "\n        merge = self._merge_statement_lists\n        newCases = []\n        for (c, caseA), (_, caseB) in zip(self.cases, other.cases):\n            newCases.append((c, merge(caseA, caseB)))\n\n        self.cases = newCases\n\n        if self.default is not None:\n            self.default = merge(self.default, other.default)\n\n        self._on_merge(other)"}
{"prompt": "def  main( argv ):\n    \"\"\"main program loop\"\"\"", "output": "\n    #\n    #   get the command line arguments\n    #\n    try:\n        opts, args = getopt.getopt(argv, \"hv\", [\"help\", \"verbose\"])\n    except getopt.GetoptError:\n        print(\"usage: %s [-v] [-h]\" % argv[0])\n        sys.exit(2)\n    #\n    #   process the command line arguments\n    #\n    verbose = False\n    for opt, arg in opts:\n        if opt in (\"-h\", \"--help\"):\n            print", "solution": "\n\n    global output_dir\n\n    try:\n        opts, args = getopt.getopt( sys.argv[1:], \\\n                                    \"hb\",         \\\n                                    [\"help\", \"backup\"] )\n    except getopt.GetoptError:\n        usage()\n        sys.exit( 2 )\n\n    if args == []:\n        usage()\n        sys.exit( 1 )\n\n    # process options\n    #\n    output_dir = None\n    do_backup  = None\n\n    for opt in opts:\n        if opt[0] in ( \"-h\", \"--help\" ):\n            usage()\n            sys.exit( 0 )\n\n        if opt[0] in ( \"-b\", \"--backup\" ):\n            do_backup = 1\n\n    # create context and processor\n    source_processor = SourceProcessor()\n\n    # retrieve the list of files to process\n    file_list = make_file_list( args )\n    for filename in file_list:\n        source_processor.parse_file( filename )\n\n        for block in source_processor.blocks:\n            beautify_block( block )\n\n        new_name = filename + \".new\"\n        ok       = None\n\n        try:\n            file = open( new_name, \"wt\" )\n            for block in source_processor.blocks:\n                for line in block.lines:\n                    file.write( line )\n                    file.write( \"\\n\" )\n            file.close()\n        except:\n            ok = 0"}
{"prompt": "def checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"", "output": "\n    if b.funcname is None:\n        return False\n    if b.funcname == '__init__':\n        return False\n    if b.funcname == '__new__':\n        return False\n    if b.funcname == '__call__':\n        return False\n    if b.funcname == '__getattr__':\n        return False\n    if b.funcname == '__setattr__':\n        return False\n    if b.funcname == '__delattr__':\n        return False\n    if b.funcname == '", "solution": "\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True"}
{"prompt": "def gene_variants(self, query=None,\n                   category='snv', variant_type=['clinical'],\n                   nr_of_variants=50, skip=0):\n        \"\"\"Return all variants seen in a given gene.\n\n        If skip not equal to 0 skip the first n variants.\n\n        Arguments:\n            query(dict): A dictionary with querys for the database, including\n            variant_type: 'clinical', 'research'\n            category(str): 'sv', 'str', 'snv' or 'cancer'\n            nr_of_variants(int): if -1 return all variants\n            skip(int): How many variants to skip\n        \"\"\"", "output": "\n            nr_of_variants(int): Number of variants to return\n            skip(int): Skip the first n variants\n\n        Returns:\n            list: A list of dictionaries with variant information\n        \"\"\"\n        if query is None:\n            query = {}\n        query['gene'] = self.gene\n        query['variant_type'] = variant_type\n        query['category'] = category\n        query['skip'] = skip\n        query['nr_of_variants'] = nr_of_variants\n        return self.variants(query)", "solution": "\n\n        mongo_variant_query = self.build_variant_query(query=query,\n                                   category=category, variant_type=variant_type)\n\n        sorting = [('rank_score', pymongo.DESCENDING)]\n\n        if nr_of_variants == -1:\n            nr_of_variants = 0 # This will return all variants\n        else:\n            nr_of_variants = skip + nr_of_variants\n\n        result = self.variant_collection.find(\n            mongo_variant_query\n            ).sort(sorting).skip(skip).limit(nr_of_variants)\n\n        return result"}
{"prompt": "def filter_exclude_downhole(self, threshold, filt=True, samples=None, subset=None):\n        \"\"\"\n        Exclude all points down-hole (after) the first excluded data.\n\n        Parameters\n        ----------\n        threhold : int\n            The minimum number of contiguous excluded data points\n            that must exist before downhole exclusion occurs.\n        file : valid filter string or bool\n            Which filter to consider. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n        if filt:\n            filt = self.filter\n        else:\n            filt = None\n        if samples is None:\n            samples = self.samples\n        if subset is None:\n            subset = self.subset\n        if filt is None:\n            filt = self.filter\n        if filt is None:\n            filt = self.filter\n        if filt is None:\n            filt = self.filter\n        if filt is None:\n            filt = self.filter\n        if filt is None:\n            filt", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            self.data[s].filter_exclude_downhole(threshold, filt)"}
{"prompt": "def match(self, path):\n        \"\"\"Return route handler with arguments if path matches this route.\n\n        Arguments:\n          path (str): Request path\n\n        Returns:\n          tuple or None: A tuple of three items:\n\n            1. Route handler (callable)\n            2. Positional arguments (list)\n            3. Keyword arguments (dict)\n\n          ``None`` if the route does not match the path.\n        \"\"\"", "output": "\n        if self.matcher.match(path):\n            return self.handler, self.args, self.kwargs\n\n    def __repr__(self):\n        return '<Route %s %s>' % (self.method, self.path)\n\n\nclass Router(object):\n    \"\"\"A router that dispatches requests to routes.\n\n    Arguments:\n      routes (list): A list of :class:`Route` instances.\n    \"\"\"\n\n    def __init__(self, routes):\n        self.routes = routes\n\n    def route(self,", "solution": "\n        match = self._re.search(path)\n        if match is None:\n            return None\n        kwargs_indexes = match.re.groupindex.values()\n        args_indexes = [i for i in range(1, match.re.groups + 1)\n                          if i not in kwargs_indexes]\n        args = [match.group(i) for i in args_indexes]\n        kwargs = {}\n        for name, index in match.re.groupindex.items():\n            kwargs[name] = match.group(index)\n        return self._callback, args, kwargs"}
{"prompt": "def get_int(errmsg, arg, default=1, cmdname=None):\n    \"\"\"If arg is an int, use that otherwise take default.\"\"\"", "output": "\n    try:\n        return int(arg)\n    except ValueError:\n        if cmdname:\n            errmsg = \"Error: %s: %s\" % (cmdname, errmsg)\n        else:\n            errmsg = \"Error: %s\" % errmsg\n        print(errmsg)\n        return default\n\n\ndef get_float(errmsg, arg, default=1.0, cmdname=None):\n    \"\"\"If arg is a float, use that otherwise take default.\"\"\"\n    try:\n        return float(arg)\n    except", "solution": "\n    if arg:\n        try:\n            # eval() is used so we will allow arithmetic expressions,\n            # variables etc.\n            default = int(eval(arg))\n        except (SyntaxError, NameError, ValueError):\n            if cmdname:\n                errmsg(\"Command '%s' expects an integer; got: %s.\" %\n                       (cmdname, str(arg)))\n            else:\n                errmsg('Expecting an integer, got: %s.' % str(arg))\n                pass\n            raise ValueError\n    return default"}
{"prompt": "def get(*args, **kwargs):\n    \"\"\"Get users.\"\"\"", "output": "\n    return (\n        User.query.filter_by(**kwargs)\n        .order_by(User.id.desc())\n        .all()\n    )\n\n\ndef get_by_id(user_id):\n    \"\"\"Get user by id.\"\"\"\n    return User.query.filter_by(id=user_id).first()\n\n\ndef get_by_email(email):\n    \"\"\"Get user by email.\"\"\"\n    return User.query.filter_by(email=email).first()\n\n\ndef create(**kwargs):\n", "solution": "\n    from invenio.modules.oauth2server.models import Client\n    q = Client.query\n    return q.count(), q.all()"}
{"prompt": "def verify(\n            cls,\n            timestamp: int,\n            message_hash: SHA512Hash,\n            signature: bytes,\n    ) -> bool:\n        \"\"\"\n        Verify a given NIST message hash and signature for a beacon value.\n\n        :param timestamp: The timestamp of the record being verified.\n        :param message_hash:\n            The hash that was carried out over the message.\n            This is an object belonging to the `Crypto.Hash` module.\n        :param signature: The signature that needs to be validated.\n        :return: True if verification is correct. False otherwise.\n        \"\"\"", "output": "\n        :return:\n            True if the signature is valid, False otherwise.\n        \"\"\"\n        return cls.verify_signature(\n            message_hash=message_hash,\n            signature=signature,\n            public_key=cls.get_public_key(timestamp=timestamp),\n        )\n\n    @classmethod\n    def get_public_key(\n            cls,\n            timestamp: int,\n    ) -> RSAKey:\n        \"\"\"\n        Get the public key for a given timestamp.\n\n        :param timestamp: The timestamp of the", "solution": "\n\n        # Determine verifier type to use based on timestamp.\n        if timestamp < 1496176860:\n            verifier = cls._VERIFIER_20130905\n        elif timestamp < 1502202360:\n            verifier = None\n        else:\n            verifier = cls._VERIFIER_20170808\n\n        # If a verifier exists to handle this problem, use it directly.\n        # Else, we cannot verify the record and must mark it invalid.\n        if verifier:\n            result = verifier.verify(\n                message_hash,\n                signature,\n            )\n        else:\n            result = False\n\n        # Convert 1 to 'True', 'False' otherwise\n        if isinstance(result, int):\n            result = True if result == 1 else False\n\n        return result"}
{"prompt": "def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from function bodies.\"\"\"", "output": "\n        if node.name == \"__init__\":\n            return None\n        if node.name == \"__new__\":\n            return None\n        if node.name == \"__del__\":\n            return None\n        if node.name == \"__repr__\":\n            return None\n        if node.name == \"__str__\":\n            return None\n        if node.name == \"__format__\":\n            return None\n        if node.name == \"__getattr__\":\n            return None\n        if node.name == \"__setattr__\":\n", "solution": "\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.FunctionDef)\n        return ast.copy_location(\n            ast.FunctionDef(\n                name=new_node.name,\n                args=new_node.args,\n                body=_filter_dead_code(new_node.body),\n                decorator_list=new_node.decorator_list,\n                returns=new_node.returns,\n            ),\n            new_node,\n        )"}
{"prompt": "def register_success(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        succeeded\n        \"\"\"", "output": "\n        self.log.info(\"Registration succeeded\")\n        self.log.info(\"Command channel: %s\", cmd_channel)\n        self.join(cmd_channel)\n        self.send_message(cmd_channel, \"Registration succeeded\")\n\n    def register_fail(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, but the\n        registration failed\n        \"\"\"\n        self.log.info(\"Registration failed\")\n        self.log.info(\"Command channel:", "solution": "\n        # the boss will tell what channel to join\n        self.channel = cmd_channel\n        self.conn.join(self.channel)\n        \n        # indicate that registered so we'll stop trying\n        self.registered.set()"}
{"prompt": "def csv_to_po(trans_csv_path, meta_csv_path, locale_root,\n              po_files_path, header=None):\n    \"\"\"\n    Converts GDocs spreadsheet generated csv file into po file.\n    :param trans_csv_path: path to temporary file with translations\n    :param meta_csv_path: path to temporary file with meta information\n    :param locale_root: path to locale root folder containing directories\n                        with languages\n    :param po_files_path: path from lang directory to po file\n    \"\"\"", "output": "   :param header: list of headers to be used in po file\n    :return: None\n    \"\"\"\n    # read csv files\n    with open(trans_csv_path, 'r') as f:\n        trans_csv = csv.reader(f)\n        trans_csv_list = list(trans_csv)\n\n    with open(meta_csv_path, 'r') as f:\n        meta_csv = csv.reader(f)\n        meta_csv_list = list(meta_csv)\n\n    # get meta information\n    lang = meta", "solution": "\n    pattern = \"^\\w+.*po$\"\n    for root, dirs, files in os.walk(locale_root):\n        for f in filter(lambda x: re.match(pattern, x), files):\n            os.remove(os.path.join(root, f))\n\n    # read title row and prepare descriptors for po files in each lang\n    trans_reader = UnicodeReader(trans_csv_path)\n    meta_reader = UnicodeReader(meta_csv_path)\n    try:\n        title_row = trans_reader.next()\n    except StopIteration:\n        # empty file\n        return\n\n    trans_languages = _prepare_locale_dirs(title_row[3:], locale_root)\n\n    po_files = {}\n\n    meta_reader.next()\n    # go through every row in downloaded csv file\n    for trans_row, meta_row in izip_longest(trans_reader, meta_reader):\n        filename = trans_row[0].rstrip()\n        metadata = meta_row[0].rstrip() if meta_row else METADATA_EMPTY\n        comment = trans_row[1]\n        msgid = trans_row[2]\n\n        if filename not in po_files:\n            _prepare_polib_files(po_files, filename, trans_languages,\n                                 locale_root, po_files_path, header)\n\n        _write_entries(po_files[filename], trans_languages, msgid,\n                       trans_row[3:], metadata, comment)\n    for filename in po_files:\n        for lang in po_files[filename]:\n            po_files[filename][lang].save()\n\n    trans_reader.close()\n    meta_reader.close()"}
{"prompt": "def _add_header(self):\n        \"\"\"Add email header info.\"\"\"", "output": "\n        self.msg['From'] = self.sender\n        self.msg['To'] = self.recipients\n        self.msg['Subject'] = self.subject\n        self.msg.preamble = self.preamble\n\n    def _add_body(self):\n        \"\"\"Add email body.\"\"\"\n        self.msg.attach(MIMEText(self.body, 'plain'))\n\n    def _add_attachments(self):\n        \"\"\"Add attachments to email.\"\"\"\n        for file in self.attachments:\n            with open(", "solution": "\n        self.message[\"From\"] = self.from_\n        self.message[\"Subject\"] = self.subject\n        if self.to:\n            self.message[\"To\"] = self.list_to_string(self.to)\n        if self.cc:\n            self.message[\"Cc\"] = self.list_to_string(self.cc)\n        if self.bcc:\n            self.message[\"Bcc\"] = self.list_to_string(self.bcc)"}
{"prompt": "def url_dequery(url):\n    \"\"\"Return a URL with the query component removed.\n\n    :param url: URL to dequery.\n    :type url: str\n    :rtype: str\n    \"\"\"", "output": "\n    return urlparse.urlunparse(urlparse.urlparse(url)._replace(query=''))\n\n\ndef url_query_add(url, key, value):\n    \"\"\"Add a key/value pair to a URL's query component.\n\n    :param url: URL to add to.\n    :type url: str\n    :param key: Key to add.\n    :type key: str\n    :param value: Value to add.\n    :type value: str\n    :rtype: str\n    \"\"\"\n    query = urlparse.parse", "solution": "\n    url = urlparse.urlparse(url)\n    return urlparse.urlunparse((url.scheme,\n                                url.netloc,\n                                url.path,\n                                url.params,\n                                '',\n                                url.fragment))"}
{"prompt": "def rendered_content(self):\n        \"\"\"Returns a 'minified' version of the javascript content\"\"\"", "output": "\n        return self.content\n\n    def rendered_content_with_comments(self):\n        \"\"\"Returns the javascript content with comments\"\"\"\n        return self.content\n\n    def rendered_content_with_comments_and_whitespace(self):\n        \"\"\"Returns the javascript content with comments and whitespace\"\"\"\n        return self.content\n\n    def rendered_content_with_comments_and_whitespace_and_line_breaks(self):\n        \"\"\"Returns the javascript content with comments and whitespace and line breaks\"\"\"\n        return self.content", "solution": "\n        template = self.resolve_template(self.template_name)\n        if django.VERSION[1] < 8:\n            if template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        else:\n            if template.template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        # if no minified template exists, minify the response\n        content = super(MinifiedJsTemplateResponse, self).rendered_content\n        content = jsmin.jsmin(content)\n        return content"}
{"prompt": "def get_except_handler_name(self, node):\n        \"\"\"\n        Helper to get the exception name from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"", "output": "\n        if node.type == 'except_handler':\n            if node.type_name:\n                return node.type_name.name\n            else:\n                return 'Exception'\n        else:\n            return node.name\n\n    def get_except_handler_type(self, node):\n        \"\"\"\n        Helper to get the exception type from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"\n        if node.type == 'except_handler':\n            if node.type_name:\n                return node.type_name.name", "solution": "\n        name = node.name\n        if not name:\n            return None\n\n        if version_info < (3,):\n            return name.id\n        return name"}
{"prompt": "def _get_live_streams(self, lang, path):\n        \"\"\"\n        Get the live stream in a particular language\n        :param lang:\n        :param path:\n        :return:\n        \"\"\"", "output": "\n        if lang == \"en\":\n            return self._get_live_streams_en(path)\n        elif lang == \"fr\":\n            return self._get_live_streams_fr(path)\n        elif lang == \"es\":\n            return self._get_live_streams_es(path)\n        elif lang == \"de\":\n            return self._get_live_streams_de(path)\n        elif lang == \"it\":\n            return self._get_live_streams_it(path)\n        elif lang == \"pt\":", "solution": "\n        res = self.session.http.get(self._live_api_url.format(lang, path))\n        live_res = self.session.http.json(res)['default']['uid']\n        post_data = '{\"channel_url\":\"/api/channels/%s/\"}' % live_res\n        try:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['stream_url']\n        except BaseException:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['channel_url']\n        return HLSStream.parse_variant_playlist(self.session, stream_data)"}
{"prompt": "def rotatePoint(self, pointX, pointY):\n        \"\"\"\n        Rotates a point relative to the mesh origin by the angle specified in the angle property.\n        Uses the angle formed between the segment linking the point of interest to the origin and\n        the parallel intersecting the origin. This angle is called beta in the code.\n        \"\"\"", "output": "\n        # Calculate the angle between the point of interest and the origin\n        beta = math.atan2(pointY, pointX)\n\n        # Calculate the distance between the point of interest and the origin\n        distance = math.sqrt(pointX**2 + pointY**2)\n\n        # Calculate the new x and y coordinates of the point\n        newX = math.cos(beta + self.angle) * distance\n        newY = math.sin(beta + self.angle) * distance\n\n        return newX, newY\n\n    def rotatePoint", "solution": "    \n        if(self.angle == 0 or self.angle == None):\n            return(pointX, pointY)\n              \n        # 1. Compute the segment length\n        length = math.sqrt((pointX - self.xll) ** 2 + (pointY - self.yll) ** 2)\n        \n        # 2. Compute beta\n        beta = math.acos((pointX - self.xll) / length) \n        if(pointY < self.yll):\n            beta = math.pi * 2 - beta\n           \n        # 3. Compute offsets\n        offsetX = math.cos(beta) * length - math.cos(self._angle_rd + beta) * length\n        offsetY = math.sin(self._angle_rd + beta) * length - math.sin(beta) * length \n        return (pointX - offsetX, pointY + offsetY)"}
{"prompt": "def get_term_by_sis_id(self, sis_term_id):\n        \"\"\"\n        Return a term resource for the passed SIS ID.\n        \"\"\"", "output": "\n        return self.get_by_path(\n            'terms/sis_id:{sis_term_id}'.format(sis_term_id=sis_term_id)\n        )\n\n    def get_term_by_sis_term_slug(self, sis_term_slug):\n        \"\"\"\n        Return a term resource for the passed SIS term slug.\n        \"\"\"\n        return self.get_by_path(\n            'terms/sis_term_slug:{sis_term_slug}'.", "solution": "\n        for term in self.get_all_terms():\n            if term.sis_term_id == sis_term_id:\n                return term"}
{"prompt": "def set_hvac_mode(self, index, hvac_mode):\n        \"\"\" possible hvac modes are auto, auxHeatOnly, cool, heat, off \"\"\"", "output": "\n        self.hvac_modes[index] = hvac_mode\n\n    def set_hvac_mode_by_name(self, name, hvac_mode):\n        \"\"\" possible hvac modes are auto, auxHeatOnly, cool, heat, off \"\"\"\n        for index, mode in enumerate(self.hvac_modes):\n            if mode == name:\n                self.set_hvac_mode(index, hvac_mode)\n                return\n        raise Exception(\"Invalid hvac mode name: \"", "solution": "\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"hvacMode\": hvac_mode\n                                  }\n                              }}\n        log_msg_action = \"set HVAC mode\"\n        return self.make_request(body, log_msg_action)"}
{"prompt": "def process_non_raw_string_token(self, prefix, string_body, start_row):\n        \"\"\"check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.\n        \"\"\"", "output": "\n        # check for bad escapes in a non-raw string.\n        if prefix == 'ur':\n            # check for bad escapes in a non-raw string.\n            if string_body.find('\\\\') != -1:\n                self.add_message('bad-escape-in-unicode-string',\n                                 node=self.node,\n                                 args=(prefix, string_body),\n                                 confidence=HIGH,\n                                 column=self.node.col_offset)\n\n    def process_raw_string_token(self,", "solution": "\n        # Walk through the string; if we see a backslash then escape the next\n        # character, and skip over it.  If we see a non-escaped character,\n        # alert, and continue.\n        #\n        # Accept a backslash when it escapes a backslash, or a quote, or\n        # end-of-line, or one of the letters that introduce a special escape\n        # sequence <http://docs.python.org/reference/lexical_analysis.html>\n        #\n        # TODO(mbp): Maybe give a separate warning about the rarely-used\n        # \\a \\b \\v \\f?\n        #\n        # TODO(mbp): We could give the column of the problem character, but\n        # add_message doesn't seem to have a way to pass it through at present.\n        i = 0\n        while True:\n            i = string_body.find(\"\\\\\", i)\n            if i == -1:\n                break\n            # There must be a next character; having a backslash at the end\n            # of the string would be a SyntaxError.\n            next_char = string_body[i + 1]\n            match = string_body[i : i + 2]\n            if next_char in self.UNICODE_ESCAPE_CHARACTERS:\n                if \"u\" in prefix:\n                    pass\n                elif (_PY3K or self._unicode_literals) and \"b\" not in prefix:\n                    pass  # unicode by default\n                else:\n                    self.add_message(\n                        \"anomalous-unicode-escape-in-string\",\n                        line=start_row,\n                        args=(match,),\n                    )\n            elif next_char not in self.ESCAPE_CHARACTERS:\n                self.add_message(\n                    \"anomalous-backslash-in-string\", line=start_row, args=(match,)\n                )\n            # Whether it was a valid escape or not, backslash followed by\n            # another character can always be consumed whole: the second\n            # character can never be the start of a new backslash escape.\n            i += 2"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.for flat segmentation.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated times for the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n        # Get the number of frames\n        N = self.X.shape[0]\n\n        # Get the number of frames\n        N = self.X.shape[0]\n\n        # Get the number of frames\n        N = self.X.shape[0]\n\n        # Get the number of frames\n        N = self.X.shape[0]\n\n        # Get the number of frames\n        N = self.X.shape[0]\n\n        # Get the number of frames\n        N = self.X.shape[0]\n\n       ", "solution": "\n        self.config[\"hier\"] = False\n        est_idxs, est_labels, F = self.process()\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1\n        return self._postprocess(est_idxs, est_labels)"}
{"prompt": "def finish_registration(self, heart):\n        \"\"\"Second half of engine registration, called after our HeartMonitor\n        has received a beat from the Engine's Heart.\"\"\"", "output": "\n        self.heart = heart\n        self.heart.add_listener(self)\n        self.heart.add_listener(self.heart_monitor)\n        self.heart.add_listener(self.heart_monitor.heart_monitor)\n        self.heart.add_listener(self.heart_monitor.heart_monitor.heart_monitor)\n        self.heart.add_listener(self.heart_monitor.heart_monitor.heart_monitor.heart_monitor)\n        self.heart.add_listener(self", "solution": "\n        try:\n            (eid,queue,reg,purge) = self.incoming_registrations.pop(heart)\n        except KeyError:\n            self.log.error(\"registration::tried to finish nonexistant registration\", exc_info=True)\n            return\n        self.log.info(\"registration::finished registering engine %i:%r\", eid, queue)\n        if purge is not None:\n            purge.stop()\n        control = queue\n        self.ids.add(eid)\n        self.keytable[eid] = queue\n        self.engines[eid] = EngineConnector(id=eid, queue=queue, registration=reg,\n                                    control=control, heartbeat=heart)\n        self.by_ident[queue] = eid\n        self.queues[eid] = list()\n        self.tasks[eid] = list()\n        self.completed[eid] = list()\n        self.hearts[heart] = eid\n        content = dict(id=eid, queue=self.engines[eid].queue.decode('ascii'))\n        if self.notifier:\n            self.session.send(self.notifier, \"registration_notification\", content=content)\n        self.log.info(\"engine::Engine Connected: %i\", eid)"}
{"prompt": "def get_ticker_price(self, ticker,\n                         startDate=None, endDate=None,\n                         fmt='json', frequency='daily'):\n        \"\"\"By default, return latest EOD Composite Price for a stock ticker.\n           On average, each feed contains 3 data sources.\n\n            Supported tickers + Available Day Ranges are here:\n            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip\n\n            Args:\n                ticker (string): Unique identifier for stock ticker\n                startDate (string): Start of ticker range in YYYY-MM-DD format\n                endDate (string): End of ticker range in YYYY-MM-DD format\n                fmt (string): 'csv' or 'json'\n                frequency (string): Resample frequency\n        \"\"\"", "output": "ticker symbol for a security.\n                startDate (string): Start date for the data.\n                endDate (string): End date for the data.\n                fmt (string): Format of the output.\n                frequency (string): Frequency of the data.\n\n            Returns:\n                dict: Dictionary of data.\n        \"\"\"\n        if startDate is None:\n            startDate = self.startDate\n        if endDate is None:\n            endDate = self.endDate\n\n        url = self.base_url + 'tiingo/daily/{}/prices'.", "solution": "\n        url = self._get_url(ticker, frequency)\n        params = {\n            'format': fmt if fmt != \"object\" else 'json',  # conversion local\n            'resampleFreq': frequency\n        }\n\n        if startDate:\n            params['startDate'] = startDate\n        if endDate:\n            params['endDate'] = endDate\n\n        # TODO: evaluate whether to stream CSV to cache on disk, or\n        # load as array in memory, or just pass plain text\n        response = self._request('GET', url, params=params)\n        if fmt == \"json\":\n            return response.json()\n        elif fmt == \"object\":\n            data = response.json()\n            return [dict_to_object(item, \"TickerPrice\") for item in data]\n        else:\n            return response.content.decode(\"utf-8\")"}
{"prompt": "def _plant_trie(strings: _List[str]) -> dict:\n    \"\"\"Create a Trie out of a list of words and return an atomic regex pattern.\n\n    The corresponding Regex should match much faster than a simple Regex union.\n    \"\"\"", "output": "\n    trie = {}\n    for string in strings:\n        node = trie\n        for char in string:\n            node = node.setdefault(char, {})\n        node[\"#\"] = True\n    return trie\n\n\ndef _plant_trie_to_regex(trie: dict) -> str:\n    \"\"\"Convert a Trie to a Regex pattern.\"\"\"\n    return \"\".join(\n        [\n            \"(\" + \"|\".join(\n                [\n                    _plant_trie_to_regex(node)\n                   ", "solution": "\n    # plant the trie\n    trie = {}\n    for string in strings:\n        d = trie\n        for char in string:\n            d[char] = char in d and d[char] or {}\n            d = d[char]\n        d[''] = None  # EOS\n    return trie"}
{"prompt": "def normalize_profile(in_profile, log=False, return_offset = True):\n    \"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n    return_offset : bool, optional\n        return the log of the scale factor for each row\n\n    Returns\n    -------\n    tuple\n        normalized profile (fresh np object) and offset (if return_offset==True)\n    \"\"\"", "output": "offset (np object)\n    \"\"\"\n    if log:\n        in_profile = np.exp(in_profile)\n    in_profile = in_profile / np.sum(in_profile, axis=1, keepdims=True)\n    if log:\n        in_profile = np.log(in_profile)\n    if return_offset:\n        return in_profile, np.log(np.sum(in_profile, axis=1))\n    else:\n        return in_profile\n\n\ndef normalize_profile_by_row(in", "solution": "\n    if log:\n        tmp_prefactor = in_profile.max(axis=1)\n        tmp_prof = np.exp(in_profile.T - tmp_prefactor).T\n    else:\n        tmp_prefactor = 0.0\n        tmp_prof = in_profile\n\n    norm_vector = tmp_prof.sum(axis=1)\n    return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)),\n            (np.log(norm_vector) + tmp_prefactor) if return_offset else None)"}
{"prompt": "def delete_dashboard(self, dashboard):\n        \"\"\"**Description**\n            Deletes a dashboard.\n\n        **Arguments**\n            - **dashboard**: the dashboard object as returned by :func:`~SdcClient.get_dashboards`.\n\n        **Success Return Value**\n            `None`.\n\n        **Example**\n            `examples/delete_dashboard.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_dashboard.py>`_\n        \"\"\"", "output": "\n        self._delete_resource(dashboard)\n\n    def get_dashboards(self, query_parameters=None):\n        \"\"\"**Description**\n            Returns a list of dashboards.\n\n        **Arguments**\n            - **query_parameters**: `dict` of query parameters to append to the request.\n\n        **Success Return Value**\n            A list of dashboard objects as returned by :func:`~SdcClient.get_dashboard`.\n\n        **Example**\n            `examples/get_dashboards.py <https://github.com/draios", "solution": "\n        if 'id' not in dashboard:\n            return [False, \"Invalid dashboard format\"]\n\n        res = requests.delete(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        return [True, None]"}
{"prompt": "def _make_masked_initializer(mask, initializer):\n  \"\"\"Returns a masked version of the given initializer.\"\"\"", "output": "\n  def _initializer(shape, dtype=None, partition_info=None):\n    if dtype is None:\n      raise ValueError(\"If a partition_info is passed, a dtype must be \"\n                       \"specified\")\n    if mask is None:\n      return initializer(shape, dtype, partition_info)\n    else:\n      return tf.where(mask, initializer(shape, dtype, partition_info),\n                      tf.zeros(shape, dtype))\n  return _initializer\n\n\ndef _make_masked_variable(mask", "solution": "\n  initializer = tf.keras.initializers.get(initializer)\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    # If no `partition_info` is given, then don't pass it to `initializer`, as\n    # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which\n    # don't accept a `partition_info` argument).\n    if partition_info is None:\n      x = initializer(shape, dtype)\n    else:\n      x = initializer(shape, dtype, partition_info)\n    return tf.cast(mask, x.dtype) * x\n  return masked_initializer"}
{"prompt": "def wipe(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"", "output": "\n    if ctx.invoked_subcommand is None:\n        print(\"No subcommand given.\")\n\n\n@wipe.command()\n@click.argument('db')\ndef database(db):\n    \"\"\"Drop the mongo database given.\"\"\"\n    db = get_db(db)\n    db.drop_database(db)\n\n\n@wipe.command()\n@click.argument('collection')\ndef collection(collection):\n    \"\"\"Drop the mongo collection given.\"\"\"\n    collection = get_collection(collection)\n    collection.drop()\n\n\n@", "solution": "\n    LOG.info(\"Running scout wipe\")\n    db_name = ctx.obj['mongodb']\n    LOG.info(\"Dropping database %s\", db_name)\n    try:\n        ctx.obj['client'].drop_database(db_name)\n    except Exception as err:\n        LOG.warning(err)\n        ctx.abort()\n    LOG.info(\"Dropped whole database\")"}
{"prompt": "def new_contact(cls, address_book, supported_private_objects, version,\n            localize_dates):\n        \"\"\"Use this to create a new and empty contact.\"\"\"", "output": "\n        return cls(address_book, supported_private_objects, version,\n            localize_dates)\n\n    def __init__(self, address_book, supported_private_objects, version,\n            localize_dates):\n        \"\"\"Initialize a new contact.\"\"\"\n        self._address_book = address_book\n        self._supported_private_objects = supported_private_objects\n        self._version = version\n        self._localize_dates = localize_dates\n        self._properties = {}\n        self._properties_by_id = {}\n", "solution": "\n        return cls(address_book, None, supported_private_objects, version,\n                localize_dates)"}
{"prompt": "def sample_variants(self, variants, sample_name, category = 'snv'):\n        \"\"\"Given a list of variants get variant objects found in a specific patient\n\n        Args:\n            variants(list): a list of variant ids\n            sample_name(str): a sample display name\n            category(str): 'snv', 'sv' ..\n\n        Returns:\n            result(iterable(Variant))\n        \"\"\"", "output": "\n        if category == 'snv':\n            return self.get_variants_by_ids(variants, sample_name)\n        elif category == 'sv':\n            return self.get_sv_variants_by_ids(variants, sample_name)\n        else:\n            raise ValueError('Unsupported category: {}'.format(category))\n\n    def get_variants_by_ids(self, variant_ids, sample_name):\n        \"\"\"Given a list of variant ids get variant objects found in a specific patient\n\n        Ar", "solution": "\n        LOG.info('Retrieving variants for subject : {0}'.format(sample_name))\n        has_allele = re.compile('1|2') # a non wild-type allele is called at least once in this sample\n\n        query = {\n            '$and': [\n                {'_id' : { '$in' : variants}},\n                {'category' : category},\n                {'samples': {\n                    '$elemMatch': { 'display_name' : sample_name, 'genotype_call': { '$regex' : has_allele } }\n                }}\n            ]\n        }\n\n        result = self.variant_collection.find(query)\n        return result"}
{"prompt": "def _register_by_id_managed_msg(self, msgid, line, is_disabled=True):\n        \"\"\"If the msgid is a numeric one, then register it to inform the user\n        it could furnish instead a symbolic msgid.\"\"\"", "output": "\n        if msgid.isdigit():\n            self.register_by_id(msgid, line, is_disabled)\n\n    def register_by_id(self, msgid, line, is_disabled=True):\n        \"\"\"Register a message to the catalog.\n\n        :param msgid: the message identifier\n        :param line: the line where the message was found\n        :param is_disabled: whether the message is disabled or not\n        \"\"\"\n        if msgid in self.catalog:\n            self.catalog[msgid].add_line(line)", "solution": "\n        try:\n            message_definitions = self.msgs_store.get_message_definitions(msgid)\n            for message_definition in message_definitions:\n                if msgid == message_definition.msgid:\n                    MessagesHandlerMixIn.__by_id_managed_msgs.append(\n                        (\n                            self.current_name,\n                            message_definition.msgid,\n                            message_definition.symbol,\n                            line,\n                            is_disabled,\n                        )\n                    )\n        except UnknownMessageError:\n            pass"}
{"prompt": "def pretty(obj, verbose=False, max_width=79, newline='\\n'):\n    \"\"\"\n    Pretty print the object's representation.\n    \"\"\"", "output": "\n    if verbose:\n        return repr(obj)\n    else:\n        return _pretty(obj, max_width, newline)\n\n\ndef _pretty(obj, max_width, newline):\n    \"\"\"\n    Pretty print the object's representation.\n    \"\"\"\n    if isinstance(obj, str):\n        return obj\n    elif isinstance(obj, bytes):\n        return repr(obj)\n    elif isinstance(obj, (list, tuple)):\n        return _pretty_list(obj, max_width, newline)\n    elif", "solution": "\n    stream = StringIO()\n    printer = RepresentationPrinter(stream, verbose, max_width, newline)\n    printer.pretty(obj)\n    printer.flush()\n    return stream.getvalue()"}
{"prompt": "def __match_intervals(intervals_from, intervals_to, strict=True):  # pragma: no cover\n    \"\"\"Numba-accelerated interval matching algorithm.\n\n    \"\"\"", "output": "\n    from numba import njit\n\n    @njit\n    def match_intervals(intervals_from, intervals_to, strict):\n        \"\"\"Numba-accelerated interval matching algorithm.\n\n        \"\"\"\n        n_from = intervals_from.shape[0]\n        n_to = intervals_to.shape[0]\n\n        matches = np.zeros((n_from, n_to), dtype=np.bool_)\n\n        for i in range(n_from):\n            for j in range(n_to):\n", "solution": "\n    # sort index of the interval starts\n    start_index = np.argsort(intervals_to[:, 0])\n\n    # sort index of the interval ends\n    end_index = np.argsort(intervals_to[:, 1])\n\n    # and sorted values of starts\n    start_sorted = intervals_to[start_index, 0]\n    # and ends\n    end_sorted = intervals_to[end_index, 1]\n\n    search_ends = np.searchsorted(start_sorted, intervals_from[:, 1], side='right')\n    search_starts = np.searchsorted(end_sorted, intervals_from[:, 0], side='left')\n\n    output = np.empty(len(intervals_from), dtype=numba.uint32)\n    for i in range(len(intervals_from)):\n        query = intervals_from[i]\n\n        # Find the intervals that start after our query ends\n        after_query = search_ends[i]\n        # And the intervals that end after our query begins\n        before_query = search_starts[i]\n\n        # Candidates for overlapping have to (end after we start) and (begin before we end)\n        candidates = set(start_index[:after_query]) & set(end_index[before_query:])\n\n        # Proceed as before\n        if len(candidates) > 0:\n            output[i] = __match_interval_overlaps(query, intervals_to, candidates)\n        elif strict:\n            # Numba only lets us use compile-time constants in exception messages\n            raise ParameterError\n        else:\n            # Find the closest interval\n            # (start_index[after_query] - query[1]) is the distance to the next interval\n            # (query[0] - end_index[before_query])\n            dist_before = np.inf\n            dist_after = np.inf\n            if search_starts[i] > 0:\n                dist_before = query[0] - end_sorted[search_starts[i]-1]\n            if search_ends[i] + 1 < len(intervals_to):\n                dist_after = start_sorted[search_ends[i]+1] - query[1]\n            if dist_before < dist_after:\n                output[i] = end_index[search_starts[i]-1]\n            else:\n                output[i] = start_index[search_ends[i]+1]\n    return output"}
{"prompt": "def on_key_press(self,symbol,modifiers):\n        \"\"\"\n        Keyboard event handler handling only the escape key.\n        \n        If an escape key press is detected, mouse exclusivity is toggled via :py:meth:`PengWindow.toggle_exclusivity()`\\ .\n        \"\"\"", "output": "\n        if symbol == pyglet.window.key.ESCAPE:\n            self.toggle_exclusivity()\n\n    def on_mouse_press(self,x,y,button,modifiers):\n        \"\"\"\n        Mouse event handler handling only the left mouse button.\n        \n        If the left mouse button is pressed, the mouse is locked via :py:meth:`PengWindow.set_exclusive_mouse()`\\ .\n        \"\"\"\n        if button == pyglet.window.mouse.LEFT:\n            self.set_exclusive_mouse", "solution": "\n        if symbol == key.ESCAPE:\n            self.world.peng.window.toggle_exclusivity()\n            return pyglet.event.EVENT_HANDLED"}
{"prompt": "def pprint_simple_array(val, displaywidth, msg_nocr, msg, lineprefix=''):\n    \"\"\"Try to pretty print a simple case where a list is not nested.\n    Return True if we can do it and False if not. \"\"\"", "output": "\n    if not isinstance(val, list):\n        return False\n    if len(val) == 0:\n        return False\n    if not isinstance(val[0], list):\n        return False\n    if len(val[0]) != len(val):\n        return False\n    if not all(isinstance(v, str) for v in val):\n        return False\n    if not all(len(v) == len(val[0]) for v in val):\n        return False\n    if not all(len(v) == len(val[", "solution": "\n\n    if type(val) != list:\n        return False\n\n    numeric = True\n    for i in range(len(val)):\n        if not (type(val[i]) in [bool, float, int]):\n            numeric = False\n            if not (type(val[i]) in [bool, float, int, bytes]):\n                return False\n            pass\n        pass\n    mess = columnize([repr(v) for v in val],\n                     opts={\"arrange_array\": True,\n                           \"lineprefix\": lineprefix,\n                           \"displaywidth\": int(displaywidth)-3,\n                           'ljust': not numeric})\n    msg_nocr(mess)\n    return True"}
{"prompt": "def size(self, source):\n    \"\"\"Get the size component of the given s3url. If it is a\n       directory, combine the sizes of all the files under\n       that directory. Subdirectories will not be counted unless\n       --recursive option is set.\n    \"\"\"", "output": "\n    if source.endswith('/'):\n        source = source[:-1]\n    if source.startswith('s3://'):\n        source = source[5:]\n    if source.startswith('s3n://'):\n        source = source[6:]\n    if source.startswith('s3a://'):\n        source = source[6:]\n    if source.startswith('s3n+ssl://'):\n        source = source[11:]\n    if source.startswith('s3a+ssl://'):\n        source = source[11:]", "solution": "\n    result = []\n    for src in self.source_expand(source):\n      size = 0\n      for f in self.s3walk(src):\n        size += f['size']\n      result.append((src, size))\n\n    return result"}
{"prompt": "def parse_compounds(compound_info, case_id, variant_type):\n    \"\"\"Get a list with compounds objects for this variant.\n\n        Arguments:\n            compound_info(str): A Variant dictionary\n            case_id (str): unique family id\n            variant_type(str): 'research' or 'clinical'\n\n        Returns:\n            compounds(list(dict)): A list of compounds\n    \"\"\"", "output": "\n    compounds = []\n    for compound in compound_info:\n        compound_id = compound.get('compound_id')\n        if compound_id:\n            compound_obj = {\n                'case_id': case_id,\n                'variant_id': compound.get('variant_id'),\n                'variant_type': variant_type,\n                'compound_id': compound_id,\n                'compound_name': compound.get('compound_name'),\n                'compound_type': comp", "solution": "\n    # We need the case to construct the correct id\n    compounds = []\n    if compound_info:\n        for family_info in compound_info.split(','):\n            splitted_entry = family_info.split(':')\n            # This is the family id\n            if splitted_entry[0] == case_id:\n                for compound in splitted_entry[1].split('|'):\n                    splitted_compound = compound.split('>')\n                    compound_obj = {}\n                    compound_name = splitted_compound[0]\n                    compound_obj['variant'] = generate_md5_key(compound_name.split('_') +\n                                                               [variant_type, case_id])\n\n                    try:\n                        compound_score = float(splitted_compound[1])\n                    except (TypeError, IndexError):\n                        compound_score = 0.0\n\n                    compound_obj['score'] = compound_score\n                    compound_obj['display_name'] = compound_name\n\n                    compounds.append(compound_obj)\n\n    return compounds"}
{"prompt": "def any_slug_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for SlugField\r\n    >>> result = any_field(models.SlugField())\r\n    >>> type(result)\r\n    <type 'str'>\r\n    >>> from django.core.validators import slug_re\r\n    >>> re.match(slug_re, result) is not None\r\n    True\r\n    \"\"\"", "output": "\r\n    return any_field(field, **kwargs)\r\n\r\n\r\ndef any_url_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for URLField\r\n    >>> result = any_field(models.URLField())\r\n    >>> type(result)\r\n    <type 'str'>\r\n    >>> result.startswith('http://')\r\n    True\r\n    \"\"\"\r\n    return any_field(field, **kwargs)\r\n\r\n\r\ndef any_ip_address_field(field, **kwargs):\r\n   ", "solution": "\r\n    letters = ascii_letters + digits + '_-'\r\n    return xunit.any_string(letters = letters, max_length = field.max_length)"}
{"prompt": "def factorize(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n            .C : updated values for C.\n            .U : updated values for U.\n            .R : updated values for R.\n        \"\"\"", "output": "\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a valid matrix\n        # TODO: Check if CUR is a", "solution": "\n\n        [prow, pcol] = self.sample_probability()\n\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._cmdinit()\n\n        self.computeUCR()"}
{"prompt": "def _deduplicate_indexed_slices(values, indices):\n    \"\"\"Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank >= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n      dimension of `values` (as in an IndexedSlices object).\n    Returns:\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n      de-duplicated version of `indices` and `summed_values` contains the sum of\n      `values` slices associated with each unique index.\n    \"\"\"", "output": "indices` and `summed_values` contains the sum of\n      `values` at each unique index. The returned `unique_indices` will be a\n      de-duplicated version of `indices` with no guaranteed ordering.\n    \"\"\"\n    unique_indices, idx = tf.unique(indices)\n    summed_values = tf.unsorted_segment_sum(values, idx, tf.size(unique_indices))\n    return summed_values, unique_indices\n\n\ndef _deduplicate_indexed_slices(index", "solution": "\n    unique_indices, new_index_positions = tf.unique(indices)\n    summed_values = tf.unsorted_segment_sum(values,\n                                            new_index_positions,\n                                            tf.shape(unique_indices)[0])\n    return (summed_values, unique_indices)"}
{"prompt": "def token_validate_with_login(self, **kwargs):\n        \"\"\"\n        Authenticate a user with a TMDb username and password.  The user\n        must have a verified email address and be registered on TMDb.\n\n        Args:\n            request_token: The token you generated for the user to approve.\n            username: The user's username on TMDb.\n            password: The user's password on TMDb.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "\n        return self.request(\n            'GET',\n            'authentication/token/validate_with_login',\n            **kwargs\n        )\n\n    def token_validate_with_token(self, **kwargs):\n        \"\"\"\n        Authenticate a user with a TMDb username and password.  The user\n        must have a verified email address and be registered on TMDb.\n\n        Args:\n            request_token: The token you generated for the user to approve.\n            username: The user's username on TMDb.\n            password: The user'", "solution": "\n        path = self._get_path('token_validate_with_login')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def setLang(self,lang):\n        \"\"\"\n        Sets the default language for all domains.\n        \n        For recommendations regarding the format of the language code, see\n        :py:class:`TranslationManager`\\ .\n        \n        Note that the ``lang`` parameter of both :py:meth:`translate()` and\n        :py:meth:`translate_lazy()` will override this setting.\n        \n        Also note that the code won't be checked for existence or plausibility.\n        This may cause the fallback strings to be displayed instead if the language\n        does not exist.\n        \n        Calling this method will cause the ``setlang`` action and the\n        :peng3d:event`peng3d:i18n.set_lang` event to be triggered. Note that both\n        action and event will be triggered even if the language did not actually change.\n        \n        This method also automatically updates the :confval:`i18n.lang` config value.\n        \"\"\"", "output": "the\n        language is invalid.\n        \n        :param lang: The language code.\n        :type lang: str\n        \"\"\"\n        self._lang = lang\n    \n    def getLang(self):\n        \"\"\"\n        Returns the default language for all domains.\n        \n        :return: The language code.\n        :rtype: str\n        \"\"\"\n        return self._lang\n    \n    def getDomain(self,domain):\n        \"\"\"\n        Returns the translation domain for the given domain name.\n        \n        :param domain: The domain name.\n       ", "solution": "\n        self.lang = lang\n        self.peng.cfg[\"i18n.lang\"] = lang\n        \n        if lang not in self.cache:\n            self.cache[lang]={}\n        \n        self.doAction(\"setlang\")\n        self.peng.sendEvent(\"peng3d:i18n.set_lang\",{\"lang\":self.lang,\"i18n\":self})"}
{"prompt": "def end_group(self, dedent=0, close=''):\n        \"\"\"End a group. See `begin_group` for more details.\"\"\"", "output": "\n        self.write(self.indent * dedent, close)\n        self.indent -= 1\n\n    def begin_group(self, name, dedent=0, open=''):\n        \"\"\"Begin a group.\n\n        A group is a block of code that is indented.\n\n        Parameters\n        ----------\n        name : str\n            The name of the group.\n        dedent : int, optional\n            The number of indents to dedent the group.\n        open : str, optional\n            The opening string of the group.\n\n       ", "solution": "\n        self.indentation -= dedent\n        group = self.group_stack.pop()\n        if not group.breakables:\n            self.group_queue.remove(group)\n        if close:\n            self.text(close)"}
{"prompt": "def revoke_token(self, token, token_type_hint, request, *args, **kwargs):\n        \"\"\"Revoke an access or refresh token.\n        \"\"\"", "output": "\n        token = self.get_token(token)\n        if token is None:\n            raise errors.InvalidRequestError(\n                description='Invalid token: {}'.format(token))\n\n        if token_type_hint == 'access_token':\n            token.revoke()\n        elif token_type_hint == 'refresh_token':\n            token.refresh_token.revoke()\n        else:\n            raise errors.InvalidRequestError(\n                description='Unsupported token type: {}'.format(\n                    token_type_hint))", "solution": "\n        if token_type_hint:\n            tok = self._tokengetter(**{token_type_hint: token})\n        else:\n            tok = self._tokengetter(access_token=token)\n            if not tok:\n                tok = self._tokengetter(refresh_token=token)\n\n        if tok:\n            request.client_id = tok.client_id\n            request.user = tok.user\n            tok.delete()\n            return True\n\n        msg = 'Invalid token supplied.'\n        log.debug(msg)\n        request.error_message = msg\n        return False"}
{"prompt": "def is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"", "output": "\n    return node.root().get_bool_value(POSTPONED_EVALUATION_ENABLED_KEY, default=False)\n\n\ndef is_postponed_evaluation_enabled_for_node(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled for a node\"\"\"\n    return is_postponed_evaluation_enabled(node) and not is_postponed_evaluation_disabled_for_node(node)\n\n\ndef is_post", "solution": "\n    name = \"annotations\"\n    module = node.root()\n    stmt = module.locals.get(name)\n    return (\n        stmt\n        and isinstance(stmt[0], astroid.ImportFrom)\n        and stmt[0].modname == \"__future__\"\n    )"}
{"prompt": "def _log(cls, level, msg, **kwargs):\n        \"\"\"\n        Logs a message with pre-formatted prefix.\n\n        :param int level:\n            Logging level as specified in the\n            `login module <http://docs.python.org/2/library/logging.html>`_ of\n            Python standard library.\n\n        :param str msg:\n            The actual message.\n\n        \"\"\"", "output": "\n        if not cls.logger:\n            return\n\n        if cls.logger.isEnabledFor(level):\n            cls.logger.log(level, \"[%s] %s\", cls.__name__, msg, **kwargs)\n\n    @classmethod\n    def debug(cls, msg, **kwargs):\n        \"\"\"\n        Logs a message with DEBUG level.\n\n        :param str msg:\n            The actual message.\n\n        \"\"\"\n        cls._log(logging.DEBUG, msg, **kwargs)\n\n    @classmethod\n", "solution": "\n\n        logger = getattr(cls, '_logger', None) or authomatic.core._logger\n        logger.log(\n            level, ': '.join(\n                ('authomatic', cls.__name__, msg)), **kwargs)"}
{"prompt": "def print_list_lines(self, filename, first, last):\n        \"\"\"The printing (as opposed to the parsing part of a 'list'\n        command.\"\"\"", "output": "\n        if first is None:\n            first = 1\n        if last is None:\n            last = self.num_lines\n        if last < first:\n            first, last = last, first\n        if first < 1:\n            first = 1\n        if last > self.num_lines:\n            last = self.num_lines\n        if first > last:\n            first, last = last, first\n        if first == last:\n            self.write('%d\\n' % first)\n        else:\n            self.write('%d", "solution": "\n        try:\n            Colors = self.color_scheme_table.active_colors\n            ColorsNormal = Colors.Normal\n            tpl_line = '%%s%s%%s %s%%s' % (Colors.lineno, ColorsNormal)\n            tpl_line_em = '%%s%s%%s %s%%s%s' % (Colors.linenoEm, Colors.line, ColorsNormal)\n            src = []\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno)\n                if not line:\n                    break\n\n                if lineno == self.curframe.f_lineno:\n                    line = self.__format_line(tpl_line_em, filename, lineno, line, arrow = True)\n                else:\n                    line = self.__format_line(tpl_line, filename, lineno, line, arrow = False)\n\n                src.append(line)\n                self.lineno = lineno\n\n            print >>io.stdout, ''.join(src)\n\n        except KeyboardInterrupt:\n            pass"}
{"prompt": "def _propagate_mean(mean, linop, dist):\n  \"\"\"Propagate a mean through linear Gaussian transformation.\"\"\"", "output": "\n  if linop is None:\n    return mean\n  if dist is None:\n    return linop.matvec(mean)\n  else:\n    return linop.matvec(mean) + dist.mean()\n\n\ndef _propagate_covariance(cov, linop, dist):\n  \"\"\"Propagate a covariance through linear Gaussian transformation.\"\"\"\n  if linop is None:\n    return cov\n  if dist is None:\n    return linop.matvec(cov) @ linop.matvec(cov).T\n ", "solution": "\n  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]"}
{"prompt": "def get_cdata(self, *args):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Returns-->cdata (if a SINGLE argument was provided)\n                  LIST of cdata (if a args was a tuple or list)\n        \"\"\"", "output": "\n        if len(args) == 1:\n            return self._cffi_backend.buffer(args[0])\n        else:\n            return [self._cffi_backend.buffer(arg) for arg in args]\n\n    def get_cdata_pointer(self, *args):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Returns-->cdata (if a SINGLE argument was provided)\n                  LIST of cdata (if a args was a tuple or list)\n        \"\"\"\n        if len(args", "solution": "\n        res = tuple([\n            self.from_buffer(x) for x in args\n        ])\n\n        if len(res) == 0:\n            return None\n        elif len(res) == 1:\n            return res[0]\n        else:\n            return res"}
{"prompt": "def sourceWatchdog(self):\n        \"\"\"Watchdog timer function. \n\n        Recreates sources which have not generated events in 10*interval if\n        they have watchdog set to true in their configuration\n        \"\"\"", "output": "\n        if self.watchdog_timer is not None:\n            self.watchdog_timer.cancel()\n        self.watchdog_timer = None\n        for source in self.sources:\n            if source.watchdog:\n                if source.last_event_time is None:\n                    self.logger.info(\"Watchdog: recreating source %s\" % source.name)\n                    self.createSource(source.name)\n\n    def createSource(self, name):\n        \"\"\"Creates a source.\n\n        Args:\n            name", "solution": "\n        for i, source in enumerate(self.sources):\n            if not source.config.get('watchdog', False):\n                continue \n            sn = repr(source)\n            last = self.lastEvents.get(source, None)\n            if last:\n                try:\n                    if last < (time.time()-(source.inter*10)):\n                        log.msg(\"Trying to restart stale source %s: %ss\" % (\n                            sn, int(time.time() - last)\n                        ))\n\n                        s = self.sources.pop(i)\n                        try:\n                            s.t.stop()\n                        except Exception as e:\n                            log.msg(\"Could not stop timer for %s: %s\" % (\n                                sn, e))\n\n                        config = copy.deepcopy(s.config)\n\n                        del self.lastEvents[source]\n                        del s, source\n\n                        source = self.createSource(config)\n\n                        reactor.callLater(0, self._startSource, source)\n                except Exception as e:\n                    log.msg(\"Could not reset source %s: %s\" % (\n                        sn, e))"}
{"prompt": "def get_key_for_purpose_and_type(self, purpose, key_type):\n        \"\"\"\n        Gets a list of keys that match the purpose and key_type, and returns the first key in that list\n        Note, if there are many keys that match the criteria, the one you get back will be random from that list\n        :returns: A key object that matches the criteria\n        \"\"\"", "output": "\n        keys = self.get_keys_for_purpose_and_type(purpose, key_type)\n        if len(keys) == 0:\n            raise Exception(\"No keys found for purpose: \" + str(purpose) + \" and key_type: \" + str(key_type))\n        return keys[0]\n\n    def get_keys_for_purpose_and_type(self, purpose, key_type):\n        \"\"\"\n        Gets a list of keys that match the purpose and key_type\n        :returns: A list", "solution": "\n        key = [key for key in self.keys.values() if key.purpose == purpose and key.key_type == key_type]\n        try:\n            return key[0]\n        except IndexError:\n            return None"}
{"prompt": "def _trj_fill_run_table(self, traj, start, stop):\n        \"\"\"Fills the `run` overview table with information.\n\n        Will also update new information.\n\n        \"\"\"", "output": "\n        # Get the run table\n        run_table = self.run_table\n\n        # Get the run table model\n        run_model = run_table.model()\n\n        # Get the run table selection\n        run_selection = run_table.selectionModel()\n\n        # Get the run table selection model\n        run_selection_model = run_selection.model()\n\n        # Get the run table selection model row count\n        run_selection_model_row_count = run_selection_model.rowCount()\n\n        # Get the run table selection model", "solution": "\n\n        def _make_row(info_dict):\n            row = (info_dict['idx'],\n                   info_dict['name'],\n                   info_dict['time'],\n                   info_dict['timestamp'],\n                   info_dict['finish_timestamp'],\n                   info_dict['runtime'],\n                   info_dict['parameter_summary'],\n                   info_dict['short_environment_hexsha'],\n                   info_dict['completed'])\n            return row\n\n        runtable = getattr(self._overview_group, 'runs')\n\n        rows = []\n        updated_run_information = traj._updated_run_information\n        for idx in range(start, stop):\n            info_dict = traj._run_information[traj._single_run_ids[idx]]\n            rows.append(_make_row(info_dict))\n            updated_run_information.discard(idx)\n\n        if rows:\n            runtable.append(rows)\n            runtable.flush()\n\n        # Store all runs that are updated and that have not been stored yet\n        rows = []\n        indices = []\n        for idx in updated_run_information:\n            info_dict = traj.f_get_run_information(idx, copy=False)\n            rows.append(_make_row(info_dict))\n            indices.append(idx)\n\n        if rows:\n            runtable.modify_coordinates(indices, rows)\n\n        traj._updated_run_information = set()"}
{"prompt": "def to_service(self, service, version):\n        \"\"\"Sets the service name and version the request should target\n\n        Args:\n            service (str): The name of the service as displayed in the services.json file\n            version (str): The version of the service as displayed in the services.json file\n\n        Returns:\n            The request builder instance in order to chain calls\n        \"\"\"", "output": "\n        self._service = service\n        self._version = version\n        return self\n\n    def with_path(self, path):\n        \"\"\"Sets the path the request should target\n\n        Args:\n            path (str): The path the request should target\n\n        Returns:\n            The request builder instance in order to chain calls\n        \"\"\"\n        self._path = path\n        return self\n\n    def with_method(self, method):\n        \"\"\"Sets the method the request should use\n\n        Args:\n            method (str): The method the request", "solution": "\n        service_url = self._service_locator.get_service_url(service, version)\n        return self.__copy_and_set('service_url', self.__strip_trailing_slashes(service_url))"}
{"prompt": "def _cmd(self, cmd, *args, **kw):\n        \"\"\"\n        write a single command, with variable number of arguments. after the\n        command, the device must return ACK\n        \"\"\"", "output": "\n        self.write(cmd)\n        self.write(*args)\n        self.write(b'\\r')\n        self.expect(b'ACK')\n\n    def _cmd_ack(self, cmd, *args, **kw):\n        \"\"\"\n        write a single command, with variable number of arguments. after the\n        command, the device must return ACK\n        \"\"\"\n        self.write(cmd)\n        self.write(*args)\n        self.write(b'\\r')\n        self.expect(b'ACK')\n\n    def _cmd", "solution": "\n        ok = kw.setdefault('ok', False)\n\n        self._wakeup()\n        if args:\n            cmd = \"%s %s\" % (cmd, ' '.join(str(a) for a in args))\n        for i in xrange(3):\n            log.info(\"send: \" + cmd)\n            self.port.write(cmd + '\\n')\n            if ok:\n                ack = self.port.read(len(self.OK))  # read OK\n                log_raw('read', ack)\n                if ack == self.OK:\n                    return\n            else:\n                ack = self.port.read(len(self.ACK))  # read ACK\n                log_raw('read', ack)\n                if ack == self.ACK:\n                    return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        for line in lines:\n            self.prefilter_line(line, continue_prompt)\n\n    def prefilter_line(self, line, continue_prompt=False):\n        \"\"\"Prefilter a single input line of text.\n\n        This is the main entry point for prefiltering a single line of input.\n\n        This covers cases where there is a single line of input, which is the\n        case when the user presses enter.\n        \"\"\"\n        if self.prefilter_func is None:\n            return line\n", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def app_template_filter(self, name=None):\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"", "output": "\n        def decorator(f):\n            self.add_app_template_filter(f, name=name)\n            return f\n        return decorator\n\n    def add_app_template_filter(self, f, name=None):\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.add_template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"\n        if name is None:\n           ", "solution": "\n        def decorator(f):\n            self.add_app_template_filter(f, name=name)\n            return f\n        return decorator"}
{"prompt": "def _replace_at_index(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"", "output": "\n  return x[:index] + [replacement] + x[index + 1:]\n\n\ndef _replace_at_index_in_list(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"\n  return [_replace_at_index(x_i, index, replacement) for x_i in x]\n\n\ndef _replace_at_index_in_dict(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"\n  return {k: _replace_at_index(x_i,", "solution": "\n  x_new = tf.concat([x[:index], tf.expand_dims(replacement, axis=0),\n                     x[(index + 1):]], axis=0)\n  return x_new"}
{"prompt": "def converted(self):\n        \"\"\"\n        :rtype: {str: object}\n        \"\"\"", "output": "\n        return self._converted\n\n    @converted.setter\n    def converted(self, value):\n        \"\"\"\n        :param value: {str: object}\n        :type value: dict\n        \"\"\"\n        self._converted = value\n\n    @property\n    def converted_to_string(self):\n        \"\"\"\n        :rtype: {str: str}\n        \"\"\"\n        return self._converted_to_string\n\n    @converted_to_string.setter\n    def converted_to_string(self, value):\n", "solution": "\n        if self._converted is None:\n            self._converted = self.converter(self.name, self.value)\n        return self._converted"}
{"prompt": "def categories(self):\n        \"\"\"\n        Return the list of levels for an enum (categorical) column.\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"", "output": "\n        if self._is_categorical:\n            return self._categorical_column.categories\n        else:\n            raise ValueError(\"This function can only be applied to single-column categorical frame.\")\n\n    def is_categorical(self):\n        \"\"\"\n        Return True if the frame is categorical.\n        \"\"\"\n        return self._is_categorical\n\n    def is_continuous(self):\n        \"\"\"\n        Return True if the frame is continuous.\n        \"\"\"\n        return self._is_continuous\n\n    def is_", "solution": "\n        if self.ncols != 1:\n            raise H2OValueError(\"This operation only applies to a single factor column\")\n        if self.types[self.names[0]] != \"enum\":\n            raise H2OValueError(\"Input is not a factor. This operation only applies to a single factor column\")\n        return self.levels()[0]"}
{"prompt": "def mixin(cls, mixin_cls):\n        \"\"\"Decorator for mixing in additional functionality into field type\n\n        Example:\n\n        >>> @Integer.mixin\n        ... class IntegerPostgresExtensions:\n        ...     postgres_type = 'INT'\n        ...\n        ...     def postgres_dump(self, obj):\n        ...         self.dump(obj) + \"::integer\"\n\n        Is roughly equivalent to:\n\n        >>> Integer.postgres_type = 'INT'\n        ...\n        ... def postgres_dump(self, obj):\n        ...     self.dump(obj) + \"::integer\"\n        ...\n        ... Integer.postgres_dump = postgres_dump\n\n        \"\"\"", "output": ", obj):\n        ...     self.dump(obj) + \"::integer\"\n        ...\n        ... Integer.postgres_dump = postgres_dump\n\n        \"\"\"\n        for name, value in mixin_cls.__dict__.items():\n            if name.startswith('_'):\n                continue\n            setattr(cls, name, value)\n        return cls\n\n    def __init__(self, *args, **kwargs):\n        super(Field, self).__init__(*args, **kwargs)\n        self.name = self.name or self.__", "solution": "\n        for item_name in dir(mixin_cls):\n            if item_name.startswith(\"__\"):\n                # don't copy magic properties\n                continue\n            item = getattr(mixin_cls, item_name)\n\n            if isinstance(item, types.MethodType):\n                # unbound method will cause problems\n                # so get the underlying function instead\n                item = item.im_func\n\n            setattr(cls, item_name, item)\n        return mixin_cls"}
{"prompt": "def get_annotation_data_between_times(self, id_tier, start, end):\n        \"\"\"Gives the annotations within the times.\n        When the tier contains reference annotations this will be returned,\n        check :func:`get_ref_annotation_data_between_times` for the format.\n\n        :param str id_tier: Name of the tier.\n        :param int start: Start time of the annotation.\n        :param int end: End time of the annotation.\n        :returns: List of annotations within that time.\n        :raises KeyError: If the tier is non existent.\n        \"\"\"", "output": "TierNotFoundError: When the tier is not found.\n        :raises TierNotReferenceError: When the tier is not a reference tier.\n        \"\"\"\n        tier = self.get_tier(id_tier)\n        if not tier.is_reference:\n            raise TierNotReferenceError(\n                \"Tier {} is not a reference tier\".format(id_tier))\n        return tier.get_ref_annotation_data_between_times(start, end)\n\n    def get_ref_annotation_data_between_times(self, id_t", "solution": "\n        if self.tiers[id_tier][1]:\n            return self.get_ref_annotation_data_between_times(\n                id_tier, start, end)\n        anns = ((self.timeslots[a[0]], self.timeslots[a[1]], a[2])\n                for a in self.tiers[id_tier][0].values())\n        return sorted(a for a in anns if a[1] >= start and a[0] <= end)"}
{"prompt": "def __deftype_method_recur_to_py_ast(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"", "output": "\n    return ctx.py_ast_from_node(node.expr)\n\n\ndef __deftype_method_recur_to_ast(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedAST:\n    \"\"\"Return an AST node for `recur` occurring inside a `deftype*` method.\"\"\"\n    return ctx.ast_from_node(node.expr)\n\n\ndef __deftype_method_recur_to_c_ast(\n    ctx: GeneratorContext, node: Recur\n)", "solution": "\n    assert node.op == NodeOp.RECUR\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n    assert this_entry is not None, \"Field type local must have this\"\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain(\n                    [\n                        ast.NameConstant(ctx.recur_point.is_variadic),\n                        ast.Name(id=this_entry.munged, ctx=ast.Load()),\n                    ],\n                    recur_nodes,\n                )\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )"}
{"prompt": "def _random_adjspecies_pair():\n    \"\"\"Return an ordered 2-tuple containing a species and a describer.\"\"\"", "output": "\n    return (\n        random.choice(species),\n        random.choice(describers),\n    )\n\n\ndef _random_adjspecies_pair():\n    \"\"\"Return an ordered 2-tuple containing a species and a describer.\"\"\"\n    return (\n        random.choice(species),\n        random.choice(describers),\n    )\n\n\ndef _random_adjspecies_pair_with_species_in_first_place():\n    \"\"\"Return an ordered 2-tuple containing a species and a describer.\"\"\"\n    return (", "solution": "\n    describer, desc_position = random_describer()\n    if desc_position == 'prefix':\n        return (describer, random_species())\n    elif desc_position == 'suffix':\n        return (random_species(), describer)"}
{"prompt": "def _run_cmd_line_code(self):\n        \"\"\"Run code or file specified at the command-line\"\"\"", "output": "\n        if self.args.code:\n            self.run_code(self.args.code)\n        elif self.args.file:\n            self.run_file(self.args.file)\n        else:\n            self.run_file(self.args.file)\n\n    def _run_file(self, filename):\n        \"\"\"Run a file\"\"\"\n        with open(filename, 'r') as f:\n            code = f.read()\n        self.run_code(code)\n\n    def _run_code(self, code", "solution": "\n        if self.code_to_run:\n            line = self.code_to_run\n            try:\n                self.log.info(\"Running code given at command line (c=): %s\" %\n                              line)\n                self.shell.run_cell(line, store_history=False)\n            except:\n                self.log.warn(\"Error in executing line in user namespace: %s\" %\n                              line)\n                self.shell.showtraceback()\n\n        # Like Python itself, ignore the second if the first of these is present\n        elif self.file_to_run:\n            fname = self.file_to_run\n            try:\n                self._exec_file(fname)\n            except:\n                self.log.warn(\"Error in executing file in user namespace: %s\" %\n                              fname)\n                self.shell.showtraceback()"}
{"prompt": "def _first_glimpse_sensor(self, x_t):\n        \"\"\"\n        Compute first glimpse position using down-sampled image.\n        \"\"\"", "output": "\n        x_t = x_t.view(x_t.size(0), -1)\n        x_t = self.first_glimpse_sensor(x_t)\n        x_t = x_t.view(x_t.size(0), -1, 1, 1)\n        return x_t\n\n    def _second_glimpse_sensor(self, x_t):\n        \"\"\"\n        Compute second glimpse position using down-sampled image.\n        \"\"\"\n        x_t = x", "solution": "\n        downsampled_img = theano.tensor.signal.downsample.max_pool_2d(x_t, (4,4))\n        downsampled_img = downsampled_img.flatten()\n        first_l = T.dot(downsampled_img, self.W_f)\n        if self.disable_reinforce:\n            wf_grad = self.W_f\n            if self.random_glimpse:\n                first_l = self.srng.uniform((2,), low=-1.7, high=1.7)\n        else:\n            sampled_l_t = self._sample_gaussian(first_l, self.cov)\n            sampled_pdf = self._multi_gaussian_pdf(disconnected_grad(sampled_l_t), first_l)\n            wf_grad = T.grad(T.log(sampled_pdf), self.W_f)\n            first_l = sampled_l_t\n        return first_l, wf_grad"}
{"prompt": "def makeReturnFormat(self):\n        \"\"\" Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. \"\"\"", "output": "\n        return self.m_req.strip(self.m_reserved)\n\n    def makeReturnFormat(self):\n        \"\"\" Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. \"\"\"\n        return self.m_req.strip(self.m_reserved)\n\n    def makeReturnFormat(self):\n        \"\"\" Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. \"\"\"\n        return self.m_req.strip(self.m_reserved)\n\n", "solution": "\n        for fld in self.m_blk_a:\n            compare_fld = fld.upper()\n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                self.m_req[fld] = self.m_blk_a[fld]\n        pass"}
{"prompt": "def add_attachment(self, filename, open_file):\n        \"\"\"\n        Adds an attachment to this card.\n        \"\"\"", "output": "\n        return self.trello.add_attachment(self.id, filename, open_file)\n\n    def add_checklist(self, name):\n        \"\"\"\n        Adds a checklist to this card.\n        \"\"\"\n        return self.trello.add_checklist(self.id, name)\n\n    def add_label(self, label):\n        \"\"\"\n        Adds a label to this card.\n        \"\"\"\n        return self.trello.add_label(self.id, label)\n\n    def add_member", "solution": "\n        fields = {\n            'api_key': self.client.api_key,\n            'token': self.client.user_auth_token\n        }\n\n        content_type, body = self.encode_multipart_formdata(\n            fields=fields,\n            filename=filename,\n            file_values=open_file\n        )\n\n        return self.fetch_json(\n            uri_path=self.base_uri + '/attachments',\n            http_method='POST',\n            body=body,\n            headers={'Content-Type': content_type},\n        )"}
{"prompt": "def configure(self, options, conf):\n        \"\"\"\n        Configure plugin.\n        \"\"\"", "output": "\n        self.options = options\n        self.conf = conf\n\n    def get_name(self):\n        \"\"\"\n        Return plugin name.\n        \"\"\"\n        return self.name\n\n    def get_description(self):\n        \"\"\"\n        Return plugin description.\n        \"\"\"\n        return self.description\n\n    def get_version(self):\n        \"\"\"\n        Return plugin version.\n        \"\"\"\n        return self.version\n\n    def get_author(self):\n        \"\"\"\n        Return plugin author.\n        \"\"\"\n        return self.author\n\n", "solution": "\n        try:\n            self.status.pop('active')\n        except KeyError:\n            pass\n        super(Coverage, self).configure(options, conf)\n        if conf.worker:\n            return\n        if self.enabled:\n            try:\n                import coverage\n            except ImportError:\n                log.error(\"Coverage not available: \"\n                          \"unable to import coverage module\")\n                self.enabled = False\n                return\n        self.conf = conf\n        self.coverErase = options.cover_erase\n        self.coverTests = options.cover_tests\n        self.coverPackages = []\n        if options.cover_packages:\n            for pkgs in [tolist(x) for x in options.cover_packages]:\n                self.coverPackages.extend(pkgs)\n        self.coverInclusive = options.cover_inclusive\n        if self.coverPackages:\n            log.info(\"Coverage report will include only packages: %s\",\n                     self.coverPackages)\n        self.coverHtmlDir = None\n        if options.cover_html:\n            self.coverHtmlDir = options.cover_html_dir\n            log.debug('Will put HTML coverage report in %s', self.coverHtmlDir)\n        self.coverBranches = options.cover_branches\n        self.coverXmlFile = None\n        if options.cover_min_percentage:\n            self.coverMinPercentage = int(options.cover_min_percentage.rstrip('%'))\n        if options.cover_xml:\n            self.coverXmlFile = options.cover_xml_file\n            log.debug('Will put XML coverage report in %s', self.coverXmlFile)\n        if self.enabled:\n            self.status['active'] = True\n            self.coverInstance = coverage.coverage(auto_data=False,\n                branch=self.coverBranches, data_suffix=None)"}
{"prompt": "def _select_next_server(self):\n        \"\"\"\n        Looks up in the server pool for an available server\n        and attempts to connect.\n        \"\"\"", "output": "\n        # TODO: implement\n        pass\n\n    def _select_next_server_and_connect(self):\n        \"\"\"\n        Looks up in the server pool for an available server\n        and attempts to connect.\n        \"\"\"\n        # TODO: implement\n        pass\n\n    def _select_next_server_and_connect_with_timeout(self, timeout):\n        \"\"\"\n        Looks up in the server pool for an available server\n        and attempts to connect.\n        \"\"\"\n        # TODO: implement\n        pass\n\n    def _select_next_server_", "solution": "\n\n        while True:\n            if len(self._server_pool) == 0:\n                self._current_server = None\n                raise ErrNoServers\n\n            now = time.monotonic()\n            s = self._server_pool.pop(0)\n            if self.options[\"max_reconnect_attempts\"] > 0:\n                if s.reconnects > self.options[\"max_reconnect_attempts\"]:\n                    # Discard server since already tried to reconnect too many times\n                    continue\n\n            # Not yet exceeded max_reconnect_attempts so can still use\n            # this server in the future.\n            self._server_pool.append(s)\n            if s.last_attempt is not None and now < s.last_attempt + self.options[\"reconnect_time_wait\"]:\n                # Backoff connecting to server if we attempted recently.\n                yield from asyncio.sleep(self.options[\"reconnect_time_wait\"], loop=self._loop)\n            try:\n                s.last_attempt = time.monotonic()\n                r, w = yield from asyncio.open_connection(\n                    s.uri.hostname,\n                    s.uri.port,\n                    loop=self._loop,\n                    limit=DEFAULT_BUFFER_SIZE)\n                self._current_server = s\n\n                # We keep a reference to the initial transport we used when\n                # establishing the connection in case we later upgrade to TLS\n                # after getting the first INFO message. This is in order to\n                # prevent the GC closing the socket after we send CONNECT\n                # and replace the transport.\n                #\n                # See https://github.com/nats-io/asyncio-nats/issues/43\n                self._bare_io_reader = self._io_reader = r\n                self._bare_io_writer = self._io_writer = w\n                break\n            except Exception as e:\n                s.last_attempt = time.monotonic()\n                s.reconnects += 1\n\n                self._err = e\n                if self._error_cb is not None:\n                    yield from self._error_cb(e)\n                continue"}
{"prompt": "def time_remaining_est(self, completed_iter):\n        \"\"\"Estimate the remaining time left.\n\n        Parameters:\n            completed_iter (int): Number of iterations completed.\n\n        Returns:\n            est_time: Estimated time remaining.\n        \"\"\"", "output": "\n        if self.time_est is None:\n            return None\n        if self.time_est.total_time is None:\n            return None\n        if self.time_est.total_time == 0:\n            return None\n        if self.time_est.total_time < 1:\n            return None\n        if self.time_est.total_time < 10:\n            return None\n        if self.time_est.total_time < 60:\n            return None\n        if self.time_est.total_time", "solution": "\n        if completed_iter:\n            t_r_est = (time.time() - self.t_start) / \\\n                completed_iter*(self.iter-completed_iter)\n        else:\n            t_r_est = 0\n        date_time = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=t_r_est)\n        time_string = \"%02d:%02d:%02d:%02d\" % \\\n            (date_time.day - 1, date_time.hour, date_time.minute, date_time.second)\n\n        return time_string"}
{"prompt": "def bkg_subtract(self, analytes=None, errtype='stderr', focus_stage='despiked'):\n        \"\"\"\n        Subtract calculated background from data.\n\n        Must run bkg_calc first!\n\n        Parameters\n        ----------\n        analytes : str or iterable\n            Which analyte(s) to subtract.\n        errtype : str\n            Which type of error to propagate. default is 'stderr'.\n        focus_stage : str\n            Which stage of analysis to apply processing to. \n            Defaults to 'despiked' if present, or 'rawdata' if not. \n            Can be one of:\n            * 'rawdata': raw data, loaded from csv file.\n            * 'despiked': despiked data.\n            * 'signal'/'background': isolated signal and background data.\n              Created by self.separate, after signal and background\n              regions have been identified by self.autorange.\n            * 'bkgsub': background subtracted data, created by \n              self.bkg_correct\n            * 'ratios': element ratio data, created by self.ratio.\n            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.\n        \"\"\"", "output": "default is 'despiked'.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if analytes is None:\n            analytes = self.analytes\n        if not isinstance(analytes, (list, tuple, np.ndarray)):\n            analytes = [analytes]\n        if focus_stage not in self.stages:\n            raise ValueError('focus_stage must be one of the following: {}'.format(self.stages))\n        if errtype not in ['stderr', 'std']:\n            raise ValueError('err", "solution": "\n        if analytes is None:\n            analytes = self.analytes\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        if focus_stage == 'despiked':\n            if 'despiked' not in self.stages_complete:\n                focus_stage = 'rawdata'\n\n        # make uncertainty-aware background interpolators\n        bkg_interps = {}\n        for a in analytes:\n            bkg_interps[a] = un_interp1d(x=self.bkg['calc']['uTime'],\n                                         y=un.uarray(self.bkg['calc'][a]['mean'],\n                                                     self.bkg['calc'][a][errtype]))\n        self.bkg_interps = bkg_interps\n\n        # apply background corrections\n        with self.pbar.set(total=len(self.data), desc='Background Subtraction') as prog:\n            for d in self.data.values():\n                # [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), None, focus_stage=focus_stage) for a in analytes]\n                [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), ~d.sig, focus_stage=focus_stage) for a in analytes]\n                d.setfocus('bkgsub')\n\n                prog.update()\n\n        self.stages_complete.update(['bkgsub'])\n        self.focus_stage = 'bkgsub'\n        return"}
{"prompt": "def find_bp(self, filename, lineno, frame):\n        \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n        Called only if we know there is a bpt at this\n        location.  Returns breakpoint that was triggered and a flag\n        that indicates if it is ok to delete a temporary breakpoint.\n\n        \"\"\"", "output": "\n        # First, check if we have a temporary breakpoint at this\n        # location.  If so, we must delete it.\n        bp = self.get_temporary_breakpoint(filename, lineno)\n        if bp is not None:\n            return bp, True\n\n        # Next, check if we have a permanent breakpoint at this\n        # location.  If so, we must return it.\n        bp = self.get_breakpoint(filename, lineno)\n        if bp is not None:\n            return bp,", "solution": "\n        possibles = self.bplist[filename, lineno]\n        for i in range(0, len(possibles)):\n            b = possibles[i]\n            if not b.enabled:\n                continue\n            if not checkfuncname(b, frame):\n                continue\n            # Count every hit when bp is enabled\n            b.hits += 1\n            if not b.condition:\n                # If unconditional, and ignoring, go on to next, else\n                # break\n                if b.ignore > 0:\n                    b.ignore = b.ignore -1\n                    continue\n                else:\n                    # breakpoint and marker that's ok to delete if\n                    # temporary\n                    return (b, True)\n            else:\n                # Conditional bp.\n                # Ignore count applies only to those bpt hits where the\n                # condition evaluates to true.\n                try:\n                    val = eval(b.condition, frame.f_globals, frame.f_locals)\n                    if val:\n                        if b.ignore > 0:\n                            b.ignore = b.ignore -1\n                            # continue\n                        else:\n                            return (b, True)\n                    # else:\n                    #   continue\n                except:\n                    # if eval fails, most conservative thing is to\n                    # stop on breakpoint regardless of ignore count.\n                    # Don't delete temporary, as another hint to user.\n                    return (b, False)\n                pass\n            pass\n        return (None, None)"}
{"prompt": "def _to_dataframe(self):\n        \"\"\"Read and return the dataset contents as a pandas DataFrame.\"\"\"", "output": "\n        return self._to_pandas()\n\n    def _to_pandas(self):\n        \"\"\"Read and return the dataset contents as a pandas DataFrame.\"\"\"\n        return self._to_table().to_pandas()\n\n    def _to_table(self):\n        \"\"\"Read and return the dataset contents as a pyarrow.Table.\"\"\"\n        return self._to_pyarrow()\n\n    def _to_pyarrow(self):\n        \"\"\"Read and return the dataset contents as a pyarrow.Table.\"\"\"\n        return self._to_arrow()\n\n   ", "solution": "\n        #TODO: figure out why passing in the opened stream directly gives invalid data\n        data = self.read_as_binary()\n        reader = BytesIO(data)\n        return deserialize_dataframe(reader, self.data_type_id)"}
{"prompt": "def create_tfs_git_client(url, token=None):\n    \"\"\"\n    Creates a TFS Git Client to pull Git repo info\n    \"\"\"", "output": "\n    if token:\n        tfs_git_client = TfsGitClient(url, token)\n    else:\n        tfs_git_client = TfsGitClient(url)\n    return tfs_git_client\n\n\ndef get_git_repo_info(tfs_git_client, repo_name):\n    \"\"\"\n    Gets the Git repo info for the given repo name\n    \"\"\"\n    git_repo_info = tfs_git_client.get_git_repo_info(repo_name)\n    return git_repo_", "solution": "\n    if token is None:\n        token = os.environ.get('TFS_API_TOKEN', None)\n\n    tfs_connection = create_tfs_connection(url, token)\n    tfs_git_client = tfs_connection.get_client('vsts.git.v4_1.git_client.GitClient')\n\n    if tfs_git_client is None:\n        msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'\n        raise RuntimeError(msg, url)\n\n    return tfs_git_client"}
{"prompt": "def canFetchMore(self, index):\n        \"\"\"Return if more data available for *index*.\"\"\"", "output": "\n        return self.index < len(self.data)\n\n    def fetchMore(self, index):\n        \"\"\"Fetch more data for *index*.\"\"\"\n        self.beginInsertRows(QtCore.QModelIndex(), index, index)\n        self.endInsertRows()\n\n    def data(self, index, role):\n        \"\"\"Return data for *index*.\"\"\"\n        if not index.isValid():\n            return None\n        if role == QtCore.Qt.DisplayRole:\n            return self.data[index.row()]\n        return None", "solution": "\n        if not index.isValid():\n            item = self.root\n        else:\n            item = index.internalPointer()\n\n        return item.canFetchMore()"}
{"prompt": "def info_formatter(info):\n    \"\"\"Produce a sequence of formatted lines from info.\n\n    `info` is a sequence of pairs (label, data).  The produced lines are\n    nicely formatted, ready to print.\n\n    \"\"\"", "output": "\n    lines = []\n    for label, data in info:\n        if isinstance(data, str):\n            lines.append(f'{label}: {data}')\n        elif isinstance(data, (list, tuple)):\n            lines.append(f'{label}:')\n            for line in data:\n                lines.append(f'  {line}')\n        elif isinstance(data, dict):\n            lines.append(f'{label}:')\n            for key, value in data.items():\n                lines.append(f'  {key", "solution": "\n    label_len = max([len(l) for l, _d in info])\n    for label, data in info:\n        if data == []:\n            data = \"-none-\"\n        if isinstance(data, (list, tuple)):\n            prefix = \"%*s:\" % (label_len, label)\n            for e in data:\n                yield \"%*s %s\" % (label_len+1, prefix, e)\n                prefix = \"\"\n        else:\n            yield \"%*s: %s\" % (label_len, label, data)"}
{"prompt": "async def handle_request(self, request: Request\n                             ) -> typing.Tuple[AsyncGeneratorType, Response]:\n        \"\"\"\n        Wrap request with middleware.\n        :param request:\n        :return:\n        \"\"\"", "output": "\n        for middleware in self.middlewares:\n            request = middleware(request)\n        return self.app(request)\n\n    def __call__(self, request: Request) -> typing.Tuple[AsyncGeneratorType, Response]:\n        \"\"\"\n        Wrap request with middleware.\n        :param request:\n        :return:\n        \"\"\"\n        return self.handle_request(request)\n\n    def add_middleware(self, middleware: typing.Callable) -> None:\n        \"\"\"\n        Add middleware to the list.\n       ", "solution": "\n        callback_result, response = None, None\n\n        await self._run_request_middleware(request)\n        try:\n            callback_result, response = await request.fetch_callback(self.sem)\n        except NotImplementedParseError as e:\n            self.logger.error(e)\n        except NothingMatchedError as e:\n            self.logger.error(f'<Item: {str(e).lower()}>')\n        except Exception as e:\n            self.logger.error(f'<Callback[{request.callback.__name__}]: {e}')\n        await self._run_response_middleware(request, response)\n        await self._process_response(request=request, response=response)\n        return callback_result, response"}
{"prompt": "def get_connection(host='localhost', port=27017, username=None, password=None,\n                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):\n    \"\"\"Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect\n\n    \"\"\"", "output": "   Returns:\n        pymongo.MongoClient: Client to the database\n    \"\"\"\n    if uri is not None:\n        client = MongoClient(uri, *args, **kwargs)\n    elif mongodb is not None:\n        client = MongoClient(mongodb, *args, **kwargs)\n    else:\n        client = MongoClient(host, port, *args, **kwargs)\n\n    if username is not None and password is not None:\n        client.admin.authenticate(username, password, source=authdb)\n\n    return client\n\n\ndef", "solution": "\n    authdb = authdb or mongodb\n    if uri is None:\n        if username and password:\n            uri = (\"mongodb://{}:{}@{}:{}/{}\"\n                   .format(quote_plus(username), quote_plus(password), host, port, authdb))\n            log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n                   .format(quote_plus(username), host, port, authdb))\n        else:\n            log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n            \n\n    LOG.info(\"Try to connect to %s\" % log_uri)\n    try:\n        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)\n    except ServerSelectionTimeoutError as err:\n        LOG.warning(\"Connection Refused\")\n        raise ConnectionFailure\n\n    LOG.info(\"Connection established\")\n    return client"}
{"prompt": "def _get_job(self, project_id, job_id):\n        \"\"\"\n        Gets a MLEngine job based on the job name.\n\n        :return: MLEngine job object if succeed.\n        :rtype: dict\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server\n        \"\"\"", "output": "\n        try:\n            return self.get_job(project_id, job_id)\n        except HttpError as e:\n            if e.resp.status == 404:\n                return None\n            else:\n                raise\n\n    def _get_job_status(self, project_id, job_id):\n        \"\"\"\n        Gets the status of a MLEngine job based on the job name.\n\n        :return: MLEngine job status if succeed.\n        :rtype: dict\n\n        Raises:\n           ", "solution": "\n        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    # polling after 30 seconds when quota failure occurs\n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))\n                    raise"}
{"prompt": "def _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list: list[datetime.date]\n    :param before_target: closest before or after the target\n    :type before_target: bool or None\n    :returns: The closest date\n    :rtype: datetime.date or None\n    \"\"\"", "output": "target: bool\n    :return: The closest date\n    :rtype: datetime.date\n    \"\"\"\n    if before_target is None:\n        before_target = False\n\n    if before_target:\n        date_list = sorted(date_list, reverse=True)\n\n    closest_date = None\n    closest_date_diff = None\n\n    for date in date_list:\n        date_diff = (target_dt - date).days\n        if closest_date is None or (before_target and date_diff > closest_date_diff)", "solution": "\n    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max\n    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()"}
{"prompt": "def _has_parent_of_type(node, node_type, statement):\n    \"\"\"Check if the given node has a parent of the given type.\"\"\"", "output": "\n    if node.parent is None:\n        return False\n    if node.parent.type == node_type:\n        return True\n    return _has_parent_of_type(node.parent, node_type, statement)\n\n\ndef _get_parent_of_type(node, node_type, statement):\n    \"\"\"Get the parent of the given type.\"\"\"\n    if node.parent is None:\n        return None\n    if node.parent.type == node_type:\n        return node.parent\n    return _get_parent_of_type", "solution": "\n    parent = node.parent\n    while not isinstance(parent, node_type) and statement.parent_of(parent):\n        parent = parent.parent\n    return isinstance(parent, node_type)"}
{"prompt": "def _wrap_command(cmds, cls, strict=True):\n    \"\"\"Wrap a setup command\n\n    Parameters\n    ----------\n    cmds: list(str)\n        The names of the other commands to run prior to the command.\n    strict: boolean, optional\n        Whether to raise errors when a pre-command fails.\n    \"\"\"", "output": "\n    def _wrap(func):\n        @wraps(func)\n        def _wrapper(self, *args, **kwargs):\n            for cmd in cmds:\n                if not hasattr(self, cmd):\n                    raise AttributeError(\n                        \"Command {0} not found. \"\n                        \"Did you forget to import it?\".format(cmd))\n                getattr(self, cmd)(strict=strict)\n            return func(self, *args, **kwargs)\n        return _wrapper\n    return _wrap\n\n\ndef _", "solution": "\n    class WrappedCommand(cls):\n\n        def run(self):\n            if not getattr(self, 'uninstall', None):\n                try:\n                    [self.run_command(cmd) for cmd in cmds]\n                except Exception:\n                    if strict:\n                        raise\n                    else:\n                        pass\n            # update package data\n            update_package_data(self.distribution)\n\n            result = cls.run(self)\n            return result\n    return WrappedCommand"}
{"prompt": "def as_tuple(self):\n        \"\"\"\n        :rtype: (str, object)\n        \"\"\"", "output": "\n        return (self.name, self.value)\n\n    def __repr__(self):\n        return \"Parameter({0}, {1})\".format(self.name, self.value)\n\n    def __str__(self):\n        return \"{0}={1}\".format(self.name, self.value)\n\n    def __eq__(self, other):\n        if isinstance(other, Parameter):\n            return self.name == other.name and self.value == other.value\n        return False\n\n    def __ne__(self, other):\n", "solution": "\n        if self._as_tuple is None:\n            self._as_tuple = self.converted.items()[0]\n        return self._as_tuple"}
{"prompt": "def add_virtual_columns_cartesian_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", alpha=\"l\", delta=\"b\", distance=\"distance\", radians=False, center=None, center_name=\"solar_position\"):\n        \"\"\"Convert cartesian to spherical coordinates.\n\n\n\n        :param x:\n        :param y:\n        :param z:\n        :param alpha:\n        :param delta: name for polar angle, ranges from -90 to 90 (or -pi to pi when radians is True).\n        :param distance:\n        :param radians:\n        :param center:\n        :param center_name:\n        :return:\n        \"\"\"", "output": "ians=True)\n        :param distance: name for distance from center\n        :param radians: if True, use radians instead of degrees\n        :param center: center of the coordinate system\n        :param center_name: name for center of the coordinate system\n        :return:\n        \"\"\"\n        if center is None:\n            center = self.get_center(center_name)\n        if center is not None:\n            x = x - center[0]\n            y = y - center[1]\n            z = z - center[2]\n        if rad", "solution": "\n        transform = \"\" if radians else \"*180./pi\"\n\n        if center is not None:\n            self.add_variable(center_name, center)\n        if center is not None and center[0] != 0:\n            x = \"({x} - {center_name}[0])\".format(**locals())\n        if center is not None and center[1] != 0:\n            y = \"({y} - {center_name}[1])\".format(**locals())\n        if center is not None and center[2] != 0:\n            z = \"({z} - {center_name}[2])\".format(**locals())\n        self.add_virtual_column(distance, \"sqrt({x}**2 + {y}**2 + {z}**2)\".format(**locals()))\n        # self.add_virtual_column(alpha, \"((arctan2({y}, {x}) + 2*pi) % (2*pi)){transform}\".format(**locals()))\n        self.add_virtual_column(alpha, \"arctan2({y}, {x}){transform}\".format(**locals()))\n        self.add_virtual_column(delta, \"(-arccos({z}/{distance})+pi/2){transform}\".format(**locals()))"}
{"prompt": "def unix_ts(dtval):\n    \"\"\"Convert datetime into a unix timestamp.\n    This is the equivalent to Python 3's int(datetime.timestamp()).\n\n    :param dt: datetime to convert\n    \"\"\"", "output": "\n    return int(dtval.timestamp())\n\n\ndef unix_ts_to_datetime(ts):\n    \"\"\"Convert a unix timestamp into a datetime.\n\n    :param ts: unix timestamp to convert\n    \"\"\"\n    return datetime.fromtimestamp(ts)\n\n\ndef unix_ts_to_datetime_str(ts):\n    \"\"\"Convert a unix timestamp into a datetime string.\n\n    :param ts: unix timestamp to convert\n    \"\"\"\n    return unix_ts_to_datetime(ts).strftime(\"%Y-%m-%d %H:%M:%", "solution": "\n    epoch = datetime(1970, 1, 1, 0, 0, tzinfo=tzutc())\n    delta = (dtval - epoch)\n    return delta.days * 24 * 3600 + delta.seconds"}
{"prompt": "def path_dispatch_kwarg(mname, path_default, returns_model):\n    \"\"\"\n    Parameterized decorator for methods that accept path as a second\n    argument.\n    \"\"\"", "output": "\n    def decorator(f):\n        @wraps(f)\n        def wrapper(self, path=path_default, **kwargs):\n            if path is None:\n                path = path_default\n            if path is not None:\n                path = self._get_path(path)\n            return f(self, path, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef path_dispatch_arg(mname, path_default, returns_model):\n    \"\"\"\n    Parameterized decorator for methods that accept path as a first\n   ", "solution": "\n    def _wrapper(self, path=path_default, **kwargs):\n        prefix, mgr, mgr_path = _resolve_path(path, self.managers)\n        result = getattr(mgr, mname)(path=mgr_path, **kwargs)\n        if returns_model and prefix:\n            return _apply_prefix(prefix, result)\n        else:\n            return result\n    return _wrapper"}
{"prompt": "def get_label ( self, object ):\n        \"\"\" Gets the label to display for a specified object.\n        \"\"\"", "output": "\n        return object.name\n\n    def get_icon ( self, object ):\n        \"\"\" Gets the icon to display for a specified object.\n        \"\"\"\n        return object.icon\n\n    def get_tooltip ( self, object ):\n        \"\"\" Gets the tooltip to display for a specified object.\n        \"\"\"\n        return object.tooltip\n\n    def get_display_name ( self, object ):\n        \"\"\" Gets the display name to display for a specified object.\n        \"\"\"\n        return object.display_name\n\n    def get_", "solution": "\n        label = self.label\n        if label[:1] == '=':\n            return label[1:]\n\n        label = xgetattr( object, label, '' )\n\n        if self.formatter is None:\n            return label\n\n        return self.formatter( object, label )"}
{"prompt": "def create(self, request):\n        \"\"\"\n        Log in django staff user\n        \"\"\"", "output": "\n        data = request.data\n        username = data.get('username')\n        password = data.get('password')\n        user = authenticate(username=username, password=password)\n        if user is not None:\n            if user.is_active:\n                login(request, user)\n                return Response(status=status.HTTP_200_OK)\n            else:\n                return Response(status=status.HTTP_400_BAD_REQUEST)\n        else:\n            return Response(status=status.HTTP_400_B", "solution": "\n        # TODO: Decorate api with sensitive post parameters as Django admin do?\n        # from django.utils.decorators import method_decorator\n        # from django.views.decorators.debug import sensitive_post_parameters\n        # sensitive_post_parameters_m = method_decorator(sensitive_post_parameters())\n\n        login_form = AuthenticationForm(request, data=request.data)\n\n        if not login_form.is_valid():\n            raise serializers.ValidationError(login_form.errors)\n\n        auth_login(request, login_form.get_user())\n\n        serializer = UserSerializer(request.user)\n        return Response(serializer.data, status=status.HTTP_200_OK)"}
{"prompt": "def remove(self, recursive=True, ignore_error=True):\n        \"\"\"\n        Remove the directory.\n        \"\"\"", "output": "\n        if not self.exists():\n            return\n        if recursive:\n            for f in self.listdir():\n                if f.isdir():\n                    f.remove(recursive=True)\n                else:\n                    f.remove()\n        try:\n            os.rmdir(self.path)\n        except OSError:\n            if not ignore_error:\n                raise\n\n    def listdir(self):\n        \"\"\"\n        Return a list of the files and directories in the directory.\n        \"\"\"\n        return [self.joinpath(f", "solution": "\n        try:\n            if recursive or self._cleanup == 'recursive':\n                shutil.rmtree(self.path)\n            else:\n                os.rmdir(self.path)\n        except Exception as e:\n            if not ignore_error:\n                raise e"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        \"\"\"\n        if status.farthest is None:\n            return self\n        if self.farthest is None:\n            return status\n        if self.farthest > status.farthest:\n            return self\n        if self.farthest < status.farthest:\n            return status\n        if self.expected is None:\n            return status\n        if status.expected is None:\n            return self\n        return self.copy(expected=self.expected + status.expected)\n\n    def __repr__(self) -> str:\n", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def activity(self, value):\n        \"\"\"\n        Used to set TurnContext._activity when a context object is created. Only takes instances of Activities.\n        :param value:\n        :return:\n        \"\"\"", "output": "\n        if not isinstance(value, Activity):\n            raise TypeError(\"TurnContext.activity must be of type Activity\")\n        self._activity = value\n\n    @property\n    def adapter(self):\n        \"\"\"\n        Gets the Adapter for this context.\n        :return:\n        \"\"\"\n        return self._adapter\n\n    @adapter.setter\n    def adapter(self, value):\n        \"\"\"\n        Used to set TurnContext._adapter when a context object is created. Only takes instances of BotAdapter.\n        :param value:\n        :return:", "solution": "\n        if not isinstance(value, Activity):\n            raise TypeError('TurnContext: cannot set `activity` to a type other than Activity.')\n        else:\n            self._activity = value"}
{"prompt": "def _post_cutout_no_chunking_blosc(self, token, channel,\n                                       x_start, y_start, z_start,\n                                       data, resolution):\n        \"\"\"\n        Accepts data in zyx. !!!\n        \"\"\"", "output": "\n        # get the data\n        data = data.astype(np.float32)\n        # get the shape\n        shape = data.shape\n        # get the resolution\n        resolution = resolution.astype(np.float32)\n        # get the size\n        size = np.prod(shape)\n        # get the dtype\n        dtype = data.dtype\n        # get the chunking\n        chunking = self.get_chunking(shape)\n        # get the blosc compression\n        compression = self.get_blosc_compression(", "solution": "\n        data = numpy.expand_dims(data, axis=0)\n        blosc_data = blosc.pack_array(data)\n        url = self.url(\"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\".format(\n            token, channel,\n            resolution,\n            x_start, x_start + data.shape[3],\n            y_start, y_start + data.shape[2],\n            z_start, z_start + data.shape[1]\n        ))\n\n        req = self.remote_utils.post_url(url, data=blosc_data, headers={\n            'Content-Type': 'application/octet-stream'\n        })\n\n        if req.status_code is not 200:\n            raise RemoteDataUploadError(req.text)\n        else:\n            return True"}
{"prompt": "def current(self):\n        \"\"\" bool: Whether this window is the window in which commands are being executed. \"\"\"", "output": "\n        return self._current\n\n    @property\n    def focused(self):\n        \"\"\" bool: Whether this window is the focused window. \"\"\"\n        return self._focused\n\n    @property\n    def visible(self):\n        \"\"\" bool: Whether this window is visible. \"\"\"\n        return self._visible\n\n    @property\n    def active(self):\n        \"\"\" bool: Whether this window is the active window. \"\"\"\n        return self._active\n\n    @property\n    def focused_view(self):\n        \"\"\" sublime.View: The focused view.", "solution": "\n        try:\n            return self.driver.current_window_handle == self.handle\n        except self.driver.no_such_window_error:\n            return False"}
{"prompt": "def autodetect_files(self):\n        \"\"\" Attempt to detect requirements files in the current working directory \"\"\"", "output": "\n        if self.files:\n            return\n        for filename in ('requirements.txt', 'requirements.pip', 'requirements.in'):\n            if os.path.exists(filename):\n                self.files.append(filename)\n\n    def parse_requirements(self, filename):\n        \"\"\" Parse a requirements file \"\"\"\n        with open(filename) as f:\n            for line in f:\n                line = line.strip()\n                if line and not line.startswith('#'):\n                    self.parse_line(line)\n\n    def parse", "solution": "\n        if self._is_valid_requirements_file('requirements.txt'):\n            self.filenames.append('requirements.txt')\n\n        if self._is_valid_requirements_file('requirements.pip'):  # pragma: nocover\n            self.filenames.append('requirements.pip')\n\n        if os.path.isdir('requirements'):\n            for filename in os.listdir('requirements'):\n                file_path = os.path.join('requirements', filename)\n                if self._is_valid_requirements_file(file_path):\n                    self.filenames.append(file_path)\n        self._check_inclusions_recursively()"}
{"prompt": "def format_extension(self):\n        \"\"\"The format extension of asset.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n        \"\"\"", "output": "\n        return self.extension.split('.')[0]\n\n    @property\n    def format_extension_with_dot(self):\n        \"\"\"The format extension of asset with a leading dot.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_extension_with_dot\n            '.js'\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.format_extension_with", "solution": "\n        for extension in reversed(self.extensions):\n            compiler = self.environment.compilers.get(extension)\n            if not compiler and self.environment.mimetypes.get(extension):\n                return extension"}
