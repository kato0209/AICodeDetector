--------------------
Prompt: 
def print_help(self, classes=False):
        """Print the help for each Configurable class in self.classes.

        If classes=False (the default), only flags and aliases are printed.
        """
----------
Output: 

        if classes:
            self.print_classes()
        else:
            self.print_flags()
            self.print_aliases()

    def print_classes(self):
        """Print the help for each Configurable class in self.classes."""
        for name in sorted(self.classes):
            self.print_class(name)

    def print_class(self, name):
        """Print the help for the Configurable class with the given name."""
        cls = self.classes[name]
        print(cls.__
----------
Solution: 

        self.print_subcommands()
        self.print_options()

        if classes:
            if self.classes:
                print "Class parameters"
                print "----------------"
                print
                for p in wrap_paragraphs(self.keyvalue_description):
                    print p
                    print

            for cls in self.classes:
                cls.class_print_help()
                print
        else:
            print "To see all available configurables, use `--help-all`"
            print
--------------------
Prompt: 
def derive_fallback_fernet_keys(passwords, user_id):
    """
    Derive a list of per-user Fernet keys from a list of master keys and a
    username.

    If a None is encountered in ``passwords``, it is forwarded.

    Parameters
    ----------
    passwords : list[unicode]
        List of ascii-encodable keys to derive.
    user_id : unicode or None
        ascii-encodable user_id to use as salt
    """
----------
Output: 

    if user_id is None:
        return passwords
    return [
        derive_fernet_key(password, user_id)
        for password in passwords
    ]


def derive_fernet_key(password, user_id):
    """
    Derive a Fernet key from a password and a username.

    Parameters
    ----------
    password : unicode
        ascii-encodable password to derive.
    user_id : unicode
        ascii-encodable user_id to use as salt
    """
----------
Solution: 

    # Normally I wouldn't advocate for these kinds of assertions, but we really
    # really really don't want to mess up deriving encryption keys.
    assert isinstance(passwords, (list, tuple)), \
        "Expected list or tuple of keys, got %s." % type(passwords)

    def derive_single_allow_none(k):
        if k is None:
            return None
        return derive_single_fernet_key(k, user_id).decode('ascii')

    return list(map(derive_single_allow_none, passwords))
--------------------
Prompt: 
def format_help(self, checkerref=False):
        """return the help string for the given message id"""
----------
Output: 

        if checkerref:
            return self.checkerref
        else:
            return self.help

    def get_checkerref(self):
        """return the checkerref string for the given message id"""
        return self.checkerref

    def get_msgid(self):
        """return the message id for the given checkerref"""
        return self.msgid

    def get_msgid_short(self):
        """return the short message id for the given checkerref"""
        return self.
----------
Solution: 

        desc = self.descr
        if checkerref:
            desc += " This message belongs to the %s checker." % self.checker.name
        title = self.msg
        if self.symbol:
            msgid = "%s (%s)" % (self.symbol, self.msgid)
        else:
            msgid = self.msgid
        if self.minversion or self.maxversion:
            restr = []
            if self.minversion:
                restr.append("< %s" % ".".join([str(n) for n in self.minversion]))
            if self.maxversion:
                restr.append(">= %s" % ".".join([str(n) for n in self.maxversion]))
            restr = " or ".join(restr)
            if checkerref:
                desc += " It can't be emitted when using Python %s." % restr
            else:
                desc += " This message can't be emitted when using Python %s." % restr
        desc = normalize_text(" ".join(desc.split()), indent="  ")
        if title != "%s":
            title = title.splitlines()[0]

            return ":%s: *%s*\n%s" % (msgid, title.rstrip(" "), desc)
        return ":%s:\n%s" % (msgid, desc)
--------------------
Prompt: 
def f_get(self, *args):
        """Returns annotations

        If len(args)>1, then returns a list of annotations.

        `f_get(X)` with *X* integer will return the annotation with name `annotation_X`.

        If the annotation contains only a single entry you can call `f_get()` without arguments.
        If you call `f_get()` and the annotation contains more than one element a ValueError is
        thrown.

        """
----------
Output: 

        if len(args) == 0:
            if len(self.annotations) == 1:
                return self.annotations[0]
            else:
                raise ValueError("Annotation contains more than one entry")
        elif len(args) == 1:
            return self.annotations[args[0]]
        else:
            return [self.annotations[i] for i in args]

    def f_set(self, *args):
        """Sets annotations

        If len(args)>1, then sets the annotations
----------
Solution: 


        if len(args) == 0:
            if len(self._dict) == 1:
                return self._dict[list(self._dict.keys())[0]]
            elif len(self._dict) > 1:
                raise ValueError('Your annotation contains more than one entry: '
                                 '`%s` Please use >>f_get<< with one of these.' %
                                 (str(list(self._dict.keys()))))
            else:
                raise AttributeError('Your annotation is empty, cannot access data.')

        result_list = []
        for name in args:
            name = self._translate_key(name)
            try:
                result_list.append(self._dict[name])
            except KeyError:
                raise AttributeError('Your annotation does not contain %s.' % name)

        if len(args) == 1:
            return result_list[0]
        else:
            return tuple(result_list)
--------------------
Prompt: 
def size(full_path):
    """Shows file size.
    :param full_path: full path to the file.
    """
----------
Output: 
""
    return os.path.getsize(full_path)


def create_dir(path):
    """Creates a directory.
    :param path: path to the directory.
    """""
    if not os.path.exists(path):
        os.makedirs(path)


def create_file(path):
    """Creates a file.
    :param path: path to the file.
    """""
    if not os.path.exists(path):
       
----------
Solution: 


    file_size = os.path.getsize(full_path)
    str_file_size = str(file_size)
    print(str_file_size, 'b')

    # Show size in b, kb, mb or gb depending on the dimension
    if len(str_file_size) >= 10:
        print('{0:.2f}'.format(file_size / 1073741824), 'gb')
    elif len(str_file_size) >= 7:
        print('{0:.2f}'.format(file_size / 1048576), 'mb')
    elif len(str_file_size) >= 4:
        print('{0:.2f}'.format(file_size / 1024), 'kb')
--------------------
Prompt: 
def init_cinder_consumer(self, mq):
        """
        Init openstack cinder mq

        1. Check if enable listening cinder notification
        2. Create consumer

        :param mq: class ternya.mq.MQ
        """
----------
Output: 

        if not self.conf.enable_cinder_notification:
            return

        self.cinder_consumer = mq.create_consumer(
            self.conf.cinder_notification_topic,
            self.cinder_notification_callback,
            'cinder-notification-consumer')

    def cinder_notification_callback(self, body):
        """
        Callback function for cinder notification

        :param body: body of notification
        """
        try:
            if body['event_type'] == 'volume
----------
Solution: 

        if not self.enable_component_notification(Openstack.Cinder):
            log.debug("disable listening cinder notification")
            return

        for i in range(self.config.cinder_mq_consumer_count):
            mq.create_consumer(self.config.cinder_mq_exchange,
                               self.config.cinder_mq_queue,
                               ProcessFactory.process(Openstack.Cinder))

        log.debug("enable listening openstack cinder notification.")
--------------------
Prompt: 
def max_global_iteration(self):
        """Return global iterator with last iteration number"""
----------
Output: 

        return self.global_iteration

    def max_local_iteration(self):
        """Return local iterator with last iteration number"""
        return self.local_iteration

    def max_global_epoch(self):
        """Return global epoch with last epoch number"""
        return self.global_epoch

    def max_local_epoch(self):
        """Return local epoch with last epoch number"""
        return self.local_epoch

    def max_global_batch(self):

----------
Solution: 

        return self.indices_to_global_iterator({
            symbol_pos_int(var_name): end-1 for var_name, start, end, incr in self._loop_stack
        })
--------------------
Prompt: 
def get_arguments(self):
        """
        Extracts the specific arguments of this CLI
        """
----------
Output: 

        parser = argparse.ArgumentParser(
            description='Runs a single test case',
            prog='test',
            usage='%(prog)s [options]',
            add_help=False)
        parser.add_argument(
            '-h', '--help',
            action='help',
            help='Show this help message and exit.')
        parser.add_argument(
            '-v', '--verbose',
            action='store_true',
            help='Verbose output')
        parser.add_argument(
           
----------
Solution: 

        ApiCli.get_arguments(self)

        # Get the host group name
        if self.args.host_group_name is not None:
            self.host_group_name = self.args.host_group_name

        # Get the list of sources separated by commas
        if self.args.sources is not None:
            self.sources = self.args.sources

        payload = {}
        if self.host_group_name is not None:
            payload['name'] = self.host_group_name

        if self.sources is not None:
            source_list = str.split(self.sources, ',')
            if 'hostnames' not in payload:
                payload['hostnames'] = []

            for s in source_list:
                payload['hostnames'].append(s)
        self.data = json.dumps(payload, sort_keys=True)
        self.headers = {'Content-Type': 'application/json', "Accept": "application/json"}
--------------------
Prompt: 
def next_retry_datetime(self):
        """
        Get datetime of the next retry if the task instance fails. For exponential
        backoff, retry_delay is used as base and will be converted to seconds.
        """
----------
Output: 

        if self.retry_delay:
            return (
                timezone.utcnow() +
                timedelta(seconds=self.retry_delay * (2 ** (self.retry_count - 1))))
        else:
            return None

    @property
    def next_run_date(self):
        """
        Return the next scheduled date for the task instance.
        """
        if self.start_date:
            if self.start_date > timezone.utcnow():
                return self.start_date
       
----------
Solution: 

        delay = self.task.retry_delay
        if self.task.retry_exponential_backoff:
            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))
            # deterministic per task instance
            hash = int(hashlib.sha1("{}#{}#{}#{}".format(self.dag_id,
                                                         self.task_id,
                                                         self.execution_date,
                                                         self.try_number)
                                    .encode('utf-8')).hexdigest(), 16)
            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)
            modded_hash = min_backoff + hash % min_backoff
            # timedelta has a maximum representable value. The exponentiation
            # here means this value can be exceeded after a certain number
            # of tries (around 50 if the initial delay is 1s, even fewer if
            # the delay is larger). Cap the value here before creating a
            # timedelta object so the operation doesn't fail.
            delay_backoff_in_seconds = min(
                modded_hash,
                timedelta.max.total_seconds() - 1
            )
            delay = timedelta(seconds=delay_backoff_in_seconds)
            if self.task.max_retry_delay:
                delay = min(self.task.max_retry_delay, delay)
        return self.end_date + delay
--------------------
Prompt: 
def showCode(self, width=80):
        """Show all words of the code in a nice format.
        """
----------
Output: 

        if self.code is None:
            return
        print("Code:")
        for i, word in enumerate(self.code):
            print("{:02d}: {}".format(i, word))

    def showWords(self, width=80):
        """Show all words of the code in a nice format.
        """
        if self.words is None:
            return
        print("Words:")
        for i, word in enumerate(self.words):
            print("{:02d}: {}".format(i
----------
Solution: 

        #make table of all symbols with binary strings
        symbolStrings = [
            (self.bitPattern(s.index), self.mnemonic(s.index))
            for s in self
            ]
        #determine column widths the way Lisp programmers do it
        leftColWidth, rightColWidth = map(max, map(
            map,
            repeat(len),
            zip(*symbolStrings)
            ))
        colwidth = leftColWidth+rightColWidth
        columns = 81//(colwidth+2)
        rows = -(-len(symbolStrings)//columns)
        def justify(bs):
            b,s = bs
            return b.rjust(leftColWidth)+':'+s.ljust(rightColWidth)
        for i in range(rows):
            print(' '.join(map(justify, symbolStrings[i::rows])).rstrip())
--------------------
Prompt: 
def normalize(ast: Node) -> Node:
    """
    Normalize an AST nodes.

    all builtins containers are replace by referencable subclasses
    """
----------
Output: 

    if isinstance(ast, (List, Dict, Set)):
        return ast.__class__(
            [normalize(item) for item in ast.items],
            ast.location,
        )
    elif isinstance(ast, (Tuple, Starred, Arguments)):
        return ast.__class__(
            [normalize(item) for item in ast.items],
            ast.location,
        )
    elif isinstance(ast, (Name, Constant, Ellipsis, Ellipsis2)):
        return ast
    elif isinstance(
----------
Solution: 

    res = ast
    typemap = {DictNode, ListNode, TupleNode}
    if type(ast) is dict:
        res = DictNode(ast)
    elif type(ast) is list:
        res = ListNode(ast)
    elif type(ast) is tuple:
        res = TupleNode(ast)
    # in-depth change
    if hasattr(res, 'items'):
        for k, v in res.items():
            res[k] = normalize(v)
    elif hasattr(res, '__getitem__'):
        for idx, v in zip(range(len(res)), res):
            res[idx] = normalize(v)
    if type(res) not in typemap and hasattr(res, '__dict__'):
        subattr = vars(res)
        for k, v in subattr.items():
            setattr(res, k, normalize(v))
    return res
--------------------
Prompt: 
def get_memory_maps(self):
        """Return process's mapped memory regions as a list of nameduples.
        Fields are explained in 'man proc'; here is an updated (Apr 2012)
        version: http://goo.gl/fmebo
        """
----------
Output: 

        # TODO: use /proc/self/smaps instead of /proc/self/maps
        maps = []
        with open('/proc/%d/maps' % self.pid) as f:
            for line in f:
                try:
                    addr, perms, offset, dev, inode, pathname = line.split()
                    maps.append(MemoryMap(addr, perms, offset, dev, inode, pathname))
                except ValueError:
                    pass
        return maps

    def get_memory_map(self
----------
Solution: 

        f = None
        try:
            f = open("/proc/%s/smaps" % self.pid)
            first_line = f.readline()
            current_block = [first_line]

            def get_blocks():
                data = {}
                for line in f:
                    fields = line.split(None, 5)
                    if len(fields) >= 5:
                        yield (current_block.pop(), data)
                        current_block.append(line)
                    else:
                        data[fields[0]] = int(fields[1]) * 1024
                yield (current_block.pop(), data)

            if first_line:  # smaps file can be empty
                for header, data in get_blocks():
                    hfields = header.split(None, 5)
                    try:
                        addr, perms, offset, dev, inode, path = hfields
                    except ValueError:
                        addr, perms, offset, dev, inode, path = hfields + ['']
                    if not path:
                        path = '[anon]'
                    else:
                        path = path.strip()
                    yield (addr, perms, path,
                           data['Rss:'],
                           data['Size:'],
                           data.get('Pss:', 0),
                           data['Shared_Clean:'], data['Shared_Clean:'],
                           data['Private_Clean:'], data['Private_Dirty:'],
                           data['Referenced:'],
                           data['Anonymous:'],
                           data['Swap:'])
            f.close()
        except EnvironmentError:
            # XXX - Can't use wrap_exceptions decorator as we're
            # returning a generator;  this probably needs some
            # refactoring in order to avoid this code duplication.
            if f is not None:
                f.close()
            err = sys.exc_info()[1]
            if err.errno in (errno.ENOENT, errno.ESRCH):
                raise NoSuchProcess(self.pid, self._process_name)
            if err.errno in (errno.EPERM, errno.EACCES):
                raise AccessDenied(self.pid, self._process_name)
            raise
        except:
            if f is not None:
                f.close()
            raise
--------------------
Prompt: 
def add_path(path, config=None):
    """Ensure that the path, or the root of the current package (if
    path is in a package), is in sys.path.
    """
----------
Output: 

    if path not in sys.path:
        if config is None:
            config = Config()
        if path.startswith(config.package_path):
            path = config.package_path
        sys.path.insert(0, path)


def get_config():
    """Get the configuration for the current package.

    This will return the configuration for the current package, or None if
    the current package is not a package.
    """
    if not hasattr(sys, 'frozen'):
        return None
    if not
----------
Solution: 


    # FIXME add any src-looking dirs seen too... need to get config for that
    
    log.debug('Add path %s' % path)    
    if not path:
        return []
    added = []
    parent = os.path.dirname(path)
    if (parent
        and os.path.exists(os.path.join(path, '__init__.py'))):
        added.extend(add_path(parent, config))
    elif not path in sys.path:
        log.debug("insert %s into sys.path", path)
        sys.path.insert(0, path)
        added.append(path)
    if config and config.srcDirs:
        for dirname in config.srcDirs:
            dirpath = os.path.join(path, dirname)
            if os.path.isdir(dirpath):
                sys.path.insert(0, dirpath)
                added.append(dirpath)
    return added
--------------------
Prompt: 
def _kl_pareto_pareto(a, b, name=None):
  """Calculate the batched KL divergence KL(a || b) with a and b Pareto.

  Args:
    a: instance of a Pareto distribution object.
    b: instance of a Pareto distribution object.
    name: (optional) Name to use for created operations.
      default is "kl_pareto_pareto".

  Returns:
    Batchwise KL(a || b)
  """
----------
Output: 
"""
  with tf.name_scope(name, "kl_pareto_pareto", [a, b]):
    a_scale = a.scale
    a_shape = a.shape
    b_scale = b.scale
    b_shape = b.shape
    a_scale_log = tf.log(a_scale)
    b_scale_log = tf.log(b_scale)
    a_scale_log_b_scale = a_scale_log - b_scale_log
    a_shape_log =
----------
Solution: 

  with tf.name_scope(name or "kl_pareto_pareto"):
    # Consistent with
    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 55
    # Terminology is different from source to source for Pareto distributions.
    # The 'concentration' parameter corresponds to 'a' in that source, and the
    # 'scale' parameter corresponds to 'm'.
    final_batch_shape = distribution_util.get_broadcast_shape(
        a.concentration, b.concentration, a.scale, b.scale)
    common_type = dtype_util.common_dtype(
        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)
    return tf.where(
        a.scale >= b.scale,
        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +
        tf.math.log(a.concentration) - tf.math.log(b.concentration) +
        b.concentration / a.concentration - 1.0,
        tf.broadcast_to(tf.cast(np.inf, common_type), final_batch_shape))
--------------------
Prompt: 
def flush(self):
        """Flush incomming socket messages."""
----------
Output: 

        while self.socket.poll(0):
            self.socket.recv()

    def send(self, message):
        """Send a message to the server."""
        self.socket.send(message)

    def recv(self):
        """Receive a message from the server."""
        return self.socket.recv()

    def close(self):
        """Close the socket."""
        self.socket.close()

    def __enter__(self):
        """Enter the context manager."""
        return self
----------
Solution: 

        debug('flushing incomming socket messages')
        try:
            while True:
                msg = self.socket.recv(self.buffer_size)
                debug(b'< ' + msg)
        except socket.error:
            pass
--------------------
Prompt: 
def vcf_records(self, format_tags=None, qualified=False):
        """Generates parsed VcfRecord objects.

        Typically called in a for loop to process each vcf record in a
        VcfReader. VcfReader must be opened in advanced and closed when
        complete. Skips all headers.

        Args:
            qualified: When True, sample names are prefixed with file name

        Returns:
            Parsed VcfRecord

        Raises:
            StopIteration: when reader is exhausted.
            TypeError: if reader is closed.
        """
----------
Output: 
       """
        if not self.is_open:
            raise ValueError("Reader must be opened before records can be read.")

        if format_tags is None:
            format_tags = []

        if qualified:
            prefix = self.filename + ":"
        else:
            prefix = ""

        for line in self.reader:
            if line.startswith("#"):
                continue
            record = self.parse_record(line)
            record.sample_names = [prefix + sample for sample in record.sample_names]
           
----------
Solution: 

        if qualified:
            sample_names = self.qualified_sample_names
        else:
            sample_names = self.sample_names

        for line in self._file_reader.read_lines():
            if line.startswith("#"):
                continue
            vcf_record = vcf.VcfRecord.parse_record(line, sample_names)
            if format_tags:
                vcf_record = self.modify_format_tag(vcf_record, format_tags)
            yield vcf_record
--------------------
Prompt: 
def main(args,parser,subparser):
    """the list command corresponds with listing images for an external
       resource. This is different from listing images that are local to the
       database, which should be done with "images"
    """
----------
Output: 

    parser.add_argument('-r','--resource',
                        help='the resource to list images from',
                        required=True)
    parser.add_argument('-f','--format',
                        help='the format to list images in',
                        required=True)
    parser.add_argument('-o','--output',
                        help='the output file to write the list to',
                        required=True)
    parser.add_argument('-d','--directory',
                        help='the directory to list images from',

----------
Solution: 

    from sregistry.main import get_client
    cli = get_client(quiet=args.quiet)
    
    for query in args.query:
        if query in ['','*']:
            query = None

        cli.ls(query=query)
--------------------
Prompt: 
def parse_hpo_genes(hpo_lines):
    """Parse HPO gene information
    
        Args:
            hpo_lines(iterable(str))
        
        Returns:
            diseases(dict): A dictionary with hgnc symbols as keys
    """
----------
Output: 

    diseases = {}
    for line in hpo_lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith('#'):
            continue
        line = line.split('\t')
        if len(line) < 4:
            continue
        hgnc_symbol = line[0]
        hpo_id = line[1]
        hpo_name = line[2]
        hpo_description = line[3]
        if hgnc_symbol not in diseases:

----------
Solution: 

    LOG.info("Parsing HPO genes ...")
    genes = {}
    for index, line in enumerate(hpo_lines):
        # First line is header
        if index == 0:
            continue
        if len(line) < 5:
            continue
        gene_info = parse_hpo_gene(line)
        hgnc_symbol = gene_info['hgnc_symbol']
        description = gene_info['description']
        
        if hgnc_symbol not in genes:
            genes[hgnc_symbol] = {
                'hgnc_symbol': hgnc_symbol
            }
        
        gene = genes[hgnc_symbol]
        if description == 'Incomplete penetrance':
            gene['incomplete_penetrance'] = True
        if description == 'Autosomal dominant inheritance':
            gene['ad'] = True
        if description == 'Autosomal recessive inheritance':
            gene['ar'] = True
        if description == 'Mithochondrial inheritance':
            gene['mt'] = True
        if description == 'X-linked dominant inheritance':
            gene['xd'] = True
        if description == 'X-linked recessive inheritance':
            gene['xr'] = True
        if description == 'Y-linked inheritance':
            gene['x'] = True
        if description == 'X-linked inheritance':
            gene['y'] = True
    LOG.info("Parsing done.")
    return genes
--------------------
Prompt: 
def start_tag(el):
    """
    The text representation of the start tag for a tag.
    """
----------
Output: 

    return "<" + el.tag


def end_tag(el):
    """
    The text representation of the end tag for a tag.
    """
    return "</" + el.tag + ">"


def text(el):
    """
    The text representation of the text content for a tag.
    """
    return el.text


def attr(el, name):
    """
    The text representation of the attribute for a tag.
    """
    return el.attrib.get(name)


def children
----------
Solution: 

    return '<%s%s>' % (
        el.tag, ''.join([' %s="%s"' % (name, html_escape(value, True))
                         for name, value in el.attrib.items()]))
--------------------
Prompt: 
def get_file_content(url, comes_from=None, session=None):
    """Gets the content of a file; it may be a filename, file: URL, or
    http: URL.  Returns (location, content).  Content is unicode."""
----------
Output: 

    if session is None:
        session = requests.Session()
    if url.startswith('file:'):
        url = url[len('file:'):]
        if url.startswith('/'):
            url = url[1:]
        location = url
        with open(url, 'rb') as f:
            content = f.read().decode('utf-8')
    elif url.startswith('http:'):
        if comes_from is None:
            raise ValueError("comes_from must be set for HTTP URLs")
        location = url
----------
Solution: 

    if session is None:
        raise TypeError(
            "get_file_content() missing 1 required keyword argument: 'session'"
        )

    match = _scheme_re.search(url)
    if match:
        scheme = match.group(1).lower()
        if (scheme == 'file' and comes_from and
                comes_from.startswith('http')):
            raise InstallationError(
                'Requirements file %s references URL %s, which is local'
                % (comes_from, url))
        if scheme == 'file':
            path = url.split(':', 1)[1]
            path = path.replace('\\', '/')
            match = _url_slash_drive_re.match(path)
            if match:
                path = match.group(1) + ':' + path.split('|', 1)[1]
            path = urllib_parse.unquote(path)
            if path.startswith('/'):
                path = '/' + path.lstrip('/')
            url = path
        else:
            # FIXME: catch some errors
            resp = session.get(url)
            resp.raise_for_status()

            if six.PY3:
                return resp.url, resp.text
            else:
                return resp.url, resp.content
    try:
        with open(url) as f:
            content = f.read()
    except IOError as exc:
        raise InstallationError(
            'Could not open requirements file: %s' % str(exc)
        )
    return url, content
--------------------
Prompt: 
def _get_repetition(extractor, text, *, bounds=(0, None), ignore_whitespace=False):
  """Tries to pull text with extractor repeatedly.

  Bounds is a 2-tuple of (lbound, ubound) where lbound is a number and ubound is a number or None.
  If the ubound is None, this method will execute extractor on text until extrator raises DeadEnd.
  Otherwise, extractor will be called until it raises DeadEnd, or it has extracted ubound times.

  If the number of children extracted is >= lbound, then a ParseNode with type repetition is
  returned. Otherwise, DeadEnd is raised.

  Bounds are interpreted as (lbound, ubound]

  This method is used to implement:
  - option (0, 1)
  - zero_or_more (0, None)
  - one_or_more (1, None)
  - exact_repeat (n, n)
  """
----------
Output: 
 If ignore_whitespace is True, this method will ignore whitespace when pulling text.
  """
  if ignore_whitespace:
    text = text.replace(' ', '')
  lbound, ubound = bounds
  if ubound is None:
    ubound = float('inf')
  extracted = []
  for _ in range(lbound, ubound):
    try:
      extracted.append(extractor(text))
    except DeadEnd:
      break
  return extracted


def _get_repetition_with_separator
----------
Solution: 

  minr, maxr = bounds
  children = []

  while maxr is None or len(children) <= maxr:
    ignored_ws, use_text = _split_ignored(text, ignore_whitespace)
    try:
      child = _call_extractor(extractor, use_text)
      child.add_ignored(ignored_ws)
    except DeadEnd:
      break

    if child.is_empty:
      break

    children.append(child)
    text = text[child.consumed:]


  if len(children) >= minr:
    return ParseNode(ParseNodeType.repetition,
                      children=children)
  else:
    raise DeadEnd()
--------------------
Prompt: 
def get_query(self, query):
        """ Grab all issues matching a github query """
----------
Output: 

        issues = []
        for page in range(1, 100):
            url = 'https://api.github.com/search/issues?q={}&page={}&per_page=100'.format(query, page)
            response = requests.get(url, headers=self.headers)
            if response.status_code == 200:
                issues += response.json()['items']
            else:
                print('Error: {}'.format(response.status_code))
                break
        return issues

    def
----------
Solution: 

        issues = {}
        for issue in self.client.get_query(query):
            url = issue['html_url']
            try:
                repo = self.get_repository_from_issue(issue)
            except ValueError as e:
                log.critical(e)
            else:
                issues[url] = (repo, issue)
        return issues
--------------------
Prompt: 
def build(self, secret_key):
        """Builds a final copy of the token using the given secret key.

        :param secret_key(string): The secret key that corresponds to this builder's access key.
        """
----------
Output: 

        if not self.access_key:
            raise ValueError('Access key must be set before building.')
        if not self.secret_key:
            raise ValueError('Secret key must be set before building.')
        if not self.expiration:
            raise ValueError('Expiration must be set before building.')

        self.secret_key = secret_key
        self.token = self.access_key + ':' + self.secret_key + ':' + self.expiration
        return self

    def get_token(self):
----------
Solution: 

        key = jwk.JWK(
            kty='oct',
            k=base64url_encode(uuid.UUID(secret_key).bytes),
        )

        header = {
            'alg': 'dir',
            'enc': 'A128GCM',
            'zip': 'DEF',
            'cty': 'JWT',
            'kid': self._access_key,
        }

        now = int(time.time())

        payload = {
            'iat': now,
            'nbf': now,
        }

        if self._expiration is not None:
            payload['exp'] = int(calendar.timegm(self._expiration.utctimetuple()))

        if len(self._view_identifiers) > 0:
            payload[VIEW_IDENTIFIERS_CLAIM_NAME] = self._view_identifiers

        if len(self._parameters) > 0:
            parameters = []
            for parameter in self._parameters:
                serialized = {
                    'field': parameter.field,
                    'op': parameter.op,
                }

                if hasattr(parameter, '__iter__'):
                    serialized['any'] = list(parameter.value)
                else:
                    serialized['value'] = parameter.value

                parameters.append(serialized)

            payload[PARAMETERS_CLAIM_NAME] = parameters

        if len(self._attributes) > 0:
            payload[ATTRIBUTES_CLAIM_NAME] = self._attributes

        tok = jwe.JWE(json_encode(payload), protected=header)
        tok.add_recipient(key)

        return tok.serialize(compact=True)
--------------------
Prompt: 
def cpu_percent(interval=0.1, percpu=False):
    """Return a float representing the current system-wide CPU
    utilization as a percentage.

    When interval is > 0.0 compares system CPU times elapsed before
    and after the interval (blocking).

    When interval is 0.0 or None compares system CPU times elapsed
    since last call or module import, returning immediately.
    In this case is recommended for accuracy that this function be
    called with at least 0.1 seconds between calls.

    When percpu is True returns a list of floats representing the
    utilization as a percentage for each CPU.
    First element of the list refers to first CPU, second element
    to second CPU and so on.
    The order of the list is consistent across calls.
    """
----------
Output: 
When percpu is True returns a list of floats representing
    CPU utilization as a percentage for each processor.

    """
    if interval is None or interval < 0.0:
        interval = 0.0
    if interval > 0.0:
        if percpu:
            prev_times = _cpu_times_percpu()
        else:
            prev_times = _cpu_times()
        time.sleep(interval)
        if percpu:
            curr_times = _cpu_times_percpu()
        else:

----------
Solution: 

    global _last_cpu_times
    global _last_per_cpu_times
    blocking = interval is not None and interval > 0.0

    def calculate(t1, t2):
        t1_all = sum(t1)
        t1_busy = t1_all - t1.idle

        t2_all = sum(t2)
        t2_busy = t2_all - t2.idle

        # this usually indicates a float precision issue
        if t2_busy <= t1_busy:
            return 0.0

        busy_delta = t2_busy - t1_busy
        all_delta = t2_all - t1_all
        busy_perc = (busy_delta / all_delta) * 100
        return round(busy_perc, 1)

    # system-wide usage
    if not percpu:
        if blocking:
            t1 = cpu_times()
            time.sleep(interval)
        else:
            t1 = _last_cpu_times
        _last_cpu_times = cpu_times()
        return calculate(t1, _last_cpu_times)
    # per-cpu usage
    else:
        ret = []
        if blocking:
            tot1 = cpu_times(percpu=True)
            time.sleep(interval)
        else:
            tot1 = _last_per_cpu_times
        _last_per_cpu_times = cpu_times(percpu=True)
        for t1, t2 in zip(tot1, _last_per_cpu_times):
            ret.append(calculate(t1, t2))
        return ret
--------------------
Prompt: 
def load_filters():
    """
    Loads and returns all filters.
    """
----------
Output: 

    filters = []
    for file in os.listdir(os.path.join(os.path.dirname(__file__), 'filters')):
        if file.endswith('.py') and not file.startswith('_'):
            filters.append(file[:-3])
    return filters


def load_filter(filter_name):
    """
    Loads and returns the specified filter.
    """
    try:
        module = importlib.import_module('filters.%s' % filter_name)
        return module.filter

----------
Solution: 

    all_filters = {}
    for m in JINJA_FILTERS:
        if hasattr(m, "filters"):
            all_filters.update(m.filters())
    return all_filters
--------------------
Prompt: 
def weighted(loads):
    """Pick two at random using inverse load as weight.

    Return the less loaded of the two.
    """
----------
Output: 

    loads = sorted(loads, key=lambda x: x[1], reverse=True)
    return loads[0]


def weighted_random(loads):
    """Pick two at random using inverse load as weight.

    Return the less loaded of the two.
    """
    loads = sorted(loads, key=lambda x: x[1], reverse=True)
    return loads[random.randint(0, len(loads) - 1)]


def weighted_random_with_replacement(loads):
    """P
----------
Solution: 

    # weight 0 a million times more than 1:
    weights = 1./(1e-6+numpy.array(loads))
    sums = weights.cumsum()
    t = sums[-1]
    x = random()*t
    y = random()*t
    idx = 0
    idy = 0
    while sums[idx] < x:
        idx += 1
    while sums[idy] < y:
        idy += 1
    if weights[idy] > weights[idx]:
        return idy
    else:
        return idx
--------------------
Prompt: 
def print_figure(fig, fmt='png'):
    """Convert a figure to svg or png for inline display."""
----------
Output: 

    fig.patch.set_alpha(0)
    fig.patch.set_linewidth(0)
    fig.patch.set_facecolor('white')
    fig.patch.set_edgecolor('white')
    fig.patch.set_zorder(-1)
    fig.patch.set_visible(True)
    fig.patch.set_rasterized(True)
    fig.patch.set_animated(True)
    fig.patch.set_clip_box(None)
    fig.patch.set_clip_path(
----------
Solution: 

    # When there's an empty figure, we shouldn't return anything, otherwise we
    # get big blank areas in the qt console.
    if not fig.axes and not fig.lines:
        return

    fc = fig.get_facecolor()
    ec = fig.get_edgecolor()
    fig.set_facecolor('white')
    fig.set_edgecolor('white')
    try:
        bytes_io = BytesIO()
        fig.canvas.print_figure(bytes_io, format=fmt, bbox_inches='tight')
        data = bytes_io.getvalue()
    finally:
        fig.set_facecolor(fc)
        fig.set_edgecolor(ec)
    return data
--------------------
Prompt: 
def json_to_bounds(segments_json):
    """Extracts the boundaries from a json file and puts them into
        an np array."""
----------
Output: 

    bounds = []
    for segment in segments_json:
        bounds.append(segment['bounds'])
    return np.array(bounds)


def json_to_labels(segments_json):
    """Extracts the labels from a json file and puts them into
        an np array."""
    labels = []
    for segment in segments_json:
        labels.append(segment['label'])
    return np.array(labels)


def json_to_points(segments_json):
    """Extracts the points from
----------
Solution: 

    f = open(segments_json)
    segments = json.load(f)["segments"]
    bounds = []
    for segment in segments:
        bounds.append(segment["start"])
    bounds.append(bounds[-1] + segments[-1]["duration"]) # Add last boundary
    f.close()
    return np.asarray(bounds)
--------------------
Prompt: 
def _bio_to_string(bio):
    """
    Copy the contents of an OpenSSL BIO object into a Python byte string.
    """
----------
Output: 

    bio_len = BIO_ctrl_pending(bio)
    bio_data = ctypes.create_string_buffer(bio_len)
    BIO_read(bio, ctypes.byref(bio_data), bio_len)
    return bio_data.raw


def _bio_to_bytes(bio):
    """
    Copy the contents of an OpenSSL BIO object into a Python byte string.
    """
    bio_len = BIO_ctrl_pending(bio)
    bio_data = ctypes.
----------
Solution: 

    result_buffer = _ffi.new('char**')
    buffer_length = _lib.BIO_get_mem_data(bio, result_buffer)
    return _ffi.buffer(result_buffer[0], buffer_length)[:]
--------------------
Prompt: 
def create_from(cls, backend):
        """
        Create device specification with values in backend configuration.
        Args:
            backend(Backend): backend configuration
        Returns:
            DeviceSpecification: created device specification
        Raises:
            PulseError: when an invalid backend is specified
        """
----------
Output: 

        return cls(backend.configuration().n_qubits,
                   backend.configuration().dt,
                   backend.configuration().memory,
                   backend.configuration().n_registers,
                   backend.configuration().coupling_map,
                   backend.configuration().basis_gates,
                   backend.configuration().simulator,
                   backend.configuration().local,
                   backend.configuration().conditional,
                   backend.configuration().open_pulse,
                   backend.configuration().memory_slots,
                   backend.
----------
Solution: 

        backend_config = backend.configuration()

        # TODO : Remove usage of config.defaults when backend.defaults() is updated.
        try:
            backend_default = backend.defaults()
        except ModelValidationError:
            from collections import namedtuple
            BackendDefault = namedtuple('BackendDefault', ('qubit_freq_est', 'meas_freq_est'))

            backend_default = BackendDefault(
                qubit_freq_est=backend_config.defaults['qubit_freq_est'],
                meas_freq_est=backend_config.defaults['meas_freq_est']
            )

        # system size
        n_qubits = backend_config.n_qubits
        n_registers = backend_config.n_registers
        n_uchannels = backend_config.n_uchannels

        if n_uchannels > 0 and n_uchannels != n_qubits:
            raise PulseError("This version assumes no U-channels or #U-cannels==#qubits.")

        # frequency information
        qubit_lo_freqs = backend_default.qubit_freq_est
        qubit_lo_ranges = backend_config.qubit_lo_range
        meas_lo_freqs = backend_default.meas_freq_est
        meas_lo_ranges = backend_config.meas_lo_range

        # generate channels with assuming their numberings are aligned with qubits
        drives = [
            DriveChannel(i, qubit_lo_freqs[i], tuple(qubit_lo_ranges[i]))
            for i in range(n_qubits)
        ]
        measures = [
            MeasureChannel(i, meas_lo_freqs[i], tuple(meas_lo_ranges[i]))
            for i in range(n_qubits)
        ]
        acquires = [AcquireChannel(i) for i in range(n_qubits)]
        controls = [ControlChannel(i) for i in range(n_uchannels)]

        qubits = []
        for i in range(n_qubits):
            # TODO: get qubits <-> channels relationship from backend
            qubit = Qubit(i,
                          drive_channels=[drives[i]],
                          control_channels=None if n_uchannels == 0 else controls[i],
                          measure_channels=[measures[i]],
                          acquire_channels=[acquires[i]])
            qubits.append(qubit)

        registers = [RegisterSlot(i) for i in range(n_registers)]
        # TODO: get #mem_slots from backend
        mem_slots = [MemorySlot(i) for i in range(len(qubits))]

        return DeviceSpecification(qubits, registers, mem_slots)
--------------------
Prompt: 
def remove_memberships(self, team, users):
        """
        **Description**
            Remove user memberships from specified team.

        **Arguments**
            - **team**: the name of the team from which user memberships are removed
            - **users**: list of usernames which should be removed from team

        **Example**
            `examples/user_team_mgmt_extended.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt_extended.py>`_
        """
----------
Output: 
extended.py>`_
        """
        if not isinstance(users, list):
            raise ValueError("users must be a list")
        if not isinstance(team, str):
            raise ValueError("team must be a string")
        if not users:
            raise ValueError("users must not be empty")
        if not team:
            raise ValueError("team must not be empty")

        url = self._url + "/teams/" + team + "/members"
        data = {"users": users}
        return self._http_delete(
----------
Solution: 


        res = self.list_memberships(team)

        if res[0] is False:
            return res

        old_memberships = res[1]
        new_memberships = {k: v for k, v in old_memberships.items() if k not in users}

        res = self.edit_team(team, new_memberships)

        if res[0] is False:
            return res
        else:
            return [True, None]
--------------------
Prompt: 
def price_converter(obj):
    """Ensures that string prices are converted into Price objects."""
----------
Output: 

    if isinstance(obj, str):
        return Price(obj)
    return obj


class Price(object):
    """Represents a price."""

    def __init__(self, value):
        """Initializes a new instance of the Price class.

        Args:
            value: The value of the price.
        """
        self.value = value

    def __str__(self):
        """Returns a string representation of the price."""
        return self.value

    def __repr__(self):

----------
Solution: 

    if isinstance(obj, str):
        obj = PriceClass.parse(obj)
    return obj
--------------------
Prompt: 
def finalize(self, remove_all_handlers=True):
        """Finalizes the manager, closes and removes all handlers if desired."""
----------
Output: 

        if remove_all_handlers:
            self.remove_all_handlers()
        self.close()

    def close(self):
        """Closes the manager."""
        self.closed = True

    def remove_all_handlers(self):
        """Removes all handlers from the manager."""
        for handler in self.handlers:
            self.remove_handler(handler)

    def remove_handler(self, handler):
        """Removes a handler from the manager."""
        if handler in
----------
Solution: 

        for tool in self._tools:
            tool.finalize()
        self._tools = []
        self._stdout_to_logger = None
        for config in (self._sp_config, self._mp_config):
            if hasattr(config, 'close'):
                config.close()
        self._sp_config = None
        self._mp_config = None
        if remove_all_handlers:
            self.tabula_rasa()
--------------------
Prompt: 
def _experiments_to_circuits(qobj):
    """Return a list of QuantumCircuit object(s) from a qobj

    Args:
        qobj (Qobj): The Qobj object to convert to QuantumCircuits
    Returns:
        list: A list of QuantumCircuit objects from the qobj

    """
----------
Output: 

    circuits = []
    for experiment in qobj.experiments:
        if experiment.config.shots == 1:
            circuits.append(experiment.instructions)
        else:
            for shot in experiment.shots:
                circuits.append(shot)
    return circuits


def _experiments_to_counts(qobj):
    """Return a list of dicts of counts from a qobj

    Args:
        qobj (Qobj): The Qobj object to convert to counts

----------
Solution: 

    if qobj.experiments:
        circuits = []
        for x in qobj.experiments:
            quantum_registers = [QuantumRegister(i[1], name=i[0])
                                 for i in x.header.qreg_sizes]
            classical_registers = [ClassicalRegister(i[1], name=i[0])
                                   for i in x.header.creg_sizes]
            circuit = QuantumCircuit(*quantum_registers,
                                     *classical_registers,
                                     name=x.header.name)
            qreg_dict = {}
            creg_dict = {}
            for reg in quantum_registers:
                qreg_dict[reg.name] = reg
            for reg in classical_registers:
                creg_dict[reg.name] = reg
            for i in x.instructions:
                instr_method = getattr(circuit, i.name)
                qubits = []
                try:
                    for qubit in i.qubits:
                        qubit_label = x.header.qubit_labels[qubit]
                        qubits.append(
                            qreg_dict[qubit_label[0]][qubit_label[1]])
                except Exception:  # pylint: disable=broad-except
                    pass
                clbits = []
                try:
                    for clbit in i.memory:
                        clbit_label = x.header.clbit_labels[clbit]
                        clbits.append(
                            creg_dict[clbit_label[0]][clbit_label[1]])
                except Exception:  # pylint: disable=broad-except
                    pass
                params = []
                try:
                    params = i.params
                except Exception:  # pylint: disable=broad-except
                    pass
                if i.name in ['snapshot']:
                    instr_method(
                        i.label,
                        snapshot_type=i.snapshot_type,
                        qubits=qubits,
                        params=params)
                elif i.name == 'initialize':
                    instr_method(params, qubits)
                else:
                    instr_method(*params, *qubits, *clbits)
            circuits.append(circuit)
        return circuits
    return None
--------------------
Prompt: 
def visit_assign(self, node):
        """check that if assigning to a function call, the function is
        possibly returning something valuable
        """
----------
Output: 

        if isinstance(node.value, ast.Call):
            self.check_function_call(node.value)
        return self.generic_visit(node)

    def visit_Call(self, node):
        """check that if calling a function, the function is possibly
        returning something valuable
        """
        if isinstance(node.func, ast.Name):
            self.check_function_call(node)
        return self.generic_visit(node)

    def visit_FunctionDef(self, node):
        """
----------
Solution: 

        if not isinstance(node.value, astroid.Call):
            return
        function_node = safe_infer(node.value.func)
        # skip class, generator and incomplete function definition
        funcs = (astroid.FunctionDef, astroid.UnboundMethod, astroid.BoundMethod)
        if not (
            isinstance(function_node, funcs)
            and function_node.root().fully_defined()
            and not function_node.decorators
        ):
            return
        if (
            function_node.is_generator()
            or function_node.is_abstract(pass_is_abstract=False)
            or isinstance(function_node, astroid.AsyncFunctionDef)
        ):
            return
        returns = list(
            function_node.nodes_of_class(astroid.Return, skip_klass=astroid.FunctionDef)
        )
        if not returns:
            self.add_message("assignment-from-no-return", node=node)
        else:
            for rnode in returns:
                if not (
                    isinstance(rnode.value, astroid.Const)
                    and rnode.value.value is None
                    or rnode.value is None
                ):
                    break
            else:
                self.add_message("assignment-from-none", node=node)
--------------------
Prompt: 
def read_array(self, start, end):
        """Return floats from `start` to `end` inclusive, indexed from 1.

        The entire range of floats is immediately read into memory from
        the file, making this efficient for small sequences of floats
        whose values are all needed immediately.

        """
----------
Output: 

        return self.read_array_from_file(start, end, self.file)

    def read_array_from_file(self, start, end, file):
        """Return floats from `start` to `end` inclusive, indexed from 1.

        The entire range of floats is immediately read into memory from
        the file, making this efficient for small sequences of floats
        whose values are all needed immediately.

        """
        if start < 1:
            raise ValueError("start must be >= 1")
       
----------
Solution: 

        f = self.file
        f.seek(8 * (start - 1))
        length = 1 + end - start
        data = f.read(8 * length)
        return ndarray(length, self.endian + 'd', data)
--------------------
Prompt: 
def batches(dataset):
    """Returns a callable that chooses sequences from netcdf data."""
----------
Output: 

    def _batches(batch_size):
        """Returns a batch of sequences from netcdf data."""
        # Get a random batch of sequences.
        batch = []
        for _ in range(batch_size):
            # Get a random sequence.
            seq = dataset.random_sequence()
            # Add it to the batch.
            batch.append(seq)
        return batch
    return _batches


def batches_from_file(filename, batch_size):
    """Returns a callable that chooses sequences from
----------
Solution: 

    seq_lengths = dataset.variables['seqLengths'].data
    seq_begins = np.concatenate(([0], np.cumsum(seq_lengths)[:-1]))

    def sample():
        chosen = np.random.choice(
            list(range(len(seq_lengths))), BATCH_SIZE, replace=False)
        return batch_at(dataset.variables['inputs'].data,
                        dataset.variables['targetClasses'].data,
                        seq_begins[chosen],
                        seq_lengths[chosen])

    return sample
--------------------
Prompt: 
def _raise_if_wrong_file_signature(stream):
    """ Reads the 4 first bytes of the stream to check that is LASF"""
----------
Output: 

    signature = stream.read(4)
    if signature != b'LASF':
        raise ValueError('File signature is not LASF')


def _read_header(stream):
    """ Reads the header of the stream and returns a Header object"""
    header = Header()
    header.file_signature = stream.read(4)
    _raise_if_wrong_file_signature(stream)
    header.file_source_id = stream.read(2)
    header.global_encoding = stream.read
----------
Solution: 

    file_sig = stream.read(len(headers.LAS_FILE_SIGNATURE))
    if file_sig != headers.LAS_FILE_SIGNATURE:
        raise errors.PylasError(
            "File Signature ({}) is not {}".format(file_sig, headers.LAS_FILE_SIGNATURE)
        )
--------------------
Prompt: 
def find_data_files(self, package, src_dir):
        """Return filenames for package's data files in 'src_dir'"""
----------
Output: 

        data_files = []
        for dirpath, dirnames, filenames in os.walk(src_dir):
            for i, dirname in enumerate(dirnames):
                if dirname.startswith('.'):
                    del dirnames[i]
            for filename in filenames:
                if filename.endswith('.py') or filename.startswith('.'):
                    continue
                path = os.path.join(dirpath, filename)
                data_files.append(path)
        return data_files

    def find
----------
Solution: 

        globs = (self.package_data.get('', [])
                 + self.package_data.get(package, []))
        files = self.manifest_files.get(package, [])[:]
        for pattern in globs:
            # Each pattern has to be converted to a platform-specific path
            files.extend(glob(os.path.join(src_dir, convert_path(pattern))))
        return self.exclude_data_files(package, src_dir, files)
--------------------
Prompt: 
def to_id(string: str) -> str:
    """Get a spotify ID from a URI or open.spotify URL.

    Paramters
    ---------
    string : str
        The string to operate on.

    Returns
    -------
    id : str
        The Spotify ID from the string.
    """
----------
Output: 

    if string.startswith("spotify:"):
        return string.split(":")[2]
    elif string.startswith("open.spotify.com"):
        return string.split("/")[-1]
    else:
        raise ValueError("Invalid string.")


def to_uri(id: str) -> str:
    """Get a URI from a Spotify ID.

    Paramters
    ---------
    id : str
        The Spotify ID to operate on.

    Returns
    -------
    uri
----------
Solution: 

    string = string.strip()

    match = _URI_RE.match(string)

    if match is None:
        match = _OPEN_RE.match(string)

        if match is None:
            return string
        else:
            return match.group(2)
    else:
        return match.group(1)
--------------------
Prompt: 
def marquee(txt='',width=78,mark='*'):
    """Return the input string centered in a 'marquee'.

    :Examples:

        In [16]: marquee('A test',40)
        Out[16]: '**************** A test ****************'

        In [17]: marquee('A test',40,'-')
        Out[17]: '---------------- A test ----------------'

        In [18]: marquee('A test',40,' ')
        Out[18]: '                 A test                 '

    """
----------
Output: 
18]: '                 A test                 '

        In [19]: marquee('A test',40,'')
        Out[19]: 'A test'

    """
    if len(txt) > width:
        return txt
    else:
        return mark*(width-len(txt))+txt


def get_file_size(file_path):
    """Return the size of a file in bytes.

    :Examples:

        In [1]: get_file_size('test.txt')
        Out[1
----------
Solution: 

    if not txt:
        return (mark*width)[:width]
    nmark = (width-len(txt)-2)//len(mark)//2
    if nmark < 0: nmark =0
    marks = mark*nmark
    return '%s %s %s' % (marks,txt,marks)
--------------------
Prompt: 
def add(self, gate, qargs, cargs):
        """Add an instruction and its context (where it's attached)."""
----------
Output: 

        if gate.name == 'measure':
            self.measure_count += 1
        if gate.name == 'reset':
            self.reset_count += 1
        if gate.name == 'barrier':
            self.barrier_count += 1
        if gate.name == 'snapshot':
            self.snapshot_count += 1
        if gate.name == 'save_expval':
            self.save_expval_count += 1
        if gate.name == 'save_statevector':
            self.save
----------
Solution: 

        if not isinstance(gate, Instruction):
            raise QiskitError("attempt to add non-Instruction" +
                              " to InstructionSet")
        self.instructions.append(gate)
        self.qargs.append(qargs)
        self.cargs.append(cargs)
--------------------
Prompt: 
def read_char(self, c: str) -> bool:
        """
        Consume the c head byte, increment current index and return True
        else return False. It use peekchar and it's the same as '' in BNF.
        """
----------
Output: 

        if self.peekchar() == c:
            self.index += 1
            return True
        return False

    def read_string(self, s: str) -> bool:
        """
        Consume the s string, increment current index and return True
        else return False. It use peekstring and it's the same as '' in BNF.
        """
        if self.peekstring(len(s)) == s:
            self.index += len(s)
            return True
        return False

    def read_
----------
Solution: 

        if self.read_eof():
            return False
        self._stream.save_context()
        if c == self._stream.peek_char:
            self._stream.incpos()
            return self._stream.validate_context()
        return self._stream.restore_context()
--------------------
Prompt: 
def disassemble(msg, msg_nocr, section, co, lasti=-1, start_line=-1,
                end_line=None, relative_pos=False, highlight='light',
                start_offset=0, end_offset=None):
    """Disassemble a code object."""
----------
Output: 

    if end_offset is None:
        end_offset = len(msg_nocr)
    if start_line == -1:
        start_line = 1
    if end_line is None:
        end_line = start_line + 1
    if relative_pos:
        start_offset = 0
        end_offset = len(msg_nocr)
    if lasti == -1:
        lasti = len(msg_nocr)
    if start_offset > end_offset:
        start_offset, end
----------
Solution: 

    return disassemble_bytes(msg, msg_nocr, co.co_code, lasti, co.co_firstlineno,
                             start_line, end_line, relative_pos,
                        co.co_varnames, co.co_names, co.co_consts,
                        co.co_cellvars, co.co_freevars,
                        dict(findlinestarts(co)), highlight,
                        start_offset=start_offset, end_offset=end_offset)
--------------------
Prompt: 
def _unpack_storm_date(date):
        """
        given a packed storm date field, unpack and return 'YYYY-MM-DD' string.
        """
----------
Output: 

        year = date >> 9
        month = (date >> 5) & 0x0f
        day = date & 0x1f
        return '{:04d}-{:02d}-{:02d}'.format(year, month, day)

    def _unpack_storm_time(time):
        """
        given a packed storm time field, unpack and return 'HH:MM' string.
        """
        hour = time >> 11
        minute = (time >> 5) & 0x
----------
Solution: 

        year = (date & 0x7f) + 2000  # 7 bits
        day = (date >> 7) & 0x01f  # 5 bits
        month = (date >> 12) & 0x0f  # 4 bits
        return "%s-%s-%s" % (year, month, day)
--------------------
Prompt: 
def poll_operation_until_done(self, name, polling_interval_in_seconds):
        """
        Poll backup operation state until it's completed.

        :param name: the name of the operation resource
        :type name: str
        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.
        :type polling_interval_in_seconds: int
        :return: a resource operation instance.
        :rtype: dict
        """
----------
Output: 

        operation_id = name
        operation_url = self._get_operation_url(operation_id)
        while True:
            operation_response = self._client.get(operation_url)
            if operation_response.status_code != 200:
                raise CloudError(operation_response)
            operation_result = operation_response.json()
            if operation_result['status'] == 'Succeeded':
                return operation_result
            elif operation_result['status'] == 'Failed':
                raise CloudError(operation_response
----------
Solution: 

        while True:
            result = self.get_operation(name)

            state = result['metadata']['common']['state']
            if state == 'PROCESSING':
                self.log.info('Operation is processing. Re-polling state in {} seconds'
                              .format(polling_interval_in_seconds))
                time.sleep(polling_interval_in_seconds)
            else:
                return result
--------------------
Prompt: 
def cost(self, t_node, branch_length, multiplicity=2.0):
        """
        returns the cost associated with a branch starting at t_node
        t_node is time before present, the branch goes back in time

        Args:
            - t_node:           time of the node
            - branch_length:    branch length, determines when this branch merges with sister
            - multiplicity:     2 if merger is binary, higher if this is a polytomy
        """
----------
Output: 

        return self.time_to_present(t_node) * branch_length * multiplicity

    def time_to_present(self, t_node):
        """
        returns the time to present for a given node

        Args:
            - t_node:           time of the node
        """
        return self.t_present - t_node

    def get_branch_length(self, t_node, t_sister):
        """
        returns the branch length for a given node

        Args:
            -
----------
Solution: 

        merger_time = t_node+branch_length
        return self.integral_merger_rate(merger_time) - self.integral_merger_rate(t_node)\
                 - np.log(self.total_merger_rate(merger_time))*(multiplicity-1.0)/multiplicity
--------------------
Prompt: 
def run_cell_magic(self, magic_name, line, cell):
        """Execute the given cell magic.
        
        Parameters
        ----------
        magic_name : str
          Name of the desired magic function, without '%' prefix.

        line : str
          The rest of the first input line as a single string.

        cell : str
          The body of the cell as a (possibly multiline) string.
        """
----------
Output: 

        # Check for the magic function
        magic_name = magic_name.strip()
        if magic_name not in self.magics:
            raise KeyError('Unknown cell magic %s' % magic_name)
        magic_func = self.magics[magic_name]
        # Run the magic function
        try:
            self.shell.run_cell_magic(magic_name, line, cell)
        except Exception:
            self.log.error("Error running cell magic %s", magic_name, exc_info=True
----------
Solution: 

        fn = self.find_cell_magic(magic_name)
        if fn is None:
            lm = self.find_line_magic(magic_name)
            etpl = "Cell magic function `%%%%%s` not found%s."
            extra = '' if lm is None else (' (But line magic `%%%s` exists, '
                                    'did you mean that instead?)' % magic_name )
            error(etpl % (magic_name, extra))
        else:
            # Note: this is the distance in the stack to the user's frame.
            # This will need to be updated if the internal calling logic gets
            # refactored, or else we'll be expanding the wrong variables.
            stack_depth = 2
            magic_arg_s = self.var_expand(line, stack_depth)
            with self.builtin_trap:
                result = fn(line, cell)
            return result
--------------------
Prompt: 
def update_trace_watch(self):
        """Parses the nextflow trace file and retrieves the path of report JSON
        files that have not been sent to the service yet.
        """
----------
Output: 

        if self.trace_file_path is None:
            return

        if not os.path.exists(self.trace_file_path):
            return

        with open(self.trace_file_path, 'r') as trace_file:
            trace_lines = trace_file.readlines()

        if len(trace_lines) == 0:
            return

        for line in trace_lines:
            if line.startswith('report'):
                report_path = line.split(' ')[1]
                self.
----------
Solution: 


        # Check the size stamp of the tracefile. Only proceed with the parsing
        # if it changed from the previous size.
        size_stamp = os.path.getsize(self.trace_file)
        self.trace_retry = 0
        if size_stamp and size_stamp == self.trace_sizestamp:
            return
        else:
            logger.debug("Updating trace size stamp to: {}".format(size_stamp))
            self.trace_sizestamp = size_stamp

        with open(self.trace_file) as fh:

            # Skip potential empty lines at the start of file
            header = next(fh).strip()
            while not header:
                header = next(fh).strip()

            # Get header mappings before parsing the file
            hm = self._header_mapping(header)

            for line in fh:
                # Skip empty lines
                if line.strip() == "":
                    continue

                fields = line.strip().split("\t")

                # Skip if task ID was already processes
                if fields[hm["task_id"]] in self.stored_ids:
                    continue

                if fields[hm["process"]] == "report":
                    self.report_queue.append(
                        self._expand_path(fields[hm["hash"]])
                    )
                    self.send = True

                # Add the processed trace line to the stored ids. It will be
                # skipped in future parsers
                self.stored_ids.append(fields[hm["task_id"]])
--------------------
Prompt: 
def permission_check(apikey, endpoint):
        """
        return (user, seckey) if url end point is in allowed entry point list
        """
----------
Output: 

        if endpoint in allowed_entry_points:
            return apikey
        else:
            return None

    @app.route('/')
    def index():
        """
        return index page
        """
        return render_template('index.html')

    @app.route('/<endpoint>', methods=['GET', 'POST'])
    def entry_point(endpoint):
        """
        return entry point page
        """
        if request.method == 'GET':
            return render_template('entry_point.html', endpoint=endpoint)
----------
Solution: 

        try:
            ak = APIKeys.objects.get(apikey=apikey)
            apitree = cPickle.loads(ak.apitree.encode("ascii"))
            if apitree.match(endpoint):
                return ak.user if ak.user else AnonymousUser(), ak.seckey
        except APIKeys.DoesNotExist:
            pass
        return None, None
--------------------
Prompt: 
def __create_criterion(self, criterion_str):
        """ instantiate criterion from config string """
----------
Output: 

        if criterion_str == "MSE":
            criterion = nn.MSELoss()
        elif criterion_str == "MAE":
            criterion = nn.L1Loss()
        elif criterion_str == "CE":
            criterion = nn.CrossEntropyLoss()
        elif criterion_str == "BCE":
            criterion = nn.BCELoss()
        elif criterion_str == "BCEL":
            criterion = nn.BCEWithLogitsL
----------
Solution: 

        parsed = criterion_str.split("(")
        type_str = parsed[0].strip().lower()
        parsed[1] = parsed[1].split(")")[0].strip()

        for criterion_class in self.custom_criterions:
            if criterion_class.get_type_string() == type_str:
                return criterion_class(self, parsed[1])
        raise ValueError(
            "Unsupported autostop criterion type: %s" % criterion_str)
--------------------
Prompt: 
def verboseRead(self, alphabet, context='', skipExtra=False):
        """Read symbol and extra from stream and explain what happens.
        Returns the value of the symbol
        >>> olleke.pos = 0
        >>> l = Layout(olleke)
        >>> l.verboseRead(WindowSizeAlphabet())
        0000  1b                   1011 WSIZE   windowsize=(1<<22)-16=4194288
        4194288
        """
----------
Output: 

        if self.pos == len(self.stream):
            raise EOFError
        if context:
            print(context)
        symbol = self.read(alphabet)
        if not skipExtra:
            extra = self.read(ExtraAlphabet())
            print('%04x  %s' % (self.pos, extra))
        return symbol

    def verboseReadBits(self, n, context=''):
        """Read n bits and explain what happens.
        Returns the value of the bits
        >>> olleke.pos =
----------
Solution: 

        #TODO 2: verbosity level, e.g. show only codes and maps in header
        stream = self.stream
        pos = stream.pos
        if skipExtra:
            length, symbol = alphabet.readTuple(stream)
            extraBits, extra = 0, None
        else:
            length, symbol, extraBits, extra = alphabet.readTupleAndExtra(
                stream)
        #fields: address, hex data, binary data, name of alphabet, explanation
        hexdata = self.makeHexData(pos)
        addressField = '{:04x}'.format(pos+7>>3) if hexdata else ''
        bitdata = self.formatBitData(pos, length, extraBits)
        #bitPtr moves bitdata so that the bytes are easier to read
        #jump back to right if a new byte starts
        if '|' in bitdata[1:]:
            #start over on the right side
            self.bitPtr = self.width
        fillWidth = self.bitPtr-(len(hexdata)+len(bitdata))
        if fillWidth<0: fillWidth = 0
        print('{:<5s} {:<{}s} {:7s} {}'.format(
            addressField,
            hexdata+' '*fillWidth+bitdata, self.width,
            context+alphabet.name,
            symbol if skipExtra else symbol.explanation(extra),
            ))
        #jump to the right if we started with a '|'
        #because we didn't jump before printing
        if bitdata.startswith('|'): self.bitPtr = self.width
        else: self.bitPtr -= len(bitdata)
        return symbol if skipExtra else symbol.value(extra)
--------------------
Prompt: 
def builtin(cls, name):
        """
        Generate a default legend.

        Args:
            name (str): The name of the legend you want. Not case sensitive.
                 'nsdoe': Nova Scotia Dept. of Energy
                 'canstrat': Canstrat
                 'nagmdm__6_2': USGS N. Am. Geol. Map Data Model 6.2
                 'nagmdm__6_1': USGS N. Am. Geol. Map Data Model 6.1
                 'nagmdm__4_3': USGS N. Am. Geol. Map Data Model 4.3
                 'sgmc': USGS State Geologic Map Compilation

            Default 'nagmdm__6_2'.

        Returns:
            Legend: The legend stored in `defaults.py`.
        """
----------
Output: 
nagmdm__6_0': USGS N. Am. Geol. Map Data Model 6.0
                 'nagmdm__5_2': USGS N. Am. Geol. Map Data Model 5.2
                 'nagmdm__5_1': USGS N. Am. Geol. Map Data Model 5.1
                 'nagmdm__5_0': USGS N. Am. Geol. Map Data Model 5.0
                 'nagmdm__4_2': USGS N. Am. Ge
----------
Solution: 

        names = {
                 'nsdoe': LEGEND__NSDOE,
                 'canstrat': LEGEND__Canstrat,
                 'nagmdm__6_2': LEGEND__NAGMDM__6_2,
                 'nagmdm__6_1': LEGEND__NAGMDM__6_1,
                 'nagmdm__4_3': LEGEND__NAGMDM__4_3,
                 'sgmc': LEGEND__SGMC,
                 }
        return cls.from_csv(text=names[name.lower()])
--------------------
Prompt: 
def parse_email(data, strip_attachment_payloads=False):
    """
    A simplified email parser

    Args:
        data: The RFC 822 message string, or MSG binary
        strip_attachment_payloads (bool): Remove attachment payloads

    Returns (dict): Parsed email data
    """
----------
Output: 

    if isinstance(data, bytes):
        data = data.decode('utf-8')

    # Parse the email
    email = emailparser.parse_from_string(data)

    # Remove attachment payloads
    if strip_attachment_payloads:
        for attachment in email.attachments:
            attachment.payload = None

    # Return the email data
    return email


def parse_email_attachments(email):
    """
    Parse email attachments

    Args:
        email: The email object

----------
Solution: 


    if type(data) == bytes:
        if is_outlook_msg(data):
            data = convert_outlook_msg(data)
        data = data.decode("utf-8", errors="replace")
    parsed_email = mailparser.parse_from_string(data)
    headers = json.loads(parsed_email.headers_json).copy()
    parsed_email = json.loads(parsed_email.mail_json).copy()
    parsed_email["headers"] = headers

    if "received" in parsed_email:
        for received in parsed_email["received"]:
            if "date_utc" in received:
                if received["date_utc"] is None:
                    del received["date_utc"]
                else:
                    received["date_utc"] = received["date_utc"].replace("T",
                                                                        " ")

    if "from" not in parsed_email:
        if "From" in parsed_email["headers"]:
            parsed_email["from"] = parsed_email["Headers"]["From"]
        else:
            parsed_email["from"] = None

    if parsed_email["from"] is not None:
        parsed_email["from"] = parse_email_address(parsed_email["from"][0])

    if "date" in parsed_email:
        parsed_email["date"] = parsed_email["date"].replace("T", " ")
    else:
        parsed_email["date"] = None
    if "reply_to" in parsed_email:
        parsed_email["reply_to"] = list(map(lambda x: parse_email_address(x),
                                            parsed_email["reply_to"]))
    else:
        parsed_email["reply_to"] = []

    if "to" in parsed_email:
        parsed_email["to"] = list(map(lambda x: parse_email_address(x),
                                      parsed_email["to"]))
    else:
        parsed_email["to"] = []

    if "cc" in parsed_email:
        parsed_email["cc"] = list(map(lambda x: parse_email_address(x),
                                      parsed_email["cc"]))
    else:
        parsed_email["cc"] = []

    if "bcc" in parsed_email:
        parsed_email["bcc"] = list(map(lambda x: parse_email_address(x),
                                       parsed_email["bcc"]))
    else:
        parsed_email["bcc"] = []

    if "delivered_to" in parsed_email:
        parsed_email["delivered_to"] = list(
            map(lambda x: parse_email_address(x),
                parsed_email["delivered_to"])
        )

    if "attachments" not in parsed_email:
        parsed_email["attachments"] = []
    else:
        for attachment in parsed_email["attachments"]:
            if "payload" in attachment:
                payload = attachment["payload"]
                try:
                    if "content_transfer_encoding" in attachment:
                        if attachment["content_transfer_encoding"] == "base64":
                            payload = decode_base64(payload)
                        else:
                            payload = str.encode(payload)
                    attachment["sha256"] = hashlib.sha256(payload).hexdigest()
                except Exception as e:
                    logger.debug("Unable to decode attachment: {0}".format(
                        e.__str__()
                    ))
        if strip_attachment_payloads:
            for attachment in parsed_email["attachments"]:
                if "payload" in attachment:
                    del attachment["payload"]

    if "subject" not in parsed_email:
        parsed_email["subject"] = None

    parsed_email["filename_safe_subject"] = get_filename_safe_string(
        parsed_email["subject"])

    if "body" not in parsed_email:
        parsed_email["body"] = None

    return parsed_email
--------------------
Prompt: 
def get_news(self, tickers=[], tags=[], sources=[], startDate=None,
                 endDate=None, limit=100, offset=0, sortBy="publishedDate",
                 fmt='json'):
        """Return list of news articles matching given search terms
            https://api.tiingo.com/docs/tiingo/news

            # Dates are in YYYY-MM-DD Format.

            Args:
                tickers [string] : List of unique Stock Tickers to search
                tags [string] : List of topics tagged by Tiingo Algorithms
                sources [string]: List of base urls to include as news sources
                startDate, endDate [date]: Boundaries of news search window
                limit (int): Max results returned. Default 100, max 1000
                offset (int): Search results offset, used for paginating
                sortBy (string): "publishedDate" OR (#TODO: UPDATE THIS)
        """
----------
Output: 
: List of tags to search
                sources [string] : List of sources to search
                startDate [string] : Start date for search
                endDate [string] : End date for search
                limit [int] : Number of results to return
                offset [int] : Number of results to skip
                sortBy [string] : Field to sort by
                fmt [string] : Format to return results

            Returns:
                dict : Dictionary of results
        """
        params = {}
        if tickers:
            params['tickers'] = ','.join(tick
----------
Solution: 

        url = "tiingo/news"
        params = {
            'limit': limit,
            'offset': offset,
            'sortBy': sortBy,
            'tickers': tickers,
            'sources': sources,
            'tags': tags,
            'startDate': startDate,
            'endDate': endDate
        }
        response = self._request('GET', url, params=params)
        data = response.json()
        if fmt == 'json':
            return data
        elif fmt == 'object':
            return [dict_to_object(item, "NewsArticle") for item in data]
--------------------
Prompt: 
def _iter_nodes(self, node, recursive=False, max_depth=float('inf'),
                    with_links=True, in_search=False, predicate=None):
        """Returns an iterator over nodes hanging below a given start node.

        :param node:

            Start node

        :param recursive:

            Whether recursively also iterate over the children of the start node's children

        :param max_depth:

            Maximum depth to search for

        :param in_search:

            if it is used during get search and if detailed info should be returned

        :param with_links:

            If links should be considered

        :param predicate:

            A predicate to filter nodes

        :return: Iterator

        """
----------
Output: 
is in search

        :param predicate:

            Predicate function to filter nodes

        :return:

            Iterator over nodes
        """
        if node is None:
            return

        if in_search:
            if predicate is not None:
                if predicate(node):
                    yield node
            else:
                yield node

        if recursive:
            if max_depth > 1:
                for child in node.children:
                    for n in self._iter_nodes(child, recursive=True, max_depth=
----------
Solution: 

        def _run_predicate(x, run_name_set):
            branch = x.v_run_branch
            return branch == 'trajectory' or branch in run_name_set

        if max_depth is None:
            max_depth = float('inf')

        if predicate is None:
            predicate = lambda x: True
        elif isinstance(predicate, (tuple, list)):
            # Create a predicate from a list of run names or run indices
            run_list = predicate
            run_name_set = set()
            for item in run_list:
                if item == -1:
                    run_name_set.add(self._root_instance.f_wildcard('$', -1))
                elif isinstance(item, int):
                    run_name_set.add(self._root_instance.f_idx_to_run(item))
                else:
                    run_name_set.add(item)
            predicate = lambda x: _run_predicate(x, run_name_set)

        if recursive:
            return NaturalNamingInterface._recursive_traversal_bfs(node,
                                            self._root_instance._linked_by,
                                            max_depth, with_links,
                                            in_search, predicate)
        else:
            iterator = (x for x in self._make_child_iterator(node, with_links) if
                        predicate(x[2]))
            if in_search:
                return iterator # Here we return tuples: (depth, name, object)
            else:
                return (x[2] for x in iterator)
--------------------
Prompt: 
def du_handler(self, args):
    """Handler for size command"""
----------
Output: 

    if len(args) == 0:
        self.send_message(self.get_size())
    elif len(args) == 1:
        if args[0] == "--help":
            self.send_message("Usage: size [file]")
        else:
            self.send_message(self.get_size(args[0]))
    else:
        self.send_message("Usage: size [file]")


def du_help(self):
    """Help for size command"""
    self.send
----------
Solution: 

    for src, size in self.s3handler().size(args[1:]):
      message('%s\t%s' % (size, src))
--------------------
Prompt: 
def gauge(self, name, value, rate=1):
        # type: (str, float, float) -> None
        """Send a Gauge metric with the specified value"""
----------
Output: 

        self.send_metric(name, value, rate, 'gauge')

    def histogram(self, name, value, rate=1):
        # type: (str, float, float) -> None
        """Send a Histogram metric with the specified value"""
        self.send_metric(name, value, rate, 'histogram')

    def increment(self, name, rate=1):
        # type: (str, float) -> None
        """Send an Increment metric"""
        self.send_metric(name
----------
Solution: 


        if self._should_send_metric(name, rate):
            if not is_numeric(value):
                value = float(value)
            self._request(
                Gauge(
                    self._create_metric_name_for_request(name),
                    value,
                    rate
                ).to_request()
            )
--------------------
Prompt: 
def watch_port_events(port, chip, pin_function_maps, event_queue,
                      return_after_kbdint=False):
    """Waits for a port event. When a port event occurs it is placed onto the
    event queue.

    :param port: The port we are waiting for interrupts on (GPIOA/GPIOB).
    :type port: int
    :param chip: The chip we are waiting for interrupts on.
    :type chip: :class:`pifacecommon.mcp23s17.MCP23S17`
    :param pin_function_maps: A list of classes that have inheritted from
        :class:`FunctionMap`\ s describing what to do with events.
    :type pin_function_maps: list
    :param event_queue: A queue to put events on.
    :type event_queue: :py:class:`multiprocessing.Queue`
    """
----------
Output: 
17.MCP23S17`
    :param pin_function_maps: A dictionary of pin to function mappings.
    :type pin_function_maps: dict
    :param event_queue: The queue to place events onto.
    :type event_queue: :class:`queue.Queue`
    :param return_after_kbdint: If True, return after a keyboard interrupt
        occurs.
    :type return_after_kbdint: bool
    :return: The event that was placed onto the event queue.
    :rtype: :
----------
Solution: 

    # set up epoll
    gpio25 = open(GPIO_INTERRUPT_DEVICE_VALUE, 'r')  # change to use 'with'?
    epoll = select.epoll()
    epoll.register(gpio25, select.EPOLLIN | select.EPOLLET)

    while True:
        # wait here until input
        try:
            events = epoll.poll()
        except KeyboardInterrupt as e:
            if return_after_kbdint:
                return
            else:
                raise e
        except IOError as e:
            # ignore "Interrupted system call" error.
            # I don't really like this solution. Ignoring problems is bad!
            if e.errno != errno.EINTR:
                raise

        # find out where the interrupt came from and put it on the event queue
        if port == pifacecommon.mcp23s17.GPIOA:
            interrupt_flag = chip.intfa.value
        else:
            interrupt_flag = chip.intfb.value

        if interrupt_flag == 0:
            continue  # The interrupt has not been flagged on this board
        else:
            if port == pifacecommon.mcp23s17.GPIOA:
                interrupt_capture = chip.intcapa.value
            else:
                interrupt_capture = chip.intcapb.value
            event_queue.add_event(InterruptEvent(
                interrupt_flag, interrupt_capture, chip, time.time()))

    epoll.close()
--------------------
Prompt: 
def _parse_retry_after(self, response):
        """Parse Retry-After header from response if it is set."""
----------
Output: 

        retry_after = response.headers.get('Retry-After')
        if retry_after:
            try:
                retry_after = int(retry_after)
            except ValueError:
                retry_after = None
        return retry_after

    def _parse_retry_after_header(self, response):
        """Parse Retry-After header from response if it is set."""
        retry_after = response.headers.get('Retry-After')
        if retry_after:
           
----------
Solution: 

        value = response.headers.get('Retry-After')

        if not value:
            seconds = 0
        elif re.match(r'^\s*[0-9]+\s*$', value):
            seconds = int(value)
        else:
            date_tuple = email.utils.parsedate(value)
            if date_tuple is None:
                seconds = 0
            else:
                seconds = time.mktime(date_tuple) - time.time()
        return max(0, seconds)
--------------------
Prompt: 
def _utf8_params(params):
    """encode a dictionary of URL parameters (including iterables) as utf-8"""
----------
Output: 

    return dict([(k.encode('utf-8'), _utf8_param_value(v)) for k, v in params.items()])


def _utf8_param_value(value):
    """encode a URL parameter value (including iterables) as utf-8"""
    if isinstance(value, unicode):
        return value.encode('utf-8')
    elif isinstance(value, list):
        return [_utf8_param_value(v) for v in value]
    else:
        return value


def
----------
Solution: 

    assert isinstance(params, dict)
    encoded_params = []
    for k, v in params.items():
        if v is None:
            continue
        if isinstance(v, integer_types + (float,)):
            v = str(v)
        if isinstance(v, (list, tuple)):
            v = [to_bytes(x) for x in v]
        else:
            v = to_bytes(v)
        encoded_params.append((k, v))
    return dict(encoded_params)
--------------------
Prompt: 
def parse_variant_id(chrom, pos, ref, alt, variant_type):
    """Parse the variant id for a variant

    variant_id is used to identify variants within a certain type of
    analysis. It is not human readable since it is a md5 key.

    Args:
        chrom(str)
        pos(str)
        ref(str)
        alt(str)
        variant_type(str): 'clinical' or 'research'

    Returns:
        variant_id(str): The variant id converted to md5 string
    """
----------
Output: 
to a md5 key
    """
    variant_id = f'{chrom}_{pos}_{ref}_{alt}_{variant_type}'
    variant_id = variant_id.encode('utf-8')
    variant_id = hashlib.md5(variant_id).hexdigest()
    return variant_id


def parse_variant_id_from_vcf(chrom, pos, ref, alt, variant_type, vcf_id):
    """Parse the variant id for a variant

    variant_id is used to identify variants within a certain type
----------
Solution: 

    return generate_md5_key([chrom, pos, ref, alt, variant_type])
--------------------
Prompt: 
def _construct_schema(elements, nsmap):
    """Consruct fiona schema based on given elements

    :param list Element: list of elements
    :param dict nsmap: namespace map

    :return dict: schema
    """
----------
Output: 

    schema = {}
    for element in elements:
        name = element.get('name')
        type = element.get('type')
        if type == 'geometry':
            schema['geometry'] = 'Point'
        elif type == 'attribute':
            schema[name] = 'str'
        elif type == 'attribute_int':
            schema[name] = 'int'
        elif type == 'attribute_float':
            schema[name] = 'float'
        elif type == 'attribute_bool':
            schema[name] = 'bool'
----------
Solution: 


    schema = {
        'properties': {},
        'geometry': None
    }

    schema_key = None
    gml_key = None

    # if nsmap is defined, use it
    if nsmap:
        for key in nsmap:
            if nsmap[key] == XS_NAMESPACE:
                schema_key = key
            if nsmap[key] in GML_NAMESPACES:
                gml_key = key
    # if no nsmap is defined, we have to guess
    else:
        gml_key = 'gml'
        schema_key = 'xsd'

    mappings = {
        'PointPropertyType': 'Point',
        'PolygonPropertyType': 'Polygon',
        'LineStringPropertyType': 'LineString',
        'MultiPointPropertyType': 'MultiPoint',
        'MultiLineStringPropertyType': 'MultiLineString',
        'MultiPolygonPropertyType': 'MultiPolygon',
        'MultiGeometryPropertyType': 'MultiGeometry',
        'GeometryPropertyType': 'GeometryCollection',
        'SurfacePropertyType': '3D Polygon',
        'MultiSurfacePropertyType': '3D MultiPolygon'
    }

    for element in elements:
        data_type = element.attrib['type'].replace(gml_key + ':', '')
        name = element.attrib['name']

        if data_type in mappings:
            schema['geometry'] = mappings[data_type]
            schema['geometry_column'] = name
        else:
            schema['properties'][name] = data_type.replace(schema_key+':', '')

    if schema['properties'] or schema['geometry']:
        return schema
    else:
        return None
--------------------
Prompt: 
def _flatten(child, parent):
    """Custom flattening method for the parse tree."""
----------
Output: 

    if isinstance(child, list):
        for item in child:
            _flatten(item, parent)
    else:
        parent.append(child)


def _parse_tree(tree):
    """Parse the parse tree."""
    if isinstance(tree, list):
        for item in tree:
            _parse_tree(item)
    else:
        if isinstance(tree, tuple):
            if tree[0] == 'TOP':
                _flatten(tree[1], tree)
            else:

----------
Solution: 

    return parent.is_type(TokenType.expression) and child.node_type == parent.node_type
--------------------
Prompt: 
def read(self, resource_id):
        """Read object in elasticsearch using the resource_id.
        :param resource_id: id of the object to be read.
        :return: object value from elasticsearch.
        """
----------
Output: 

        try:
            return self.es.get(index=self.index, id=resource_id)
        except NotFoundError:
            return None

    def update(self, resource_id, data):
        """Update object in elasticsearch using the resource_id.
        :param resource_id: id of the object to be updated.
        :param data: new object value.
        :return: object value from elasticsearch.
        """
        try:
            return self.es.update(index=self.index, id=resource_id
----------
Solution: 

        self.logger.debug('elasticsearch::read::{}'.format(resource_id))
        return self.driver._es.get(
            index=self.driver._index,
            id=resource_id,
            doc_type='_doc'
        )['_source']
--------------------
Prompt: 
def date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):
        """use previously calculated variation of the rate to estimate
        the uncertainty in a particular numdate due to rate variation.

        Parameters
        ----------
        node : PhyloTree.Clade
            node for which the confidence interval is to be calculated
        interval : tuple, optional
            Array of length two, or tuple, defining the bounds of the confidence interval

        """
----------
Output: 

        if node.numdate is None:
            return None
        if node.numdate.rate_var is None:
            return None
        if node.numdate.rate_var == 0:
            return None
        if node.numdate.rate_var < 0:
            return None
        if node.numdate.rate_var > 1:
            return None
        if node.numdate.rate_var > 0.95:
            return None
        if node.numdate.rate_var < 0.05
----------
Solution: 

        if hasattr(node, "numdate_rate_variation"):
            from scipy.special import erfinv
            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0
                    for x in interval]
            l,c,u = [x[1] for x in node.numdate_rate_variation]
            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])

        else:
            return None
--------------------
Prompt: 
def score(self, x, w=None, **kwargs):
        """Compute R^2 coefficient of determination for a given input.

        Parameters
        ----------
        x : ndarray (num-examples, num-inputs)
            An array containing data to be fed into the network. Multiple
            examples are arranged as rows in this array, with columns containing
            the variables for each example.

        Returns
        -------
        r2 : float
            The R^2 correlation between the prediction of this netork and its
            input. This can serve as one measure of the information loss of the
            autoencoder.
        """
----------
Output: 
.

        Notes
        -----
        The R^2 coefficient of determination is defined as:

        .. math::

            R^2 = 1 - \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2

        where :math:`y_i` is the true value of the target variable for the
        :math:`i`th example and :math:`\\hat{y_i}` is the predicted value.
        """
        if w is None:
            w = self
----------
Solution: 

        return super(Autoencoder, self).score(x, x, w=w, **kwargs)
--------------------
Prompt: 
def set_parent(self, parent):
        """Set parent ``Expression`` for this object.

        Args:
            parent (Expression): The ``Expression`` which contains this object.

        Raises:
            FiqlObjectException: Parent must be of type ``Expression``.
        """
----------
Output: 

        if not isinstance(parent, Expression):
            raise FiqlObjectException("Parent must be of type Expression")
        self._parent = parent

    def get_parent(self):
        """Get parent ``Expression`` for this object.

        Returns:
            Expression: The ``Expression`` which contains this object.
        """
        return self._parent

    def get_value(self):
        """Get the value of this object.

        Returns:
            str: The value of this object.
        """
        return self._value

----------
Solution: 

        if not isinstance(parent, Expression):
            raise FiqlObjectException("Parent must be of %s not %s" % (
                Expression, type(parent)))
        self.parent = parent
--------------------
Prompt: 
def _build_kernel_function_declaration(self, name='kernel'):
        """Build and return kernel function declaration"""
----------
Output: 

        return self._build_function_declaration(name=name,
                                               return_type=self.return_type,
                                               args=self.args)

    def _build_kernel_function_definition(self, name='kernel'):
        """Build and return kernel function definition"""
        return self._build_function_definition(name=name,
                                               return_type=self.return_type,
                                               args=self.args,
                                               body=self.body)

    def _build_function
----------
Solution: 

        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)
        scalar_declarations = self._build_scalar_declarations(with_init=False)
        const_declarations = self._build_const_declartions(with_init=False)
        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +
                                                          const_declarations),
                              type=c_ast.TypeDecl(declname=name,
                                                  quals=[],
                                                  type=c_ast.IdentifierType(names=['void'])))
--------------------
Prompt: 
def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,
                                   length=8):
  """Visualizes a qualitative analysis of a given model.

  Args:
    inputs: A tensor of the original inputs, of shape [batch, timesteps,
      h, w, c].
    model: A DisentangledSequentialVAE model.
    samples: Number of samples to draw from the latent distributions.
    batch_size: Number of sequences to generate.
    length: Number of timesteps to generate for each sequence.
  """
----------
Output: 
of timesteps to generate.
  """
  # Get the latent distribution.
  _, z_mean, z_log_var = model.encode(inputs)
  z_mean = tf.squeeze(z_mean, axis=1)
  z_log_var = tf.squeeze(z_log_var, axis=1)
  z_std = tf.exp(0.5 * z_log_var)
  z_samples = tf.random.normal(shape=[batch_size, samples, z_mean
----------
Solution: 

  average = lambda dist: tf.reduce_mean(
      input_tensor=dist.mean(), axis=0)  # avg over samples
  with tf.compat.v1.name_scope("val_reconstruction"):
    reconstruct = functools.partial(model.reconstruct, inputs=inputs,
                                    samples=samples)
    visualize_reconstruction(inputs, average(reconstruct()))
    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),
                             name="static_prior")
    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),
                             name="dynamic_prior")
    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),
                             name="swap_static")
    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),
                             name="swap_dynamic")

  with tf.compat.v1.name_scope("generation"):
    generate = functools.partial(model.generate, batch_size=batch_size,
                                 length=length, samples=samples)
    image_summary(average(generate(fix_static=True)), "fix_static")
    image_summary(average(generate(fix_dynamic=True)), "fix_dynamic")
--------------------
Prompt: 
def __single_arity_fn_to_py_ast(
    ctx: GeneratorContext,
    node: Fn,
    method: FnMethod,
    def_name: Optional[str] = None,
    meta_node: Optional[MetaNode] = None,
) -> GeneratedPyAST:
    """Return a Python AST node for a function with a single arity."""
----------
Output: 

    if def_name is None:
        def_name = node.name

    if meta_node is None:
        meta_node = node.meta

    # Create a function definition.
    fn_def = ast.FunctionDef(
        name=def_name,
        args=ast.arguments(
            args=[],
            vararg=None,
            kwonlyargs=[],
            kw_defaults=[],
            kwarg=None,
            defaults=[],
        ),
        body=[],
        decorator_list=[],
----------
Solution: 

    assert node.op == NodeOp.FN
    assert method.op == NodeOp.FN_METHOD

    lisp_fn_name = node.local.name if node.local is not None else None
    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)
    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef
    with ctx.new_symbol_table(py_fn_name), ctx.new_recur_point(
        method.loop_id, RecurType.FN, is_variadic=node.is_variadic
    ):
        # Allow named anonymous functions to recursively call themselves
        if lisp_fn_name is not None:
            ctx.symbol_table.new_symbol(
                sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN
            )

        fn_args, varg, fn_body_ast = __fn_args_to_py_ast(
            ctx, method.params, method.body
        )
        meta_deps, meta_decorators = __fn_meta(ctx, meta_node)
        return GeneratedPyAST(
            node=ast.Name(id=py_fn_name, ctx=ast.Load()),
            dependencies=list(
                chain(
                    meta_deps,
                    [
                        py_fn_node(
                            name=py_fn_name,
                            args=ast.arguments(
                                args=fn_args,
                                kwarg=None,
                                vararg=varg,
                                kwonlyargs=[],
                                defaults=[],
                                kw_defaults=[],
                            ),
                            body=fn_body_ast,
                            decorator_list=list(
                                chain(
                                    meta_decorators,
                                    [_BASILISP_FN_FN_NAME],
                                    [_TRAMPOLINE_FN_NAME]
                                    if ctx.recur_point.has_recur
                                    else [],
                                )
                            ),
                            returns=None,
                        )
                    ],
                )
            ),
        )
--------------------
Prompt: 
def activate(fn=None):
    """
    Enables the HTTP traffic interceptors.

    This function can be used as decorator.

    Arguments:
        fn (function|coroutinefunction): Optional function argument
            if used as decorator.

    Returns:
        function: decorator wrapper function, only if called as decorator,
            otherwise ``None``.

    Example::

        # Standard use case
        pook.activate()
        pook.mock('server.com/foo').reply(404)

        res = requests.get('server.com/foo')
        assert res.status_code == 404
        pook.disable()

        # Decorator use case
        @pook.activate
        def test_request():
            pook.mock('server.com/foo').reply(404)

            res = requests.get('server.com/foo')
            assert res.status_code == 404
    """
----------
Output: 
4)
        requests.get('http://server.com/foo')

        # Use as decorator
        @pook.activate
        def test():
            pook.mock('server.com/foo').reply(404)
            requests.get('http://server.com/foo')

        test()

    """
    if fn is None:
        return _activate
    return _activate(fn)


def _activate(fn):
    """
    Activates the HTTP traffic interceptors.

   
----------
Solution: 

    # If not used as decorator, activate the engine and exit
    if not isfunction(fn):
        _engine.activate()
        return None

    # If used as decorator for an async coroutine, wrap it
    if iscoroutinefunction is not None and iscoroutinefunction(fn):
        return activate_async(fn, _engine)

    @functools.wraps(fn)
    def wrapper(*args, **kw):
        _engine.activate()
        try:
            fn(*args, **kw)
        finally:
            _engine.disable()

    return wrapper
--------------------
Prompt: 
def parse(self, string):
        """
        Parse lipid definition from string:

            alhead=C P, allink=A A, altail=TCC CCCC, alname=DPSM, charge=0.0
        """
----------
Output: 

        self.lipid_string = string
        self.lipid_string = self.lipid_string.replace(' ', '')
        self.lipid_string = self.lipid_string.replace('=', ':')
        self.lipid_string = self.lipid_string.replace(',', ':')
        self.lipid_string = self.lipid_string.replace(';', ':')
        self.lipid_string = self.lipid_string.replace(';', ':')
        self.lipid_
----------
Solution: 

        fields = [i.split("=") for i in string.split(', ')]
        for what, val in fields:
            what = what.strip()
            val  = val.split()
            if what.endswith("head"):
                self.head = val
            elif what.endswith("link"):
                self.link = val
            elif what.endswith("tail"):
                self.tail = val
            elif what == "charge":
                self.charge = float(val[0])
            elif what.endswith("name") and not self.name:
                self.name = val[0]
        if self.charge is None:
            # Infer charge from head groups
            self.charge = sum([headgroup_charges[bead] for bead in self.head])
--------------------
Prompt: 
def combine_columns_aggregation(
        df,
        id_cols: List[str],
        cols_for_combination: Dict[str, str],
        agg_func: Union[str, List[str], Dict[str, str]] = 'sum'
):
    """
    Aggregates data to reproduce "All" category for requester

    ---

    ### Parameters

    *mandatory :*
    - `id_cols` (*list*): the columns id to group
    - `cols_for_combination` (*dict*): colums corresponding to
       the filters as key and their default value as value

    *optional :*
    - `agg_func` (*str*, *list* or *dict*): the function(s) to use for aggregating the data.
       Accepted combinations are:
       - string function name
       - list of functions and/or function names, e.g. [np.sum, 'mean']
       - dict of axis labels -> functions, function names or list of such.
    """
----------
Output: 
for_combination` (*dict*): the columns to combine
    - `agg_func` (*str, list, dict*): the aggregation function to apply

    ---

    ### Example

    ```python
    combine_columns_aggregation(
        df,
        id_cols=['country', 'year'],
        cols_for_combination={'value': 'value_2019'},
        agg_func='sum'
    )
    ```

    ---

    ### Returns

    *pandas.DataFrame*
----------
Solution: 

    requesters_cols = list(cols_for_combination.keys())
    requester_combination = [
        list(item) for i in range(0, len(requesters_cols) + 1)
        for item in itertools.combinations(requesters_cols, i)]
    dfs_result = []
    for comb in requester_combination:
        df_tmp = df.groupby(id_cols + comb).agg(agg_func).reset_index()
        for key in (set(cols_for_combination.keys()) - set(comb)):
            df_tmp[key] = cols_for_combination[key]
        dfs_result.append(df_tmp)

    return pd.concat(dfs_result, sort=False, ignore_index=True)
--------------------
Prompt: 
def _backup_file(self, file, patch):
        """ Creates a backup of file """
----------
Output: 

        backup_file = file + '.bak'
        if os.path.exists(backup_file):
            os.remove(backup_file)
        os.rename(file, backup_file)
        if patch:
            self.patch_file(backup_file)

    def _restore_file(self, file, backup_file):
        """ Restores file from backup """
        if os.path.exists(file):
            os.remove(file)
        os.rename(backup_file, file)
----------
Solution: 

        dest_dir = self.quilt_pc + patch.get_name()
        file_dir = file.get_directory()
        if file_dir:
            #TODO get relative path
            dest_dir = dest_dir + file_dir
        backup = Backup()
        backup.backup_file(file, dest_dir, copy_empty=True)
--------------------
Prompt: 
def delete(self, url, **kwargs):
        """Sends a DELETE request.

        Args:
            url(basestring): The URL of the API endpoint.
            **kwargs:
                erc(int): The expected (success) response code for the request.
                others: Passed on to the requests package.

        Raises:
            ApiError: If anything other than the expected response code is
                returned by the Webex Teams API endpoint.

        """
----------
Output: 

        return self._request('DELETE', url, **kwargs)

    def get(self, url, **kwargs):
        """Sends a GET request.

        Args:
            url(basestring): The URL of the API endpoint.
            **kwargs:
                erc(int): The expected (success) response code for the request.
                others: Passed on to the requests package.

        Raises:
            ApiError: If anything other than the expected response code is
                returned by the Webex Teams API endpoint.

       
----------
Solution: 

        check_type(url, basestring, may_be_none=False)

        # Expected response code
        erc = kwargs.pop('erc', EXPECTED_RESPONSE_CODE['DELETE'])

        self.request('DELETE', url, erc, **kwargs)
--------------------
Prompt: 
def meteor_random_id(name=None, length=17):
    """Generate a new ID, optionally using namespace of given `name`."""
----------
Output: 

    if name is None:
        name = ''
    else:
        name = name.lower()
    return ''.join([name, str(random.randint(0, 0x7fffffff))])[:length]


def meteor_random_id_with_prefix(prefix, length=17):
    """Generate a new ID, optionally using namespace of given `name`."""
    return ''.join([prefix, str(random.randint(0, 0x7fffffff))])[:length]


def meteor
----------
Solution: 

    if name is None:
        stream = THREAD_LOCAL.alea_random
    else:
        stream = THREAD_LOCAL.random_streams[name]
    return stream.random_string(length, METEOR_ID_CHARS)
--------------------
Prompt: 
def INIT(self):
        """INIT state.

        [:rfc:`2131#section-4.4.1`]::

            The client SHOULD wait a random time between one and ten
            seconds to desynchronize the use of DHCP at startup

        .. todo::
           - The initial delay is implemented, but probably is not in other
             implementations. Check what other implementations do.
        """
----------
Output: 

        self.delay_init()
        self.delay_init_done = True
        self.delay_init_timer = self.schedule_delay_init()
        self.state = 'INIT-REBOOT'

    def INIT_REBOOT(self):
        """INIT-REBOOT state.

        [:rfc:`2131#section-4.4.1`]::

            The client SHOULD wait a random time between one and ten
            seconds to desynchronize the use of DHCP at startup
----------
Solution: 

        # NOTE: in case INIT is reached from other state, initialize attributes
        # reset all variables.
        logger.debug('In state: INIT')
        if self.current_state is not STATE_PREINIT:
            self.reset()
        self.current_state = STATE_INIT
        # NOTE: see previous TODO, maybe this is not needed.
        if self.delay_selecting:
            if self.delay_before_selecting is None:
                delay_before_selecting = gen_delay_selecting()
            else:
                delay_before_selecting = self.delay_before_selecting
        else:
            delay_before_selecting = 0
        self.set_timeout(self.current_state,
                         self.timeout_delay_before_selecting,
                         delay_before_selecting)
        if self.timeout_select is not None:
            self.set_timeout(STATE_SELECTING,
                             self.timeout_selecting,
                             self.timeout_select)
--------------------
Prompt: 
def bisect(seq, func=bool):
	"""
	Split a sequence into two sequences:  the first is elements that
	return False for func(element) and the second for True for
	func(element).
	By default, func is ``bool``, so uses the truth value of the object.

	>>> is_odd = lambda n: n%2
	>>> even, odd = bisect(range(5), is_odd)
	>>> list(odd)
	[1, 3]
	>>> list(even)
	[0, 2, 4]

	>>> other, zeros = bisect(reversed(range(5)))
	>>> list(zeros)
	[0]
	>>> list(other)
	[4, 3, 2, 1]

	"""
----------
Output: 
)
	[0, 2, 4]
	"""
	# bisect_left and bisect_right are not used because they would
	# require the sequence to be sorted.
	# bisect_right(seq, value) returns the index of the first element
	# greater than or equal to value.
	# bisect_left(seq, value) returns the index of the first element
	# greater than value.
	# bisect_left(seq, value) is equivalent to bisect_right(seq, value) - 1
	
----------
Solution: 

	queues = GroupbySaved(seq, func)
	return queues.get_first_n_queues(2)
--------------------
Prompt: 
def confirm(self, prompt, default):
        """ Called when a dangerous action is about to be done to make sure
        it's okay. `prompt' is printed; user response is returned."""
----------
Output: 

        if default:
            prompt = '%s [y/N]? ' % prompt
        else:
            prompt = '%s [y/N]? ' % prompt
        while True:
            sys.stdout.write(prompt)
            choice = raw_input().lower()
            if default and not choice:
                return True
            if choice in ('y', 'yes'):
                return True
            if choice in ('n', 'no'):
                return False

    def query_yes_no(self, question, default=None):
        """
----------
Solution: 

        while True:
            try:
                self.write_confirm(prompt, default)
                reply = self.readline('').strip().lower()
            except EOFError:
                return default
            if reply in ('y', 'yes'):
                return True
            elif reply in ('n', 'no'):
                return False
            else:
                self.msg("Please answer y or n.")
                pass
            pass
        return default
--------------------
Prompt: 
def check_type(self, value, attr, data):
        """Validates a value against the correct type of the field.

        It calls ``_expected_types`` to get a list of valid types.

        Subclasses can do one of the following:

            1. They can override the ``valid_types`` property with a tuple with
            the expected types for this field.

            2. They can override the ``_expected_types`` method to return a
            tuple of expected types for the field.

            3. They can change ``check_type`` completely to customize
            validation.

        This method or the overrides must return the ``value`` parameter
        untouched.
        """
----------
Output: 
type`` to do something else.

        :param value: The value to check.
        :param attr: The attribute name.
        :param data: The data passed to the ``Schema``.
        :return: The value.
        """
        if not isinstance(value, self._expected_types):
            raise ValidationError(
                self.error_messages['invalid'],
                field_names=[attr],
                validator_value=self._expected_types,
                value=value
            )
        return value

    def _expected_types
----------
Solution: 

        expected_types = self._expected_types()
        if not isinstance(value, expected_types):
            raise self._not_expected_type(
                value, expected_types, fields=[self], field_names=attr, data=data)
        return value
--------------------
Prompt: 
def _get_selection_cursor(self, start, end):
        """ Convenience method that returns a cursor with text selected between
            the positions 'start' and 'end'.
        """
----------
Output: 

        cursor = self.textCursor()
        cursor.setPosition(start)
        cursor.movePosition(QtGui.QTextCursor.Right, QtGui.QTextCursor.KeepAnchor, end - start)
        return cursor

    def _get_selection_text(self, start, end):
        """ Convenience method that returns the text selected between the
            positions 'start' and 'end'.
        """
        cursor = self._get_selection_cursor(start, end)
        return cursor.selectedText()

    def _get_
----------
Solution: 

        cursor = self._control.textCursor()
        cursor.setPosition(start)
        cursor.setPosition(end, QtGui.QTextCursor.KeepAnchor)
        return cursor
--------------------
Prompt: 
def delay(self, sender, receiver, packet, add_delay=0):
        """
        bandwidths are inaccurate, as we don't account for parallel transfers here
        """
----------
Output: 

        if self.delay_model == 'exponential':
            delay = self.delay_exponential(sender, receiver, packet)
        elif self.delay_model == 'normal':
            delay = self.delay_normal(sender, receiver, packet)
        elif self.delay_model == 'poisson':
            delay = self.delay_poisson(sender, receiver, packet)
        elif self.delay_model == 'uniform':
            delay = self.delay_uniform(sender, receiver, packet)
        else:
            raise Exception('Unknown
----------
Solution: 

        bw = min(sender.ul_bandwidth, receiver.dl_bandwidth)
        delay = sender.base_latency + receiver.base_latency
        delay += len(packet) / bw
        delay += add_delay
        return delay
--------------------
Prompt: 
def look_ahead_match(rating, tokens):
    """iterate through all tokens looking for matches of cleansed tokens
    or token regexes, skipping tokens left empty by cleansing and
    coping with Token objects that produce multiple space-separated
    strings when cleansed.  Yields tokens that match.

    """
----------
Output: 

    for token in tokens:
        if token.empty:
            continue
        if token.cleansed:
            if token.match(rating):
                yield token
        else:
            for regex in token.regexes:
                if regex.match(rating):
                    yield token

def look_ahead_match_all(rating, tokens):
    """iterate through all tokens looking for matches of cleansed tokens
    or token regexes, skipping tokens left empty by cleansing and
    coping with Token objects that produce multiple space-
----------
Solution: 

    ## this ensures that all cleansed tokens are non-zero length
    all_mregexes = []
    for m in rating.mentions:
        mregexes = []
        mpatterns = m.decode('utf8').split(' ')
        for mpat in mpatterns:
            if mpat.startswith('ur"^') and mpat.endswith('$"'): # is not regex
                ## chop out the meat of the regex so we can reconstitute it below
                mpat = mpat[4:-2]
            else:
                mpat = cleanse(mpat)
            if mpat:
                ## make a unicode raw string
                ## https://docs.python.org/2/reference/lexical_analysis.html#string-literals
                mpat = ur'^%s$' % mpat
                logger.debug('look_ahead_match compiling regex: %s', mpat)
                mregexes.append(re.compile(mpat, re.UNICODE | re.IGNORECASE))

        if not mregexes:
            logger.warn('got empty cleansed mention: %r\nrating=%r' % (m, rating))

        all_mregexes.append(mregexes)

    ## now that we have all_mregexes, go through all the tokens
    for i in range(len(tokens)):
        for mregexes in all_mregexes:
            if mregexes[0].match(tokens[i][0][0]):
                ## found the start of a possible match, so iterate
                ## through the tuples of cleansed strings for each
                ## Token while stepping through the cleansed strings
                ## for this mention.
                m_j = 1
                i_j = 0
                last_token_matched = 0
                matched = True
                while m_j < len(mregexes):
                    i_j += 1
                    if i_j == len(tokens[i + last_token_matched][0]):
                        i_j = 0
                        last_token_matched += 1
                        if i + last_token_matched == len(tokens):
                            matched = False
                            break
                    target_token = tokens[i + last_token_matched][0][i_j]
                    ## this next line is the actual string comparison
                    if mregexes[m_j].match(target_token):
                        m_j += 1
                    elif target_token == '':
                        continue
                    else:
                        matched = False
                        break
                if matched:
                    ## yield each matched token only once
                    toks = set()
                    for j in xrange(last_token_matched + 1):
                        toks.add(tokens[i + j][1])
                    for tok in toks:
                        yield tok
--------------------
Prompt: 
def stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports):
    """return lines with leading/trailing whitespace and any ignored code
    features removed
    """
----------
Output: 

    if ignore_comments:
        lines = [line for line in lines if not line.lstrip().startswith('#')]
    if ignore_docstrings:
        lines = [line for line in lines if not line.lstrip().startswith('"""')]
    if ignore_imports:
        lines = [line for line in lines if not line.lstrip().startswith('import')]
    return [line.strip() for line in lines]


def count_lines(lines, ignore_comments, ignore_docstrings, ignore_imports
----------
Solution: 

    if ignore_imports:
        tree = astroid.parse("".join(lines))
        node_is_import_by_lineno = (
            (node.lineno, isinstance(node, (astroid.Import, astroid.ImportFrom)))
            for node in tree.body
        )
        line_begins_import = {
            lineno: all(is_import for _, is_import in node_is_import_group)
            for lineno, node_is_import_group in groupby(
                node_is_import_by_lineno, key=lambda x: x[0]
            )
        }
        current_line_is_import = False

    strippedlines = []
    docstring = None
    for lineno, line in enumerate(lines, start=1):
        line = line.strip()
        if ignore_docstrings:
            if not docstring and any(
                line.startswith(i) for i in ['
--------------------
Prompt: 
def add_field(self, model, field):
        """Ran when a field is added to a model."""
----------
Output: 

        pass

    def remove_field(self, model, field):
        """Ran when a field is removed from a model."""
        pass

    def alter_field(self, model, old_field, new_field, strict=False):
        """Ran when a field is altered on a model."""
        pass

    def alter_unique_together(self, model, old_unique_together, new_unique_together):
        """Ran when the unique_together is altered on a model."""
       
----------
Solution: 


        for key in self._iterate_required_keys(field):
            self._create_hstore_required(
                model._meta.db_table,
                field,
                key
            )
--------------------
Prompt: 
def get_organisation_information(self, query_params=None):
        """
        Get information fot this organisation. Returns a dictionary of values.
        """
----------
Output: 

        return self.get_resource(
            'organisation',
            query_params=query_params
        )

    def get_organisation_members(self, query_params=None):
        """
        Get information fot this organisation. Returns a dictionary of values.
        """
        return self.get_resource(
            'organisation/members',
            query_params=query_params
        )

    def get_organisation_members_invited(self, query_params=None):
        """
        Get information fot this organisation. Returns a dictionary of
----------
Solution: 

        return self.fetch_json(
            uri_path=self.base_uri,
            query_params=query_params or {}
        )
--------------------
Prompt: 
def verified(context, collaborator, test, outpath=None):
    """Export variants which have been verified for an institute
        and write them to an excel file.

    Args:
        collaborator(str): institute id
        test(bool): True if the function is called for testing purposes
        outpath(str): path to output file

    Returns:
        written_files(int): number of written or simulated files
    """
----------
Output: 

    # Get the institute object
    institute = context.driver.institute(collaborator)

    # Get the verified variants
    variants = institute.verified_variants()

    # Write the variants to an excel file
    written_files = write_verified_variants(variants, outpath)

    # Return the number of written files
    return written_files


def write_verified_variants(variants, outpath=None):
    """Write verified variants to an excel file.

    Args
----------
Solution: 

    written_files = 0
    collaborator = collaborator or 'cust000'
    LOG.info('Exporting verified variants for cust {}'.format(collaborator))

    adapter = context.obj['adapter']
    verified_vars = adapter.verified(institute_id=collaborator)
    LOG.info('FOUND {} verified variants for institute {}'.format(len(verified_vars), collaborator))


    if not verified_vars:
        LOG.warning('There are no verified variants for institute {} in database!'.format(collaborator))
        return None

    document_lines = export_verified_variants(verified_vars)

    today = datetime.datetime.now().strftime('%Y-%m-%d')
    document_name = '.'.join(['verified_variants', collaborator, today]) + '.xlsx'

    # If this was a test and lines are created return success
    if test and document_lines:
        written_files +=1
        LOG.info('Success. Verified variants file contains {} lines'.format(len(document_lines)))
        return written_files

    # create workbook and new sheet
    # set up outfolder
    if not outpath:
        outpath = str(os.getcwd())
    workbook = Workbook(os.path.join(outpath,document_name))
    Report_Sheet = workbook.add_worksheet()

    # Write the column header
    row = 0
    for col,field in enumerate(VERIFIED_VARIANTS_HEADER):
        Report_Sheet.write(row,col,field)

    # Write variant lines, after header (start at line 1)
    for row, line in enumerate(document_lines,1): # each line becomes a row in the document
        for col, field in enumerate(line): # each field in line becomes a cell
            Report_Sheet.write(row,col,field)
    workbook.close()

    if os.path.exists(os.path.join(outpath,document_name)):
        LOG.info('Success. Verified variants file of {} lines was written to disk'. format(len(document_lines)))
        written_files += 1

    return written_files
--------------------
Prompt: 
def _plot_result(self, traj, result_name):
        """Plots a state variable graph for several neurons into one figure"""
----------
Output: 

        # TODO: add a legend
        # TODO: add a title
        # TODO: add a colorbar
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a label
        # TODO: add a
----------
Solution: 

        result = traj.f_get(result_name)
        varname = result.record_variables[0]
        values = result[varname]
        times = result.t

        record = result.record

        for idx, celia_neuron in enumerate(record):
            plt.subplot(len(record), 1, idx+1)
            plt.plot(times, values[idx,:])
            if idx==0:
                plt.title('%s' % varname)
            if idx==1:
                plt.ylabel('%s' % ( varname))
            if idx == len(record)-1:
                plt.xlabel('t')
--------------------
Prompt: 
def unregister_sizer(self, attr_name):
        """
        Unregister the SizedImage subclass currently assigned to `attr_name`.

        If a SizedImage subclass isn't already registered to `attr_name`
        NotRegistered will raise.
        """
----------
Output: 

        if attr_name not in self._sizer_registry:
            raise NotRegistered(attr_name)
        del self._sizer_registry[attr_name]

    def register_sizer(self, attr_name, sizer_class):
        """
        Register a SizedImage subclass to `attr_name`.

        If a SizedImage subclass is already registered to `attr_name`
        AlreadyRegistered will raise.
        """
        if attr_name in self._sizer_registry:
            raise Al
----------
Solution: 

        if attr_name not in self._sizedimage_registry:
            raise NotRegistered(
                'No SizedImage subclass is registered to %s' % attr_name
            )
        else:
            del self._sizedimage_registry[attr_name]
--------------------
Prompt: 
def assert_no_title(self, title, **kwargs):
        """
        Asserts that the page doesn't have the given title.

        Args:
            title (str | RegexObject): The string that the title should include.
            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.

        Returns:
            True

        Raises:
            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.
        """
----------
Output: 

        return self.assert_title(title, **kwargs)

    def assert_title(self, title, **kwargs):
        """
        Asserts that the page has the given title.

        Args:
            title (str | RegexObject): The string that the title should include.
            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.

        Returns:
            True

        Raises:
            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.
        """
        return self.
----------
Solution: 


        query = TitleQuery(title, **kwargs)

        @self.synchronize(wait=query.wait)
        def assert_no_title():
            if query.resolves_for(self):
                raise ExpectationNotMet(query.negative_failure_message)

            return True

        return assert_no_title()
--------------------
Prompt: 
def __fetch(self, url, payload):
        """Fetch requests from groupsio API"""
----------
Output: 

        try:
            r = requests.post(url, data=payload)
            r.raise_for_status()
            return r.json()
        except requests.exceptions.HTTPError as err:
            raise GroupioError(err)

    def __get_token(self):
        """Get a token from the groupsio API"""
        payload = {
            'username': self.username,
            'password': self.password,
            'grant_type': 'password',
            'client_id': self.client_id,
----------
Solution: 


        r = requests.get(url, params=payload, auth=self.auth, verify=self.verify)
        try:
            r.raise_for_status()
        except requests.exceptions.HTTPError as e:
            raise e

        return r
--------------------
Prompt: 
def f_shrink(self, force=False):
        """ Shrinks the trajectory and removes all exploration ranges from the parameters.
        Only possible if the trajectory has not been stored to disk before or was loaded as new.

        :param force:

            Usually you cannot shrink the trajectory if it has been stored to disk,
            because there's no guarantee that it is actually shrunk if there
            still exist explored parameters on disk. In case you are certain that
            you did not store explored parameters to disk set or you deleted all
            of them from disk set `force=True`.

        :raises: TypeError if the trajectory was stored before.

        """
----------
Output: 

            explored parameters, you can set force=True to force the shrinking.

        :return:

            True if the trajectory was shrunk, False otherwise.
        """
        if self.stored:
            if force:
                self.shrink()
                return True
            else:
                return False
        else:
            self.shrink()
            return True

    def shrink(self):
        """ Shrinks the trajectory and removes all exploration ranges from the parameters.
        """
        self.traject
----------
Solution: 

        if self._stored and not force:
            raise TypeError('Your trajectory is already stored to disk or database, shrinking is '
                            'not allowed.')

        for param in self._explored_parameters.values():
            param.f_unlock()
            try:
                param._shrink()
            except Exception as exc:
                self._logger.error('Could not shrink `%s` because of:`%s`' %
                                   (param.v_full_name, repr(exc)))

        # If we shrink, we do not have any explored parameters left and we can erase all
        # run information, and the length of the trajectory is 1 again.
        self._explored_parameters = {}
        self._run_information = {}
        self._single_run_ids = {}
        self._add_run_info(0)
        self._test_run_addition(1)
--------------------
Prompt: 
def get_data_disk(self, service_name, deployment_name, role_name, lun):
        """
        Retrieves the specified data disk from a virtual machine.

        service_name:
            The name of the service.
        deployment_name:
            The name of the deployment.
        role_name:
            The name of the role.
        lun:
            The Logical Unit Number (LUN) for the disk.
        """
----------
Output: 

        return self.get_data_disks(service_name, deployment_name, role_name)[lun]

    def get_data_disks(self, service_name, deployment_name, role_name):
        """
        Retrieves the data disks from a virtual machine.

        service_name:
            The name of the service.
        deployment_name:
            The name of the deployment.
        role_name:
            The name of the role.
        """
        return self.get_role(service_name, deployment
----------
Solution: 

        _validate_not_none('service_name', service_name)
        _validate_not_none('deployment_name', deployment_name)
        _validate_not_none('role_name', role_name)
        _validate_not_none('lun', lun)
        return self._perform_get(
            self._get_data_disk_path(
                service_name, deployment_name, role_name, lun),
            DataVirtualHardDisk)
--------------------
Prompt: 
def for_type(typ, func):
    """
    Add a pretty printer for a given type.
    """
----------
Output: 

    if typ not in _pretty_printers:
        _pretty_printers[typ] = []
    _pretty_printers[typ].append(func)


def for_type_and_subtypes(typ, func):
    """
    Add a pretty printer for a given type and all its subtypes.
    """
    if typ not in _pretty_printers:
        _pretty_printers[typ] = []
    _pretty_printers[typ].append(func)

----------
Solution: 

    oldfunc = _type_pprinters.get(typ, None)
    if func is not None:
        # To support easy restoration of old pprinters, we need to ignore Nones.
        _type_pprinters[typ] = func
    return oldfunc
--------------------
Prompt: 
def structured_mesh(shape = (2,2,2), dim = (1.,1.,1.)):
    """
    Returns a structured mesh. 
    
    :arg shape: 2 or 3 integers (eg: shape = (10, 10, 10)).
    :type shape: tuple
    :arg dim: 2 or 3 floats (eg: dim = (4., 2., 1.))
    :type dim: tuple
    
    .. note::

       This function does not use GMSH for mesh generation.


    
    >>> import argiope as ag
    >>> mesh = ag.mesh.structured_mesh(shape =(10,10,10), dim=(1.,1.,1.)))
    """
----------
Output: 
msh.
    """
    from dolfin import *
    mesh = Mesh(shape, dim)
    return mesh

def unstructured_mesh(mesh_file):
    """
    Returns an unstructured mesh. 
    
    :arg mesh_file: Path to the mesh file.
    :type mesh_file: string
    
    .. note::

       This function does not use Gmsh.
    """
    from dolfin import *
    mesh = Mesh(mesh_file)
    return mesh
----------
Solution: 

    # PREPROCESSING
    shape = np.array(shape)
    dim   = np.array(dim) 
    Ne = shape.prod()
    Nn = (shape + 1).prod()
    # LABELS
    nindex = np.arange(Nn) + 1
    eindex = np.arange(Ne) + 1
    # COORDINATES
    coords = [ np.linspace(0., dim[i], shape[i] + 1) for i in range(len(shape))]
    coords = np.array(np.meshgrid(*coords))
    coords = np.array([c.swapaxes(0,1).flatten("F") for c in coords]).T
    if len(shape) == 2:
        c = coords
        coords = np.zeros((Nn, 3))
        coords[:, :2] = c  
    # CONNECTIVITY    
    conn = _make_conn(shape)
    # MESH INSTANCE
    mesh = Mesh(nlabels = nindex,
                coords  = coords,
                elabels = eindex,
                conn = conn,)
    if len(shape) == 2: mesh.elements[("type", "argiope")] = "quad4"
    if len(shape) == 3: mesh.elements[("type", "argiope")] = "hexa8"    
    return mesh
--------------------
Prompt: 
def inshape(shape, axes):
    """
    Checks to see if a list of axes are contained within an array shape.

    Parameters
    ----------
    shape : tuple[int]
        the shape of a BoltArray

    axes : tuple[int]
        the axes to check against shape
    """
----------
Output: 

    if len(axes) != len(shape):
        raise ValueError("axes and shape must be the same length")
    for i, axis in enumerate(axes):
        if axis >= shape[i]:
            raise ValueError("axis %d is out of bounds" % axis)


def check_axes(axes, shape):
    """
    Checks to see if a list of axes are contained within an array shape.

    Parameters
    ----------
    axes : tuple[int]
        the axes to check against shape
----------
Solution: 

    valid = all([(axis < len(shape)) and (axis >= 0) for axis in axes])
    if not valid:
        raise ValueError("axes not valid for an ndarray of shape: %s" % str(shape))
--------------------
Prompt: 
def desc(value):
    """ str: A normalized representation for a user-provided value. """
----------
Output: 

    return value.replace(' ', '_').lower()


def _get_value(value, default):
    """
    Get a value from a dictionary or a default value.

    Parameters
    ----------
    value : str or dict
        The value to get.
    default : str
        The default value.

    Returns
    -------
    str
        The value.
    """
    if isinstance(value, dict):
        return value.get('value', default)
    return value


def _get_values
----------
Solution: 


    def normalize_strings(value):
        if isinstance(value, list):
            value = [normalize_strings(e) for e in value]

        if isinstance(value, dict):
            value = {normalize_strings(k): normalize_strings(v) for k, v in iter(value.items())}

        if isregex(value):
            value = value.pattern

        if isbytes(value):
            value = decode_bytes(value)

        if PY2:
            if isstring(value):
                # In Python 2, strings (``unicode`` objects) represent as ``u'...'``, so ensure
                # the string is encoded (as a ``str`` object) for cleaner representation.
                value = encode_string(value)

        return value

    value = normalize_strings(value)

    return repr(value)
--------------------
Prompt: 
def recommend_k_items_slow(self, test, top_k=10, remove_seen=True):
        """Recommend top K items for all users which are in the test set.

        Args:
            test: test Spark dataframe
            top_k: top n items to return
            remove_seen: remove items test users have already seen in the past from the recommended set.
        """
----------
Output: 

        # TODO: implement this method
        raise NotImplementedError

    def recommend_k_items_fast(self, test, top_k=10, remove_seen=True):
        """Recommend top K items for all users which are in the test set.

        Args:
            test: test Spark dataframe
            top_k: top n items to return
            remove_seen: remove items test users have already seen in the past from the recommended set.
        """
        # TODO: implement this method
        raise NotImplementedError
----------
Solution: 


        # TODO: remove seen
        if remove_seen:
            raise ValueError("Not implemented")

        self.get_user_affinity(test)\
            .write.mode("overwrite")\
            .saveAsTable(self.f("{prefix}user_affinity"))

        # user_affinity * item_similarity
        # filter top-k
        query = self.f(
            
--------------------
Prompt: 
def basis_state(str_state, num):
    """
    Return a basis state ndarray.

    Args:
        str_state (string): a string representing the state.
        num (int): the number of qubits
    Returns:
        ndarray:  state(2**num) a quantum state with basis basis state.
     Raises:
        QiskitError: if the dimensions is wrong
    """
----------
Output: 

    if num < 0:
        raise QiskitError('Number of qubits must be non-negative.')
    if len(str_state) != 2 ** num:
        raise QiskitError('Length of state string must equal 2**num.')
    state = np.zeros(2 ** num, dtype=complex)
    for i, state_i in enumerate(str_state):
        if state_i == '1':
            state[i] = 1
        elif state_i == '0':
            state[i
----------
Solution: 

    n = int(str_state, 2)
    if num >= len(str_state):
        state = np.zeros(1 << num, dtype=complex)
        state[n] = 1
        return state
    else:
        raise QiskitError('size of bitstring is greater than num.')
--------------------
Prompt: 
def _sparse_tensor_dense_matmul(sp_a, b, **kwargs):
  """Returns (batched) matmul of a SparseTensor with a Tensor.

  Args:
    sp_a: `SparseTensor` representing a (batch of) matrices.
    b: `Tensor` representing a (batch of) matrices, with the same batch shape of
      `sp_a`. The shape must be compatible with the shape of `sp_a` and kwargs.
    **kwargs: Keyword arguments to `tf.sparse_tensor_dense_matmul`.

  Returns:
    product: A dense (batch of) matrix-shaped Tensor of the same batch shape and
    dtype as `sp_a` and `b`. If `sp_a` or `b` is adjointed through `kwargs` then
    the shape is adjusted accordingly.
  """
----------
Output: 
tf.sparse.sparse_dense_matmul`.

  Returns:
    A `Tensor` with shape `[sp_a.shape[0], b.shape[1]]`.
  """
  if sp_a.dtype != b.dtype:
    raise TypeError("SparseTensor and Tensor must have the same dtype, got %s "
                    "and %s." % (sp_a.dtype, b.dtype))
  if not sp_a.shape.is_compatible_with(b.shape):
   
----------
Solution: 

  batch_shape = _get_shape(sp_a)[:-2]

  # Reshape the SparseTensor into a rank 3 SparseTensors, with the
  # batch shape flattened to a single dimension. If the batch rank is 0, then
  # we add a batch dimension of rank 1.
  sp_a = tf.sparse.reshape(sp_a, tf.concat([[-1], _get_shape(sp_a)[-2:]],
                                           axis=0))
  # Reshape b to stack the batch dimension along the rows.
  b = tf.reshape(b, tf.concat([[-1], _get_shape(b)[-1:]], axis=0))

  # Convert the SparseTensor to a matrix in block diagonal form with blocks of
  # matrices [M, N]. This allow us to use tf.sparse_tensor_dense_matmul which
  # only accepts rank 2 (Sparse)Tensors.
  out = tf.sparse.sparse_dense_matmul(_sparse_block_diag(sp_a), b, **kwargs)

  # Finally retrieve the original batch shape from the resulting rank 2 Tensor.
  # Note that we avoid inferring the final shape from `sp_a` or `b` because we
  # might have transposed one or both of them.
  return tf.reshape(
      out,
      tf.concat([batch_shape, [-1], _get_shape(out)[-1:]], axis=0))
--------------------
Prompt: 
def require_parents(packages):
        """
        Exclude any apparent package that apparently doesn't include its
        parent.

        For example, exclude 'foo.bar' if 'foo' is not present.
        """
----------
Output: 

        for package in packages:
            if '.' in package:
                parent = package.rsplit('.', 1)[0]
                if parent not in packages:
                    packages.remove(package)

    def require_packages(packages):
        """
        Exclude any apparent package that apparently doesn't exist.

        For example, exclude 'foo.bar' if 'foo' is not present.
        """
        for package in packages:
            if '.' in package:
                parent = package.rsplit('.', 1)[0]

----------
Solution: 

        found = []
        for pkg in packages:
            base, sep, child = pkg.rpartition('.')
            if base and base not in found:
                continue
            found.append(pkg)
            yield pkg
--------------------
Prompt: 
def make_report(self,traceback):
        """Return a string containing a crash report."""
----------
Output: 

        return traceback

    def get_traceback_text(self,traceback):
        """Return a string containing a traceback."""
        return traceback

    def get_report_label(self):
        """Return a string containing a label for the crash report."""
        return "Traceback"

    def get_traceback_label(self):
        """Return a string containing a label for the traceback."""
        return "Traceback"

    def get_crash_report_entry_text(self,crash_report
----------
Solution: 


        sec_sep = self.section_sep

        report = ['*'*75+'\n\n'+'IPython post-mortem report\n\n']
        rpt_add = report.append
        rpt_add(sys_info())

        try:
            config = pformat(self.app.config)
            rpt_add(sec_sep)
            rpt_add('Application name: %s\n\n' % self.app_name)
            rpt_add('Current user configuration structure:\n\n')
            rpt_add(config)
        except:
            pass
        rpt_add(sec_sep+'Crash traceback:\n\n' + traceback)

        return ''.join(report)
--------------------
Prompt: 
def _fmt(self, string):
        """Helper method to format our name into a string."""
----------
Output: 

        return string.format(name=self.name,
                             number=self.number,
                             type=self.type)

    def __str__(self):
        """Returns a string representation of the object."""
        return self._fmt('{name} {number} {type}')

    def __repr__(self):
        """Returns a string representation of the object."""
        return self._fmt('{name} {number} {type}')

    def __eq__(self, other):
        """Returns True if
----------
Solution: 

        if '{' not in string:
            string = '{}.' + string
        return string.format(self.name)
--------------------
Prompt: 
def build_fake_input_fns(batch_size):
  """Builds fake MNIST-style data for unit testing."""
----------
Output: 

  def fake_input_fn(is_training):
    """Fake input function."""
    # Create a fake dataset that yields 100 images.
    images = tf.random_uniform([batch_size, 28, 28, 1])
    labels = tf.random_uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)
    return tf.data.Dataset.from_tensors((images, labels)).repeat()
  return fake_input_fn



----------
Solution: 

  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype("float32")

  def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices(
        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()
    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()

  def eval_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices(
        random_sample).map(lambda row: (row, 0)).batch(batch_size)
    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()

  return train_input_fn, eval_input_fn
--------------------
Prompt: 
def marker(self, *args):
        """
        Defines markers one at a time for your graph
        args are of the form::
            <marker type>,
            <color>,
            <data set index>,
            <data point>,
            <size>,
            <priority>
        see the official developers doc for the complete spec
        APIPARAM: chm
        """
----------
Output: 

        self._marker_args = args
        return self

    def marker_size(self, *args):
        """
        Defines markers one at a time for your graph
        args are of the form::
            <marker type>,
            <color>,
            <data set index>,
            <data point>,
            <size>,
            <priority>
        see the official developers doc for the complete spec
        APIPARAM: chm
        """
        self._marker_args = args
        return self

    def marker_priority(
----------
Solution: 

        if len(args[0]) == 1:
            assert args[0] in MARKERS, 'Invalid marker type: %s'%args[0]
        assert len(args) <= 6, 'Incorrect arguments %s'%str(args)
        args = color_args(args, 1)
        self.markers.append(','.join(map(str,args)) )
        return self
--------------------
Prompt: 
def camera_event_motion_enum(self, camera_id, **kwargs):
        """Return motion settings matching camera_id."""
----------
Output: 

        return self.camera_event_motion_enum_by_id(camera_id, **kwargs)

    def camera_event_motion_enum_by_id(self, camera_id, **kwargs):
        """Return motion settings matching camera_id."""
        return self.camera_event_motion_enum_by_name(
            self.camera_id_to_name(camera_id), **kwargs)

    def camera_event_motion_enum_by_name(self, camera_name, **kwargs):
       
----------
Solution: 

        api = self._api_info['camera_event']
        payload = dict({
            '_sid': self._sid,
            'api': api['name'],
            'method': 'MotionEnum',
            'version': api['version'],
            'camId': camera_id,
        }, **kwargs)
        response = self._get_json_with_retry(api['url'], payload)

        return MotionSetting(camera_id, response['data']['MDParam'])
--------------------
Prompt: 
def register(self, managed_object):
        """
        Register a managed object with a KMIP appliance.

        Args:
            managed_object (ManagedObject): A managed object to register. An
                instantiatable subclass of ManagedObject from the Pie API.

        Returns:
            string: The uid of the newly registered managed object.

        Raises:
            ClientConnectionNotOpen: if the client connection is unusable
            KmipOperationFailure: if the operation result is a failure
            TypeError: if the input argument is invalid
        """
----------
Output: 

        """
        if not isinstance(managed_object, ManagedObject):
            raise TypeError("managed_object must be a ManagedObject")

        request = RegisterRequestPayload(
            object_type=managed_object.object_type,
            attributes=managed_object.attributes
        )

        operation = RegisterOperation(request)
        response = self.send_request(operation)

        if response.result.result_status.value == ResultStatusEnum.SUCCESS:
            return response.result.object_uuid
        else:
           
----------
Solution: 

        # Check input
        if not isinstance(managed_object, pobjects.ManagedObject):
            raise TypeError("managed object must be a Pie ManagedObject")

        # Extract and create attributes
        object_attributes = list()

        if hasattr(managed_object, 'cryptographic_usage_masks'):
            if managed_object.cryptographic_usage_masks is not None:
                mask_attribute = self.attribute_factory.create_attribute(
                    enums.AttributeType.CRYPTOGRAPHIC_USAGE_MASK,
                    managed_object.cryptographic_usage_masks
                )
                object_attributes.append(mask_attribute)
        if hasattr(managed_object, 'operation_policy_name'):
            if managed_object.operation_policy_name is not None:
                opn_attribute = self.attribute_factory.create_attribute(
                    enums.AttributeType.OPERATION_POLICY_NAME,
                    managed_object.operation_policy_name
                )
                object_attributes.append(opn_attribute)
        if hasattr(managed_object, 'names'):
            if managed_object.names:
                for name in managed_object.names:
                    name_attribute = self.attribute_factory.create_attribute(
                        enums.AttributeType.NAME,
                        name
                    )
                    object_attributes.append(name_attribute)

        template = cobjects.TemplateAttribute(attributes=object_attributes)
        object_type = managed_object.object_type

        # Register the managed object and handle the results
        secret = self.object_factory.convert(managed_object)
        result = self.proxy.register(object_type, template, secret)

        status = result.result_status.value
        if status == enums.ResultStatus.SUCCESS:
            return result.uuid
        else:
            reason = result.result_reason.value
            message = result.result_message.value
            raise exceptions.KmipOperationFailure(status, reason, message)
--------------------
Prompt: 
def visit_return(self, node):
        """1 - check is the node has a right sibling (if so, that's some
        unreachable code)
        2 - check is the node is inside the finally clause of a try...finally
        block
        """
----------
Output: 

        if node.sibling:
            self.warn(node, "Unreachable code")
        if node.parent and node.parent.type == 'try':
            if node.parent.finally_block:
                self.warn(node, "Unreachable code")

    def visit_raise(self, node):
        """1 - check if the node is inside a finally clause
        """
        if node.parent and node.parent.type == 'try':
            if node.parent.finally_block:
                self.
----------
Solution: 

        self._check_unreachable(node)
        # Is it inside final body of a try...finally bloc ?
        self._check_not_in_finally(node, "return", (astroid.FunctionDef,))
--------------------
Prompt: 
def read(self, size=-1):
        """
        read([size]) -> read at most size bytes, returned as a string.

        If the size argument is negative or omitted, read until EOF is reached.
        Notice that when in non-blocking mode, less data than what was
        requested may be returned, even if no size parameter was given.
        """
----------
Output: 

        if size < 0:
            size = None
        return self._sock.recv(size)

    def readline(self, size=-1):
        """
        readline([size]) -> read a line, ending in a newline character.

        If the size argument is negative or omitted, read until a newline
        character is encountered, or until EOF is reached.
        """
        if size < 0:
            size = None
        return self._sock.recv(size)

    def readlines(self, sizehint
----------
Solution: 

        if self.left is not None:
            size = min(size, self.left)
        if self.closed:
            raise ValueError('I/O operation on closed file')
        if size < 0:
            return ''.join(self)
        elif not size:
            chunk = ''
        elif self.buf:
            chunk = self.buf
            self.buf = None
        else:
            try:
                chunk = next(self.iterator)
            except StopIteration:
                return ''
        if len(chunk) > size:
            self.buf = chunk[size:]
            chunk = chunk[:size]
        if self.left is not None:
            self.left -= len(chunk)
        return chunk
--------------------
Prompt: 
def balanced_rows(n, iterable, fillvalue=None):
	"""
	Like grouper, but balance the rows to minimize fill per row.
	balanced_rows(3, 'ABCDEFG', 'x') --> ABC DEx FGx"
	"""
----------
Output: 

	args = [iter(iterable)] * n
	return zip_longest(*args, fillvalue=fillvalue)


def balanced_cols(n, iterable, fillvalue=None):
	"""
	Like grouper, but balance the columns to minimize fill per column.
	balanced_cols(3, 'ABCDEFG', 'x') --> AB CDEFGx"
	"""
	args = [iter(iterable)] * n
	return zip(*args, fillvalue=fillvalue)


def balanced
----------
Solution: 

	iterable, iterable_copy = itertools.tee(iterable)
	count = len(tuple(iterable_copy))
	for allocation in partition_items(count, n):
		row = itertools.islice(iterable, allocation)
		if allocation < n:
			row = itertools.chain(row, [fillvalue])
		yield tuple(row)
--------------------
Prompt: 
def timetree_likelihood(self):
        """
        Return the likelihood of the data given the current branch length in the tree
        """
----------
Output: 

        return self.likelihood_function(self.branch_length)

    def timetree_likelihood_gradient(self):
        """
        Return the gradient of the likelihood of the data given the current branch length in the tree
        """
        return self.likelihood_gradient_function(self.branch_length)

    def timetree_likelihood_hessian(self):
        """
        Return the hessian of the likelihood of the data given the current branch length in the tree
        """
        return self.likelihood_
----------
Solution: 

        LH = 0
        for node in self.tree.find_clades(order='preorder'):  # sum the likelihood contributions of all branches
            if node.up is None: # root node
                continue
            LH -= node.branch_length_interpolator(node.branch_length)

        # add the root sequence LH and return
        if self.aln:
            LH += self.gtr.sequence_logLH(self.tree.root.cseq, pattern_multiplicity=self.multiplicity)
        return LH
--------------------
Prompt: 
def read(*paths):
    """Build a file path from *paths* and return the contents."""
----------
Output: 

    with open(os.path.join(*paths), 'r') as f:
        return f.read()


def get_version(*file_paths):
    """Get version string."""
    version_file = read(*file_paths)
    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]",
                              version_file, re.M)
    if version_match:
        return version_match.group(1)
    raise RuntimeError("Unable to find version string.")
----------
Solution: 

    filename = os.path.join(*paths)
    with codecs.open(filename, mode='r', encoding='utf-8') as handle:
        return handle.read()
--------------------
Prompt: 
def bootstrap(self, path_or_uri):
        """ Initialize a database.

        :param database_path: The absolute path to the database to initialize.
        """
----------
Output: 

        if not os.path.exists(path_or_uri):
            os.makedirs(path_or_uri)
        self.db_path = path_or_uri
        self.db_uri = 'sqlite:///' + path_or_uri
        self.engine = create_engine(self.db_uri)
        self.session_factory = sessionmaker(bind=self.engine)
        self.session = self.session_factory()
        self.Base = declarative_base()
        self.Base.metadata.create_all
----------
Solution: 

        _logger.debug("Bootstrapping new database: %s", path_or_uri)
        self.database_uri = _urify_db(path_or_uri)
        db = sa.create_engine(self.database_uri)
        Base.metadata.create_all(db)
--------------------
Prompt: 
def apply_mirroring(storage, debug):
    """Applies L4: mirroring

    See: http://unicode.org/reports/tr9/#L4

    """
----------
Output: 

    if debug:
        print("Applying L4: mirroring")

    # 1. For each character in the input,
    for char in storage:
        # 2. if the character has a mirrored glyph,
        if char.mirrored:
            # 3. add the mirrored glyph to the output.
            storage.append(char.mirrored)
        else:
            # 4. otherwise, add the character to the output.
            storage.append(char)


def apply_combining_
----------
Solution: 

    # L4. A character is depicted by a mirrored glyph if and only if (a) the
    # resolved directionality of that character is R, and (b) the
    # Bidi_Mirrored property value of that character is true.
    for _ch in storage['chars']:
        unichar = _ch['ch']
        if mirrored(unichar) and \
                _embedding_direction(_ch['level']) == 'R':
            _ch['ch'] = MIRRORED.get(unichar, unichar)

    if debug:
        debug_storage(storage)
--------------------
Prompt: 
def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool:
    """return True if <name> is a method overridden from an ancestor"""
----------
Output: 

    for ancestor in class_node.ancestors():
        if ancestor.name == name:
            return True
    return False


def get_overridden_methods(class_node: astroid.node_classes.NodeNG) -> Set[str]:
    """return a set of method names overridden from an ancestor"""
    return {
        method.name
        for method in class_node.body
        if isinstance(method, astroid.node_classes.FunctionDef)
        and overrides_a_method(class
----------
Solution: 

    for ancestor in class_node.ancestors():
        if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef):
            return True
    return False
--------------------
Prompt: 
def _add_empty_child(self, parent, code, final=False):
        """
        Добавление ребёнка к вершине parent по символу с кодом code
        """
----------
Output: 

        child = self.add_child(parent, code)
        if final:
            self.set_final(child)
        return child

    def _add_empty_children(self, parent, codes):
        """
        Добавление ребёнков к вершине parent по символам с кодами codes
        """
        for code in codes:
            self._add_empty_child(parent, code)

    def _add_empty_siblings(self, parent, codes):
        """
        Добав
----------
Solution: 

        self.graph[parent][code] = self.nodes_number
        self.graph.append(self._make_default_node())
        self.data.append(None)
        self.final.append(final)
        self.nodes_number += 1
        return (self.nodes_number - 1)
--------------------
Prompt: 
def _check_year(year, month, error, error_msg):
    """Checks that the year is within 50 years from now."""
----------
Output: 

    if year < datetime.now().year - 50:
        raise error(error_msg.format(year))
    if year > datetime.now().year + 50:
        raise error(error_msg.format(year))
    if month < 1 or month > 12:
        raise error(error_msg.format(month))


def _check_month(year, month, error, error_msg):
    """Checks that the month is within 12 months from now."""
    if month < 1 or
----------
Solution: 

    if year not in xrange((now.year - 50), (now.year + 51)):
        year = now.year
        month = now.month
        error = error_msg
    return year, month, error
--------------------
Prompt: 
def transpose(self):
        """Return the transpose of the QuantumChannel."""
----------
Output: 

        return self.adjoint()

    def is_cptp(self, atol=None, rtol=None):
        """Return True if the QuantumChannel is completely-positive trace-preserving.

        Args:
            atol (float): absolute tolerance value
            rtol (float): relative tolerance value

        Returns:
            bool: is completely-positive and trace-preserving
        """
        return True

    def is_hermitian(self, atol=None, rtol=None):

----------
Solution: 

        din, dout = self.dim
        dtr = self._data[0].shape[0] // dout
        stine = [None, None]
        for i, mat in enumerate(self._data):
            if mat is not None:
                stine[i] = np.reshape(
                    np.transpose(np.reshape(mat, (dout, dtr, din)), (2, 1, 0)),
                    (din * dtr, dout))
        return Stinespring(
            tuple(stine),
            input_dims=self.output_dims(),
            output_dims=self.input_dims())
--------------------
Prompt: 
def newick(self):
        """The representation of the Node in Newick format."""
----------
Output: 

        return self.newick_str

    @property
    def newick_str(self):
        """The representation of the Node in Newick format."""
        return self._newick_str

    @newick_str.setter
    def newick_str(self, newick_str):
        """Set the representation of the Node in Newick format."""
        self._newick_str = newick_str

    @property
    def name(self):
        """The name of the Node."""
        return self._
----------
Solution: 

        label = self.name or ''
        if self._length:
            label += ':' + self._length
        descendants = ','.join([n.newick for n in self.descendants])
        if descendants:
            descendants = '(' + descendants + ')'
        return descendants + label
--------------------
Prompt: 
async def get_postcode_random() -> Postcode:
    """
    Gets a random postcode object..
    Acts as a middleware between us and the API, caching results.
    :return: The PostCode object else None if the postcode does not exist.
    """
----------
Output: 

    postcode = Postcode.objects.all().order_by('?').first()
    if postcode:
        return postcode
    else:
        return None


def get_postcode_by_id(postcode_id: int) -> Postcode:
    """
    Gets a postcode object by its id.
    Acts as a middleware between us and the API, caching results.
    :param postcode_id: The id of the postcode to get.
    :return: The PostCode object else None if the postcode
----------
Solution: 

    try:
        postcode = await fetch_postcode_random()
    except (ApiError, CircuitBreakerError):
        raise CachingError(f"Requested postcode is not cached, and can't be retrieved.")

    if postcode is not None:
        postcode.save()
    return postcode
--------------------
Prompt: 
def tasks(self, from_date=DEFAULT_DATETIME):
        """Retrieve tasks.

        :param from_date: retrieve tasks that where updated from that date;
            dates are converted epoch time.
        """
----------
Output: 

        return self._get(
            'tasks',
            params={'from_date': from_date}
        )

    def task(self, task_id):
        """Retrieve a task.

        :param task_id: the task id.
        """
        return self._get('tasks/%s' % task_id)

    def create_task(self, task):
        """Create a task.

        :param task: the task to create.
        """
        return self._post('tasks', task)


----------
Solution: 

        # Convert 'from_date' to epoch timestamp.
        # Zero value (1970-01-01 00:00:00) is not allowed for
        # 'modifiedStart' so it will be set to 1, by default.
        ts = int(datetime_to_utc(from_date).timestamp()) or 1

        consts = {
            self.PMODIFIED_START: ts
        }

        attachments = {
            self. PPROJECTS: True
        }

        params = {
            self.PCONSTRAINTS: consts,
            self.PATTACHMENTS: attachments,
            self.PORDER: self.VOUTDATED,
        }

        while True:
            r = self._call(self.MANIPHEST_TASKS, params)
            yield r
            j = json.loads(r)
            after = j['result']['cursor']['after']
            if not after:
                break
            params[self.PAFTER] = after
--------------------
Prompt: 
def ensure_dir(path):
    """Ensure that a needed directory exists, creating it if it doesn't"""
----------
Output: 

    if not os.path.exists(path):
        os.makedirs(path)


def get_file_list(path):
    """Get a list of files in a directory"""
    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]


def get_file_list_with_extension(path, extension):
    """Get a list of files in a directory with a given extension"""
    return [f for f in os.listdir(
----------
Solution: 

    try:
        log.info('Ensuring directory exists: %s' % path)
        os.makedirs(path)
    except OSError:
        if not os.path.isdir(path):
            raise
--------------------
Prompt: 
def parse_mim2gene(lines):
    """Parse the file called mim2gene
    
    This file describes what type(s) the different mim numbers have.
    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',
    'phenotype', 'predominantly phenotypes'
    Where:
        gene: Is a gene entry
        gene/phenotype: This entry describes both a phenotype and a gene
        moved/removed: No explanation needed
        phenotype: Describes a phenotype
        predominantly phenotype: Not clearly established (probably phenotype)
    
    Args:
        lines(iterable(str)): The mim2gene lines
    
    Yields:
        parsed_entry(dict)
    
        {
            "mim_number": int, 
            "entry_type": str, 
            "entrez_gene_id": int, 
            "hgnc_symbol": str, 
            "ensembl_gene_id": str,
            "ensembl_transcript_id": str,
        }
    
    """
----------
Output: 
phenotype: Is a phenotype entry
        predominantly phenotypes: Is a phenotype entry

    Args:
        lines (list): A list of lines from the file

    Returns:
        dict: A dictionary with the mim number as key and the entry type as value
    """
    mim2gene = {}
    for line in lines:
        line = line.strip()
        if line.startswith('#'):
            continue
        if line.startswith('MIM'):
            continue
        if line.startswith
----------
Solution: 

    LOG.info("Parsing mim2gene")
    header = ["mim_number", "entry_type", "entrez_gene_id", "hgnc_symbol", "ensembl_gene_id"]
    for i, line in enumerate(lines):
        if line.startswith('#'):
            continue
        
        if not len(line) > 0:
            continue

        line = line.rstrip()
        parsed_entry = parse_omim_line(line, header)
        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])
        parsed_entry['raw'] = line
        
        if 'hgnc_symbol' in parsed_entry:
            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']
        
        if parsed_entry.get('entrez_gene_id'):
            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])
        
        if parsed_entry.get('ensembl_gene_id'):
            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')
            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()
            if len(ensembl_info) > 1:
                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()
        
        yield parsed_entry
--------------------
Prompt: 
def directive(directname=None):
    """Attach a class to a parsing class and register it as a parser directive.

        The class is registered with its name unless directname is provided.
    """
----------
Output: 

    def decorator(cls):
        if directname is None:
            directname = cls.__name__
        cls.directname = directname
        cls.directive = True
        return cls
    return decorator


def directive_parser(directname=None):
    """Attach a function to a parsing class and register it as a parser directive.

        The function is registered with its name unless directname is provided.
    """
    def decorator(func):
        if directname is None:
            directname = func
----------
Solution: 

    global _directives
    class_dir_list = _directives

    def wrapper(f):
        nonlocal directname
        if directname is None:
            directname = f.__name__
        f.ns_name = directname
        set_one(class_dir_list, directname, f)
        return f
    return wrapper
--------------------
Prompt: 
def get_conn(self):
        """
        Returns a Redis connection.
        """
----------
Output: 

        return self.connection

    def get_key(self, key):
        """
        Returns a key from the Redis connection.
        """
        return self.connection.get(key)

    def set_key(self, key, value):
        """
        Sets a key in the Redis connection.
        """
        return self.connection.set(key, value)

    def delete_key(self, key):
        """
        Deletes a key from the Redis connection.
        """
        return self.connection
----------
Solution: 

        conn = self.get_connection(self.redis_conn_id)
        self.host = conn.host
        self.port = conn.port
        self.password = None if str(conn.password).lower() in ['none', 'false', ''] else conn.password
        self.db = conn.extra_dejson.get('db', None)

        if not self.redis:
            self.log.debug(
                'Initializing redis object for conn_id "%s" on %s:%s:%s',
                self.redis_conn_id, self.host, self.port, self.db
            )
            self.redis = Redis(
                host=self.host,
                port=self.port,
                password=self.password,
                db=self.db)

        return self.redis
--------------------
Prompt: 
def decode(self, packet):
        """
        Decode a PUBREL control packet. 
        """
----------
Output: 

        self.logger.debug("PUBREL received")
        packet.logger.debug("PUBREL received")
        packet.logger.debug("Packet: %s" % packet)
        packet.logger.debug("Packet.data: %s" % packet.data)
        packet.logger.debug("Packet.data.decode('utf-8'): %s" % packet.data.decode('utf-8'))
        packet.logger.debug("Packet.data.decode('utf-8').split(' '): %s" % packet.
----------
Solution: 

        self.encoded = packet
        lenLen = 1
        while packet[lenLen] & 0x80:
            lenLen += 1
        packet_remaining = packet[lenLen+1:]
        self.msgId  = decode16Int(packet_remaining)
        self.dup = (packet[0] & 0x08) == 0x08
--------------------
Prompt: 
def init_log( quiet, debug ):
   """
   setup system logging to desired verbosity.
   """
----------
Output: 

   if debug:
      log_level = logging.DEBUG
   elif quiet:
      log_level = logging.ERROR
   else:
      log_level = logging.INFO

   logging.basicConfig(
      level=log_level,
      format='%(asctime)s %(levelname)s %(message)s',
      datefmt='%Y-%m-%d %H:%M:%S'
   )


def get_logger( name ):
   """
   get a logger by name.
   """
  
----------
Solution: 

   from logging.handlers import SysLogHandler
   fmt = logging.Formatter( os.path.basename(sys.argv[0]) +
         ".%(name)s %(levelname)s - %(message)s")
   facility = SysLogHandler.LOG_DAEMON
   syslog = SysLogHandler(address='/dev/log',facility=facility)
   syslog.setFormatter( fmt )
   log.addHandler(syslog)
   if not quiet:
      console = logging.StreamHandler()
      console.setFormatter( fmt )
      log.addHandler(console)
      log.setLevel(logging.INFO)
      if debug:
         log.setLevel(logging.DEBUG)
--------------------
Prompt: 
def loadPlugins(self):
        """Load plugins in nose.plugins.builtin
        """
----------
Output: 

        from nose.plugins.builtin import (
            CapturePlugin, CoveragePlugin, DiffPlugin, ExcludePlugin,
            FileNamePlugin, FileNameAndLineNumberPlugin,
            FileNameAndDescriptionPlugin, FileNameAndIdPlugin,
            FileNameAndFuncNamePlugin, FileNameAndModulePlugin,
            FileNameAndClassPlugin, FileNameAndMethodPlugin,
            FileNameAndLineNumberAndDescriptionPlugin,
            FileNameAndLineNumberAndIdPlugin,
            FileNameAndLineNumberAndFuncNamePlugin,
            FileNameAndLineNumberAndModule
----------
Solution: 

        from nose.plugins import builtin
        for plug in builtin.plugins:
            self.addPlugin(plug())
        super(BuiltinPluginManager, self).loadPlugins()
--------------------
Prompt: 
def verify(self, pkey):
        """
        Verifies the signature on this certificate signing request.

        :param PKey key: A public key.

        :return: ``True`` if the signature is correct.
        :rtype: bool

        :raises OpenSSL.crypto.Error: If the signature is invalid or there is a
            problem verifying the signature.
        """
----------
Output: 

        return self._cert_req.verify(pkey)

    def set_pubkey(self, pkey):
        """
        Sets the public key for this certificate signing request.

        :param PKey pkey: The public key.
        """
        self._cert_req.set_pubkey(pkey)

    def get_pubkey(self):
        """
        Returns the public key for this certificate signing request.

        :return: The public key.
        :rtype: PKey
        """
        return self
----------
Solution: 

        if not isinstance(pkey, PKey):
            raise TypeError("pkey must be a PKey instance")

        result = _lib.X509_REQ_verify(self._req, pkey._pkey)
        if result <= 0:
            _raise_current_error()

        return result
--------------------
Prompt: 
def add_card(self, query_params=None):
        """
        Create a card for this list. Returns a Card object.
        """
----------
Output: 

        return self._create_card(query_params=query_params)

    def add_comment(self, comment, query_params=None):
        """
        Add a comment to this list. Returns a Comment object.
        """
        return self._create_comment(comment, query_params=query_params)

    def add_label(self, label, query_params=None):
        """
        Add a label to this list. Returns a Label object.
        """
        return self._create_label(label, query_params=query
----------
Solution: 

        card_json = self.fetch_json(
            uri_path=self.base_uri + '/cards',
            http_method='POST',
            query_params=query_params or {}
        )

        return self.create_card(card_json)
--------------------
Prompt: 
def get_default_value(self):
        """Instantiate a default value instance.

        This is called when the containing HasTraits classes'
        :meth:`__new__` method is called to ensure that a unique instance
        is created for each HasTraits instance.
        """
----------
Output: 

        return self.default_value()

    def __get__(self, obj, cls):
        """Get the value of the trait from the HasTraits instance.

        This is called when the trait is accessed as an attribute of the
        HasTraits instance.
        """
        if obj is None:
            return self
        return obj.get(self.name)

    def __set__(self, obj, value):
        """Set the value of the trait on the HasTraits instance.

        This is called when the trait is assigned to
----------
Solution: 

        dv  = self.default_value
        if isinstance(dv, DefaultValueGenerator):
            return dv.generate(self.klass)
        else:
            return dv
--------------------
Prompt: 
def forward(self, input, target):
        """
        NB: It's for debug only, please use optimizer.optimize() in production.
        Takes an input object, and computes the corresponding loss of the criterion,
        compared with `target`

        :param input: ndarray or list of ndarray
        :param target: ndarray or list of ndarray
        :return: value of loss
        """
----------
Output: 

        if isinstance(input, list):
            input = [Variable(i) for i in input]
        else:
            input = [Variable(input)]
        if isinstance(target, list):
            target = [Variable(i) for i in target]
        else:
            target = [Variable(target)]
        return self.forward_batch(input, target)

    def forward_batch(self, input, target):
        """
        NB: It's for debug only, please use optimizer.optimize() in production.

----------
Solution: 

        jinput, input_is_table = Layer.check_input(input)
        jtarget, target_is_table = Layer.check_input(target)
        output = callBigDlFunc(self.bigdl_type,
                               "criterionForward",
                               self.value,
                               jinput,
                               input_is_table,
                               jtarget,
                               target_is_table)
        return output
--------------------
Prompt: 
def split_line(self, line, cursor_pos=None):
        """Split a line of text with a cursor at the given position.
        """
----------
Output: 

        if cursor_pos is None:
            cursor_pos = len(line)
        if cursor_pos == 0:
            return [line], 0
        if cursor_pos == len(line):
            return [line], len(line)
        left, right = line[:cursor_pos], line[cursor_pos:]
        return [left, right], cursor_pos

    def join_lines(self, lines):
        """Join a list of lines into a single line.
        """
        return ''.join(lines)

    def
----------
Solution: 

        l = line if cursor_pos is None else line[:cursor_pos]
        return self._delim_re.split(l)[-1]
--------------------
Prompt: 
def signing_keys_as_jwks(self):
        """
        Build a JWKS from the signing keys belonging to the self signer

        :return: Dictionary
        """
----------
Output: 

        jwks = {
            "keys": []
        }
        for key in self.signing_keys:
            jwks["keys"].append(key.as_jwk())
        return jwks

    def signing_keys_as_pem(self):
        """
        Build a PEM from the signing keys belonging to the self signer

        :return: Dictionary
        """
        pem = {
            "keys": []
        }
        for key in self.signing_keys:
            pem["
----------
Solution: 

        _l = [x.serialize() for x in self.self_signer.keyjar.get_signing_key()]
        if not _l:
            _l = [x.serialize() for x in
                  self.self_signer.keyjar.get_signing_key(owner=self.iss)]
        return {'keys': _l}
--------------------
Prompt: 
def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:
    """Check if the given node is from a fallback import block."""
----------
Output: 

    return node.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent.parent
----------
Solution: 

    context = find_try_except_wrapper_node(node)
    if not context:
        return False

    if isinstance(context, astroid.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable(
            handler.body for handler in context.handlers
        )
        handlers = context.handlers

    has_fallback_imports = any(
        isinstance(import_node, (astroid.ImportFrom, astroid.Import))
        for import_node in other_body
    )
    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)
    return ignores_import_error or has_fallback_imports
--------------------
Prompt: 
def report(self, output_file=sys.stdout):
        """Report generated model in human readable form."""
----------
Output: 

        if self.model is None:
            raise ValueError("Model is not generated yet.")
        if self.model.is_empty():
            raise ValueError("Model is empty.")
        if not self.model.is_valid():
            raise ValueError("Model is not valid.")
        if self.model.is_empty():
            raise ValueError("Model is empty.")
        if self.model.is_valid():
            raise ValueError("Model is not valid.")
        if self.model.is_empty():
            raise ValueError("Model is empty
----------
Solution: 

        if self._args and self._args.verbose > 2:
            pprint(self.results)

        for dimension, lc_info in self.results['dimensions'].items():
            print("{}D layer condition:".format(dimension), file=output_file)
            for cache, lc_solution in sorted(lc_info['caches'].items()):
                print(cache+": ", end='', file=output_file)
                if lc_solution['lt'] is sympy.true:
                    print("unconditionally fulfilled", file=output_file)
                else:
                    if lc_solution['eq'] is None:
                        print("{}".format(lc_solution['lt']), file=output_file)
                    elif type(lc_solution['eq']) is not list:
                        print("{}".format(lc_solution['eq']), file=output_file)
                    else:
                        for solu in lc_solution['eq']:
                            for s, v in solu.items():
                                print("{} <= {}".format(s, v), file=output_file)
--------------------
Prompt: 
def _draw(self, mode, vertex_list=None):
        """Draw vertices in the domain.

        If `vertex_list` is not specified, all vertices in the domain are
        drawn.  This is the most efficient way to render primitives.

        If `vertex_list` specifies a `VertexList`, only primitives in that
        list will be drawn.

        :Parameters:
            `mode` : int
                OpenGL drawing mode, e.g. ``GL_POINTS``, ``GL_LINES``, etc.
            `vertex_list` : `VertexList`
                Vertex list to draw, or ``None`` for all lists in this domain.

        """
----------
Output: 
.
            `vertex_list` : `VertexList`
                The `VertexList` to draw.

        """
        if vertex_list is None:
            vertex_list = self._vertex_list
        vertex_list.draw(mode)

    def _draw_with_colors(self, mode, vertex_list=None):
        """Draw vertices in the domain with colors.

        If `vertex_list` is not specified, all vertices in the domain are
        drawn.  This is the most efficient way to render primitives.

----------
Solution: 

        glPushClientAttrib(GL_CLIENT_VERTEX_ARRAY_BIT)
        for buffer, attributes in self.buffer_attributes:
            buffer.bind()
            for attribute in attributes:
                attribute.enable()
                attribute.set_pointer(attribute.buffer.ptr)
        if vertexbuffer._workaround_vbo_finish:
            glFinish()

        if vertex_list is not None:
            glDrawArrays(mode, vertex_list.start, vertex_list.count)
        else:
            starts, sizes = self.allocator.get_allocated_regions()
            primcount = len(starts)
            if primcount == 0:
                pass
            elif primcount == 1:
                # Common case
                glDrawArrays(mode, starts[0], int(sizes[0]))
            elif gl_info.have_version(1, 4):
                starts = (GLint * primcount)(*starts)
                sizes = (GLsizei * primcount)(*sizes)
                glMultiDrawArrays(mode, starts, sizes, primcount)
            else:
                for start, size in zip(starts, sizes):
                    glDrawArrays(mode, start, size)

        for buffer, _ in self.buffer_attributes:
            buffer.unbind()
        glPopClientAttrib()
--------------------
Prompt: 
def parse_rrset(e_rrset, connection, zone_id):
    """
    This a parser that allows the passing of any valid ResourceRecordSet
    tag. It will spit out the appropriate ResourceRecordSet object for the tag.

    :param lxml.etree._Element e_rrset: The root node of the etree parsed
        response from the API.
    :param Route53Connection connection: The connection instance used to
        query the API.
    :param str zone_id: The zone ID of the HostedZone these rrsets belong to.
    :rtype: ResourceRecordSet
    :returns: An instantiated ResourceRecordSet object.
    """
----------
Output: 
sets belong
        to.
    :rtype: Route53ResourceRecordSet
    :return: The ResourceRecordSet object that corresponds to the tag.
    """
    rrset = Route53ResourceRecordSet(connection, zone_id)
    rrset.name = e_rrset.find('Name').text
    rrset.type = e_rrset.find('Type').text
    rrset.ttl = int(e_rrset.find('TTL').text)
    rrset.resource_records = []

----------
Solution: 


    # This dict will be used to instantiate a ResourceRecordSet instance to yield.
    kwargs = {
        'connection': connection,
        'zone_id': zone_id,
    }
    rrset_type = None

    for e_field in e_rrset:
        # Cheesy way to strip off the namespace.
        tag_name = e_field.tag.split('}')[1]
        field_text = e_field.text

        if tag_name == 'Type':
            # Need to store this to determine which ResourceRecordSet
            # subclass to instantiate.
            rrset_type = field_text
            continue
        elif tag_name == 'AliasTarget':
            # A records have some special field values we need.
            alias_hosted_zone_id, alias_dns_name = parse_rrset_alias(e_field)
            kwargs['alias_hosted_zone_id'] = alias_hosted_zone_id
            kwargs['alias_dns_name'] = alias_dns_name
            # Alias A entries have no TTL.
            kwargs['ttl'] = None
            continue
        elif tag_name == 'ResourceRecords':
            kwargs['records'] = parse_rrset_record_values(e_field)
            continue

        # Map the XML tag name to a kwarg name.
        kw_name = RRSET_TAG_TO_KWARG_MAP[tag_name]
        # This will be the key/val pair used to instantiate the
        # ResourceRecordSet instance.
        kwargs[kw_name] = field_text

    if not rrset_type:
        raise Route53Error("No Type tag found in ListResourceRecordSetsResponse.")

    if 'records' not in kwargs:
        # Not all rrsets have records.
        kwargs['records'] = []

    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP[rrset_type]
    return RRSetSubclass(**kwargs)
--------------------
Prompt: 
def get_params(degrees, translate, scale_ranges, shears, img_size):
        """Get parameters for affine transformation

        Returns:
            sequence: params to be passed to the affine transformation
        """
----------
Output: 

        # 0.5 is added to the degrees to correct the rotation direction.
        if degrees is not None:
            # get_params() will convert degrees to radians for us
            degrees = get_params(degrees, 0.5)

        if translate is not None:
            max_dx = translate[0] * img_size[0]
            max_dy = translate[1] * img_size[1]

            translations = (np.random.uniform(-max_dx, max_dx),
                            np.random
----------
Solution: 

        angle = random.uniform(degrees[0], degrees[1])
        if translate is not None:
            max_dx = translate[0] * img_size[0]
            max_dy = translate[1] * img_size[1]
            translations = (np.round(random.uniform(-max_dx, max_dx)),
                            np.round(random.uniform(-max_dy, max_dy)))
        else:
            translations = (0, 0)

        if scale_ranges is not None:
            scale = random.uniform(scale_ranges[0], scale_ranges[1])
        else:
            scale = 1.0

        if shears is not None:
            shear = random.uniform(shears[0], shears[1])
        else:
            shear = 0.0

        return angle, translations, scale, shear
--------------------
Prompt: 
def topic(self, topic_id):
        """Retrive the topic with `topic_id` identifier.

        :param topic_id: identifier of the topic to retrieve
        """
----------
Output: 

        return self.get_topic(topic_id)

    def topics(self, **kwargs):
        """Retrieve the list of topics.

        :param kwargs: additional filters
        """
        return self.get_topics(**kwargs)

    def topic_posts(self, topic_id, **kwargs):
        """Retrieve the list of posts for a topic.

        :param topic_id: identifier of the topic
        :param kwargs: additional filters
        """
        return self.get_topic_posts(
----------
Solution: 

        params = {
            self.PKEY: self.api_key
        }

        # http://example.com/t/8.json
        response = self._call(self.TOPIC, topic_id,
                              params=params)

        return response
--------------------
Prompt: 
def decode(data):
    """
    Decodes a PackBit encoded data.
    """
----------
Output: 

    if not isinstance(data, bytes):
        raise TypeError("data must be bytes")

    if len(data) < 1:
        raise ValueError("data must be at least 1 byte long")

    result = []
    i = 0
    while i < len(data):
        if data[i] == 0:
            # 0x00: 0x00
            result.append(0)
            i += 1
        elif data[i] == 1:
            # 0x01
----------
Solution: 

    data = bytearray(data) # <- python 2/3 compatibility fix
    result = bytearray()
    pos = 0
    while pos < len(data):
        header_byte = data[pos]
        if header_byte > 127:
            header_byte -= 256
        pos += 1

        if 0 <= header_byte <= 127:
            result.extend(data[pos:pos+header_byte+1])
            pos += header_byte+1
        elif header_byte == -128:
            pass
        else:
            result.extend([data[pos]] * (1 - header_byte))
            pos += 1

    return bytes(result)
--------------------
Prompt: 
def get_conn(self):
        """Return the FileService object."""
----------
Output: 

        return self.conn

    def get_container(self, container_name):
        """Return the container object."""
        return self.conn.get_container(container_name)

    def get_object(self, container_name, object_name):
        """Return the object object."""
        return self.conn.get_object(container_name, object_name)

    def get_object_metadata(self, container_name, object_name):
        """Return the object metadata."""
        return self.conn.head_
----------
Solution: 

        conn = self.get_connection(self.conn_id)
        service_options = conn.extra_dejson
        return FileService(account_name=conn.login,
                           account_key=conn.password, **service_options)
--------------------
Prompt: 
def createClient(self):
        """Create a UDP connection to Riemann"""
----------
Output: 

        if self.client is None:
            self.client = RiemannClient(host=self.host, port=self.port)

    def send(self, event):
        """Send an event to Riemann"""
        self.createClient()
        self.client.send(event)

    def send_events(self, events):
        """Send a list of events to Riemann"""
        self.createClient()
        self.client.send_events(events)

    def send_state(self, state):
        """Send a state
----------
Solution: 

        server = self.config.get('server', '127.0.0.1')
        port = self.config.get('port', 5555)

        def connect(ip):
            self.protocol = riemann.RiemannUDP(ip, port)
            self.endpoint = reactor.listenUDP(0, self.protocol)

        d = reactor.resolve(server)
        d.addCallback(connect)
        return d
--------------------
Prompt: 
def _multi_gamma_sequence(self, a, p, name="multi_gamma_sequence"):
    """Creates sequence used in multivariate (di)gamma; shape = shape(a)+[p]."""
----------
Output: 

    with tf.name_scope(name):
      a = tf.convert_to_tensor(a, name="a")
      p = tf.convert_to_tensor(p, name="p")
      return tf.concat([a, p], axis=0)

  def _multi_gamma_log_prob(self, x, a, p, name="multi_gamma_log_prob"):
    """Computes log prob for multivariate (di)gamma distribution."""
    with tf.name_scope(name):
      a = tf
----------
Solution: 

    with self._name_scope(name):
      # Linspace only takes scalars, so we'll add in the offset afterwards.
      seq = tf.linspace(
          tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(
              p, tf.int32))
      return seq + tf.expand_dims(a, [-1])
--------------------
Prompt: 
def skus(self):
        """Instance depends on the API version:

           * 2017-06-01: :class:`SkusOperations<azure.mgmt.storage.v2017_06_01.operations.SkusOperations>`
           * 2017-10-01: :class:`SkusOperations<azure.mgmt.storage.v2017_10_01.operations.SkusOperations>`
           * 2018-02-01: :class:`SkusOperations<azure.mgmt.storage.v2018_02_01.operations.SkusOperations>`
           * 2018-03-01-preview: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01_preview.operations.SkusOperations>`
           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage.v2018_07_01.operations.SkusOperations>`
        """
----------
Output: 
-02-01: :class:`SkusOperations<azure.mgmt.storage.v2018_02_01.operations.SkusOperations>`
           * 2018-03-01-preview: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01_preview.operations.SkusOperations>`
           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage
----------
Solution: 

        api_version = self._get_api_version('skus')
        if api_version == '2017-06-01':
            from .v2017_06_01.operations import SkusOperations as OperationClass
        elif api_version == '2017-10-01':
            from .v2017_10_01.operations import SkusOperations as OperationClass
        elif api_version == '2018-02-01':
            from .v2018_02_01.operations import SkusOperations as OperationClass
        elif api_version == '2018-03-01-preview':
            from .v2018_03_01_preview.operations import SkusOperations as OperationClass
        elif api_version == '2018-07-01':
            from .v2018_07_01.operations import SkusOperations as OperationClass
        else:
            raise NotImplementedError("APIVersion {} is not available".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))
--------------------
Prompt: 
def delete_local_file(file_name):
    """
    Deletes the file associated with the file_name passed from local storage.
    
    :param str file_name: Filename of the file to be deleted
    :return str: Filename of the file that was just deleted
    """
----------
Output: 

    if os.path.exists(file_name):
        os.remove(file_name)
        return file_name
    else:
        return None


def delete_remote_file(file_name):
    """
    Deletes the file associated with the file_name passed from remote storage.
    
    :param str file_name: Filename of the file to be deleted
    :return str: Filename of the file that was just deleted
    """
    if os.path.exists(file_name):
        os.remove(
----------
Solution: 


    try:
        os.remove(file_name)
        log.info(f"Deletion for {file_name} has finished")
        return file_name
    except OSError:
        pass
--------------------
Prompt: 
def output_json(self, file_name='/tmp/ND.json'):
        """
        Arguments:
            file_name(str : '/tmp/ND.json'): The file name to store the json to

        Returns:
            None
        """
----------
Output: 

        with open(file_name, 'w') as f:
            json.dump(self.to_dict(), f)

    def to_dict(self):
        """
        Returns:
            dict: A dictionary representation of the ND object
        """
        return {
            'name': self.name,
            'description': self.description,
            'version': self.version,
            'author': self.author,
            'email': self.email,
            'url': self.url,
            'license': self.lic
----------
Solution: 

        complete_example = (
            self.dataset, self.project, self.channels, self.metadata)
        data = json.loads(self.nd_json(*complete_example))

        # self.verify_json(data)
        self.verify_path(data, VERIFY_BY_SLICE)

        f = open(file_name, 'w')
        f.write(str(data))
        f.close()
--------------------
Prompt: 
def open_graph(self, **args):
        """open a vcg graph
        """
----------
Output: 

        if args.get('filename') is None:
            raise Exception('filename not specified')
        filename = args.get('filename')
        if args.get('format') is None:
            raise Exception('format not specified')
        format = args.get('format')
        if args.get('flags') is None:
            raise Exception('flags not specified')
        flags = args.get('flags')
        if args.get('mask') is None:
            raise Exception('mask not specified')
        mask = args.get('mask')
        if args
----------
Solution: 

        self._stream.write("%sgraph:{\n" % self._indent)
        self._inc_indent()
        self._write_attributes(GRAPH_ATTRS, **args)
--------------------
Prompt: 
def remove(cls, name: sym.Symbol) -> Optional["Namespace"]:
        """Remove the namespace bound to the symbol `name` in the global
        namespace cache and return that namespace.
        Return None if the namespace did not exist in the cache."""
----------
Output: 

        return cls._cache.pop(name, None)

    @classmethod
    def get(cls, name: sym.Symbol) -> Optional["Namespace"]:
        """Get the namespace bound to the symbol `name` in the global
        namespace cache. Return None if the namespace did not exist in the
        cache."""
        return cls._cache.get(name)

    @classmethod
    def clear(cls) -> None:
        """Clear the global namespace cache."""
        cls._cache.clear()

    @classmethod
----------
Solution: 

        while True:
            oldval: lmap.Map = cls._NAMESPACES.deref()
            ns: Optional[Namespace] = oldval.entry(name, None)
            newval = oldval
            if ns is not None:
                newval = oldval.dissoc(name)
            if cls._NAMESPACES.compare_and_set(oldval, newval):
                return ns
--------------------
Prompt: 
def einsum_vecmul_index(gate_indices, number_of_qubits):
    """Return the index string for Numpy.eignsum matrix-vector multiplication.

    The returned indices are to perform a matrix multiplication A.v where
    the matrix A is an M-qubit matrix, vector v is an N-qubit vector, and
    M <= N, and identity matrices are implied on the subsystems where A has no
    support on v.

    Args:
        gate_indices (list[int]): the indices of the right matrix subsystems
                                  to contract with the left matrix.
        number_of_qubits (int): the total number of qubits for the right matrix.

    Returns:
        str: An indices string for the Numpy.einsum function.
    """
----------
Output: 
the gate to be applied
        number_of_qubits (int): the number of qubits in the system

    Returns:
        str: the index string for the einsum call
    """
    # pylint: disable=invalid-name
    M = len(gate_indices)
    N = number_of_qubits
    if M > N:
        raise ValueError("The number of qubits in the gate is larger than the number of qubits in the system.")

    # The indices of the gate
    indices = []
    for i
----------
Solution: 


    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,
                                                                    number_of_qubits)

    # Combine indices into matrix multiplication string format
    # for numpy.einsum function
    return "{mat_l}{mat_r}, ".format(mat_l=mat_l, mat_r=mat_r) + \
           "{tens_lin}->{tens_lout}".format(tens_lin=tens_lin,
                                            tens_lout=tens_lout)
--------------------
Prompt: 
def get_client(self):
        """
        Obtains a client for use, whether an existing unused client
        or a brand new one if none are available.
        """
----------
Output: 

        if self.clients:
            return self.clients.pop()
        else:
            return self.create_client()

    def release_client(self, client):
        """
        Releases a client back to the pool.
        """
        self.clients.append(client)

    def create_client(self):
        """
        Creates a new client.
        """
        client = self.client_class(self.host, self.port,
                                   self.username, self.password,
                                
----------
Solution: 

        client = None
        try:
            client = self.clients.get(block=False)
        except queue.Empty:
            pass
        if not client:
            self.client_id += 1
            kwargs = dict(self.kwargs)
            kwargs['verbose_id'] = kwargs.get(
                'verbose_id', '') + str(self.client_id)
            client = self.client_class(*self.args, **kwargs)
        return client
--------------------
Prompt: 
def _sem_open(name, value=None):
    """ Construct or retrieve a semaphore with the given name

    If value is None, try to retrieve an existing named semaphore.
    Else create a new semaphore with the given value
    """
----------
Output: 

    if value is None:
        return _sem_open_existing(name)
    else:
        return _sem_open_new(name, value)


def _sem_open_existing(name):
    """ Construct a semaphore with the given name

    If the named semaphore already exists, raise an exception.
    """
    sem_handle = ctypes.windll.kernel32.OpenSemaphoreW(
        SEMAPHORE_ALL_ACCESS,
        False,
        name
    )
----------
Solution: 

    if value is None:
        handle = pthread.sem_open(ctypes.c_char_p(name), 0)
    else:
        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,
                                  ctypes.c_int(value))

    if handle == SEM_FAILURE:
        e = ctypes.get_errno()
        if e == errno.EEXIST:
            raise FileExistsError("a semaphore named %s already exists" % name)
        elif e == errno.ENOENT:
            raise FileNotFoundError('cannot find semaphore named %s' % name)
        elif e == errno.ENOSYS:
            raise NotImplementedError('No semaphore implementation on this '
                                      'system')
        else:
            raiseFromErrno()

    return handle
--------------------
Prompt: 
def delete(self, request, *args, **kwargs):
        """Delete auth token when `delete` request was issued."""
----------
Output: 

        self.object = self.get_object()
        self.object.delete()
        return Response(status=status.HTTP_204_NO_CONTENT)


class UserViewSet(viewsets.ModelViewSet):
    """
    API endpoint that allows users to be viewed or edited.
    """
    queryset = User.objects.all().order_by('-date_joined')
    serializer_class = UserSerializer
    permission_classes = (permissions.IsAuthenticated,)

    def get_queryset
----------
Solution: 

        # Logic repeated from DRF because one cannot easily reuse it
        auth = get_authorization_header(request).split()

        if not auth or auth[0].lower() != b'token':
            return response.Response(status=status.HTTP_400_BAD_REQUEST)

        if len(auth) == 1:
            msg = 'Invalid token header. No credentials provided.'
            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)
        elif len(auth) > 2:
            msg = 'Invalid token header. Token string should not contain spaces.'
            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)

        try:
            token = self.model.objects.get(key=auth[1])
        except self.model.DoesNotExist:
            pass
        else:
            token.delete()
            signals.user_logged_out.send(
                type(self),
                user=token.user,
                request=request,
            )
        return response.Response(status=status.HTTP_204_NO_CONTENT)
--------------------
Prompt: 
def load_pkcs7_data(type, buffer):
    """
    Load pkcs7 data from the string *buffer* encoded with the type
    *type*.

    :param type: The file type (one of FILETYPE_PEM or FILETYPE_ASN1)
    :param buffer: The buffer with the pkcs7 data.
    :return: The PKCS7 object
    """
----------
Output: 

    if type == FILETYPE_PEM:
        return load_pkcs7_pem(buffer)
    elif type == FILETYPE_ASN1:
        return load_pkcs7_asn1(buffer)
    else:
        raise ValueError("Unknown file type")


def load_pkcs7_pem(buffer):
    """
    Load pkcs7 data from the string *buffer* encoded with pem.

    :param buffer: The buffer with the pkcs7 data.
    :return: The PKCS
----------
Solution: 

    if isinstance(buffer, _text_type):
        buffer = buffer.encode("ascii")

    bio = _new_mem_buf(buffer)

    if type == FILETYPE_PEM:
        pkcs7 = _lib.PEM_read_bio_PKCS7(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)
    elif type == FILETYPE_ASN1:
        pkcs7 = _lib.d2i_PKCS7_bio(bio, _ffi.NULL)
    else:
        raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1")

    if pkcs7 == _ffi.NULL:
        _raise_current_error()

    pypkcs7 = PKCS7.__new__(PKCS7)
    pypkcs7._pkcs7 = _ffi.gc(pkcs7, _lib.PKCS7_free)
    return pypkcs7
--------------------
Prompt: 
def create_sprites_dataset(characters, actions, directions, channels=3,
                           length=8, shuffle=False, fake_data=False):
  """Creates a tf.data pipeline for the sprites dataset.

  Args:
    characters: A list of (skin, hair, top, pants) tuples containing
      relative paths to the sprite png image for each attribute.
    actions: A list of Actions.
    directions: A list of Directions.
    channels: Number of image channels to yield.
    length: Desired length of the sequences.
    shuffle: Whether or not to shuffle the characters and sequences
      start frame.
    fake_data: Boolean for whether or not to yield synthetic data.

  Returns:
    A tf.data.Dataset yielding (seq, skin label index, hair label index,
    top label index, pants label index, action label index, skin label
    name, hair label_name, top label name, pants label name, action
    label name) tuples.
  """
----------
Output: 
length: Number of frames to yield.
    shuffle: Whether to shuffle the dataset.
    fake_data: Whether to use fake data.

  Returns:
    A tf.data.Dataset.
  """
  if fake_data:
    return _create_fake_sprites_dataset(characters, actions, directions,
                                        channels, length, shuffle)
  else:
    return _create_real_sprites_dataset(characters, actions, directions,
                                        channels, length, shuffle)


def
----------
Solution: 

  if fake_data:
    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])
  else:
    basedir = download_sprites()

  action_names = [action.name for action in actions]
  action_metadata = [(action.start_row, action.frames) for action in actions]

  direction_rows = [direction.row_offset for direction in directions]

  chars = tf.data.Dataset.from_tensor_slices(characters)
  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()
  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()
  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()

  if shuffle:
    chars = chars.shuffle(len(characters))

  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))

  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))
  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))
  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))
  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))
  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))

  def process_example(attrs, act_name, act_metadata, dir_row_offset):
    
--------------------
Prompt: 
def _prepare_subsection(data, sections, wrapper):
    """
    Creates a subsection ready to be filled.
    :param data: Semi-parsed dictionary.
    :param sections: A list of sections.
    :param wrapper: A wrapping object for key-value pairs.
    :return: A newly created subsection.
    """
----------
Output: 

    subsection = {}
    for key, value in data.items():
        if key in sections:
            subsection[key] = _prepare_subsection(value, sections, wrapper)
        else:
            subsection[key] = wrapper(key, value)
    return subsection


def _prepare_section(data, sections, wrapper):
    """
    Creates a section ready to be filled.
    :param data: Semi-parsed dictionary.
    :param sections: A list of sections.
    :param wrapper: A wrapping object
----------
Solution: 

    current = data
    for i in sections[:-1]:
        current = current[i]

    current[sections[-1]] = wrapper()
    return current[sections[-1]]
--------------------
Prompt: 
def render_tag(self, context, *tag_args, **tag_kwargs):
        """
        Rendering of the tag. It either assigns the value as variable, or renders it.
        """
----------
Output: 

        # get the value
        value = self.value.resolve(context)
        # get the name
        name = self.name.resolve(context)
        # get the type
        type = self.type.resolve(context)
        # get the options
        options = self.options.resolve(context)
        # get the id
        id = self.id.resolve(context)
        # get the class
        class_ = self.class_.resolve(context)
        # get the style
        style = self.style.resolve(context)

----------
Solution: 

        # Be very explicit about which base functionality is used:
        # Using super() for mixin support will not work nicely anyway here.
        if self.as_var:
            # Assign the value in the parent context
            return BaseAssignmentNode.render_tag(self, context, *tag_args, **tag_kwargs)
        else:
            # Render the output using the BaseInclusionNode features
            return BaseInclusionNode.render_tag(self, context, *tag_args, **tag_kwargs)
--------------------
Prompt: 
def verify_token(self, request, *args, **kwargs):
        """
        Use `token` to allow one-time access to a view.

        Set the user as a class attribute or raise an `InvalidExpiredToken`.

        Token expiry can be set in `settings` with `VERIFY_ACCOUNT_EXPIRY` and is
        set in seconds.
        """
----------
Output: 

        token = kwargs.get("token")
        if not token:
            raise InvalidExpiredToken("Token not provided")
        try:
            payload = jwt.decode(token, settings.SECRET_KEY)
        except jwt.ExpiredSignatureError:
            raise InvalidExpiredToken("Token has expired")
        except jwt.InvalidTokenError:
            raise InvalidExpiredToken("Invalid token")
        user = get_user_model().objects.get(pk=payload["user_id"])
        if user.is_active
----------
Solution: 

        User = get_user_model()

        try:
            max_age = settings.VERIFY_ACCOUNT_EXPIRY
        except AttributeError:
            max_age = self.DEFAULT_VERIFY_ACCOUNT_EXPIRY

        try:
            email_data = signing.loads(kwargs['token'], max_age=max_age)
        except signing.BadSignature:
            raise self.invalid_exception_class

        email = email_data['email']

        try:
            self.user = User.objects.get_by_natural_key(email)
        except User.DoesNotExist:
            raise self.invalid_exception_class

        if self.user.email_verified:
            raise self.permission_denied_class
--------------------
Prompt: 
def partitions(l, partition_size):
    """
    >>> list(partitions([], 10))
    []
    >>> list(partitions([1,2,3,4,5], 1))
    [[1], [2], [3], [4], [5]]
    >>> list(partitions([1,2,3,4,5], 2))
    [[1, 2], [3, 4], [5]]
    >>> list(partitions([1,2,3,4,5], 5))
    [[1, 2, 3, 4, 5]]

    :param list l: List to be partitioned
    :param int partition_size: Size of partitions
    """
----------
Output: 
[[1, 2, 3, 4, 5]]
    >>> list(partitions([1,2,3,4,5], 6))
    []
    >>> list(partitions([1,2,3,4,5], 3))
    [[1, 2, 3], [4, 5]]
    >>> list(partitions([1,2,3,4,5], 4))
    [[1, 2, 3, 4], [5]]
    >>> list(partitions([1,2,3
----------
Solution: 

    for i in xrange(0, len(l), partition_size):
        yield l[i:i + partition_size]
--------------------
Prompt: 
def download_layers(self, repo_name, digest=None, destination=None):
    """ download layers is a wrapper to do the following for a client loaded
        with a manifest for an image:
      
        1. use the manifests to retrieve list of digests (get_digests)
        2. atomically download the list to destination (get_layers)

        This function uses the MultiProcess client to download layers
        at the same time.
    """
----------
Output: 

    # get the list of digests
    digests = self.get_digests(repo_name, digest)
    # download the layers
    self.get_layers(repo_name, digests, destination)

def get_digests(self, repo_name, digest=None):
    """ get_digests is a wrapper to do the following for a client loaded
        with a manifest for an image:
      
        1. use the manifests to retrieve list of digests (get_digests)
        2. atomically download the list
----------
Solution: 

    from sregistry.main.workers import ( Workers, download_task )

    # 1. Get manifests if not retrieved
    if not hasattr(self, 'manifests'):
        self._get_manifests(repo_name, digest)

    # Obtain list of digets, and destination for download
    digests = self._get_digests()
    destination = self._get_download_cache(destination)

    # Create multiprocess download client
    workers = Workers()

    # Download each layer atomically
    tasks = []
    layers = []
    for digest in digests:

        targz = "%s/%s.tar.gz" % (destination, digest)

        # Only download if not in cache already
        if not os.path.exists(targz):
            url = "%s/%s/blobs/%s" % (self.base, repo_name, digest)
            tasks.append((url, self.headers, targz))
        layers.append(targz)

    # Download layers with multiprocess workers
    if len(tasks) > 0:
        download_layers = workers.run(func=download_task,
                                      tasks=tasks)
    # Create the metadata tar
    metadata = self._create_metadata_tar(destination)
    if metadata is not None:
        layers.append(metadata)


    return layers
--------------------
Prompt: 
def process_item(self, item, spider):
        """
        Process single item. Add item to items and then upload to S3 if size of items
        >= max_chunk_size.
        """
----------
Output: 

        self.items.append(item)
        if len(self.items) >= self.max_chunk_size:
            self.upload_items()

    def upload_items(self):
        """
        Upload items to S3.
        """
        if not self.items:
            return
        try:
            self.s3_client.put_object(
                Bucket=self.bucket,
                Key=self.key,
                Body=json.dumps(self.items, cls=DjangoJSONEncoder),
----------
Solution: 

        self.items.append(item)
        if len(self.items) >= self.max_chunk_size:
            self._upload_chunk(spider)

        return item
--------------------
Prompt: 
def handle_print(self, signame, set_print):
        """Set whether we print or not when this signal is caught."""
----------
Output: 

        if signame == 'SIGUSR1':
            self.print_on_sigusr1 = set_print
        elif signame == 'SIGUSR2':
            self.print_on_sigusr2 = set_print
        else:
            raise ValueError('Unknown signal name: %s' % signame)

    def handle_signal(self, signame, set_print):
        """Set whether we print or not when this signal is caught."""
        if signame == 'SIGUSR1':
            self.print
----------
Solution: 

        if set_print:
            self.sigs[signame].print_method = self.dbgr.intf[-1].msg
        else:
            self.sigs[signame].print_method = None
            pass
        return set_print
--------------------
Prompt: 
def _trj_check_version(self, version, python, force):
        """Checks for version mismatch

        Raises a VersionMismatchError if version of loaded trajectory and current pypet version
        do not match. In case of `force=True` error is not raised only a warning is emitted.

        """
----------
Output: 

        if not force:
            if version != self.version:
                raise VersionMismatchError(
                    'Version mismatch: Trajectory was saved with pypet version %s, '
                    'but current pypet version is %s' % (version, self.version))

    def _trj_check_python(self, python, force):
        """Checks for python version mismatch

        Raises a VersionMismatchError if python version of loaded trajectory and current python
        version do not match. In case of `force
----------
Solution: 

        curr_python = pypetconstants.python_version_string

        if (version != VERSION or curr_python != python) and not force:
            raise pex.VersionMismatchError('Current pypet version is %s used under python %s '
                                           '  but your trajectory'
                                           ' was created with version %s and python %s.'
                                           ' Use >>force=True<< to perform your load regardless'
                                           ' of version mismatch.' %
                                           (VERSION, curr_python, version, python))
        elif version != VERSION or curr_python != python:
            self._logger.warning('Current pypet version is %s with python %s but your trajectory'
                                 ' was created with version %s under python %s.'
                                 ' Yet, you enforced the load, so I will'
                                 ' handle the trajectory despite the'
                                 ' version mismatch.' %
                                 (VERSION, curr_python, version, python))
--------------------
Prompt: 
def get_branch_mutation_matrix(self, node, full_sequence=False):
        """uses results from marginal ancestral inference to return a joint
        distribution of the sequence states at both ends of the branch.

        Parameters
        ----------
        node : Phylo.clade
            node of the tree
        full_sequence : bool, optional
            expand the sequence to the full sequence, if false (default)
            the there will be one mutation matrix for each column in the
            reduced alignment

        Returns
        -------
        numpy.array
            an Lxqxq stack of matrices (q=alphabet size, L (reduced)sequence length)
        """
----------
Output: 
       mutation_matrix : ndarray
            joint distribution of the sequence states at both ends of the branch
        """
        # get the marginal ancestral inference results
        marginal_ancestral_inference_results = self.get_marginal_ancestral_inference_results(node)
        # get the mutation matrix
        mutation_matrix = self.get_mutation_matrix(marginal_ancestral_inference_results, full_sequence)
        return mutation_matrix

    def get_mutation_matrix
----------
Solution: 

        pp,pc = self.marginal_branch_profile(node)

        # calculate pc_i [e^Qt]_ij pp_j for each site
        expQt = self.gtr.expQt(self._branch_length_to_gtr(node))
        if len(expQt.shape)==3: # site specific model
            mut_matrix_stack = np.einsum('ai,aj,ija->aij', pc, pp, expQt)
        else:
            mut_matrix_stack = np.einsum('ai,aj,ij->aij', pc, pp, expQt)

        # normalize this distribution
        normalizer = mut_matrix_stack.sum(axis=2).sum(axis=1)
        mut_matrix_stack = np.einsum('aij,a->aij', mut_matrix_stack, 1.0/normalizer)

        # expand to full sequence if requested
        if full_sequence:
            return mut_matrix_stack[self.full_to_reduced_sequence_map]
        else:
            return mut_matrix_stack
--------------------
Prompt: 
def meas_gate(self, circuit, qreg, op):
        """
        Add measurement gates to a circuit.

        Args:
            circuit (QuantumCircuit): circuit to add measurement to.
            qreg (tuple(QuantumRegister,int)): quantum register being measured.
            op (str): the basis label for the measurement.
        """
----------
Output: 

        if op == 'X':
            circuit.measure(qreg[0], qreg[1])
        elif op == 'Y':
            circuit.measure(qreg[0], qreg[1])
        elif op == 'Z':
            circuit.measure(qreg[0], qreg[1])
        else:
            raise ValueError("Invalid measurement basis.")

    def meas_circuit(self, qregs, ops):
        """
        Create a circuit with measurement gates.

        Args:
            qregs (
----------
Solution: 

        if self.meas_fun is None:
            pass
        else:
            self.meas_fun(circuit, qreg, op)
--------------------
Prompt: 
def log(wave):
    r"""
    Return the natural logarithm of a waveform's dependent variable vector.

    :param wave: Waveform
    :type  wave: :py:class:`peng.eng.Waveform`

    :rtype: :py:class:`peng.eng.Waveform`

    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]
    .. Auto-generated exceptions documentation for peng.wave_functions.log

    :raises:
     * RuntimeError (Argument \`wave\` is not valid)

     * ValueError (Math domain error)

    .. [[[end]]]
    """
----------
Output: 

    .. [[[end]]]
    """
    return log(wave)


def log10(wave):
    r"""
    Return the base 10 logarithm of a waveform's dependent variable vector.

    :param wave: Waveform
    :type  wave: :py:class:`peng.eng.Waveform`

    :rtype: :py:class:`peng.eng.Waveform`

    .. [[[cog cog.out(exobj_eng.get_sph
----------
Solution: 

    pexdoc.exh.addex(
        ValueError, "Math domain error", bool((min(wave._dep_vector) <= 0))
    )
    return _operation(wave, "log", "", np.log)
--------------------
Prompt: 
async def save_tracks(self, *tracks):
        """Save one or more tracks to the current user’s ‘Your Music’ library.

        Parameters
        ----------
        tracks : Sequence[Union[Track, str]]
            A sequence of track objects or spotify IDs
        """
----------
Output: 

        if not tracks:
            raise ValueError('No tracks provided')

        tracks = [self._get_id(track) for track in tracks]
        return self._put(
            'me/tracks',
            json={'ids': tracks},
            params={'market': self.country},
        )

    def remove_tracks(self, *tracks):
        """Remove one or more tracks from the current user’s ‘Your Music’ library.

        Parameters
        ----------
        tracks : Sequence[Union[Track
----------
Solution: 

        _tracks = [(obj if isinstance(obj, str) else obj.id) for obj in tracks]
        await self.user.http.save_tracks(','.join(_tracks))
--------------------
Prompt: 
def get_logging_level():
    """get_logging_level will configure a logging to standard out based on the user's
    selected level, which should be in an environment variable called
    MESSAGELEVEL. if MESSAGELEVEL is not set, the maximum level
    (5) is assumed (all messages).
    """
----------
Output: 

    level = 5
    try:
        level = int(os.environ["MESSAGELEVEL"])
    except KeyError:
        pass
    except ValueError:
        pass
    return level


def get_logging_level_name(level):
    """get_logging_level_name will return the name of the logging level
    """
    if level == 0:
        return "CRITICAL"
    elif level == 1:
        return "ERROR"
    elif level == 2:
        return "W
----------
Solution: 

    level = os.environ.get("MESSAGELEVEL", INFO)

    # User knows logging levels and set one
    if isinstance(level, int):
        return level

    # Otherwise it's a string
    if level == "CRITICAL":
        return CRITICAL
    elif level == "ABORT":
        return ABORT
    elif level == "ERROR":
        return ERROR
    elif level == "WARNING":
        return WARNING
    elif level == "LOG":
        return LOG
    elif level == "INFO":
        return INFO
    elif level == "QUIET":
        return QUIET
    elif level.startswith("VERBOSE"):
        return VERBOSE3
    elif level == "LOG":
        return LOG
    elif level == "DEBUG":
        return DEBUG

    return level
--------------------
Prompt: 
def format_task_numbers_with_links(tasks):
    """Returns formatting for the tasks section of asana."""
----------
Output: 

    formatted_tasks = []
    for task in tasks:
        formatted_tasks.append(f"[{task['name']}]({task['permalink']})")
    return formatted_tasks


def format_section_header(section_name):
    """Returns formatting for the section headers of asana."""
    return f"**{section_name}**"


def format_section_body(section_name, section_body):
    """Returns formatting for the section bodies of asana."""
    return f"{section_
----------
Solution: 


    project_id = data.get('asana-project', None)

    def _task_format(task_id):
        if project_id:
            asana_url = tool.ToolApp.make_asana_url(project_id, task_id)
            return "[#%d](%s)" % (task_id, asana_url)
        else:
            return "#%d" % task_id

    return "\n".join([_task_format(tid) for tid in tasks])
--------------------
Prompt: 
def _mode(self):
    """The mode of the von Mises-Fisher distribution is the mean direction."""
----------
Output: 

    return self.mean()

  def _mean(self):
    """The mean of the von Mises-Fisher distribution is the mean direction."""
    return self.mean_direction

  def _variance(self):
    """The variance of the von Mises-Fisher distribution is the variance of the
    mean direction.
    """
    return self.mean_direction.variance()

  def _stddev(self):
    """The standard deviation of the von Mises-Fisher distribution is the
    standard deviation of the mean direction
----------
Solution: 

    return (self.mean_direction +
            tf.zeros_like(self.concentration)[..., tf.newaxis])
--------------------
Prompt: 
def is_detached(self):
        """Check if the repo is in a detached state.

        The repository is in a detached state when HEAD is not a symbolic
        reference.

        :returns: whether the repository is detached or not

        :raises RepositoryError: when an error occurs checking the state
            of the repository
        """
----------
Output: 

        try:
            return self.repo.head.is_detached
        except (AttributeError, ValueError):
            raise RepositoryError(
                "Unable to check if repository is detached"
            )

    def is_dirty(self):
        """Check if the repo is dirty.

        The repository is dirty when there are uncommitted changes.

        :returns: whether the repository is dirty or not

        :raises RepositoryError: when an error occurs checking the state
            of the repository
        """
        try:
----------
Solution: 

        cmd_sym = ['git', 'symbolic-ref', 'HEAD']

        try:
            self._exec(cmd_sym, cwd=self.dirpath, env=self.gitenv)
        except RepositoryError as e:
            if e.msg.find("ref HEAD is not a symbolic ref") == -1:
                raise e
            return True
        else:
            return False
--------------------
Prompt: 
def publish_events(
            self, topic_hostname, events, custom_headers=None, raw=False, **operation_config):
        """Publishes a batch of events to an Azure Event Grid topic.

        :param topic_hostname: The host name of the topic, e.g.
         topic1.westus2-1.eventgrid.azure.net
        :type topic_hostname: str
        :param events: An array of events to be published to Event Grid.
        :type events: list[~azure.eventgrid.models.EventGridEvent]
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :return: None or ClientRawResponse if raw=true
        :rtype: None or ~msrest.pipeline.ClientRawResponse
        :raises:
         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`
        """
----------
Output: 
GridEvent]
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :return: None or ClientRawResponse if raw=true
        :rtype: None or ~msrest.pipeline.ClientRawResponse
        :raises:
         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`
        """
       
----------
Solution: 

        # Construct URL
        url = self.publish_events.metadata['url']
        path_format_arguments = {
            'topicHostname': self._serialize.url("topic_hostname", topic_hostname, 'str', skip_quote=True)
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.api_version", self.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if custom_headers:
            header_parameters.update(custom_headers)

        # Construct body
        body_content = self._serialize.body(events, '[EventGridEvent]')

        # Construct and send request
        request = self._client.post(url, query_parameters)
        response = self._client.send(
            request, header_parameters, body_content, stream=False, **operation_config)

        if response.status_code not in [200]:
            raise HttpOperationError(self._deserialize, response)

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response
--------------------
Prompt: 
def get_imap_capabilities(server):
    """
    Returns a list of an IMAP server's capabilities

    Args:
        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient

    Returns (list): A list of capabilities
    """
----------
Output: 

    return server.capabilities


def get_imap_status(server):
    """
    Returns the status of an IMAP server

    Args:
        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient

    Returns (dict): A dictionary of status
    """
    return server.status()


def get_imap_folders(server):
    """
    Returns a list of folders on an IMAP server

    Args:
        server (imapclient.IMAPClient
----------
Solution: 


    capabilities = list(map(str, list(server.capabilities())))
    for i in range(len(capabilities)):
        capabilities[i] = str(capabilities[i]).replace("b'",
                                                       "").replace("'",
                                                                   "")
    logger.debug("IMAP server supports: {0}".format(capabilities))

    return capabilities
--------------------
Prompt: 
def update_path(self, board, color, path):
        """ update win/loss count along path """
----------
Output: 

        for i in range(len(path) - 1):
            if path[i] == path[i + 1]:
                if path[i] == color:
                    board[path[i]][path[i + 1]] += 1
                else:
                    board[path[i]][path[i + 1]] -= 1

    def check_win(self, board, color):
        """ check if the color has won """
        for i in range(self.size):
            if board[i][i]
----------
Solution: 

        wins = board.score(BLACK) >= board.score(WHITE)
        for node in path:
            if color == BLACK:
                color = WHITE
            else:
                color = BLACK
            if wins == (color == BLACK):
                node.wins += 1
            else:
                node.losses += 1
            if node.parent:
                node.parent.bestchild = node.parent.best_child()
--------------------
Prompt: 
def read(self):
        """
        Load the metrics file from the given path
        """
----------
Output: 

        if not self.path:
            return

        try:
            with open(self.path, 'r') as f:
                self.metrics = json.load(f)
        except IOError:
            self.metrics = {}

    def write(self):
        """
        Save the metrics file to the given path
        """
        if not self.path:
            return

        try:
            with open(self.path, 'w') as f:
                json.dump(self.metrics, f)
       
----------
Solution: 

        f = open(self.path, "r")
        self.manifest_json = f.read()
--------------------
Prompt: 
def update(self, teamId, name=None, **request_parameters):
        """Update details for a team, by ID.

        Args:
            teamId(basestring): The team ID.
            name(basestring): A user-friendly name for the team.
            **request_parameters: Additional request parameters (provides
                support for parameters that may be added in the future).

        Returns:
            Team: A Team object with the updated Webex Teams team details.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError: If the Webex Teams cloud returns an error.

        """
----------
Output: 
input is invalid.
            ApiError: If the API returns an error.
        """
        if not teamId:
            raise TypeError("Missing required parameter: teamId")

        if not name:
            raise TypeError("Missing required parameter: name")

        if not isinstance(teamId, basestring):
            raise TypeError("Expected a string for parameter 'teamId'")

        if not isinstance(name, basestring):
            raise TypeError("Expected a string for parameter 'name'")

        url = self.build_url
----------
Solution: 

        check_type(teamId, basestring, may_be_none=False)
        check_type(name, basestring)

        put_data = dict_from_items_with_values(
            request_parameters,
            name=name,
        )

        # API request
        json_data = self._session.put(API_ENDPOINT + '/' + teamId,
                                      json=put_data)

        # Return a team object created from the response JSON data
        return self._object_factory(OBJECT_TYPE, json_data)
--------------------
Prompt: 
def _odds_val(self):
        """Determine a new random value derived from the
        defined :any:`gramfuzz.fields.Field.odds` value.

        :returns: The derived value
        """
----------
Output: 

        if self.odds is None:
            return None
        if self.odds == 0:
            return None
        if self.odds == 1:
            return self.value
        if self.odds == 2:
            return self.value
        if self.odds == 3:
            return self.value
        if self.odds == 4:
            return self.value
        if self.odds == 5:
            return self.value
        if self.odds == 6:
            return
----------
Solution: 

        if len(self.odds) == 0:
            self.odds = [(1.00, [self.min, self.max])]

        rand_val = rand.random()
        total = 0
        for percent,v in self.odds:
            if total <= rand_val < total+percent:
                found_v = v
                break
            total += percent

        res = None
        if isinstance(v, (tuple,list)):
            rand_func = rand.randfloat if type(v[0]) is float else rand.randint

            if len(v) == 2:
                res = rand_func(v[0], v[1])
            elif len(v) == 1:
                res = v[0]
        else:
            res = v

        return res
--------------------
Prompt: 
def _check_key_value_types(obj, key_type, value_type, key_check=isinstance, value_check=isinstance):
    """Ensures argument obj is a dictionary, and enforces that the keys/values conform to the types
    specified by key_type, value_type.
    """
----------
Output: 

    if not isinstance(obj, dict):
        raise TypeError("obj must be a dictionary")
    for key, value in obj.items():
        if not key_check(key, key_type):
            raise TypeError("obj keys must be of type {}".format(key_type))
        if not value_check(value, value_type):
            raise TypeError("obj values must be of type {}".format(value_type))


def _check_key_value_types_or_none(obj, key_type, value_type, key_
----------
Solution: 

    if not isinstance(obj, dict):
        raise_with_traceback(_type_mismatch_error(obj, dict))

    if key_type is str:
        key_type = string_types

    if value_type is str:
        value_type = string_types

    for key, value in obj.items():
        if key_type and not key_check(key, key_type):
            raise_with_traceback(
                CheckError(
                    'Key in dictionary mismatches type. Expected {key_type}. Got {obj_repr}'.format(
                        key_type=repr(key_type), obj_repr=repr(key)
                    )
                )
            )
        if value_type and not value_check(value, value_type):
            raise_with_traceback(
                CheckError(
                    'Value in dictionary mismatches expected type for key {key}. Expected value '
                    'of type {vtype}. Got value {value} of type {obj_type}.'.format(
                        vtype=repr(value_type), obj_type=type(value), key=key, value=value
                    )
                )
            )
    return obj
--------------------
Prompt: 
def _clear_temp(self):
        """
        Clear temp directory from created csv and ods files during
        communicator operations.
        """
----------
Output: 

        if os.path.exists(self.temp_dir):
            for file in os.listdir(self.temp_dir):
                if file.endswith(".csv") or file.endswith(".ods"):
                    os.remove(os.path.join(self.temp_dir, file))

    def _get_temp_file(self, file_type):
        """
        Get a temporary file name for a given file type.

        :param file_type: File type to get a temporary file name for.
        :type file
----------
Solution: 

        temp_files = [LOCAL_ODS, GDOCS_TRANS_CSV, GDOCS_META_CSV,
                      LOCAL_TRANS_CSV, LOCAL_META_CSV]
        for temp_file in temp_files:
            file_path = os.path.join(self.temp_path, temp_file)
            if os.path.exists(file_path):
                os.remove(file_path)
--------------------
Prompt: 
def to_zyz(self):
        """Converts a unit-length quaternion to a sequence
        of ZYZ Euler angles.

        Returns:
            ndarray: Array of Euler angles.
        """
----------
Output: 

        q = self.data
        return np.array([np.arctan2(2 * (q[0] * q[1] + q[2] * q[3]),
                                    1 - 2 * (q[1] * q[1] + q[2] * q[2])),
                         np.arcsin(2 * (q[0] * q[2] - q[3] * q[1])),
                         np.arctan2(2 * (q[0] * q[3] + q[
----------
Solution: 

        mat = self.to_matrix()
        euler = np.zeros(3, dtype=float)
        if mat[2, 2] < 1:
            if mat[2, 2] > -1:
                euler[0] = math.atan2(mat[1, 2], mat[0, 2])
                euler[1] = math.acos(mat[2, 2])
                euler[2] = math.atan2(mat[2, 1], -mat[2, 0])
            else:
                euler[0] = -math.atan2(mat[1, 0], mat[1, 1])
                euler[1] = np.pi
        else:
            euler[0] = math.atan2(mat[1, 0], mat[1, 1])
        return euler
--------------------
Prompt: 
def export_variants(adapter, collaborator, document_id=None, case_id=None):
    """Export causative variants for a collaborator

    Args:
        adapter(MongoAdapter)
        collaborator(str)
        document_id(str): Search for a specific variant
        case_id(str): Search causative variants for a case

    Yields:
        variant_obj(scout.Models.Variant): Variants marked as causative ordered by position.
    """
----------
Output: 

    if document_id:
        variant_obj = adapter.variant_collection.find_one(
            {"_id": document_id, "case_id": case_id}
        )
        if variant_obj:
            yield variant_obj
        return

    if case_id:
        variants = adapter.variant_collection.find(
            {"case_id": case_id, "causative": True},
            sort=[("chromosome", 1), ("position", 1)],
        )
    else:
        variants
----------
Solution: 


    # Store the variants in a list for sorting
    variants = []
    if document_id:
        yield adapter.variant(document_id)
        return

    variant_ids = adapter.get_causatives(
        institute_id=collaborator,
        case_id=case_id
        )
    ##TODO add check so that same variant is not included more than once
    for document_id in variant_ids:

        variant_obj = adapter.variant(document_id)
        chrom = variant_obj['chromosome']
        # Convert chromosome to integer for sorting
        chrom_int = CHROMOSOME_INTEGERS.get(chrom)
        if not chrom_int:
            LOG.info("Unknown chromosome %s", chrom)
            continue

        # Add chromosome and position to prepare for sorting
        variants.append((chrom_int, variant_obj['position'], variant_obj))

    # Sort varants based on position
    variants.sort(key=lambda x: (x[0], x[1]))

    for variant in variants:
        variant_obj = variant[2]
        yield variant_obj
--------------------
Prompt: 
def delete_uneeded(self):
        """
        Delete the directory which are not registered into our structure.
        """
----------
Output: 

        for directory in os.listdir(self.directory):
            if directory not in self.structure:
                shutil.rmtree(os.path.join(self.directory, directory))

    def create_structure(self):
        """
        Create the directory structure.
        """
        for directory in self.structure:
            os.mkdir(os.path.join(self.directory, directory))

    def move_files(self):
        """
        Move the files into the directory structure.
        """
        for file in os.
----------
Solution: 


        # We get the structure we have to apply.
        structure = self._get_structure()

        # We get the list of key which is implicitly the list of directory we do not bave to delete.
        list_of_key = list(structure.keys())

        # We move to the content of the parent as we know that we are creating only one directory.
        # Note: if one day we will have to create multiple directory, we will have to change
        # the following.
        structure = structure[list_of_key[0]]

        # We also set the parent directory as we are going to construct its childen.
        parent_path = list_of_key[0]

        if not parent_path.endswith(PyFunceble.directory_separator):
            parent_path += PyFunceble.directory_separator

        for root, _, _ in PyFunceble.walk(parent_path):
            # We loop through each directories of the parent path.

            # We fix the path in order to avoid issues.
            root = Directory(root).fix_path()

            if root.replace(parent_path, "") not in structure:
                # The currently read directory is not in our structure.

                # We delete it.
                PyFunceble.rmtree(root)
--------------------
Prompt: 
def is_effective_member(self, group_id, netid):
        """
        Returns True if the netid is in the group, False otherwise.
        """
----------
Output: 

        return self.is_member(group_id, netid) and self.is_active(group_id, netid)

    def is_member(self, group_id, netid):
        """
        Returns True if the netid is in the group, False otherwise.
        """
        return self.is_in_group(group_id, netid)

    def is_active(self, group_id, netid):
        """
        Returns True if the netid is in the group, False otherwise.
        """
       
----------
Solution: 

        self._valid_group_id(group_id)

        # GWS doesn't accept EPPNs on effective member checks, for UW users
        netid = re.sub('@washington.edu', '', netid)

        url = "{}/group/{}/effective_member/{}".format(self.API,
                                                       group_id,
                                                       netid)

        try:
            data = self._get_resource(url)
            return True  # 200
        except DataFailureException as ex:
            if ex.status == 404:
                return False
            else:
                raise
--------------------
Prompt: 
def find_modules(rootpath, skip):
    """
    Look for every file in the directory tree and return a dict
    Hacked from sphinx.autodoc
    """
----------
Output: 

    modules = {}
    for dirpath, dirnames, filenames in os.walk(rootpath):
        # Ignore certain directories and files
        for skip_path in skip:
            if os.path.join(dirpath, skip_path) in dirnames:
                dirnames.remove(skip_path)
        for filename in filenames:
            if filename.endswith('.py') and filename != '__init__.py':
                module_name = filename[:-3]
                module_path =
----------
Solution: 


    INITPY = '__init__.py'

    rootpath = os.path.normpath(os.path.abspath(rootpath))
    if INITPY in os.listdir(rootpath):
        root_package = rootpath.split(os.path.sep)[-1]
        print "Searching modules in", rootpath
    else:
        print "No modules in", rootpath
        return

    def makename(package, module):
        
--------------------
Prompt: 
def Rock(*args, **kwargs):
    """
    Graceful deprecation for old class name.
    """
----------
Output: 

    from .Rock import Rock
    return Rock(*args, **kwargs)


def Rock_v2(*args, **kwargs):
    """
    Graceful deprecation for old class name.
    """
    from .Rock_v2 import Rock_v2
    return Rock_v2(*args, **kwargs)


def Rock_v3(*args, **kwargs):
    """
    Graceful deprecation for old class name.
    """
    from .Rock_v3 import Rock_v3
    return Rock_v3
----------
Solution: 


    with warnings.catch_warnings():
        warnings.simplefilter("always")
        w = "The 'Rock' class was renamed 'Component'. "
        w += "Please update your code."
        warnings.warn(w, DeprecationWarning, stacklevel=2)

    return Component(*args, **kwargs)
--------------------
Prompt: 
def metadata(self):
        """Get metadata information in XML format."""
----------
Output: 

        return self.request('metadata')

    def metadata_json(self):
        """Get metadata information in JSON format."""
        return self.request('metadata', 'json')

    def metadata_xml(self):
        """Get metadata information in XML format."""
        return self.request('metadata', 'xml')

    def metadata_csv(self):
        """Get metadata information in CSV format."""
        return self.request('metadata', 'csv')

    def metadata_tsv(self):
        """Get metadata information in TSV
----------
Solution: 


        params = {
            self.PCTYPE: self.CTYPE_XML
        }

        response = self.call(self.CGI_BUG, params)

        return response
--------------------
Prompt: 
def put(self, thing_id='0', action_name=None, action_id=None):
        """
        Handle a PUT request.

        TODO: this is not yet defined in the spec

        thing_id -- ID of the thing this request is for
        action_name -- name of the action from the URL path
        action_id -- the action ID from the URL path
        """
----------
Output: 

        raise NotImplementedError

    def delete(self, thing_id='0', action_name=None, action_id=None):
        """
        Handle a DELETE request.

        TODO: this is not yet defined in the spec

        thing_id -- ID of the thing this request is for
        action_name -- name of the action from the URL path
        action_id -- the action ID from the URL path
        """
        raise NotImplementedError

    def options(self, thing_id='0', action_name
----------
Solution: 

        thing = self.get_thing(thing_id)
        if thing is None:
            self.set_status(404)
            return

        self.set_status(200)
--------------------
Prompt: 
def safe_infer(
    node: astroid.node_classes.NodeNG, context=None
) -> Optional[astroid.node_classes.NodeNG]:
    """Return the inferred value for the given node.

    Return None if inference failed or if there is some ambiguity (more than
    one node has been inferred).
    """
----------
Output: 

    if context is None:
        context = astroid.InferenceContext()
    inferred = context.infer(node)
    if len(inferred) == 1:
        return inferred[0]
    return None


def safe_infer_string(node: astroid.node_classes.NodeNG) -> Optional[str]:
    """Return the inferred value for the given node.

    Return None if inference failed or if there is some ambiguity (more than
    one node has been inferred).
    """
    infer
----------
Solution: 

    try:
        inferit = node.infer(context=context)
        value = next(inferit)
    except astroid.InferenceError:
        return None
    try:
        next(inferit)
        return None  # None if there is ambiguity on the inferred node
    except astroid.InferenceError:
        return None  # there is some kind of ambiguity
    except StopIteration:
        return value
--------------------
Prompt: 
def IsNotNone(*fields, default=None):
    """Selects whichever field is not None, in the specified order.

    Arguments:
        fields:
            The fields to attempt to get a value from,
            in order.

        default:
            The value to return in case all values are None.

    Returns:
        A Case-When expression that tries each field and
        returns the specified default value when all of
        them are None.
    """
----------
Output: 

    return Case(
        *[When(field, field) for field in fields],
        default=default,
    )


def IsNotNoneOrEmpty(*fields, default=None):
    """Selects whichever field is not None or empty, in the specified order.

    Arguments:
        fields:
            The fields to attempt to get a value from,
            in order.

        default:
            The value to return in case all values are None
            or empty.

    Returns:
        A Case-When expression that tries
----------
Solution: 


    when_clauses = [
        expressions.When(
            ~expressions.Q(**{field: None}),
            then=expressions.F(field)
        )
        for field in reversed(fields)
    ]

    return expressions.Case(
        *when_clauses,
        default=expressions.Value(default),
        output_field=CharField()
    )
--------------------
Prompt: 
def authorize_redirect(self, oauth_scope, callback_uri=None,
                           ax_attrs=["name","email","language","username"]):
        """Authenticates and authorizes for the given Google resource.

        Some of the available resources are:

        * Gmail Contacts - http://www.google.com/m8/feeds/
        * Calendar - http://www.google.com/calendar/feeds/
        * Finance - http://finance.google.com/finance/feeds/

        You can authorize multiple resources by separating the resource
        URLs with a space.
        """
----------
Output: 
Args:
          oauth_scope: The scope of the resource to authorize for.
          callback_uri: The callback URI to use for the authorization.
          ax_attrs: The list of attributes to request from the user.

        Returns:
          The URL to redirect the user to in order to authorize the request.
        """
        if callback_uri is None:
            callback_uri = self.request.uri

        # Generate the OAuth request token.
        request_token = self.get_request_token(oauth_scope, callback
----------
Solution: 

        callback_uri = callback_uri or self.request.uri
        args = self._openid_args(callback_uri, ax_attrs=ax_attrs,
                                 oauth_scope=oauth_scope)
        self.redirect(self._OPENID_ENDPOINT + "?" + urllib.urlencode(args))
--------------------
Prompt: 
def main(self,argv=None):
        """Run as a command-line script."""
----------
Output: 

        if argv is None:
            argv = sys.argv[1:]
        parser = argparse.ArgumentParser(description=__doc__)
        parser.add_argument('--version',action='version',version=__version__)
        parser.add_argument('-v','--verbose',action='store_true',
            help='Print verbose output.')
        parser.add_argument('-q','--quiet',action='store_true',
            help='Print only errors.')
        parser.add_argument('-c','--
----------
Solution: 


        parser = optparse.OptionParser(usage=USAGE % self.__class__.__name__)
        newopt = parser.add_option
        newopt('-i','--interact',action='store_true',default=False,
               help='Interact with the program after the script is run.')

        opts,args = parser.parse_args(argv)

        if len(args) != 1:
            print >> sys.stderr,"You must supply exactly one file to run."
            sys.exit(1)

        self.run_file(args[0],opts.interact)
--------------------
Prompt: 
def init_s3_keys(opt):
    """Initialize s3 access keys from environment variable or s3cfg config file."""
----------
Output: 

    if opt.s3cfg_path is not None:
        s3cfg_path = opt.s3cfg_path
    else:
        s3cfg_path = os.path.join(os.path.expanduser('~'), '.s3cfg')
    if not os.path.exists(s3cfg_path):
        raise ValueError('S3 config file not found: {}'.format(s3cfg_path))
    with open(s3cfg_path, 'r') as f:
        s3cfg = f.read()

----------
Solution: 

    S3Handler.S3_KEYS = S3Handler.s3_keys_from_cmdline(opt) or S3Handler.s3_keys_from_env() \
                        or S3Handler.s3_keys_from_s3cfg(opt)
--------------------
Prompt: 
def _reshuffle(mat, shape):
    """Reshuffle the indicies of a bipartite matrix A[ij,kl] -> A[lj,ki]."""
----------
Output: 

    mat = mat.reshape(shape)
    mat = mat.transpose(1, 0, 3, 2)
    mat = mat.reshape(shape[1], shape[0], shape[3], shape[2])
    return mat


def _reshape_bipartite(mat, shape):
    """Reshape a bipartite matrix A[ij,kl] -> A[ij,kl]."""
    mat = mat.reshape(shape)
    mat = mat.transpose(1, 0, 3
----------
Solution: 

    return np.reshape(
        np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)),
        (shape[3] * shape[1], shape[0] * shape[2]))
--------------------
Prompt: 
def get_by_symbol_name(self, name: str) -> Scope:
        """ Retrieve a Set of all signature by symbol name """
----------
Output: 

        return self.get_by_symbol(name)

    def get_by_symbol(self, symbol: str) -> Scope:
        """ Retrieve a Set of all signature by symbol """
        return self.get_by_symbol_type(symbol, None)

    def get_by_symbol_type(self, symbol: str, symbol_type: str) -> Scope:
        """ Retrieve a Set of all signature by symbol and symbol type """
        return self.get_by_symbol_type_and_scope(symbol,
----------
Solution: 

        lst = []
        for s in self.values():
            if s.name == name:
                # create an EvalCtx only when necessary
                lst.append(EvalCtx.from_sig(s))
        # include parent
        # TODO: see all case of local redefinition for
        #       global overloads
        # possible algos... take all with different internal_name
        if len(lst) == 0:
            p = self.get_parent()
            if p is not None:
                return p.get_by_symbol_name(name)
        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)
        # inherit type/translation from parent
        rscope.set_parent(self)
        return rscope
--------------------
Prompt: 
def to_ndarray(self):
        """
        Transfer JTensor to ndarray.
        As SparseTensor may generate an very big ndarray, so we don't support this function for SparseTensor.
        :return: a ndarray
        """
----------
Output: 

        if self.is_sparse:
            raise RuntimeError("SparseTensor can't be transfer to ndarray.")
        return self.data.asnumpy()

    def to_tensor(self):
        """
        Transfer JTensor to Tensor.
        :return: a Tensor
        """
        return Tensor(self.data)

    def __str__(self):
        return self.data.__str__()

    def __repr__(self):
        return self.data.__repr__()

    def
----------
Solution: 

        assert self.indices is None, "sparseTensor to ndarray is not supported"
        return np.array(self.storage, dtype=get_dtype(self.bigdl_type)).reshape(self.shape)
--------------------
Prompt: 
def deprecated(msg=''):
    """This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emitted
    when the function is used.

    :param msg:

        Additional message added to the warning.

    """
----------
Output: 


    def deprecated_decorator(func):
        """Decorator function.

        :param func:

            Function to be decorated.

        """

        def new_func(*args, **kwargs):
            """Decorated function.

            :param args:

                Function arguments.

            :param kwargs:

                Function keyword arguments.

            """

            warnings.warn(
                "Call to deprecated function {}.".format(func.__name__) +
                (msg and " {}".format(msg
----------
Solution: 


    def wrapper(func):
        @functools.wraps(func)
        def new_func(*args, **kwargs):
            warning_string = "Call to deprecated function or property `%s`." % func.__name__
            warning_string = warning_string + ' ' + msg
            warnings.warn(
                warning_string,
                category=DeprecationWarning,
            )
            return func(*args, **kwargs)

        return new_func

    return wrapper
--------------------
Prompt: 
def uncache_zipdir(path):
    """Ensure that the importer caches dont have stale info for `path`"""
----------
Output: 

    import zipfile
    import sys
    import os
    import importlib.util

    if not os.path.exists(path):
        raise FileNotFoundError(path)

    if os.path.isdir(path):
        path = os.path.join(path, '__init__.py')

    if not os.path.exists(path):
        raise FileNotFoundError(path)

    if not path.endswith('.zip'):
        raise ValueError(f'path must be a zip file, not {path}')

----------
Solution: 

    from zipimport import _zip_directory_cache as zdc
    _uncache(path, zdc)
    _uncache(path, sys.path_importer_cache)
--------------------
Prompt: 
def adjust_frame(self, pos, absolute_pos):
        """Adjust stack frame by pos positions. If absolute_pos then
        pos is an absolute number. Otherwise it is a relative number.

        A negative number indexes from the other end."""
----------
Output: 

        if absolute_pos:
            self.frame = pos
        else:
            self.frame += pos

    def get_frame(self):
        """Return the current frame number."""
        return self.frame

    def get_frame_count(self):
        """Return the number of frames in the stack."""
        return self.frame_count

    def get_frame_rate(self):
        """Return the frame rate of the stack."""
        return self.frame_rate

    def get_frame_time(self):
----------
Solution: 

        if not self.curframe:
            Mmsg.errmsg(self, "No stack.")
            return

        # Below we remove any negativity. At the end, pos will be
        # the new value of self.curindex.
        if absolute_pos:
            if pos >= 0:
                pos = len(self.stack)-pos-1
            else:
                pos = -pos-1
        else:
            pos += self.curindex

        if pos < 0:
            Mmsg.errmsg(self,
                        "Adjusting would put us beyond the oldest frame.")
            return
        elif pos >= len(self.stack):
            Mmsg.errmsg(self,
                        "Adjusting would put us beyond the newest frame.")
            return

        self.curindex = pos
        self.curframe = self.stack[self.curindex][0]
        self.print_location()
        self.list_lineno = None
        return
--------------------
Prompt: 
def filter_unused_variable(line, previous_line=''):
    """Return line if used, otherwise return None."""
----------
Output: 

    if line.startswith('var ') and not line.startswith('var _'):
        return line
    if line.startswith('var ') and previous_line.startswith('var '):
        return line
    return None


def filter_unused_function(line, previous_line=''):
    """Return line if used, otherwise return None."""
    if line.startswith('function '):
        return line
    if line.startswith('function '):
        return line
    return None


def filter_unused_class
----------
Solution: 

    if re.match(EXCEPT_REGEX, line):
        return re.sub(r' as \w+:$', ':', line, count=1)
    elif multiline_statement(line, previous_line):
        return line
    elif line.count('=') == 1:
        split_line = line.split('=')
        assert len(split_line) == 2
        value = split_line[1].lstrip()
        if ',' in split_line[0]:
            return line

        if is_literal_or_name(value):
            # Rather than removing the line, replace with it "pass" to avoid
            # a possible hanging block with no body.
            value = 'pass' + get_line_ending(line)

        return get_indentation(line) + value
    else:
        return line
--------------------
Prompt: 
def initialize(cls):
        """
        Creates an **"Example"** entity of kind **"NDBConfig"** in the
        datastore if the model is empty and raises and error to inform you that
        you should populate the model with data.

        .. note::

            The *Datastore Viewer* in the ``_ah/admin/`` won't let you add
            properties to a model if there is not an entity with that
            property already. Therefore it is a good idea to keep the
            **"Example"** entity (which has all possible properties set) in
            the datastore.

        """
----------
Output: 
in the datastore.

        """
        if not cls.query().get():
            cls(name='Example').put()
            raise Exception('Please populate the model with data.')

    @classmethod
    def get_config(cls, name):
        """
        Returns the configuration for the given **name**.

        :param name: The name of the configuration.
        :type name: str

        :return: The configuration for the given **name**.
        :rtype: :class:`~ndb.model.Model
----------
Solution: 


        if not len(cls.query().fetch()):

            example = cls.get_or_insert('Example')

            example.class_ = 'Provider class e.g. ' + \
                             '"authomatic.providers.oauth2.Facebook".'
            example.provider_name = 'Your custom provider name e.g. "fb".'

            # AuthorizationProvider
            example.consumer_key = 'Consumer key.'
            example.consumer_secret = 'Consumer secret'
            example.provider_id = 1

            # OAuth2
            example.scope = 'coma, separated, list, of, scopes'

            # AuthenticationProvider
            example.identifier_param = 'Querystring parameter for claimed ' + \
                                       'id. default is "id"'

            # Save the example
            example.put()

            # Raise an information error.
            raise GAEError(
                'A NDBConfig data model was created! Go to Datastore Viewer '
                'in your dashboard and populate it with data!')
--------------------
Prompt: 
def _config_session():
        """
        Configure session for particular device

        Returns:
            tensorflow.Session
        """
----------
Output: 

        config = tf.ConfigProto(
            device_count={'GPU': 0},
            allow_soft_placement=True,
            log_device_placement=False
        )
        return tf.Session(config=config)

    def _get_model_path(self):
        """
        Get path to model

        Returns:
            str
        """
        return os.path.join(self.model_dir, self.model_name)

    def _get_model_path_latest(self):

----------
Solution: 

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.visible_device_list = '0'
        return tf.Session(config=config)
--------------------
Prompt: 
def _ensure_executor_running(self):
        """ensures all workers and management thread are running
        """
----------
Output: 

        if not self._executor_running:
            self._executor_running = True
            self._start_executor()

    def _start_executor(self):
        """starts the executor thread
        """
        self._executor_thread = Thread(target=self._executor_thread_main)
        self._executor_thread.daemon = True
        self._executor_thread.start()

    def _executor_thread_main(self):
        """main loop of the executor thread
        """
       
----------
Solution: 

        with self._processes_management_lock:
            if len(self._processes) != self._max_workers:
                self._adjust_process_count()
            self._start_queue_management_thread()
--------------------
Prompt: 
def getPos(self):
        """
        Returns the absolute position and size of the layer.
        
        This method is intended for use in vertex position calculation, as the border and offset have already been applied.
        
        The returned value is a 4-tuple of ``(sx,sy,ex,ey)``\ .
        The two values starting with an s are the "start" position, or the lower-left corner.
        The second pair of values signify the "end" position, or upper-right corner.
        """
----------
Output: 

        return self.sx,self.sy,self.ex,self.ey

    def getSize(self):
        """
        Returns the size of the layer.
        
        This method is intended for use in vertex position calculation, as the border and offset have already been applied.
        
        The returned value is a 2-tuple of ``(width,height)``\ .
        """
        return self.ex-self.sx,self.ey-self.sy

    def getBorder(self):
        """
        Returns the
----------
Solution: 

        # Returns sx,sy,ex,ey
        # sx,sy are bottom-left/lowest
        # ex,ey are top-right/highest
        sx,sy = self.widget.pos[0]+self.border[0]+self.offset[0],                       self.widget.pos[1]+self.border[1]+self.offset[1]
        ex,ey = self.widget.pos[0]+self.widget.size[0]-self.border[0]+self.offset[0],   self.widget.pos[1]+self.widget.size[1]-self.border[1]+self.offset[1]
        return sx,sy,ex,ey
--------------------
Prompt: 
def users(self):
        """
        List of users of this slack team
        """
----------
Output: 

        return self.get_users()

    def channels(self):
        """
        List of channels of this slack team
        """
        return self.get_channels()

    def groups(self):
        """
        List of groups of this slack team
        """
        return self.get_groups()

    def ims(self):
        """
        List of direct messages of this slack team
        """
        return self.get_ims()

    def get_users(self):
        """
        List of
----------
Solution: 

        if not self._users:
            self._users = self._call_api('users.list')['members']
        return self._users
--------------------
Prompt: 
def decode_instruction(instruction):
        """
        Decode whole instruction and return list of args.
        Usually, returned arg[0] is the instruction opcode.

        example:
        >> args = decode_instruction('4.size,4.1024;')
        >> args == ['size', '1024']
        >> True

        :param instruction: Instruction string.

        :return: list
        """
----------
Output: 

        args = []
        for arg in instruction.split(','):
            if arg.find('.') != -1:
                arg = arg.split('.')
                args.append(arg[0])
                args.append(arg[1])
            else:
                args.append(arg)
        return args


def decode_instruction_args(instruction):
        """
        Decode instruction args and return list of args.
        Usually, returned arg[0] is the instruction opcode.

        example:
        >> args = decode
----------
Solution: 

        if not instruction.endswith(INST_TERM):
            raise InvalidInstruction('Instruction termination not found.')

        # Use proper encoding
        instruction = utf8(instruction)

        # Get arg size
        elems = instruction.split(ELEM_SEP, 1)

        try:
            arg_size = int(elems[0])
        except Exception:
            # Expected ValueError
            raise InvalidInstruction(
                'Invalid arg length.' +
                ' Possibly due to missing element separator!')

        arg_str = elems[1][:arg_size]

        remaining = elems[1][arg_size:]

        args = [arg_str]

        if remaining.startswith(ARG_SEP):
            # Ignore the ARG_SEP to parse next arg.
            remaining = remaining[1:]
        elif remaining == INST_TERM:
            # This was the last arg!
            return args
        else:
            # The remaining is neither starting with ARG_SEP nor INST_TERM.
            raise InvalidInstruction(
                'Instruction arg (%s) has invalid length.' % arg_str)

        next_args = GuacamoleInstruction.decode_instruction(remaining)

        if next_args:
            args = args + next_args

        return args
--------------------
Prompt: 
def handle_combo(self,combo,symbol,modifiers,release=False,mod=True):
        """
        Handles a key combination and dispatches associated events.
        
        First, all keybind handlers registered via :py:meth:`add` will be handled,
        then the pyglet event :peng3d:pgevent:`on_key_combo` with params ``(combo,symbol,modifiers,release,mod)`` is sent to the :py:class:`Peng()` instance.
        
        Also sends the events :peng3d:event:`peng3d:keybind.combo`\, :peng3d:event:`peng3d:keybind.combo.press` and :peng3d:event`peng3d:keybind.combo.release`\ .
        
        :params str combo: Key combination pressed
        :params int symbol: Key pressed, passed from the same argument within pyglet
        :params int modifiers: Modifiers held while the key was pressed
        :params bool release: If the combo was released
        :params bool mod: If the combo was sent without mods
        """
----------
Output: 
:peng3d:pgevent:`on_key_combo_<combo>` and :peng3d:pgevent:`on_key_combo_<combo>_<symbol>` to the :py:class:`Peng()` instance.
        
        :param combo: The key combination.
        :type combo: str
        :param symbol: The symbol of the key.
        :type symbol: str
        :param modifiers: The modifiers of the key.
        :type modifiers: int
        :param release: Whether
----------
Solution: 

        if self.peng.cfg["controls.keybinds.debug"]:
            print("combo: nm=%s %s"%(mod,combo))
        if mod:
            for kbname in self.keybinds.get(combo,[]):
                self.kbname[kbname](symbol,modifiers,release)
        else:
            for kbname in self.keybinds_nm.get(combo,[]):
                self.kbname[kbname](symbol,modifiers,release)
        self.peng.sendPygletEvent("on_key_combo",(combo,symbol,modifiers,release,mod))
        self.peng.sendEvent("peng3d:keybind.combo",{"peng":self.peng,"combo":combo,"symbol":symbol,"modifiers":modifiers,"release":release,"mod":mod})
        if release:
            self.peng.sendEvent("peng3d:keybind.combo.release",{"peng":self.peng,"combo":combo,"symbol":symbol,"modifiers":modifiers,"release":release,"mod":mod})
        else:
            self.peng.sendEvent("peng3d:keybind.combo.press",{"peng":self.peng,"combo":combo,"symbol":symbol,"modifiers":modifiers,"release":release,"mod":mod})
--------------------
Prompt: 
def expand_tokens(tokens, equal=False):
    """Given a list of tokens, return a generator of the chunks of
    text for the data in the tokens.
    """
----------
Output: 

    for token in tokens:
        if token.is_text:
            yield token.text
        elif token.is_space:
            yield ' '
        elif token.is_newline:
            yield '\n'
        elif token.is_tab:
            yield '\t'
        elif token.is_equal and equal:
            yield '='
        elif token.is_equal and not equal:
            yield ' '
        elif token.is_open_paren:
            yield '('
        elif token.is_close_paren
----------
Solution: 

    for token in tokens:
        for pre in token.pre_tags:
            yield pre
        if not equal or not token.hide_when_equal:
            if token.trailing_whitespace:
                yield token.html() + token.trailing_whitespace
            else:
                yield token.html()
        for post in token.post_tags:
            yield post
--------------------
Prompt: 
def replay_detection_negotiated(self):
        """
        After :meth:`step` has been called, this property will be set to
        True if the security context can use replay detection for messages protected by
        :meth:`get_mic` and :meth:`wrap`. False if replay detection cannot be used.
        """
----------
Output: 

        return self._replay_detection_negotiated

    @property
    def replay_detection_supported(self):
        """
        After :meth:`step` has been called, this property will be set to
        True if the security context can use replay detection for messages protected by
        :meth:`get_mic` and :meth:`wrap`. False if replay detection cannot be used.
        """
        return self._replay_detection_supported

    @property
    def replay_detection
----------
Solution: 

        return (
            self.flags & C.GSS_C_REPLAY_FLAG
        ) and (
            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)
        )
--------------------
Prompt: 
def visit_table(self, layout):
        """display a table as text"""
----------
Output: 

        if not layout.table:
            return
        if layout.table.caption:
            self.write(layout.table.caption)
            self.write('\n')
        self.write('|')
        for col in layout.table.columns:
            self.write(' %s |' % col)
        self.write('\n')
        self.write('|')
        for col in layout.table.columns:
            self.write(' --- |')
        self.write('\n')
        for row in layout.table.rows:

----------
Solution: 

        table_content = self.get_table_content(layout)
        # get columns width
        cols_width = [0] * len(table_content[0])
        for row in table_content:
            for index, col in enumerate(row):
                cols_width[index] = max(cols_width[index], len(col))
        self.default_table(layout, table_content, cols_width)
        self.writeln()
--------------------
Prompt: 
def variants(context, collaborator, document_id, case_id, json):
    """Export causatives for a collaborator in .vcf format"""
----------
Output: 

    # TODO: Add collaborator to the context
    # TODO: Add collaborator to the document
    # TODO: Add collaborator to the case
    # TODO: Add collaborator to the case's collaborators
    # TODO: Add collaborator to the case's collaborators
    # TODO: Add collaborator to the case's collaborators
    # TODO: Add collaborator to the case's collaborators
    # TODO: Add collaborator to the case's collaborators
    # TODO: Add collaborator to the case's collaborators
    # TODO:
----------
Solution: 

    LOG.info("Running scout export variants")
    adapter = context.obj['adapter']
    collaborator = collaborator or 'cust000'

    variants = export_variants(
        adapter,
        collaborator,
        document_id=document_id,
        case_id=case_id
    )

    if json:
        click.echo(dumps([var for var in variants]))
        return

    vcf_header = VCF_HEADER

    #If case_id is given, print more complete vcf entries, with INFO,
    #and genotypes
    if case_id:
        vcf_header[-1] = vcf_header[-1] + "\tFORMAT"
        case_obj = adapter.case(case_id=case_id)
        for individual in case_obj['individuals']:
            vcf_header[-1] = vcf_header[-1] + "\t" + individual['individual_id']

    #print header
    for line in vcf_header:
        click.echo(line)

    for variant_obj in variants:
        variant_string = get_vcf_entry(variant_obj, case_id=case_id)
        click.echo(variant_string)
--------------------
Prompt: 
def _plot_formatting(title, est_file, algo_ids, last_bound, N, output_file):
    """Formats the plot with the correct axis labels, title, ticks, and
    so on."""
----------
Output: 

    plt.title(title)
    plt.xlabel('Number of samples')
    plt.ylabel('Estimated value')
    plt.xticks(np.arange(0, N, 10))
    plt.yticks(np.arange(0, last_bound, 10))
    plt.legend(algo_ids)
    plt.savefig(output_file)
    plt.close()


def _plot_estimates(est_file, algo_ids, last_bound, N, output_
----------
Solution: 

    import matplotlib.pyplot as plt
    if title is None:
        title = os.path.basename(est_file).split(".")[0]
    plt.title(title)
    plt.yticks(np.arange(0, 1, 1 / float(N)) + 1 / (float(N) * 2))
    plt.gcf().subplots_adjust(bottom=0.22)
    plt.gca().set_yticklabels(algo_ids)
    plt.xlabel("Time (seconds)")
    plt.xlim((0, last_bound))
    plt.tight_layout()
    if output_file is not None:
        plt.savefig(output_file)
    plt.show()
--------------------
Prompt: 
def f_remove(self, recursive=True, predicate=None):
        """Recursively removes the group and all it's children.

        :param recursive:

            If removal should be applied recursively. If not, node can only be removed
            if it has no children.

        :param predicate:

            In case of recursive removal, you can selectively remove nodes in the tree.
            Predicate which can evaluate for each node to ``True`` in order to remove the node or
            ``False`` if the node should be kept. Leave ``None`` if you want to remove all nodes.

        """
----------
Output: 
None`` to remove all nodes.

        :return:

            ``True`` if the group was removed, ``False`` if the group was not removed.
        """
        if not self.is_group():
            return False

        if not recursive:
            if self.children:
                return False

        if predicate is None:
            predicate = lambda node: True

        for child in self.children:
            if predicate(child):
                child.remove(recursive=recursive, predicate=predicate)

        self.parent.remove
----------
Solution: 

        parent = self.f_get_parent()
        parent.f_remove_child(self.v_name, recursive=recursive, predicate=predicate)
--------------------
Prompt: 
def _wrap(text, wrap_at=120, indent=4):
    """
    Return piece of text, wrapped around if needed.

    :param text: text that may be too long and then needs to be wrapped.
    :param wrap_at: the maximum line length.
    :param indent: number of spaces to prepend to all subsequent lines after the first.
    """
----------
Output: 

    if len(text) <= wrap_at:
        return text
    else:
        return '\n'.join(
            ' ' * indent + line
            for line in textwrap.wrap(text, wrap_at - indent)
        )


def _get_docstring(obj):
    """
    Return docstring for given object.

    :param obj: object to get docstring for.
    """
    if hasattr(obj, '__doc__'):
        return obj.__doc__
    else:
        return ''


----------
Solution: 

    out = ""
    curr_line_length = indent
    space_needed = False
    for word in text.split():
        if curr_line_length + len(word) > wrap_at:
            out += "\n" + " " * indent
            curr_line_length = indent
            space_needed = False
        if space_needed:
            out += " "
            curr_line_length += 1
        out += word
        curr_line_length += len(word)
        space_needed = True
    return out
--------------------
Prompt: 
def sanitize_for_archive(url, headers, payload):
        """Sanitize payload of a HTTP request by removing the login, password and token information
        before storing/retrieving archived items

        :param: url: HTTP url request
        :param: headers: HTTP headers request
        :param: payload: HTTP payload request

        :returns url, headers and the sanitized payload
        """
----------
Output: 

        if url.startswith('https://'):
            url = url.replace('https://', '')
        if url.startswith('http://'):
            url = url.replace('http://', '')
        if url.startswith('www.'):
            url = url.replace('www.', '')
        if url.endswith('/'):
            url = url[:-1]
        if ':' in url:
            url = url.split(':')[0]
        if '?' in url:
            url = url.split('?')[0]

----------
Solution: 

        if BugzillaRESTClient.PBUGZILLA_LOGIN in payload:
            payload.pop(BugzillaRESTClient.PBUGZILLA_LOGIN)

        if BugzillaRESTClient.PBUGZILLA_PASSWORD in payload:
            payload.pop(BugzillaRESTClient.PBUGZILLA_PASSWORD)

        if BugzillaRESTClient.PBUGZILLA_TOKEN in payload:
            payload.pop(BugzillaRESTClient.PBUGZILLA_TOKEN)

        return url, headers, payload
--------------------
Prompt: 
def cycle(self):
        """
        Cycles through notifications with latest results from data feeds.
        """
----------
Output: 

        self.logger.info("Starting cycle.")
        while True:
            self.cycle_once()

    def cycle_once(self):
        """
        Cycles through notifications with latest results from data feeds.
        """
        self.logger.info("Starting cycle once.")
        self.logger.info("Getting notifications.")
        notifications = self.get_notifications()
        self.logger.info("Got notifications.")
        self.logger.info("Getting data feeds.")
        data_feeds = self.get_
----------
Solution: 

        messages = self.poll_datafeeds()
        notifications = self.process_notifications(messages)

        self.draw_notifications(notifications)
--------------------
Prompt: 
def user_agent():
    """
    Return a string representing the user agent.
    """
----------
Output: 

    return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"


def get_page(url):
    """
    Return the page at the given URL.
    """
    headers = {'User-Agent': user_agent()}
    response = requests.get(url, headers=headers)
    return response.
----------
Solution: 

    data = {
        "installer": {"name": "pip", "version": pip.__version__},
        "python": platform.python_version(),
        "implementation": {
            "name": platform.python_implementation(),
        },
    }

    if data["implementation"]["name"] == 'CPython':
        data["implementation"]["version"] = platform.python_version()
    elif data["implementation"]["name"] == 'PyPy':
        if sys.pypy_version_info.releaselevel == 'final':
            pypy_version_info = sys.pypy_version_info[:3]
        else:
            pypy_version_info = sys.pypy_version_info
        data["implementation"]["version"] = ".".join(
            [str(x) for x in pypy_version_info]
        )
    elif data["implementation"]["name"] == 'Jython':
        # Complete Guess
        data["implementation"]["version"] = platform.python_version()
    elif data["implementation"]["name"] == 'IronPython':
        # Complete Guess
        data["implementation"]["version"] = platform.python_version()

    if sys.platform.startswith("linux"):
        distro = dict(filter(
            lambda x: x[1],
            zip(["name", "version", "id"], platform.linux_distribution()),
        ))
        libc = dict(filter(
            lambda x: x[1],
            zip(["lib", "version"], platform.libc_ver()),
        ))
        if libc:
            distro["libc"] = libc
        if distro:
            data["distro"] = distro

    if sys.platform.startswith("darwin") and platform.mac_ver()[0]:
        data["distro"] = {"name": "OS X", "version": platform.mac_ver()[0]}

    if platform.system():
        data.setdefault("system", {})["name"] = platform.system()

    if platform.release():
        data.setdefault("system", {})["release"] = platform.release()

    if platform.machine():
        data["cpu"] = platform.machine()

    return "{data[installer][name]}/{data[installer][version]} {json}".format(
        data=data,
        json=json.dumps(data, separators=(",", ":"), sort_keys=True),
    )
--------------------
Prompt: 
def headers_present(self, headers):
        """
        Defines a list of headers that must be present in the
        outgoing request in order to satisfy the matcher, no matter what value
        the headers hosts.

        Header keys are case insensitive.

        Arguments:
            headers (list|tuple): header keys to match.

        Returns:
            self: current Mock instance.

        Example::

            (pook.get('server.com/api')
                .headers_present(['content-type', 'Authorization']))
        """
----------
Output: 
               .reply(200)
        """
        self.matchers.append(
            lambda req: all(
                [req.headers.get(header) for header in headers]
            )
        )
        return self

    def headers_absent(self, headers):
        """
        Defines a list of headers that must not be present in the
        outgoing request in order to satisfy the matcher, no matter what value
        the headers hosts.

        Header keys are case insensitive.

        Arguments:
            headers
----------
Solution: 

        headers = {name: re.compile('(.*)') for name in headers}
        self.add_matcher(matcher('HeadersMatcher', headers))
--------------------
Prompt: 
def resolve_path(self, path, root_id='me/skydrive', objects=False, listdir_limit=500):
		"""Return id (or metadata) of an object, specified by chain
				(iterable or fs-style path string) of "name" attributes
				of its ancestors, or raises DoesNotExists error.

			Requires many calls to resolve each name in path, so use with care.
				root_id parameter allows to specify path relative to some folder_id
				(default: me/skydrive)."""
----------
Output: 

				(default is root of SkyDrive).

			If objects=True, returns metadata of each object in path,
				otherwise returns id of the last object in path.

			If listdir_limit is set, it will be used as limit parameter
				for listdir calls.
		"""
		if isinstance(path, basestring):
			path = path.split('/')
		if not path:
			raise ValueError('path is empty')
	
----------
Solution: 

		if path:
			if isinstance(path, types.StringTypes):
				if not path.startswith('me/skydrive'):
					# Split path by both kinds of slashes
					path = filter(None, it.chain.from_iterable(p.split('\\') for p in path.split('/')))
				else: root_id, path = path, None
			if path:
				try:
					for i, name in enumerate(path):
						offset = None
						while True:
							obj_list = self.listdir(root_id, offset=offset, limit=listdir_limit)
							try: root_id = dict(it.imap(op.itemgetter('name', 'id'), obj_list))[name]
							except KeyError:
								if len(obj_list) < listdir_limit: raise # assuming that it's the last page
								offset = (offset or 0) + listdir_limit
							else: break
				except (KeyError, ProtocolError) as err:
					if isinstance(err, ProtocolError) and err.code != 404: raise
					raise DoesNotExists(root_id, path[i:])
		return root_id if not objects else self.info(root_id)
--------------------
Prompt: 
def require(self, key):
        """
        Raises an exception if value for ``key`` is empty.
        """
----------
Output: 

        value = self.get(key)
        if value is None:
            raise ValueError("Missing required key: %s" % key)
        return value

    def __getitem__(self, key):
        """
        Returns the value for the given key.
        """
        value = self._data.get(key)
        if value is None:
            raise KeyError(key)
        return value

    def __setitem__(self, key, value):
        """
        Sets the value for the given key.
        """
----------
Solution: 

        value = self.get(key)
        if not value:
            raise ValueError('"{}" is empty.'.format(key))
        return value
--------------------
Prompt: 
def Rconverter(Robj, dataframe=False):
    """
    Convert an object in R's namespace to one suitable
    for ipython's namespace.

    For a data.frame, it tries to return a structured array.
    It first checks for colnames, then names.
    If all are NULL, it returns np.asarray(Robj), else
    it tries to construct a recarray

    Parameters
    ----------

    Robj: an R object returned from rpy2
    """
----------
Output: 

    import numpy as np
    if isinstance(Robj, np.ndarray):
        return Robj
    elif isinstance(Robj, list):
        return Robj
    elif isinstance(Robj, np.matrix):
        return Robj
    elif isinstance(Robj, np.recarray):
        return Robj
    elif isinstance(Robj, np.rec.recarray):
        return Robj
    elif isinstance(Robj, np.rec.array):
        return Robj
    elif isinstance(Robj,
----------
Solution: 

    is_data_frame = ro.r('is.data.frame')
    colnames = ro.r('colnames')
    rownames = ro.r('rownames') # with pandas, these could be used for the index
    names = ro.r('names')

    if dataframe:
        as_data_frame = ro.r('as.data.frame')
        cols = colnames(Robj)
        _names = names(Robj)
        if cols != ri.NULL:
            Robj = as_data_frame(Robj)
            names = tuple(np.array(cols))
        elif _names != ri.NULL:
            names = tuple(np.array(_names))
        else: # failed to find names
            return np.asarray(Robj)
        Robj = np.rec.fromarrays(Robj, names = names)
    return np.asarray(Robj)
--------------------
Prompt: 
def pid_exists(pid):
    """Check whether pid exists in the current process table."""
----------
Output: 

    if pid < 0:
        return False
    try:
        os.kill(pid, 0)
    except OSError:
        return False
    else:
        return True


def kill_process(pid):
    """Kill a process by pid."""
    if pid_exists(pid):
        os.kill(pid, signal.SIGTERM)


def get_pid_by_port(port):
    """Get pid by port."""
    pid = 0
    for p in psutil.process
----------
Solution: 

    if pid < 0:
        return False
    try:
        os.kill(pid, 0)
    except OSError as exc:
        logging.debug("No process[%s]: %s", exc.errno, exc)
        return exc.errno == errno.EPERM
    else:
        p = psutil.Process(pid)
        return p.status != psutil.STATUS_ZOMBIE
--------------------
Prompt: 
def start_hb(self, callback):
        """Start the heartbeating and call the callback if the kernel dies."""
----------
Output: 

        if self.hb_thread is not None:
            return
        self.hb_thread = threading.Thread(target=self._heartbeat, args=(callback,))
        self.hb_thread.daemon = True
        self.hb_thread.start()

    def stop_hb(self):
        """Stop the heartbeating."""
        if self.hb_thread is None:
            return
        self.hb_thread.join()
        self.hb_thread = None

    def
----------
Solution: 

        if not self._beating:
            self._kernel_alive = True

            def ping_or_dead():
                self.hb_stream.flush()
                if self._kernel_alive:
                    self._kernel_alive = False
                    self.hb_stream.send(b'ping')
                    # flush stream to force immediate socket send
                    self.hb_stream.flush()
                else:
                    try:
                        callback()
                    except:
                        pass
                    finally:
                        self.stop_hb()

            def beat_received(msg):
                self._kernel_alive = True

            self.hb_stream.on_recv(beat_received)
            loop = ioloop.IOLoop.instance()
            self._hb_periodic_callback = ioloop.PeriodicCallback(ping_or_dead, self.time_to_dead*1000, loop)
            loop.add_timeout(time.time()+self.first_beat, self._really_start_hb)
            self._beating= True
--------------------
Prompt: 
def _chunk_noise(noise):
    """Chunk input noise data into valid Touchstone file rows."""
----------
Output: 

    # TODO: This is a hack.
    # The Touchstone format requires the data to be chunked into 16-bit
    # words, but the data is stored as 32-bit floats.
    # This function converts the data to 16-bit words.
    # The Touchstone format also requires the data to be in the range
    # [-1, 1].
    # This function converts the data to [-1, 1].
    # The Touchstone format also requires the data to be in the order
    # [real, imag
----------
Solution: 

    data = zip(
        noise["freq"],
        noise["nf"],
        np.abs(noise["rc"]),
        np.angle(noise["rc"]),
        noise["res"],
    )
    for freq, nf, rcmag, rcangle, res in data:
        yield freq, nf, rcmag, rcangle, res
--------------------
Prompt: 
def is_lesser(a, b):
    """
    Verify that an item *a* is <= then an item *b*
    
    :param a: An item
    :param b: Another item
    :return: True or False
    """
----------
Output: 

    return a <= b


def is_greater(a, b):
    """
    Verify that an item *a* is >= then an item *b*
    
    :param a: An item
    :param b: Another item
    :return: True or False
    """
    return a >= b


def is_equal(a, b):
    """
    Verify that an item *a* is == then an item *b*
    
    :param a: An item
    :param b: Another item
   
----------
Solution: 


    if type(a) != type(b):
        return False

    if isinstance(a, str) and isinstance(b, str):
        return a == b
    elif isinstance(a, bool) and isinstance(b, bool):
        return a == b
    elif isinstance(a, list) and isinstance(b, list):
        for element in a:
            flag = 0
            for e in b:
                if is_lesser(element, e):
                    flag = 1
                    break
            if not flag:
                return False
        return True
    elif isinstance(a, dict) and isinstance(b, dict):
        if is_lesser(list(a.keys()), list(b.keys())):
            for key, val in a.items():
                if not is_lesser(val, b[key]):
                    return False
            return True
        return False
    elif isinstance(a, int) and isinstance(b, int):
        return a <= b
    elif isinstance(a, float) and isinstance(b, float):
        return a <= b

    return False
--------------------
Prompt: 
def isreshapeable(new, old):
    """
    Check to see if a proposed tuple of axes is a valid reshaping of
    the old axes by ensuring that they can be factored.

    Parameters
    ----------
    new : tuple
        tuple of proposed axes

    old : tuple
        tuple of old axes
    """
----------
Output: 

    if len(new) != len(old):
        return False
    if len(new) == 0:
        return True
    if len(new) == 1:
        return new[0] == old[0]
    if len(new) == 2:
        return new[0] == old[0] and new[1] == old[1]
    if len(new) == 3:
        return new[0] == old[0] and new[1] == old[1] and new[2] == old[
----------
Solution: 


    new, old = tupleize(new), tupleize(old)

    if not prod(new) == prod(old):
        raise ValueError("Total size of new keys must remain unchanged")
--------------------
Prompt: 
def _simple_ast_generator(gen_ast):
    """Wrap simpler AST generators to return a GeneratedPyAST."""
----------
Output: 

    def wrapper(*args, **kwargs):
        return GeneratedPyAST(gen_ast(*args, **kwargs))
    return wrapper


def _simple_ast_visitor(visit_ast):
    """Wrap simpler AST visitors to return a GeneratedPyAST."""
    def wrapper(*args, **kwargs):
        return GeneratedPyAST(visit_ast(*args, **kwargs))
    return wrapper


def _simple_ast_transformer(transform_ast):
    """Wrap simpler AST transformers to return a Gener
----------
Solution: 


    @wraps(gen_ast)
    def wrapped_ast_generator(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:
        return GeneratedPyAST(node=gen_ast(ctx, form))

    return wrapped_ast_generator
--------------------
Prompt: 
def next_opcode(code, offset):
    """Return the next opcode and offset as a tuple. Tuple (-100,
    -1000) is returned when reaching the end."""
----------
Output: 

    if offset >= len(code):
        return (-100, -1000)
    opcode = code[offset]
    if opcode == 0:
        return (opcode, offset + 1)
    if opcode == 1:
        return (opcode, offset + 2)
    if opcode == 2:
        return (opcode, offset + 3)
    if opcode == 3:
        return (opcode, offset + 1)
    if opcode == 4:
        return
----------
Solution: 

    n = len(code)
    while offset < n:
        op = code[offset]
        offset += 1
        if op >= HAVE_ARGUMENT:
            offset += 2
            pass
        yield op, offset
        pass
    yield -100, -1000
    pass
--------------------
Prompt: 
def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,
                        name=None):
  """Assert `x` has rank equal to `rank` or smaller.

  Example of adding a dependency to an operation:

  ```python
  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):
    output = tf.reduce_sum(x)
  ```

  Args:
    x:  Numeric `Tensor`.
    rank:  Scalar `Tensor`.
    data:  The tensors to print out if the condition is False.  Defaults to
      error message and first few entries of `x`.
    summarize: Print this many entries of each tensor.
    message: A string to prefix to the default message.
    name: A name for this operation (optional).
      Defaults to "assert_rank_at_most".

  Returns:
    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.
    If static checks determine `x` has correct rank, a `no_op` is returned.

  Raises:
    ValueError:  If static checks determine `x` has wrong rank.
  """
----------
Output: 
:  Scalar integer `Tensor`.  Rank must be <= rank(x).
    data:  The tensors to print out if the condition is False.  Defaults to
      error message and first few entries of `x`.
    summarize: Print this many entries of each tensor.
    message: A string to prefix to the default message.
    name: A name for this operation (optional).  Defaults to "assert_rank_at_most".

  Returns:
    Op raising `InvalidArgumentError` unless `x` has rank equal to `rank`
----------
Solution: 

  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):
    return tf.compat.v1.assert_less_equal(
        tf.rank(x), rank, data=data, summarize=summarize, message=message)
--------------------
Prompt: 
def insert_instance_template(self, body, request_id=None, project_id=None):
        """
        Inserts instance template using body specified
        Must be called with keyword arguments rather than positional.

        :param body: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again)
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """
----------
Output: 
call is retried).
        :type request_id: str
        :param project_id: Optional, project ID.
        :type project_id: str
        :return: Instance template resource
        :rtype: dict
        """
        return self.insert_resource(
            body=body,
            request_id=request_id,
            project_id=project_id,
            resource_path=self.RESOURCE_PATH,
            resource_name='instanceTemplate',
            method_name='insert',
        )

    def
----------
Solution: 

        response = self.get_conn().instanceTemplates().insert(
            project=project_id,
            body=body,
            requestId=request_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response["name"]
        except KeyError:
            raise AirflowException(
                "Wrong response '{}' returned - it should contain "
                "'name' field".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)
--------------------
Prompt: 
def _process_op_err(self, e):
        """
        Process errors which occured while reading or parsing
        the protocol. If allow_reconnect is enabled it will
        try to switch the server to which it is currently connected
        otherwise it will disconnect.
        """
----------
Output: 

        if self.allow_reconnect and e.args[0] == 'Connection reset by peer':
            self.logger.warning('Connection reset by peer, trying to reconnect')
            self.reconnect()
        else:
            self.logger.error('Disconnecting from server due to error: %s' % e)
            self.disconnect()

    def _process_op_data(self, data):
        """
        Process data which was received from the server.
        """
        if self.protocol_version == 0:
           
----------
Solution: 

        if self.is_connecting or self.is_closed or self.is_reconnecting:
            return

        if self.options["allow_reconnect"] and self.is_connected:
            self._status = Client.RECONNECTING
            self._ps.reset()

            if self._reconnection_task is not None and not self._reconnection_task.cancelled():
                # Cancel the previous task in case it may still be running.
                self._reconnection_task.cancel()

            self._reconnection_task = self._loop.create_task(self._attempt_reconnect())
        else:
            self._process_disconnect()
            self._err = e
            yield from self._close(Client.CLOSED, True)
--------------------
Prompt: 
def get_generator(self, path, *args, **kw_args):
        """
        Get a generator that allows convenient access to the streamed data.
        Elements from the dataset are returned from the generator one row at a time.
        Unlike the direct access queue, this generator also returns the remainder elements.
        Additional arguments are forwarded to get_queue.
        See the get_queue method for documentation of these parameters.

        :param path:
        :return: A generator that iterates over the rows in the dataset.
        """
----------
Output: 

        return self.get_queue(path, *args, **kw_args)

    def get_queue(self, path, *args, **kw_args):
        """
        Get a queue that allows convenient access to the streamed data.
        Elements from the dataset are returned from the queue one row at a time.
        Unlike the direct access queue, this queue also returns the remainder elements.
        Additional arguments are forwarded to get_queue.
        See the get_queue method for documentation of these parameters.

        :param path:
       
----------
Solution: 

        q = self.get_queue(path=path, *args, **kw_args)

        try:
            # This generator just implements a standard access pattern for the direct access queue.
            for guard in q.iter():
                with guard as batch:
                    batch_copy = batch.copy()

                for row in batch_copy:
                    yield row

            last_batch = self.get_remainder(path, q.block_size)
            for row in last_batch:
                yield row

        finally:
            q.close()
--------------------
Prompt: 
def running_under_virtualenv():
    """
    Return True if we're running inside a virtualenv, False otherwise.

    """
----------
Output: 

    return hasattr(sys, 'real_prefix')


def get_virtualenv_path():
    """
    Return the path to the virtualenv we're running inside.

    """
    return sys.prefix


def get_virtualenv_site_packages_path():
    """
    Return the path to the virtualenv's site-packages directory.

    """
    return os.path.join(get_virtualenv_path(), 'lib', 'python%s' % sys.version[:3], 'site-packages')


----------
Solution: 

    if hasattr(sys, 'real_prefix'):
        return True
    elif sys.prefix != getattr(sys, "base_prefix", sys.prefix):
        return True

    return False
--------------------
Prompt: 
def build(self, pre=None, shortest=False):
        """Build the String instance

        :param list pre: The prerequisites list (optional, default=None)
        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.
        """
----------
Output: 

        if pre is None:
            pre = []
        if not isinstance(pre, list):
            raise TypeError('pre must be a list')
        if not isinstance(shortest, bool):
            raise TypeError('shortest must be a boolean')
        if not self.is_built:
            self.is_built = True
            self.pre = pre
            self.shortest = shortest
            self.build_string()

    def build_string(self):
        """Build the String instance

        :raises NotImplemented
----------
Solution: 

        if pre is None:
            pre = []

        if self.value is not None and rand.maybe():
            return utils.val(self.value, pre, shortest=shortest)

        length = super(String, self).build(pre, shortest=shortest)
        res = rand.data(length, self.charset)
        return res
--------------------
Prompt: 
def chimera_block_quotient(G, blocks):
    """
    Extract the blocks from a graph, and returns a
    block-quotient graph according to the acceptability
    functions block_good and eblock_good

    Inputs:
        G: a networkx graph
        blocks: a tuple of tuples

    """
----------
Output: 

    # Create a new graph
    H = nx.Graph()

    # Add the nodes
    for block in blocks:
        H.add_nodes_from(block)

    # Add the edges
    for block in blocks:
        for node in block:
            for neighbor in G.neighbors(node):
                if neighbor in block:
                    H.add_edge(node, neighbor)

    return H


def chimera_block_quotient_with_labels(G, blocks):
    """
    Ext
----------
Solution: 

    from networkx import Graph
    from itertools import product

    BG = Graph()
    blockid = {}
    for i, b in enumerate(blocks):
        BG.add_node(i)
        if not b or not all(G.has_node(x) for x in b):
            continue
        for q in b:
            if q in blockid:
                raise(RuntimeError, "two blocks overlap")
            blockid[q] = i

    for q, u in blockid.items():
        ublock = blocks[u]
        for p in G[q]:
            if p not in blockid:
                continue
            v = blockid[p]
            if BG.has_edge(u, v) or u == v:
                continue
            vblock = blocks[v]

            if ublock[0][2] == vblock[0][2]:
                block_edges = zip(ublock, vblock)
            else:
                block_edges = product(ublock, vblock)

            if all(G.has_edge(x, y) for x, y in block_edges):
                BG.add_edge(u, v)

    return BG
--------------------
Prompt: 
def process_commands(self):
        """Handle debugger commands."""
----------
Output: 

        while True:
            try:
                self.prompt()
                cmd = self.get_command()
                if cmd is None:
                    break
                self.process_command(cmd)
            except KeyboardInterrupt:
                self.print_("KeyboardInterrupt")
                self.print_("")
                self.prompt()

    def prompt(self):
        """Print the debugger prompt."""
        self.print_("(Pdb)")

    def get_command(self):
        """Get a command from the
----------
Solution: 

        if self.core.execution_status != 'No program':
            self.setup()
            self.location()
            pass
        leave_loop = run_hooks(self, self.preloop_hooks)
        self.continue_running = False

        while not leave_loop:
            try:
                run_hooks(self, self.precmd_hooks)
                # bdb had a True return to leave loop.
                # A more straight-forward way is to set
                # instance variable self.continue_running.
                leave_loop = self.process_command()
                if leave_loop or self.continue_running: break
            except EOFError:
                # If we have stacked interfaces, pop to the next
                # one.  If this is the last one however, we'll
                # just stick with that.  FIXME: Possibly we should
                # check to see if we are interactive.  and not
                # leave if that's the case. Is this the right
                # thing?  investigate and fix.
                if len(self.debugger.intf) > 1:
                    del self.debugger.intf[-1]
                    self.last_command = ''
                else:
                    if self.debugger.intf[-1].output:
                        self.debugger.intf[-1].output.writeline('Leaving')
                        raise Mexcept.DebuggerQuit
                        pass
                    break
                pass
            pass
        return run_hooks(self, self.postcmd_hooks)
--------------------
Prompt: 
def table(name, auth=None, eager=True):
    """Returns a given table for the given user."""
----------
Output: 

    if auth is None:
        auth = request.auth
    if auth.is_admin:
        return Table.query.get(name)
    else:
        return Table.query.filter_by(name=name, user_id=auth.id).first()


def table_exists(name, auth=None):
    """Returns True if a given table exists for the given user."""
    if auth is None:
        auth = request.auth
    if auth.is_admin:
        return Table.query.get(name)
----------
Solution: 

    auth = auth or []
    dynamodb = boto.connect_dynamodb(*auth)

    table = dynamodb.get_table(name)
    return Table(table=table, eager=eager)
--------------------
Prompt: 
def _is_raising(body: typing.List) -> bool:
    """Return true if the given statement node raise an exception"""
----------
Output: 

    if body[0] == "raise":
        return True
    elif body[0] == "try":
        return _is_raising(body[1])
    elif body[0] == "with":
        return _is_raising(body[2])
    elif body[0] == "if":
        return _is_raising(body[2]) or _is_raising(body[3])
    elif body[0] == "for":
        return _is_raising(body[3])
    elif body[0
----------
Solution: 

    for node in body:
        if isinstance(node, astroid.Raise):
            return True
    return False
--------------------
Prompt: 
def get_assembly_size(assembly_file):
    """Returns the number of nucleotides and the size per contig for the
    provided assembly file path

    Parameters
    ----------
    assembly_file : str
        Path to assembly file.

    Returns
    -------
    assembly_size : int
        Size of the assembly in nucleotides
    contig_size : dict
        Length of each contig (contig name as key and length as value)

    """
----------
Output: 

    assembly_size = 0
    contig_size = {}
    with open(assembly_file, 'r') as f:
        for line in f:
            if line.startswith('>'):
                contig_name = line.strip().split(' ')[0][1:]
            else:
                contig_size[contig_name] = len(line.strip())
                assembly_size += len(line.strip())
    return assembly_size, contig_size


def get_assembly_size_from_fasta(
----------
Solution: 


    assembly_size = 0
    contig_size = {}
    header = ""

    with open(assembly_file) as fh:
        for line in fh:

            # Skip empty lines
            if line.strip() == "":
                continue

            if line.startswith(">"):
                header = line.strip()[1:]
                contig_size[header] = 0

            else:
                line_len = len(line.strip())
                assembly_size += line_len
                contig_size[header] += line_len

    return assembly_size, contig_size
--------------------
Prompt: 
def compute_index_key(self, to_instance):
        """
        Compute the index key that can be used to identify an instance
        on the link.
        """
----------
Output: 

        return self.compute_index_key_from_instance(to_instance)

    def compute_index_key_from_instance(self, to_instance):
        """
        Compute the index key that can be used to identify an instance
        on the link.
        """
        return to_instance.get_index_key()

    def compute_index_key_from_index_key(self, to_index_key):
        """
        Compute the index key that can be used to identify an instance
        on the link.

----------
Solution: 

        kwargs = dict()
        for attr in self.key_map.values():
            if _is_null(to_instance, attr):
                return None
            
            if attr in to_instance.__dict__:
                kwargs[attr] = to_instance.__dict__[attr]
            else:
                kwargs[attr] = getattr(to_instance, attr)

        return frozenset(tuple(kwargs.items()))
--------------------
Prompt: 
def p_boolean_expression(self, p):
        """
        expression : expression LE          expression
                   | expression LESSTHAN    expression
                   | expression DOUBLEEQUAL expression
                   | expression NOTEQUAL    expression
                   | expression GE          expression
                   | expression GT          expression
                   | expression AND         expression
                   | expression OR          expression
        """
----------
Output: 

        p[0] = BooleanExpression(p[1], p[2], p[3])

    def p_boolean_expression_not(self, p):
        """
        expression : NOT expression
        """
        p[0] = BooleanExpression(p[1], p[2])

    def p_boolean_expression_paren(self, p):
        """
        expression : LPAREN expression RPAREN
        """
        p[0] = p[2]

    def p_boolean_expression_literal(self,
----------
Solution: 

        p[0] = BinaryOperationNode(left=p[1],
                                   operator=p[2],
                                   right=p[3])
--------------------
Prompt: 
def pairwise_reproducibility(df, plot=False):
    """
    Calculate the reproducibility of LA-ICPMS based on unique pairs of repeat analyses.
    
    Pairwise differences are fit with a half-Cauchy distribution, and the median and 
    95% confidence limits are returned for each analyte.
    
    Parameters
    ----------
    df : pandas.DataFrame
        A dataset
    
    plot : bool
        Whether or not to plot the resulting error distributions.
    
    Returns
    -------
    pdiffs : pandas.DataFrame
        Unique pairwise differences for all analytes.
    rep_dists : dict of scipy.stats.halfcauchy
        Half-Cauchy distribution objects fitted to the
        differences.
    rep_stats : dict of tuples
        The 50% and 95% quantiles of the half-cauchy
        distribution.
    (fig, axs) : matplotlib objects
        The figure. If not made, returnes (None, None) placeholder
    
    """
----------
Output: 
Returns
    -------
    pandas.DataFrame
        A dataframe with the median and 95% confidence limits for each analyte.
    """
    # Get the analytes
    analytes = df.columns.values[1:]
    
    # Get the unique pairs of repeat analyses
    repeats = df.index.values
    pairs = []
    for i in range(len(repeats)):
        for j in range(i+1, len(repeats)):
            pairs.append((repeats[i], repeats[j]))
----------
Solution: 

    
    ans = df.columns.values
    pdifs = []
    
    # calculate differences between unique pairs
    for ind, d in df.groupby(level=0):
        d.index = d.index.droplevel(0)

        difs = []
        for i, r in d.iterrows():
            t = d.loc[i+1:, :]
            difs.append(t[ans] - r[ans])

        pdifs.append(pd.concat(difs))
    pdifs = pd.concat(pdifs).abs()

    # calculate stats
    rep_stats = {}
    rep_dists = {}
    errfn = stats.halfcauchy
    
    for a in ans:
        d = pdifs.loc[:, a].dropna().values
        hdist = errfn.fit(d, floc=0)
        rep_dists[a] = errfn(*hdist)
        rep_stats[a] = rep_dists[a].ppf((0.5, 0.95))
    
    # make plot
    if not plot:
        return pdifs, rep_dists, rep_stats, (None, None)
    
    fig, axs = plt.subplots(1, len(ans), figsize=[len(ans) * 2, 2])
    for a, ax in zip(ans, axs):
        d = pdifs.loc[:, a].dropna().values
        hist, edges, _ = ax.hist(d, 30)
        ax.plot(edges, rep_dists[a].pdf(edges) * (sum(hist) * np.mean(np.diff(edges))))
        ax.set_title(a, loc='left')

    return pdifs, rep_dists, rep_stats, (fig, axs)
--------------------
Prompt: 
def remove_redundant_nodes(self, preserve_lengths=True):
        """
        Remove all nodes which have only a single child, and attach their
        grandchildren to their parent.  The resulting tree has the minimum
        number of internal nodes required for the number of leaves.
        :param preserve_lengths: If true, branch lengths of removed nodes are \
        added to those of their children.
        """
----------
Output: 

        # TODO: This is a very inefficient implementation.  It should be
        # replaced with something more efficient.
        # TODO: This is not a very good implementation.  It does not preserve
        # branch lengths.
        # TODO: This is not a very good implementation.  It does not preserve
        # the order of leaves.
        # TODO: This is not a very good implementation.  It does not preserve
        # the order of internal nodes.
        # TODO: This is not a very good implementation.  It does not preserve
        # the order of leaves
----------
Solution: 

        for n in self.walk(mode='postorder'):
            while n.ancestor and len(n.ancestor.descendants) == 1:
                grandfather = n.ancestor.ancestor
                father = n.ancestor
                if preserve_lengths:
                    n.length += father.length

                if grandfather:
                    for i, child in enumerate(grandfather.descendants):
                        if child is father:
                            del grandfather.descendants[i]
                    grandfather.add_descendant(n)
                    father.ancestor = None
                else:
                    self.descendants = n.descendants
                    if preserve_lengths:
                        self.length = n.length
--------------------
Prompt: 
def _set_start(self):
        """ Sets the start timestamp and formatted time to the current time. """
----------
Output: 

        self.start = time.time()
        self.start_formatted = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.start))

    def _set_end(self):
        """ Sets the end timestamp and formatted time to the current time. """
        self.end = time.time()
        self.end_formatted = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.end))

    def _set_duration
----------
Solution: 

        init_time = time.time()
        formatted_time = datetime.datetime.fromtimestamp(init_time).strftime('%Y_%m_%d_%Hh%Mm%Ss')
        run_info_dict = self._run_information[self.v_crun]
        run_info_dict['timestamp'] = init_time
        run_info_dict['time'] = formatted_time
        if self._environment_hexsha is not None:
            run_info_dict['short_environment_hexsha'] = self._environment_hexsha[0:7]
--------------------
Prompt: 
def return_type(type_name, formatter=None):
    """Specify that this function returns a typed value.

    Args:
        type_name (str): A type name known to the global typedargs type system
        formatter (str): An optional name of a formatting function specified
            for the type given in type_name.
    """
----------
Output: 

    def decorator(func):
        func.__return_type__ = type_name
        func.__return_formatter__ = formatter
        return func
    return decorator


def typed_args(func):
    """Decorator for functions that take typed arguments.

    This decorator is used to specify the types of arguments that a function
    takes.  The decorated function must be defined in a module that has been
    imported by the typedargs module.  The decorated function must take a
    single argument, which is a dictionary mapping argument names
----------
Solution: 


    def _returns(func):
        annotated(func)
        func.metadata.typed_returnvalue(type_name, formatter)
        return func

    return _returns
--------------------
Prompt: 
def load_metamodel(resource=None, load_globals=True):
    """
    Load and return a metamodel expressed in ooaofooa from a *resource*.
    The resource may be either a filename, a path, or a list of filenames
    and/or paths.
    """
----------
Output: 

    from pymod import load_model
    from pymod.model import Model

    if isinstance(resource, list):
        model = Model()
        for r in resource:
            model.load(r)
    else:
        model = load_model(resource)

    if load_globals:
        model.load_globals()

    return model


def load_metamodel_from_string(string, load_globals=True):
    """
    Load and return a metamodel expressed in ooa
----------
Solution: 

    loader = _mk_loader(resource, load_globals)
    return loader.build_metamodel()
--------------------
Prompt: 
def set_s3_credentials(secret_key_id, secret_access_key):
    """Creates a new Amazon S3 client internally with specified credentials.
    There are no validations done to the credentials. Incorrect credentials are thus revealed with first S3 import call.
    
    secretKeyId Amazon S3 Secret Key ID (provided by Amazon)
    secretAccessKey Amazon S3 Secret Access Key (provided by Amazon)
    """
----------
Output: 

    global s3_client
    s3_client = boto3.client('s3', aws_access_key_id=secret_key_id, aws_secret_access_key=secret_access_key)


def get_s3_client():
    """Returns the Amazon S3 client.
    """
    return s3_client


def get_s3_bucket_names():
    """Returns a list of all buckets in the account.
    """
    return s3_client.list_buckets()['
----------
Solution: 

    if(secret_key_id is None):
        raise H2OValueError("Secret key ID must be specified")

    if(secret_access_key is None):
        raise H2OValueError("Secret access key must be specified")
    
    if(not secret_key_id):
        raise H2OValueError("Secret key ID must not be empty")
    
    if(not secret_access_key):
        raise H2OValueError("Secret access key must not be empty")
    
    
    params = {"secret_key_id": secret_key_id,
              "secret_access_key": secret_access_key
              }
    
    h2o.api(endpoint="POST /3/PersistS3", data=params)
    print("Credentials successfully set.")
--------------------
Prompt: 
def tar_and_s3_upload(self, path, key, bucket):
        """
        Tar the local file or directory and upload to s3

        :param path: local file or directory
        :type path: str
        :param key: s3 key
        :type key: str
        :param bucket: s3 bucket
        :type bucket: str
        :return: None
        """
----------
Output: 

        if not os.path.exists(path):
            raise ValueError('Path does not exist: {}'.format(path))

        # tar the file or directory
        tar_file = self.tar_file(path)

        # upload the tar file to s3
        self.s3_upload(tar_file, key, bucket)

    def tar_file(self, path):
        """
        Tar the local file or directory

        :param path: local file or directory
        :type path: str
        :return: tar file
----------
Solution: 

        with tempfile.TemporaryFile() as temp_file:
            if os.path.isdir(path):
                files = [os.path.join(path, name) for name in os.listdir(path)]
            else:
                files = [path]
            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:
                for f in files:
                    tar_file.add(f, arcname=os.path.basename(f))
            temp_file.seek(0)
            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)
--------------------
Prompt: 
def _make_default_operation_costs(self, allow_spaces=False):
        """
        sets 1.0 cost for every replacement, insertion, deletion and transposition
        """
----------
Output: 

        if allow_spaces:
            self.insertion_cost = 1.0
            self.deletion_cost = 1.0
            self.substitution_cost = 1.0
            self.transposition_cost = 1.0
        else:
            self.insertion_cost = 1.0
            self.deletion_cost = 1.0
            self.substitution_cost = 1.0
            self.transposition_cost = 1.0

    def _make_default_operation_
----------
Solution: 

        self.operation_costs = dict()
        self.operation_costs[""] = {c: 1.0 for c in list(self.alphabet) + [' ']}
        for a in self.alphabet:
            current_costs = {c: 1.0 for c in self.alphabet}
            current_costs[a] = 0.0
            current_costs[""] = 1.0
            if allow_spaces:
                current_costs[" "] = 1.0
            self.operation_costs[a] = current_costs
        # транспозиции
        for a, b in itertools.permutations(self.alphabet, 2):
            self.operation_costs[a + b] = {b + a: 1.0}
        # пробелы
        if allow_spaces:
            self.operation_costs[" "] = {c: 1.0 for c in self.alphabet}
            self.operation_costs[" "][""] = 1.0
--------------------
Prompt: 
def _dir_exists(db, user_id, db_dirname):
    """
    Internal implementation of dir_exists.

    Expects a db-style path name.
    """
----------
Output: 

    return db.exists(user_id, db_dirname)


def _dir_create(db, user_id, db_dirname):
    """
    Internal implementation of dir_create.

    Expects a db-style path name.
    """
    return db.create(user_id, db_dirname)


def _dir_delete(db, user_id, db_dirname):
    """
    Internal implementation of dir_delete.

    Expects a db-style path name.
    """
   
----------
Solution: 

    return db.execute(
        select(
            [func.count(directories.c.name)],
        ).where(
            and_(
                directories.c.user_id == user_id,
                directories.c.name == db_dirname,
            ),
        )
    ).scalar() != 0
--------------------
Prompt: 
def create(self, name, description=None, color=None):
        """
        Creates a new label and returns the response

        :param name: The label name
        :type name: str

        :param description: An optional description for the label. The name is
            used if no description is provided.
        :type description: str

        :param color: The hex color for the label (ex: 'ff0000' for red). If no
            color is provided, a random one will be assigned.
        :type color: str

        :returns: The response of your post
        :rtype: dict

        :raises: This will raise a
            :class:`ServerException<logentries_api.exceptions.ServerException>`
            if there is an error from Logentries
        """
----------
Output: 


        :returns: The label object
        :rtype: dict
        """
        data = {
            'name': name,
        }

        if description:
            data['description'] = description

        if color:
            data['color'] = color

        return self._post(self.ENDPOINT, data=data)

    def update(self, label_id, name=None, description=None, color=None):
        """
        Updates an existing label and returns the response

        :param label_id:
----------
Solution: 

        data = {
            'name': name,
            'title': name,
            'description': description or name,
            'appearance': {
                'color': color or random_color()
            }
        }
        # Yes, it's confusing. the `/tags/` endpoint is used for labels
        return self._post(
            request=ApiActions.CREATE.value,
            uri=ApiUri.TAGS.value,
            params=data
        )
--------------------
Prompt: 
def create(self, teamId, personId=None, personEmail=None,
               isModerator=False, **request_parameters):
        """Add someone to a team by Person ID or email address.

        Add someone to a team by Person ID or email address; optionally making
        them a moderator.

        Args:
            teamId(basestring): The team ID.
            personId(basestring): The person ID.
            personEmail(basestring): The email address of the person.
            isModerator(bool): Set to True to make the person a team moderator.
            **request_parameters: Additional request parameters (provides
                support for parameters that may be added in the future).

        Returns:
            TeamMembership: A TeamMembership object with the details of the
            created team membership.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError: If the Webex Teams cloud returns an error.

        """
----------
Output: 
bool): Whether the person should be a moderator.

        Returns:
            TeamMember: The created team member.

        Raises:
            :class:`pydro.core.exceptions.ApiError`: If the API returns an error.
        """
        if personId is None and personEmail is None:
            raise ValueError("Must provide either personId or personEmail")

        if personId is not None and personEmail is not None:
            raise ValueError("Must provide either personId or personEmail, not both")

        if person
----------
Solution: 

        check_type(teamId, basestring, may_be_none=False)
        check_type(personId, basestring)
        check_type(personEmail, basestring)
        check_type(isModerator, bool)

        post_data = dict_from_items_with_values(
            request_parameters,
            teamId=teamId,
            personId=personId,
            personEmail=personEmail,
            isModerator=isModerator,
        )

        # API request
        json_data = self._session.post(API_ENDPOINT, json=post_data)

        # Return a team membership object created from the response JSON data
        return self._object_factory(OBJECT_TYPE, json_data)
--------------------
Prompt: 
def visit(self, visit_uri):
        """
        Navigate to the given URL. The URL can either be a relative URL or an absolute URL. The
        behavior of either depends on the driver. ::

            session.visit("/foo")
            session.visit("http://google.com")

        For drivers which can run against an external application, such as the Selenium driver,
        giving an absolute URL will navigate to that page. This allows testing applications running
        on remote servers. For these drivers, setting :data:`capybara.app_host` will make the
        remote server the default. For example::

            capybara.app_host = "http://google.com"
            session.visit("/")  # visits the Google homepage

        Args:
            visit_uri (str): The URL to navigate to.
        """
----------
Output: 
host` will allow
        relative URLs to be used.

        For drivers which are designed to run against a single application, such as the RackTest
        driver, giving an absolute URL will raise an error. This is because the application under
        test is known and should be given explicitly.

        :param visit_uri: the URL to visit
        :type visit_uri: str
        """
        raise NotImplementedError()

    def current_url(self):
        """
        Returns the current URL.

        :return: the current URL
       
----------
Solution: 


        self.raise_server_error()

        visit_uri = urlparse(visit_uri)

        if capybara.app_host:
            uri_base = urlparse(capybara.app_host)
        elif self.server:
            uri_base = urlparse("http://{}:{}".format(self.server.host, self.server.port))
        else:
            uri_base = None

        visit_uri = ParseResult(
            scheme=visit_uri.scheme or (uri_base.scheme if uri_base else ""),
            netloc=visit_uri.netloc or (uri_base.netloc if uri_base else ""),
            path=visit_uri.path,
            params=visit_uri.params,
            query=visit_uri.query,
            fragment=visit_uri.fragment)

        self.driver.visit(visit_uri.geturl())
--------------------
Prompt: 
def from_join(cls, join: Join) -> 'ConditionalJoin':
        """Creates a new :see:ConditionalJoin from the
        specified :see:Join object.

        Arguments:
            join:
                The :see:Join object to create the
                :see:ConditionalJoin object from.

        Returns:
            A :see:ConditionalJoin object created from
            the :see:Join object.
        """
----------
Output: 

        return cls(
            join.left,
            join.right,
            join.condition,
            join.join_type,
            join.join_type_name,
            join.join_type_description,
            join.join_type_abbreviation,
            join.join_type_symbol,
            join.join_type_symbol_abbreviation,
            join.join_type_symbol_description,
            join.join_type_symbol_abbreviation_description,
            join.join_type_symbol_abbre
----------
Solution: 


        return cls(
            join.table_name,
            join.parent_alias,
            join.table_alias,
            join.join_type,
            join.join_field,
            join.nullable
        )
--------------------
Prompt: 
def process_tokens(self, tokens):
        """process tokens from the current module to search for module/block
        level options
        """
----------
Output: 

        for token in tokens:
            if token.type == 'NAME' and token.value == 'options':
                self.process_options(tokens)
            elif token.type == 'NAME' and token.value == 'block':
                self.process_block(tokens)
            elif token.type == 'NAME' and token.value == 'module':
                self.process_module(tokens)
            elif token.type == 'NAME' and token.value == 'include':
                self.process_include(tokens)
            elif token.
----------
Solution: 

        control_pragmas = {"disable", "enable"}
        for (tok_type, content, start, _, _) in tokens:
            if tok_type != tokenize.COMMENT:
                continue
            match = OPTION_RGX.search(content)
            if match is None:
                continue

            first_group = match.group(1)
            if (
                first_group.strip() == "disable-all"
                or first_group.strip() == "skip-file"
            ):
                if first_group.strip() == "disable-all":
                    self.add_message(
                        "deprecated-pragma",
                        line=start[0],
                        args=("disable-all", "skip-file"),
                    )
                self.add_message("file-ignored", line=start[0])
                self._ignore_file = True
                return
            try:
                opt, value = first_group.split("=", 1)
            except ValueError:
                self.add_message(
                    "bad-inline-option", args=first_group.strip(), line=start[0]
                )
                continue
            opt = opt.strip()
            if opt in self._options_methods or opt in self._bw_options_methods:
                try:
                    meth = self._options_methods[opt]
                except KeyError:
                    meth = self._bw_options_methods[opt]
                    # found a "(dis|en)able-msg" pragma deprecated suppression
                    self.add_message(
                        "deprecated-pragma",
                        line=start[0],
                        args=(opt, opt.replace("-msg", "")),
                    )
                for msgid in utils._splitstrip(value):
                    # Add the line where a control pragma was encountered.
                    if opt in control_pragmas:
                        self._pragma_lineno[msgid] = start[0]

                    try:
                        if (opt, msgid) == ("disable", "all"):
                            self.add_message(
                                "deprecated-pragma",
                                line=start[0],
                                args=("disable=all", "skip-file"),
                            )
                            self.add_message("file-ignored", line=start[0])
                            self._ignore_file = True
                            return
                        meth(msgid, "module", start[0])
                    except exceptions.UnknownMessageError:
                        self.add_message("bad-option-value", args=msgid, line=start[0])
            else:
                self.add_message("unrecognized-inline-option", args=opt, line=start[0])
--------------------
Prompt: 
def _flags_changed(self, name, old, new):
        """ensure flags dict is valid"""
----------
Output: 

        if new is None:
            new = {}
        elif not isinstance(new, dict):
            raise TypeError("flags must be a dict")
        if old is None:
            old = {}
        elif not isinstance(old, dict):
            raise TypeError("flags must be a dict")
        if set(old.keys()) != set(new.keys()):
            raise ValueError("flags must have the same keys")
        for k, v in new.items():
            if not isinstance(v, bool):
                raise TypeError("flags must be
----------
Solution: 

        for key,value in new.iteritems():
            assert len(value) == 2, "Bad flag: %r:%s"%(key,value)
            assert isinstance(value[0], (dict, Config)), "Bad flag: %r:%s"%(key,value)
            assert isinstance(value[1], basestring), "Bad flag: %r:%s"%(key,value)
--------------------
Prompt: 
def images(self, query=None):
    """List local images in the database, optionally with a query.

       Paramters
       =========
       query: a string to search for in the container or collection name|tag|uri

    """
----------
Output: 

    if query is None:
        query = ''
    query = query.lower()
    return [i for i in self.images if query in i.lower()]

def containers(self, query=None):
    """List local containers in the database, optionally with a query.

       Paramters
       =========
       query: a string to search for in the container or collection name|tag|uri

    """
    if query is None:
        query = ''
    query = query.lower()
    return [i for i in self
----------
Solution: 

    from sregistry.database.models import Collection, Container

    rows = []
    if query is not None:   
        like = "%" + query + "%"
        containers = Container.query.filter(or_(Container.name == query,
                                                Container.tag.like(like),
                                                Container.uri.like(like),
                                                Container.name.like(like))).all() 
    else:
        containers = Container.query.all()

    if len(containers) > 0:
        message = "  [date]   [client]\t[uri]"
        bot.custom(prefix='Containers:', message=message, color="RED")
        for c in containers:
            uri = c.get_uri()
            created_at = c.created_at.strftime('%B %d, %Y')
            rows.append([created_at, "   [%s]" %c.client, uri])
        bot.table(rows) 
    return containers
--------------------
Prompt: 
def load_tiff_multipage(tiff_filename, dtype='float32'):
    """
    Load a multipage tiff into a single variable in x,y,z format.

    Arguments:
        tiff_filename:     Filename of source data
        dtype:             data type to use for the returned tensor

    Returns:
        Array containing contents from input tiff file in xyz order
    """
----------
Output: 

    import tifffile
    import numpy as np

    with tifffile.TiffFile(tiff_filename) as tif:
        data = np.zeros((tif.series[0].shape[0], tif.series[0].shape[1], len(tif.series)), dtype=dtype)
        for i, page in enumerate(tif.series):
            data[:, :, i] = page.asarray()

    return data


def load_tiff_stack(tiff_filename,
----------
Solution: 

    if not os.path.isfile(tiff_filename):
        raise RuntimeError('could not find file "%s"' % tiff_filename)

    # load the data from multi-layer TIF files
    data = tiff.imread(tiff_filename)

    im = []

    while True:

        Xi = numpy.array(data, dtype=dtype)
        if Xi.ndim == 2:
            Xi = Xi[numpy.newaxis, ...]  # add slice dimension
        im.append(Xi)

        try:
            data.seek(data.tell()+1)
        except EOFError:
            break  # this just means hit end of file (not really an error)

    im = numpy.concatenate(im, axis=0)  # list of 2d -> tensor
    im = numpy.rollaxis(im, 1)
    im = numpy.rollaxis(im, 2)

    return im
--------------------
Prompt: 
def with_setup(setup=None, teardown=None):
    """Decorator to add setup and/or teardown methods to a test function::

      @with_setup(setup, teardown)
      def test_something():
          " ... "

    Note that `with_setup` is useful *only* for test functions, not for test
    methods or inside of TestCase subclasses.
    """
----------
Output: 

    def decorate(func):
        if setup is not None:
            func.setUp = setup
        if teardown is not None:
            func.tearDown = teardown
        return func
    return decorate


def with_teardown(teardown):
    """Decorator to add a teardown method to a test function::

      @with_teardown(teardown)
      def test_something():
          " ... "

    Note that `with_teardown` is useful *only
----------
Solution: 

    def decorate(func, setup=setup, teardown=teardown):
        if setup:
            if hasattr(func, 'setup'):
                _old_s = func.setup
                def _s():
                    setup()
                    _old_s()
                func.setup = _s
            else:
                func.setup = setup
        if teardown:
            if hasattr(func, 'teardown'):
                _old_t = func.teardown
                def _t():
                    _old_t()
                    teardown()
                func.teardown = _t
            else:
                func.teardown = teardown
        return func
    return decorate
--------------------
Prompt: 
def processFlat(self):
        """Main process.
        Returns
        -------
        est_idx : np.array(N)
            Estimated indeces for the segment boundaries in frames.
        est_labels : np.array(N-1)
            Estimated labels for the segments.
        """
----------
Output: 

        # Initialize
        est_idx = np.zeros(self.n_frames, dtype=np.int)
        est_labels = np.zeros(self.n_frames - 1, dtype=np.int)

        # Process
        for i in range(self.n_frames):
            if self.flat[i] == 0:
                est_idx[i] = 1
            else:
                est_idx[i] = 0

        # Post-process
        est_idx = est_idx.ast
----------
Solution: 

        # Preprocess to obtain features, times, and input boundary indeces
        F = self._preprocess()

        # Normalize
        F = U.normalize(F, norm_type=self.config["label_norm_feats"],
                        floor=self.config["label_norm_floor"],
                        min_db=self.config["label_norm_min_db"])

        # Find the labels using 2D-FMCs
        est_labels = compute_similarity(F, self.in_bound_idxs,
                                        dirichlet=self.config["dirichlet"],
                                        xmeans=self.config["xmeans"],
                                        k=self.config["k"],
                                        offset=self.config["2dfmc_offset"])

        # Post process estimations
        self.in_bound_idxs, est_labels = self._postprocess(self.in_bound_idxs,
                                                           est_labels)

        return self.in_bound_idxs, est_labels
--------------------
Prompt: 
def find_handfile(names=None):
    """
    尝试定位 ``handfile`` 文件，明确指定或逐级搜索父路径

    :param str names: 可选，待查找的文件名，主要用于调试，默认使用终端传入的配置
    :return: ``handfile`` 文件所在的绝对路径，默认为 None
    :rtype: str
    """
----------
Output: 
:rtype: str
    """
    if names is None:
        names = sys.argv[1:]
    if len(names) == 0:
        return None
    if len(names) == 1:
        return names[0]
    for name in names:
        if os.path.exists(name):
            return name
    return find_handfile(names[:-1])


def find_handfile_by_name(name):
    """
    尝试定位 ``handfile`` 文件，根
----------
Solution: 

    # 如果没有明确指定，则包含 env 中的值
    names = names or [env.handfile]

    # 若无 ``.py`` 扩展名，则作为待查询名称，追加到 names 末尾
    if not names[0].endswith('.py'):
        names += [names[0] + '.py']

    # name 中是否包含路径元素
    if os.path.dirname(names[0]):
        # 若存在，则扩展 Home 路径标志，并测试是否存在
        for name in names:
            expanded = os.path.expanduser(name)
            if os.path.exists(expanded):
                if name.endswith('.py') or _is_package(expanded):
                    return os.path.abspath(expanded)
    else:
        # 否则，逐级向上搜索，直到根路径
        path = '.'

        # 在到系统根路径之前停止
        while os.path.split(os.path.abspath(path))[1]:
            for name in names:
                joined = os.path.join(path, name)
                if os.path.exists(joined):
                    if name.endswith('.py') or _is_package(joined):
                        return os.path.abspath(joined)
            path = os.path.join('..', path)

    return None
--------------------
Prompt: 
def remote_upload(self, remote_url, folder_id=None, headers=None):
        """Used to make a remote file upload to openload.co

        Note:
            If folder_id is not provided, the file will be uploaded to ``Home`` folder.

        Args:
            remote_url (str): direct link of file to be remotely downloaded.
            folder_id (:obj:`str`, optional): folder-ID to upload to.
            headers (:obj:`dict`, optional): additional HTTP headers (e.g. Cookies or HTTP Basic-Auth)

        Returns:
            dict: dictionary containing ("id": uploaded file id, "folderid"). ::

                {
                    "id": "12",
                    "folderid": "4248"
                }

        """
----------
Output: 
).

        Returns:
            :obj:`dict`: response from openload.co.

        Raises:
            :obj:`ValueError`: if ``remote_url`` is not a valid URL.
            :obj:`ValueError`: if ``folder_id`` is not a valid folder-ID.
            :obj:`Exception`: if the response from openload.co is not 200.

        """
        if not is_url(remote_url):
            raise ValueError("remote_url must be a valid URL.")

        if folder_
----------
Solution: 


        kwargs = {'folder': folder_id, 'headers': headers}
        params = {'url': remote_url}
        params.update({key: value for key, value in kwargs.items() if value})

        return self._get('remotedl/add', params=params)
--------------------
Prompt: 
def list(self, **request_parameters):
        """List all roles.

        Args:
            **request_parameters: Additional request parameters (provides
                support for parameters that may be added in the future).

        Returns:
            GeneratorContainer: A GeneratorContainer which, when iterated,
            yields the roles returned by the Webex Teams query.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError: If the Webex Teams cloud returns an error.

        """
----------
Output: 

        return self.list_roles(**request_parameters)

    def list_roles(self, **request_parameters):
        """List all roles.

        Args:
            **request_parameters: Additional request parameters (provides
                support for parameters that may be added in the future).

        Returns:
            GeneratorContainer: A GeneratorContainer which, when iterated,
            yields the roles returned by the Webex Teams query.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError:
----------
Solution: 

        # API request - get items
        items = self._session.get_items(
            API_ENDPOINT,
            params=request_parameters
        )

        # Yield role objects created from the returned JSON objects
        for item in items:
            yield self._object_factory(OBJECT_TYPE, item)
--------------------
Prompt: 
def delete_all_checkpoints(self, path):
        """Delete all checkpoints for the given path."""
----------
Output: 

        for f in os.listdir(path):
            if f.startswith("checkpoint"):
                os.remove(os.path.join(path, f))

    def _get_checkpoint_path(self, path):
        """Get the path to the latest checkpoint."""
        checkpoints = [f for f in os.listdir(path) if f.startswith("checkpoint")]
        if len(checkpoints) == 0:
            return None
        return os.path.join(path, max(checkpoints, key
----------
Solution: 

        with self.engine.begin() as db:
            delete_remote_checkpoints(db, self.user_id, path)
--------------------
Prompt: 
def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:
        """
        Add hdl process to execution queue

        :param trigger: instance of SimSignal
        :param proc: python generator function representing HDL process
        """
----------
Output: 

        self._run.addHdlProc(trigger, proc)

    def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:
        """
        Add hdl process to execution queue

        :param trigger: instance of SimSignal
        :param proc: python generator function representing HDL process
        """
        self._run.addHdlProc(trigger, proc)

    def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:
        """
        Add hdl
----------
Solution: 

        # first process in time has to plan executing of apply values on the
        # end of this time
        if not self._applyValPlaned:
            # (apply on end of this time to minimalize process reevaluation)
            self._scheduleApplyValues()

        if isEvDependentOn(trigger, proc):
            if self.now == 0:
                return  # pass event dependent on startup
            self._seqProcsToRun.append(proc)
        else:
            self._combProcsToRun.append(proc)
--------------------
Prompt: 
def convert(self, amount, currency, new_currency='EUR', date=None):
        """Convert amount from a currency to another one.

        :param float amount: The amount of `currency` to convert.
        :param str currency: The currency to convert from.
        :param str new_currency: The currency to convert to.
        :param datetime.date date: Use the conversion rate of this date. If this
            is not given, the most recent rate is used.

        :return: The value of `amount` in `new_currency`.
        :rtype: float

        >>> from datetime import date
        >>> c = CurrencyConverter()
        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))
        137.5...
        >>> c.convert(100, 'USD', date=date(2014, 3, 28))
        72.67...
        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))
        Traceback (most recent call last):
        RateNotFoundError: BGN has no rate for 2010-11-21
        """
----------
Output: 

        :rtype: float
        """
        if currency == new_currency:
            return amount

        if date is None:
            date = datetime.date.today()

        rate = self.get_rate(currency, new_currency, date)
        return amount * rate

    def get_rate(self, currency, new_currency, date):
        """Get the conversion rate for a currency pair.

        :param str currency: The currency to convert from.
        :param str new_currency: The currency to convert to.
        :
----------
Solution: 

        for c in currency, new_currency:
            if c not in self.currencies:
                raise ValueError('{0} is not a supported currency'.format(c))

        if date is None:
            date = self.bounds[currency].last_date
        else:
            try:
                date = date.date()  # fallback if input was a datetime object
            except AttributeError:
                pass

        r0 = self._get_rate(currency, date)
        r1 = self._get_rate(new_currency, date)

        return float(amount) / r0 * r1
--------------------
Prompt: 
def user_config_file(self):
        """Get the absolute path to the user config file."""
----------
Output: 

        return os.path.join(self.user_dir, 'config.json')

    @property
    def user_data_dir(self):
        """Get the absolute path to the user data directory."""
        return os.path.join(self.user_dir, 'data')

    @property
    def user_dir(self):
        """Get the absolute path to the user directory."""
        return os.path.join(self.home_dir, '.config', 'sos')

    @property
    def user_log
----------
Solution: 

        return os.path.join(
            get_user_config_dir(self.app_name, self.app_author),
            self.filename)
--------------------
Prompt: 
def build_url_field(self, field_name, model_class):
        """
        This is needed due to DRF's model serializer uses the queryset to build url name

        # TODO: Move this to own serializer mixin or fix problem elsewhere?
        """
----------
Output: 

        field = self.fields[field_name]
        if isinstance(field, serializers.HyperlinkedRelatedField):
            field.view_name = model_class._meta.model_name + '-detail'

    def build_url_fields(self, model_class):
        """
        This is needed due to DRF's model serializer uses the queryset to build url name

        # TODO: Move this to own serializer mixin or fix problem elsewhere?
        """
        for field_name in self.fields:
           
----------
Solution: 

        field, kwargs = super().build_url_field(field_name, model_class)

        view = self.root.context["view"]
        kwargs["view_name"] = view.get_url_name("detail")

        return field, kwargs
--------------------
Prompt: 
def list_templates(self, extensions=None, filter_func=None):
        """Returns a list of templates for this environment.  This requires
        that the loader supports the loader's
        :meth:`~BaseLoader.list_templates` method.

        If there are other files in the template folder besides the
        actual templates, the returned list can be filtered.  There are two
        ways: either `extensions` is set to a list of file extensions for
        templates, or a `filter_func` can be provided which is a callable that
        is passed a template name and should return `True` if it should end up
        in the result list.

        If the loader does not support that, a :exc:`TypeError` is raised.

        .. versionadded:: 2.4
        """
----------
Output: 
       takes a string and returns a boolean.  If the string is a template
        name, the callable should return True if the name should be included
        in the list.

        :param extensions: a list of file extensions for templates
        :param filter_func: a callable that takes a string and returns a
                            boolean
        :return: a list of template names
        """
        return self.loader.list_templates(extensions, filter_func)

    def get_template(self, template):
        """Returns a template object for the given template
----------
Solution: 

        x = self.loader.list_templates()
        if extensions is not None:
            if filter_func is not None:
                raise TypeError('either extensions or filter_func '
                                'can be passed, but not both')
            filter_func = lambda x: '.' in x and \
                                    x.rsplit('.', 1)[1] in extensions
        if filter_func is not None:
            x = ifilter(filter_func, x)
        return x
--------------------
Prompt: 
def find_debugged_frame(frame):
    """Find the first frame that is a debugged frame. We do this
    Generally we want traceback information without polluting it with
    debugger frames. We can tell these because those are frames on the
    top which don't have f_trace set. So we'll look back from the top
    to find the fist frame where f_trace is set.
    """
----------
Output: 

    while frame.f_trace is None:
        frame = frame.f_back
    return frame


def get_traceback_info(frame):
    """Get the traceback info for the given frame.
    """
    frame = find_debugged_frame(frame)
    filename = frame.f_code.co_filename
    lineno = frame.f_lineno
    return filename, lineno


def get_traceback_info_for_exception(exc_info):
    """Get the traceback info for the given
----------
Solution: 

    f_prev = f = frame
    while f is not None and f.f_trace is None:
        f_prev = f
        f = f.f_back
        pass
    if f_prev:
        val = f_prev.f_locals.get('tracer_func_frame')
        if val == f_prev:
            if f_prev.f_back:
                f_prev = f_prev.f_back
                pass
            pass
        pass
    else:
        return frame
    return f_prev
--------------------
Prompt: 
def parent_frame_arguments():
  """Returns parent frame arguments.

  When called inside a function, returns a dictionary with the caller's function
  arguments. These are positional arguments and keyword arguments (**kwargs),
  while variable arguments (*varargs) are excluded.

  When called at global scope, this will return an empty dictionary, since there
  are no arguments.

  WARNING: If caller function argument names are overloaded before invoking
  this method, then values will reflect the overloaded value. For this reason,
  we recommend calling `parent_frame_arguments` at the beginning of the
  function.
  """
----------
Output: 
parent_frame_arguments` at the end of the function
  body.

  Returns:
    A dictionary with caller function arguments.
  """
  frame = inspect.currentframe().f_back
  args, _, _, values = inspect.getargvalues(frame)
  return dict(zip(args, values))


def get_caller_frame_arguments():
  """Returns caller frame arguments.

  When called inside a function, returns a dictionary with the caller's caller
  function arguments. These are positional arguments and keyword arguments
  (**
----------
Solution: 

  # All arguments and the names used for *varargs, and **kwargs
  arg_names, variable_arg_name, keyword_arg_name, local_vars = (
      tf_inspect._inspect.getargvalues(  # pylint: disable=protected-access
          # Get the first frame of the caller of this method.
          tf_inspect._inspect.stack()[1][0]))  # pylint: disable=protected-access

  # Remove the *varargs, and flatten the **kwargs. Both are
  # nested lists.
  local_vars.pop(variable_arg_name, {})
  keyword_args = local_vars.pop(keyword_arg_name, {})

  final_args = {}
  # Copy over arguments and their values. In general, local_vars
  # may contain more than just the arguments, since this method
  # can be called anywhere in a function.
  for arg_name in arg_names:
    final_args[arg_name] = local_vars.pop(arg_name)
  final_args.update(keyword_args)

  return final_args
--------------------
Prompt: 
def _process_bit_id(self, node):
        """Process an Id or IndexedId node as a bit or register type.

        Return a list of tuples (Register,index).
        """
----------
Output: 

        if isinstance(node, Id):
            return [(self._process_id(node), None)]
        elif isinstance(node, IndexedId):
            return [(self._process_id(node.id), node.index)]
        else:
            raise ValueError("Unexpected node type: %s" % type(node))

    def _process_id(self, node):
        """Process an Id node as a bit or register type.

        Return a Register object.
        """
        if isinstance(node, Id):
           
----------
Solution: 

        # pylint: disable=inconsistent-return-statements
        reg = None
        if node.name in self.dag.qregs:
            reg = self.dag.qregs[node.name]
        elif node.name in self.dag.cregs:
            reg = self.dag.cregs[node.name]
        else:
            raise QiskitError("expected qreg or creg name:",
                              "line=%s" % node.line,
                              "file=%s" % node.file)

        if node.type == "indexed_id":
            # An indexed bit or qubit
            return [(reg, node.index)]
        elif node.type == "id":
            # A qubit or qreg or creg
            if not self.bit_stack[-1]:
                # Global scope
                return [(reg, j) for j in range(reg.size)]
            else:
                # local scope
                if node.name in self.bit_stack[-1]:
                    return [self.bit_stack[-1][node.name]]
                raise QiskitError("expected local bit name:",
                                  "line=%s" % node.line,
                                  "file=%s" % node.file)
        return None
--------------------
Prompt: 
def _backup(self):
        """
        Save the current database into the inactive-db.json file.
        """
----------
Output: 

        self.logger.info("Backing up database...")
        self.db.backup()

    def _restore(self):
        """
        Restore the database from the inactive-db.json file.
        """
        self.logger.info("Restoring database...")
        self.db.restore()

    def _create_db(self):
        """
        Create a new database.
        """
        self.logger.info("Creating database...")
        self.db.create()

    def _delete
----------
Solution: 


        if PyFunceble.CONFIGURATION["inactive_database"]:
            # The database subsystem is activated.

            # We save the current database state into the database file.
            Dict(PyFunceble.INTERN["inactive_db"]).to_json(self.inactive_db_path)
--------------------
Prompt: 
def get_pandas_df(self, hql, schema='default'):
        """
        Get a pandas dataframe from a Hive query

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to 'default'.
        :type schema: str
        :return: result of hql execution
        :rtype: DataFrame

        >>> hh = HiveServer2Hook()
        >>> sql = "SELECT * FROM airflow.static_babynames LIMIT 100"
        >>> df = hh.get_pandas_df(sql)
        >>> len(df.index)
        100

        :return: pandas.DateFrame
        """
----------
Output: 
100"
        >>> df = hh.get_pandas_df(sql)
        >>> df.head()
        """
        df = self.get_pandas_df_from_query(hql, schema)
        return df

    def get_pandas_df_from_query(self, hql, schema='default'):
        """
        Get a pandas dataframe from a Hive query

        :param hql: hql to be executed.
        :type hql: str or list
        :param schema: target schema, default to '
----------
Solution: 

        import pandas as pd
        res = self.get_results(hql, schema=schema)
        df = pd.DataFrame(res['data'])
        df.columns = [c[0] for c in res['header']]
        return df
--------------------
Prompt: 
def get_item_abspath(self, identifier):
        """Return absolute path at which item content can be accessed.

        :param identifier: item identifier
        :returns: absolute path from which the item content can be accessed
        """
----------
Output: 

        raise NotImplementedError

    def get_item_content(self, identifier):
        """Return item content.

        :param identifier: item identifier
        :returns: item content
        """
        raise NotImplementedError

    def get_item_metadata(self, identifier):
        """Return item metadata.

        :param identifier: item identifier
        :returns: item metadata
        """
        raise NotImplementedError

    def get_item_properties(self, identifier):
        """Return item properties.

       
----------
Solution: 

        admin_metadata = self.get_admin_metadata()
        uuid = admin_metadata["uuid"]
        # Create directory for the specific dataset.
        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)
        mkdir_parents(dataset_cache_abspath)

        bucket_fpath = self.data_key_prefix + identifier
        obj = self.s3resource.Object(self.bucket, bucket_fpath)
        relpath = obj.get()['Metadata']['handle']
        _, ext = os.path.splitext(relpath)

        local_item_abspath = os.path.join(
            dataset_cache_abspath,
            identifier + ext
        )
        if not os.path.isfile(local_item_abspath):

            tmp_local_item_abspath = local_item_abspath + ".tmp"
            self.s3resource.Bucket(self.bucket).download_file(
                bucket_fpath,
                tmp_local_item_abspath
            )
            os.rename(tmp_local_item_abspath, local_item_abspath)

        return local_item_abspath
--------------------
Prompt: 
def record(self, func):
        """Registers a function that is called when the blueprint is
        registered on the application.  This function is called with the
        state as argument as returned by the :meth:`make_setup_state`
        method.
        """
----------
Output: 


        self.record_once(func)
        return func

    def record_once(self, func):
        """Registers a function that is called when the blueprint is
        registered on the application.  This function is called with the
        state as argument as returned by the :meth:`make_setup_state`
        method.  The function will only be called once.
        """

        if func not in self.record_once_funcs:
            self.record_once_funcs.append(func)

    def before_
----------
Solution: 

        if self._got_registered_once and self.warn_on_modifications:
            from warnings import warn
            warn(Warning('The blueprint was already registered once '
                         'but is getting modified now.  These changes '
                         'will not show up.'))
        self.deferred_functions.append(func)
--------------------
Prompt: 
def find_sourcemap_comment(filepath, block_size=100):
    """
    Seeks and removes the sourcemap comment. If found, the sourcemap line is
    returned.

    Bundled output files can have massive amounts of lines, and the sourceMap
    comment is always at the end. So, to extract it efficiently, we read out the
    lines of the file starting from the end. We look back at most 2 lines.

    :param:filepath: path to output bundle file containing the sourcemap comment
    :param:blocksize: integer saying how many bytes to read at once
    :return:string with the sourcemap comment or None
    """
----------
Output: 
cemap comment
    :param:block_size: number of lines to read at a time
    :return: sourcemap line if found, None otherwise
    """
    with open(filepath, 'r') as f:
        lines = []
        for line in f:
            lines.append(line)
        if len(lines) == 0:
            return None
        for i in range(len(lines) - 1, -1, -1):
            if lines[i].startswith('//# sourceMappingURL='):
                return
----------
Solution: 


    MAX_TRACKBACK = 2  # look back at most 2 lines, catching potential blank line at the end

    block_number = -1
    # blocks of size block_size, in reverse order starting from the end of the file
    blocks = []
    sourcemap = None

    try:
        # open file in binary read+write mode, so we can seek with negative offsets
        of = io.open(filepath, 'br+')
        # figure out what's the end byte
        of.seek(0, os.SEEK_END)
        block_end_byte = of.tell()

        # track back for maximum MAX_TRACKBACK lines and while we can track back
        while block_end_byte > 0 and MAX_TRACKBACK > 0:
            if (block_end_byte - block_size > 0):
                # read the last block we haven't yet read
                of.seek(block_number*block_size, os.SEEK_END)
                blocks.append(of.read(block_size))
            else:
                # file too small, start from begining
                of.seek(0, os.SEEK_SET)
                # only read what was not read
                blocks = [of.read(block_end_byte)]

            # update variables that control while loop
            content = b''.join(reversed(blocks))
            lines_found = content.count(b'\n')
            MAX_TRACKBACK -= lines_found
            block_end_byte -= block_size
            block_number -= 1

            # early check and bail out if we found the sourcemap comment
            if SOURCEMAPPING_URL_COMMENT in content:
                offset = 0
                # splitlines eats the last \n if its followed by a blank line
                lines = content.split(b'\n')
                for i, line in enumerate(lines):
                    if line.startswith(SOURCEMAPPING_URL_COMMENT):
                        offset = len(line)
                        sourcemap = line
                        break
                while i+1 < len(lines):
                    offset += 1  # for the newline char
                    offset += len(lines[i+1])
                    i += 1
                # track back until the start of the comment, and truncate the comment
                if sourcemap:
                    offset += 1  # for the newline before the sourcemap comment
                    of.seek(-offset, os.SEEK_END)
                    of.truncate()
                return force_text(sourcemap)
    finally:
        of.close()
    return sourcemap
--------------------
Prompt: 
def confirm(self, prompt, default):
        """ Called when a dangerous action is about to be done to make sure
        it's okay. `prompt' is printed; user response is returned."""
----------
Output: 

        if default:
            prompt = '%s [y/n]? ' % prompt
        else:
            prompt = '%s [y/N]? ' % prompt
        while True:
            try:
                return raw_input(prompt).lower() == 'y'
            except KeyboardInterrupt:
                print 'Interrupted'
            except EOFError:
                print 'Interrupted'
                return False

    def _get_config(self, config_file):
        """ Reads the config file and returns a dictionary of the settings.
----------
Solution: 

        while True:
            try:
                self.write_confirm(prompt, default)
                reply = self.readline('').strip().lower()
            except EOFError:
                return default
            if reply in ('y', 'yes'):
                return True
            elif reply in ('n', 'no'):
                return False
            else:
                self.msg("Please answer y or n.")
                pass
            pass
        return default
--------------------
Prompt: 
def delete(self, tag_id):
        """
        Delete the specified InactivityAlert

        :param tag_id: The tag ID to delete
        :type tag_id: str

        :raises: This will raise a
            :class:`ServerException <logentries_api.exceptions.ServerException>`
            if there is an error from Logentries
        """
----------
Output: 

        self.client.delete(self.url + tag_id)

    def get(self, tag_id):
        """
        Get the specified InactivityAlert

        :param tag_id: The tag ID to get
        :type tag_id: str

        :return: The InactivityAlert
        :rtype: :class:`InactivityAlert <logentries_api.models.InactivityAlert>`

        :raises: This will raise a
            :class:`ServerException <logentries_api.exceptions.ServerException>`
           
----------
Solution: 

        tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}'

        self._api_delete(
            url=tag_url.format(
                account_id=self.account_id,
                tag_id=tag_id
            )
        )
--------------------
Prompt: 
def normal_left_dclick(self, event):
        """ Handles the left mouse button being double-clicked when the tool
        is in the 'normal' state.

        If the event occurred on this tool's component (or any contained
        component of that component), the method opens a Traits UI view on the
        object referenced by the 'element' trait of the component that was
        double-clicked, setting the tool as the active tool for the duration
        of the view.

        """
----------
Output: 

        if self.component is not None:
            self.component.normal_left_dclick(event)

    def normal_left_press(self, event):
        """ Handles the left mouse button being pressed when the tool is in
        the 'normal' state.

        If the event occurred on this tool's component (or any contained
        component of that component), the method opens a Traits UI view on the
        object referenced by the 'element' trait of the component that was
        double-clicked, setting the tool as the active tool for the
----------
Solution: 

        x = event.x
        y = event.y

        # First determine what component or components we are going to hittest
        # on.  If our component is a container, then we add its non-container
        # components to the list of candidates.
#        candidates = []
        component = self.component
#        if isinstance(component, Container):
#            candidates = get_nested_components(self.component)
#        else:
#            # We don't support clicking on unrecognized components
#            return
#
#        # Hittest against all the candidate and take the first one
#        item = None
#        for candidate, offset in candidates:
#            if candidate.is_in(x-offset[0], y-offset[1]):
#                item = candidate
#                break

        if hasattr(component, "element"):
            if component.element is not None:
                component.active_tool = self
                component.element.edit_traits(kind="livemodal")
                event.handled = True
                component.active_tool = None
                component.request_redraw()
        return
--------------------
Prompt: 
def _read_compressed_points_data(self, laszip_vlr, point_format):
        """ reads the compressed point record
        """
----------
Output: 

        # read the compressed point record
        self.laszip_reader.seek(laszip_vlr.offset_to_point_data)
        self.laszip_reader.readinto(self.compressed_data)

        # decompress the point record
        self.laszip_reader.seek(laszip_vlr.offset_to_point_data + laszip_vlr.uncompressed_size)
        self.laszip_reader.readinto(self.uncompressed_data)

        # read the point
----------
Solution: 

        offset_to_chunk_table = struct.unpack("<q", self.stream.read(8))[0]
        size_of_point_data = offset_to_chunk_table - self.stream.tell()

        if offset_to_chunk_table <= 0:
            logger.warning(
                "Strange offset to chunk table: {}, ignoring it..".format(
                    offset_to_chunk_table
                )
            )
            size_of_point_data = -1  # Read everything

        points = record.PackedPointRecord.from_compressed_buffer(
            self.stream.read(size_of_point_data),
            point_format,
            self.header.point_count,
            laszip_vlr,
        )
        return points
--------------------
Prompt: 
def projector(state, flatten=False):
    """
    maps a pure state to a state matrix

    Args:
        state (ndarray): the number of qubits
        flatten (bool): determine if state matrix of column work
    Returns:
        ndarray:  state_mat(2**num, 2**num) if flatten is false
        ndarray:  state_mat(4**num) if flatten is true stacked on by the column
    """
----------
Output: 

    num = int(np.log2(state.shape[0]))
    state_mat = np.zeros((2 ** num, 2 ** num))
    for i in range(2 ** num):
        state_mat[i, state[i]] = 1
    if flatten:
        state_mat = np.reshape(state_mat, (2 ** (2 * num), 1))
    return state_mat


def projector_to_state(state_mat, flatten=False):
    """
    maps a
----------
Solution: 

    density_matrix = np.outer(state.conjugate(), state)
    if flatten:
        return density_matrix.flatten(order='F')
    return density_matrix
--------------------
Prompt: 
def export_transcripts(adapter, build='37'):
    """Export all transcripts from the database
    
    Args:
        adapter(scout.adapter.MongoAdapter)
        build(str)
    
    Yields:
        transcript(scout.models.Transcript)
    """
----------
Output: 

    for gene in adapter.gene_collection.find({"build": build}):
        for transcript in adapter.transcript_collection.find({"gene_id": gene["_id"]}):
            yield transcript


def export_genes(adapter, build='37'):
    """Export all genes from the database
    
    Args:
        adapter(scout.adapter.MongoAdapter)
        build(str)
    
    Yields:
        gene(scout.models.Gene)
    """
----------
Solution: 

    LOG.info("Exporting all transcripts")

    for tx_obj in adapter.transcripts(build=build):
        yield tx_obj
--------------------
Prompt: 
def write_table(self, table):
        """Send DDL to create the specified `table`

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
----------
Output: 

        self.write_ddl(table.get_ddl())

    def write_ddl(self, ddl):
        """Send DDL to create the specified `table`

        :Parameters:
          - `ddl`: a string representing the DDL to send to the server.

        Returns None
        """
        self.cursor.execute(ddl)

    def write_data(self, table, data):
        """Send data to create the specified `table`

        :Parameters:
          - `table`: an instance of
----------
Solution: 

        table_sql, serial_key_sql = super(PostgresDbWriter, self).write_table(table)
        for sql in serial_key_sql + table_sql:
            self.execute(sql)
--------------------
Prompt: 
def visit_module(self, node):
        """visit module : update consumption analysis variable
        checks globals doesn't overrides builtins
        """
----------
Output: 

        self.visit_block(node)
        self.update_consumption_analysis_variable()

    def visit_classdef(self, node):
        """visit classdef : update consumption analysis variable
        checks globals doesn't overrides builtins
        """
        self.visit_block(node)
        self.update_consumption_analysis_variable()

    def visit_block(self, node):
        """visit block : update consumption analysis variable
        checks globals doesn't overrides builtins
        """
       
----------
Solution: 

        self._to_consume = [NamesConsumer(node, "module")]
        self._postponed_evaluation_enabled = is_postponed_evaluation_enabled(node)

        for name, stmts in node.locals.items():
            if utils.is_builtin(name) and not utils.is_inside_except(stmts[0]):
                if self._should_ignore_redefined_builtin(stmts[0]) or name == "__doc__":
                    continue
                self.add_message("redefined-builtin", args=name, node=stmts[0])
--------------------
Prompt: 
def transpose(self):
        """Return the transpose of the QuantumChannel."""
----------
Output: 

        return self._evolve(self._data.transpose())

    def conjugate(self):
        """Return the conjugate of the QuantumChannel."""
        return self._evolve(self._data.conj())

    def adjoint(self):
        """Return the adjoint of the QuantumChannel."""
        return self._evolve(self._data.conj().T)

    def is_cptp(self, atol=None, rtol=None):
        """Return True if the QuantumChannel
----------
Solution: 

        return SuperOp(
            np.transpose(self._data),
            input_dims=self.output_dims(),
            output_dims=self.input_dims())
--------------------
Prompt: 
def _prepare_hiveconf(d):
        """
        This function prepares a list of hiveconf params
        from a dictionary of key value pairs.

        :param d:
        :type d: dict

        >>> hh = HiveCliHook()
        >>> hive_conf = {"hive.exec.dynamic.partition": "true",
        ... "hive.exec.dynamic.partition.mode": "nonstrict"}
        >>> hh._prepare_hiveconf(hive_conf)
        ["-hiveconf", "hive.exec.dynamic.partition=true",\
 "-hiveconf", "hive.exec.dynamic.partition.mode=nonstrict"]
        """
----------
Output: 
iveconf", "hive.exec.dynamic.partition=true",
        "-hiveconf", "hive.exec.dynamic.partition.mode=nonstrict"]
        """
        if not d:
            return []
        return ['-hiveconf', '{0}={1}'.format(key, value) for key, value in d.items()]

    def _prepare_params(self, hql, hive_conf=None, **kwargs):
        """
        This function prepares a list of parameters to be passed to the
        h
----------
Solution: 

        if not d:
            return []
        return as_flattened_list(
            zip(["-hiveconf"] * len(d),
                ["{}={}".format(k, v) for k, v in d.items()])
        )
--------------------
Prompt: 
def trimmed_pred_default(node, parent):
  """The default predicate used in Node.trimmed."""
----------
Output: 

  return node.is_visible()


class Node(object):
  """A node in the DOM tree."""

  def __init__(self, name, parent=None, predicate=None):
    """Creates a new node.

    Args:
      name: The name of the node.
      parent: The parent node.
      predicate: A predicate function that returns True if the node should be
          included in the trimmed tree.
    """
    self.name = name
    self.parent = parent
    self.predicate =
----------
Solution: 

  return isinstance(node, ParseNode) and (node.is_empty or node.is_type(ParseNodeType.terminal))
--------------------
Prompt: 
def _verify_compatibility(wave_a, wave_b, check_dep_units=True):
    """Verify that two waveforms can be combined with various mathematical functions."""
----------
Output: 

    if check_dep_units:
        if wave_a.dep_units != wave_b.dep_units:
            raise ValueError(
                "Waveforms have different dependent units: %s and %s"
                % (wave_a.dep_units, wave_b.dep_units)
            )

    if wave_a.indep_units != wave_b.indep_units:
        raise ValueError(
            "Waveforms have different independent units: %s and %s"
           
----------
Solution: 

    exobj = pexdoc.exh.addex(RuntimeError, "Waveforms are not compatible")
    ctuple = (
        bool(wave_a.indep_scale != wave_b.indep_scale),
        bool(wave_a.dep_scale != wave_b.dep_scale),
        bool(wave_a.indep_units != wave_b.indep_units),
        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),
        bool(wave_a.interp != wave_b.interp),
    )
    exobj(any(ctuple))
--------------------
Prompt: 
def flowwrite(flow, filename, quantize=False, concat_axis=0, *args, **kwargs):
    """Write optical flow to file.

    If the flow is not quantized, it will be saved as a .flo file losslessly,
    otherwise a jpeg image which is lossy but of much smaller size. (dx and dy
    will be concatenated horizontally into a single image if quantize is True.)

    Args:
        flow (ndarray): (h, w, 2) array of optical flow.
        filename (str): Output filepath.
        quantize (bool): Whether to quantize the flow and save it to 2 jpeg
            images. If set to True, remaining args will be passed to
            :func:`quantize_flow`.
        concat_axis (int): The axis that dx and dy are concatenated,
            can be either 0 or 1. Ignored if quantize is False.
    """
----------
Output: 
       filename (str): name of the file to save the flow to.
        quantize (bool): if True, save the flow as a jpeg image.
        concat_axis (int): if quantize is True, the axis to concatenate dx and dy
            on.
        args: additional args passed to cv2.imwrite.
        kwargs: additional kwargs passed to cv2.imwrite.

    """
    if not quantize:
        # write flow file in Middlebury .flo format
        flow_write_middlebury(flow, filename
----------
Solution: 

    if not quantize:
        with open(filename, 'wb') as f:
            f.write('PIEH'.encode('utf-8'))
            np.array([flow.shape[1], flow.shape[0]], dtype=np.int32).tofile(f)
            flow = flow.astype(np.float32)
            flow.tofile(f)
            f.flush()
    else:
        assert concat_axis in [0, 1]
        dx, dy = quantize_flow(flow, *args, **kwargs)
        dxdy = np.concatenate((dx, dy), axis=concat_axis)
        imwrite(dxdy, filename)
--------------------
Prompt: 
def get_singularity_version(singularity_version=None):
    """get_singularity_version will determine the singularity version for a
       build first, an environmental variable is looked at, followed by 
       using the system version.

       Parameters
       ==========
       singularity_version: if not defined, look for in environment. If still
       not find, try finding via executing --version to Singularity. Only return
       None if not set in environment or installed.
    """
----------
Output: 

    if singularity_version is None:
        singularity_version = os.environ.get('SINGULARITY_VERSION')
    if singularity_version is None:
        singularity_version = get_singularity_version_from_system()
    return singularity_version


def get_singularity_version_from_system():
    """get_singularity_version_from_system will determine the singularity version
       for a build by executing --version to Singularity.
    """
    try:
        singularity
----------
Solution: 


    if singularity_version is None:        
        singularity_version = os.environ.get("SINGULARITY_VERSION")
        
    if singularity_version is None:
        try:
            cmd = ['singularity','--version']
            output = run_command(cmd)

            if isinstance(output['message'],bytes):
                output['message'] = output['message'].decode('utf-8')
            singularity_version = output['message'].strip('\n')
            bot.info("Singularity %s being used." % singularity_version)
            
        except:
            singularity_version = None
            bot.warning("Singularity version not found, so it's likely not installed.")

    return singularity_version
--------------------
Prompt: 
def sign(pkey, data, digest):
    """
    Sign a data string using the given key and message digest.

    :param pkey: PKey to sign with
    :param data: data to be signed
    :param digest: message digest to use
    :return: signature

    .. versionadded:: 0.11
    """
----------
Output: 

    return pkey.sign(data, digest)


def verify(pkey, signature, data, digest):
    """
    Verify a signature against a data string using the given key and message digest.

    :param pkey: PKey to verify with
    :param signature: signature to verify
    :param data: data to be verified
    :param digest: message digest to use
    :return: True if signature is valid, False otherwise

    .. versionadded:: 0.11
    """
    return pkey
----------
Solution: 

    data = _text_to_bytes_and_warn("data", data)

    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))
    if digest_obj == _ffi.NULL:
        raise ValueError("No such digest method")

    md_ctx = _lib.Cryptography_EVP_MD_CTX_new()
    md_ctx = _ffi.gc(md_ctx, _lib.Cryptography_EVP_MD_CTX_free)

    _lib.EVP_SignInit(md_ctx, digest_obj)
    _lib.EVP_SignUpdate(md_ctx, data, len(data))

    length = _lib.EVP_PKEY_size(pkey._pkey)
    _openssl_assert(length > 0)
    signature_buffer = _ffi.new("unsigned char[]", length)
    signature_length = _ffi.new("unsigned int *")
    final_result = _lib.EVP_SignFinal(
        md_ctx, signature_buffer, signature_length, pkey._pkey)
    _openssl_assert(final_result == 1)

    return _ffi.buffer(signature_buffer, signature_length[0])[:]
--------------------
Prompt: 
def p_navigation_step_2(self, p):
        """navigation_step : ARROW identifier LSQBR identifier DOT phrase RSQBR"""
----------
Output: 

        p[0] = NavigationStep(p[2], p[4], p[6])

    def p_navigation_step_3(self, p):
        """navigation_step : ARROW identifier LSQBR identifier DOT phrase RSQBR LSQBR identifier DOT phrase RSQBR"""
        p[0] = NavigationStep(p[2], p[4], p[6], p[8], p[10])

    def p_navigation_step_4(self, p):
        """navigation_step :
----------
Solution: 

        p[0] = NavigationStepNode(key_letter=p[2],
                                  rel_id=p[4],
                                  phrase=p[6])
--------------------
Prompt: 
def _get_request(self, endpoint):
        """Do actual GET request to GC REST API
        Also validates responses.

        Keyword arguments:
        endpoint -- full endpoint for GET request
        """
----------
Output: 

        response = self.session.get(endpoint)
        if response.status_code != 200:
            raise Exception(
                "Error: {}".format(response.text))
        return response

    def _post_request(self, endpoint, data):
        """Do actual POST request to GC REST API
        Also validates responses.

        Keyword arguments:
        endpoint -- full endpoint for POST request
        data -- data to be sent in POST request
        """
        response = self.session.post(endpoint, data=data)

----------
Solution: 

        try:
            response = requests.get(endpoint)
        except requests.exceptions.RequestException:
            raise GoldenCheetahNotAvailable(endpoint)
        
        if response.text.startswith('unknown athlete'):
            match = re.match(
                pattern='unknown athlete (?P<athlete>.+)',
                string=response.text)
            raise AthleteDoesNotExist(
                athlete=match.groupdict()['athlete'])

        elif response.text == 'file not found':
            match = re.match(
                pattern='.+/activity/(?P<filename>.+)',
                string=endpoint)
            raise ActivityDoesNotExist(
                filename=match.groupdict()['filename'])

        return response
--------------------
Prompt: 
def _line_segment_with_colons(linefmt, align, colwidth):
    """Return a segment of a horizontal line with optional colons which
    indicate column's alignment (as in `pipe` output format)."""
----------
Output: 

    if align is None:
        return linefmt
    else:
        return linefmt.replace(':', ':' + ':' * (colwidth - 1))


def _line_segment_with_colons_and_dashes(linefmt, align, colwidth):
    """Return a segment of a horizontal line with optional colons and
    dashes which indicate column's alignment and width (as in `simple`
    output format)."""
    if align is None:
        return linefmt
    else:
        return linefmt.
----------
Solution: 

    fill = linefmt.hline
    w = colwidth
    if align in ["right", "decimal"]:
        return (fill[0] * (w - 1)) + ":"
    elif align == "center":
        return ":" + (fill[0] * (w - 2)) + ":"
    elif align == "left":
        return ":" + (fill[0] * (w - 1))
    else:
        return fill[0] * w
--------------------
Prompt: 
def report(self):
        """
        Print network statistics.
        """
----------
Output: 

        self.print_stats()

    def print_stats(self):
        """
        Print network statistics.
        """
        print('Network statistics:')
        print('  Number of nodes: %d' % self.num_nodes)
        print('  Number of edges: %d' % self.num_edges)
        print('  Number of triangles: %d' % self.num_triangles)
        print('  Number of quadrilaterals: %d' % self.num_quadrilaterals)
        print(' 
----------
Solution: 

        logging.info("network inputs: %s", " ".join(map(str, self.input_variables)))
        logging.info("network targets: %s", " ".join(map(str, self.target_variables)))
        logging.info("network parameters: %s", " ".join(map(str, self.all_parameters)))
        logging.info("parameter count: %d", self.parameter_count)
--------------------
Prompt: 
def init_handlers(self):
        """Create the default handlers."""
----------
Output: 

        self.handlers = {
            'default': {
                'level': 'INFO',
                'class': 'logging.StreamHandler',
                'formatter': 'default',
                'stream': 'ext://sys.stdout',
            },
            'file': {
                'level': 'INFO',
                'class': 'logging.handlers.RotatingFileHandler',
                'formatter': 'default',
                'filename': 'logs/app.log',
                'maxBytes': 1024 * 1024 * 1
----------
Solution: 

        self._handlers = {}
        self._esc_handlers = {}
        for handler in _default_handlers:
            handler(
                shell=self.shell, prefilter_manager=self, config=self.config
            )
--------------------
Prompt: 
def average_data(counts, observable):
    """Compute the mean value of an diagonal observable.

    Takes in a diagonal observable in dictionary, list or matrix format and then
    calculates the sum_i value(i) P(i) where value(i) is the value of the
    observable for state i.

    Args:
        counts (dict): a dict of outcomes from an experiment
        observable (dict or matrix or list): The observable to be averaged over.
        As an example, ZZ on qubits can be given as:
        * dict: {"00": 1, "11": 1, "01": -1, "10": -1}
        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]
        * matrix diagonal (list): [1, -1, -1, 1]

    Returns:
        Double: Average of the observable
    """
----------
Output: 
a list of Pauli matrices
        [Pauli.Z, Pauli.Z] or as a matrix [[1,0],[0,-1]]

    Returns:
        float: The mean value of the observable.
    """
    if isinstance(observable, dict):
        observable = observable.values()
    if isinstance(observable, list):
        observable = np.array(observable)
    if isinstance(observable, np.ndarray):
        observable = np.diag(observable)
    if not isinstance(observable,
----------
Solution: 

    if not isinstance(observable, dict):
        observable = make_dict_observable(observable)
    temp = 0
    tot = sum(counts.values())
    for key in counts:
        if key in observable:
            temp += counts[key] * observable[key] / tot
    return temp
--------------------
Prompt: 
def _get_object_name(self, line):
        """ Get second token in line
        >>> docwriter = ApiDocWriter('sphinx')
        >>> docwriter._get_object_name("  def func():  ")
        'func'
        >>> docwriter._get_object_name("  class Klass(object):  ")
        'Klass'
        >>> docwriter._get_object_name("  class Klass:  ")
        'Klass'
        """
----------
Output: 

        return line.split()[1]

    def _get_object_type(self, line):
        """ Get first token in line
        >>> docwriter = ApiDocWriter('sphinx')
        >>> docwriter._get_object_type("  def func():  ")
        'def'
        >>> docwriter._get_object_type("  class Klass(object):  ")
        'class'
        >>> docwriter._get_object_type("  class Klass:  ")
        'class'
        """
        return line.
----------
Solution: 

        name = line.split()[1].split('(')[0].strip()
        # in case we have classes which are not derived from object
        # ie. old style classes
        return name.rstrip(':')
--------------------
Prompt: 
def _build_metrics(func_name, namespace):
    """
    Builds metrics dict from function args
    It assumes that function arguments is from airflow.bin.cli module's function
    and has Namespace instance where it optionally contains "dag_id", "task_id",
    and "execution_date".

    :param func_name: name of function
    :param namespace: Namespace instance from argparse
    :return: dict with metrics
    """
----------
Output: 

    metrics = {}
    if namespace.dag_id:
        metrics["dag_id"] = namespace.dag_id
    if namespace.task_id:
        metrics["task_id"] = namespace.task_id
    if namespace.execution_date:
        metrics["execution_date"] = namespace.execution_date
    return metrics


def _build_tags(func_name, namespace):
    """
    Builds tags dict from function args
    It assumes that function arguments is from airflow.bin.cli module's function
    and has
----------
Solution: 


    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),
               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}

    assert isinstance(namespace, Namespace)
    tmp_dic = vars(namespace)
    metrics['dag_id'] = tmp_dic.get('dag_id')
    metrics['task_id'] = tmp_dic.get('task_id')
    metrics['execution_date'] = tmp_dic.get('execution_date')
    metrics['host_name'] = socket.gethostname()

    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))
    log = Log(
        event='cli_{}'.format(func_name),
        task_instance=None,
        owner=metrics['user'],
        extra=extra,
        task_id=metrics.get('task_id'),
        dag_id=metrics.get('dag_id'),
        execution_date=metrics.get('execution_date'))
    metrics['log'] = log
    return metrics
--------------------
Prompt: 
def get_func_sourcecode(func):
    """
    Try to get sourcecode using standard inspect.getsource().
    If the function comes from a module which has been created dynamically
    (not from the filesystem), then it tries to read the sourcecode on the
    filesystem anyway.
    WARNING: can do weird things if the filesystem code slightly differs from
             the original module code.
    """
----------
Output: 

    try:
        return inspect.getsource(func)
    except OSError:
        # the function comes from a module which has been created dynamically
        # (not from the filesystem), so try to read the sourcecode on the
        # filesystem anyway
        try:
            filename = inspect.getsourcefile(func)
            with open(filename, 'r') as f:
                return f.read()
        except (TypeError, IOError):
            # the function comes from a module which has been created
            # dynamically and which has no filename
            return
----------
Solution: 


    def getsource(func):
        lines, lnum = getsourcelines(func)
        return ''.join(lines)

    def getsourcelines(func):
        lines, lnum = findsource(func)
        return inspect.getblock(lines[lnum:]), lnum + 1

    def findsource(func):
        file = getfile(func)  # file path
        module = inspect.getmodule(func, file)
        lines = linecache.getlines(file, module.__dict__)
        code = func.__code__
        lnum = code.co_firstlineno - 1
        pat = re.compile(r'^(\s*def\s)|(\s*async\s+def\s)|(.*(?<!\w)lambda(:|\s))|^(\s*@)')
        while lnum > 0:
            if pat.match(lines[lnum]):
                break
            lnum = lnum - 1  # pragma: no cover
        return lines, lnum

    def getfile(func):
        module = inspect.getmodule(func)
        return module.__file__

    try:
        return inspect.getsource(func)
    except Exception:
        return getsource(func)
--------------------
Prompt: 
def x_fit(self, test_length):
        """ Test to see if the line can has enough space for the given length. """
----------
Output: 

        return self.length >= test_length

    def x_fit_all(self, test_length):
        """ Test to see if the line can has enough space for the given length. """
        return self.length >= test_length

    def x_fit_any(self, test_length):
        """ Test to see if the line can has enough space for the given length. """
        return self.length >= test_length

    def x_fit_not(self, test_length):
----------
Solution: 

        if (self.x + test_length) >= self.xmax:
            return False
        else:
            return True
--------------------
Prompt: 
def list_files(self, extensions=None):
        """
        List the ports contents by file type or all.
        :param extensions: string extensions, single string or list of extensions.
        :return: A list of full path names of each file.
        """
----------
Output: 

        if extensions is None:
            return self.list_files_all()
        elif isinstance(extensions, str):
            return self.list_files_by_extension(extensions)
        elif isinstance(extensions, list):
            return self.list_files_by_extensions(extensions)
        else:
            raise TypeError("extensions must be a string or list of strings.")

    def list_files_all(self):
        """
        List the ports contents by file type or all.
        :return: A list of full path names of each file
----------
Solution: 

        if self.type.lower() != 'directory':
            raise ValueError("Port type is not == directory")

        filesystem_location = self.path

        for root, dirs, files in os.walk(filesystem_location):
            if extensions is None:
                return [os.path.join(root, f) for f in files]
            elif not isinstance(extensions, list):
                extensions = [extensions]

            subset_files = []

            for f in files:
                for extension in extensions:
                    if f.lower().endswith(extension.lower()):
                        subset_files.append(os.path.join(root, f))
                        break
            return subset_files
--------------------
Prompt: 
def get_image(self, float_key="floats", to_chw=True):
        """
        get image list from ImageFrame
        """
----------
Output: 

        if self.is_cuda:
            return self.data[float_key].cpu().numpy()
        else:
            return self.data[float_key].numpy()

    def get_label(self, int_key="ints"):
        """
        get label list from ImageFrame
        """
        if self.is_cuda:
            return self.data[int_key].cpu().numpy()
        else:
            return self.data[int_key].numpy()

    def get_float(self, float_key
----------
Solution: 

        tensors = callBigDlFunc(self.bigdl_type,
                                   "localImageFrameToImageTensor", self.value, float_key, to_chw)
        return map(lambda tensor: tensor.to_ndarray(), tensors)
--------------------
Prompt: 
def generate_key(self, type, bits):
        """
        Generate a key pair of the given type, with the given number of bits.

        This generates a key "into" the this object.

        :param type: The key type.
        :type type: :py:data:`TYPE_RSA` or :py:data:`TYPE_DSA`
        :param bits: The number of bits.
        :type bits: :py:data:`int` ``>= 0``
        :raises TypeError: If :py:data:`type` or :py:data:`bits` isn't
            of the appropriate type.
        :raises ValueError: If the number of bits isn't an integer of
            the appropriate size.
        :return: ``None``
        """
----------
Output: 
type` is not :py:data:`TYPE_RSA` or
            :py:data:`TYPE_DSA`.
        :raises ValueError: If :py:data:`bits` is less than 0.
        """
        if type == self.TYPE_RSA:
            self._key = RSA.generate(bits)
        elif type == self.TYPE_DSA:
            self._key = DSA.generate(bits)
        else:
            raise TypeError("type must be TYPE_RSA or TYPE_DSA")


----------
Solution: 

        if not isinstance(type, int):
            raise TypeError("type must be an integer")

        if not isinstance(bits, int):
            raise TypeError("bits must be an integer")

        if type == TYPE_RSA:
            if bits <= 0:
                raise ValueError("Invalid number of bits")

            # TODO Check error return
            exponent = _lib.BN_new()
            exponent = _ffi.gc(exponent, _lib.BN_free)
            _lib.BN_set_word(exponent, _lib.RSA_F4)

            rsa = _lib.RSA_new()

            result = _lib.RSA_generate_key_ex(rsa, bits, exponent, _ffi.NULL)
            _openssl_assert(result == 1)

            result = _lib.EVP_PKEY_assign_RSA(self._pkey, rsa)
            _openssl_assert(result == 1)

        elif type == TYPE_DSA:
            dsa = _lib.DSA_new()
            _openssl_assert(dsa != _ffi.NULL)

            dsa = _ffi.gc(dsa, _lib.DSA_free)
            res = _lib.DSA_generate_parameters_ex(
                dsa, bits, _ffi.NULL, 0, _ffi.NULL, _ffi.NULL, _ffi.NULL
            )
            _openssl_assert(res == 1)

            _openssl_assert(_lib.DSA_generate_key(dsa) == 1)
            _openssl_assert(_lib.EVP_PKEY_set1_DSA(self._pkey, dsa) == 1)
        else:
            raise Error("No such key type")

        self._initialized = True
--------------------
Prompt: 
def v2_runner_on_skipped(self, result, **kwargs):
        """Run when a task is skipped."""
----------
Output: 

        self._display.display(
            'skipping: %s' % result._host.get_name(),
            color=C.COLOR_SKIP,
            stderr=True
        )

    def v2_runner_on_ok(self, result, **kwargs):
        """Run when a task passes."""
        self._display.display(
            'ok: %s' % result._host.get_name(),
            color=C.COLOR_OK,
            stderr=True
        )

    def v2
----------
Solution: 

        if self._display.verbosity > 1:
            self._print_task()
            self.last_skipped = False

            line_length = 120
            spaces = " " * (31 - len(result._host.name) - 4)

            line = "  * {}{}- {}".format(
                colorize(result._host.name, "not_so_bold"),
                spaces,
                colorize("skipped", "skipped"),
            )

            reason = result._result.get("skipped_reason", "") or result._result.get(
                "skip_reason", ""
            )
            if len(reason) < 50:
                line += " -- {}".format(reason)
                print("{} {}---------".format(line, "-" * (line_length - len(line))))
            else:
                print("{} {}".format(line, "-" * (line_length - len(line))))
                print(self._indent_text(reason, 8))
                print(reason)
--------------------
Prompt: 
def _estimate_gas(self,
                      master_copy: str,
                      initializer: bytes,
                      funder: str,
                      payment_token: str) -> int:
        """
        Gas estimation done using web3 and calling the node
        Payment cannot be estimated, as no ether is in the address. So we add some gas later.
        :param master_copy: Master Copy of Gnosis Safe already deployed
        :param initializer: Data initializer to send to GnosisSafe setup method
        :param funder: Address that should get the payment (if payment set)
        :param payment_token: Address if a token is used. If not set, 0x0 will be ether
        :return: Total gas estimation
        """
----------
Output: 
funder: Address that will be used to fund the Safe
        :param payment_token: Address of the token used to pay for the gas
        :return: Estimated gas
        """
        # Estimate gas
        gas_estimate = self.w3.eth.estimate_gas(
            {
                "from": funder,
                "to": master_copy,
                "data": initializer
            }
        )
        # Add gas to cover the payment
        gas_estimate += 100000
        return gas
----------
Solution: 


        # Estimate the contract deployment. We cannot estimate the refunding, as the safe address has not any fund
        gas: int = self._build_proxy_contract_creation_constructor(
            master_copy,
            initializer,
            funder,
            payment_token,
            0).estimateGas()

        # We estimate the refund as a new tx
        if payment_token == NULL_ADDRESS:
            # Same cost to send 1 ether than 1000
            gas += self.w3.eth.estimateGas({'to': funder, 'value': 1})
        else:
            # Top should be around 52000 when storage is needed (funder no previous owner of token),
            # we use value 1 as we are simulating an internal call, and in that calls you don't pay for the data.
            # If it was a new tx sending 5000 tokens would be more expensive than sending 1 because of data costs
            try:
                gas += get_erc20_contract(self.w3,
                                          payment_token).functions.transfer(funder, 1).estimateGas({'from':
                                                                                                    payment_token})
            except ValueError as exc:
                raise InvalidERC20Token from exc

        return gas
--------------------
Prompt: 
def reg_on_abort(self, callable_object, *args, **kwargs):
        """ Register a function/method to be called when execution is aborted"""
----------
Output: 

        self._abort_callbacks.append((callable_object, args, kwargs))

    def abort(self, msg=None):
        """ Abort execution of the current test.

        :param msg: Optional message to be printed before aborting.
        """
        if msg:
            print(msg)
        for callable_object, args, kwargs in self._abort_callbacks:
            callable_object(*args, **kwargs)
        raise AbortTest()

    def _run_test(self, test_
----------
Solution: 

        persistent = kwargs.pop('persistent', False)
        event = self._create_event(callable_object, 'abort', persistent, *args, **kwargs)
        self.abort_callbacks.append(event)
        return event
--------------------
Prompt: 
def wa(client, event, channel, nick, rest):
	"""
	A free-text query resolver by Wolfram|Alpha. Returns the first
	result, if available.
	"""
----------
Output: 

	if not rest:
		return
	try:
		result = wolfram.query(rest)
		if result:
			return result[0]['subpods'][0]['plaintext']
	except:
		return

def wiki(client, event, channel, nick, rest):
	"""
	A free-text query resolver by Wikipedia. Returns the first
	result, if available.
	"""
	if not rest:
		return
	try:
		result = wikipedia.
----------
Solution: 

	client = wolframalpha.Client(pmxbot.config['Wolfram|Alpha API key'])
	res = client.query(rest)
	return next(res.results).text
--------------------
Prompt: 
def update_(self, sct_dict, conf_arg=True):
        """Update values of configuration section with dict.

        Args:
            sct_dict (dict): dict indexed with option names. Undefined
                options are discarded.
            conf_arg (bool): if True, only options that can be set in a config
                file are updated.
        """
----------
Output: 

        if conf_arg:
            conf_dict = self.conf_dict
        else:
            conf_dict = self.dict

        for option, value in sct_dict.items():
            if option in conf_dict:
                conf_dict[option] = value

    def update(self, sct_dict, conf_arg=True):
        """Update values of configuration section with dict.

        Args:
            sct_dict (dict): dict indexed with option names. Undefined
                options are discarded.
            conf
----------
Solution: 

        for opt, val in sct_dict.items():
            if opt not in self.def_:
                continue
            if not conf_arg or self.def_[opt].conf_arg:
                self[opt] = val
--------------------
Prompt: 
def poke(self, context):
        """
        poke matching files in a directory with self.regex

        :return: Bool depending on the search criteria
        """
----------
Output: 

        if self.regex is None:
            return False

        if self.path is None:
            return False

        if self.path.startswith("~"):
            self.path = os.path.expanduser(self.path)

        if not os.path.exists(self.path):
            return False

        if not os.path.isdir(self.path):
            return False

        for root, dirs, files in os.walk(self.path):
            for name in files:
                if self.
----------
Solution: 

        sb = self.hook(self.hdfs_conn_id).get_conn()
        self.log.info(
            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern
        )
        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if
                  f['file_type'] == 'f' and
                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]
        result = self.filter_for_ignored_ext(result, self.ignored_ext,
                                             self.ignore_copying)
        result = self.filter_for_filesize(result, self.file_size)
        return bool(result)
--------------------
Prompt: 
def breadcrumbs(self, tree_alias, context):
        """Builds and returns breadcrumb trail structure for 'sitetree_breadcrumbs' tag.

        :param str|unicode tree_alias:
        :param Context context:
        :rtype: list|str
        """
----------
Output: 

        tree = self.get_tree(tree_alias)
        if not tree:
            return ''

        # Get current node
        current_node = self.get_current_node(tree_alias, context)
        if not current_node:
            return ''

        # Get current node's parents
        parents = self.get_parents(tree_alias, current_node.id)

        # Get current node's children
        children = self.get_children(tree_alias, current_node.id)

        #
----------
Solution: 

        tree_alias, sitetree_items = self.init_tree(tree_alias, context)

        if not sitetree_items:
            return ''

        current_item = self.get_tree_current_item(tree_alias)

        breadcrumbs = []

        if current_item is not None:

            context_ = self.current_page_context
            check_access = self.check_access
            get_item_by_id = self.get_item_by_id

            def climb(base_item):
                
--------------------
Prompt: 
def getCSV(self):
        """
        Returns
        -------
        filename: str
        """
----------
Output: 

        return self.csv

    def getCSVHeader(self):
        """
        Returns
        -------
        header: list
        """
        return self.header

    def getCSVData(self):
        """
        Returns
        -------
        data: list
        """
        return self.data

    def getCSVDataAsDict(self):
        """
        Returns
        -------
        data: list
        """
        return self.dataAsDict

    def getCSVDataAsDictByColumn(
----------
Solution: 

        import getpass
        import gspread

        user = raw_input("Insert Google username:")
        password = getpass.getpass(prompt="Insert password:")
        name = raw_input("SpreadSheet filename on Drive:")
        sheet = raw_input("Sheet name (first sheet is default):")

        cl = gspread.login(user, password)
        sh = cl.open(name)

        if not (sheet.strip()):
            ws = sh.sheet1
            sheet = "1"
        else:
            ws = sh.worksheet(sheet)

        filename = name + '-worksheet_' + sheet + '.csv'
        with open(filename, 'wb') as f:
            writer = UnicodeWriter(f)
            writer.writerows(ws.get_all_values())

        return filename
--------------------
Prompt: 
def hunt_repeated_yaml_keys(data):
    """Parses yaml and returns a list of repeated variables and
       the line on which they occur
    """
----------
Output: 

    repeated_keys = []
    for key in data.keys():
        if key in repeated_keys:
            repeated_keys.append(key)
        else:
            repeated_keys.append(key)
    return repeated_keys


def hunt_repeated_yaml_keys_in_file(file_path):
    """Parses yaml file and returns a list of repeated variables and
       the line on which they occur
    """
    with open(file_path, 'r') as stream:
        try:
            data
----------
Solution: 

    loader = yaml.Loader(data)

    def compose_node(parent, index):
        # the line number where the previous token has ended (plus empty lines)
        line = loader.line
        node = Composer.compose_node(loader, parent, index)
        node.__line__ = line + 1
        return node

    def construct_mapping(node, deep=False):
        mapping = dict()
        errors = dict()
        for key_node, value_node in node.value:
            key = key_node.value
            if key in mapping:
                if key in errors:
                    errors[key].append(key_node.__line__)
                else:
                    errors[key] = [mapping[key], key_node.__line__]

            mapping[key] = key_node.__line__

        return errors

    loader.compose_node = compose_node
    loader.construct_mapping = construct_mapping
    data = loader.get_single_data()
    return data
--------------------
Prompt: 
def deserialize_dataframe(reader, data_type_id):
    """
    Deserialize a dataframe.

    Parameters
    ----------
    reader : file
        File-like object to read from. Must be opened in binary mode.
    data_type_id : dict
        Serialization format of the raw data.
        See the azureml.DataTypeIds class for constants.

    Returns
    -------
    pandas.DataFrame
        Dataframe object.
    """
----------
Output: 

    if data_type_id == azureml.DataTypeIds.PANDAS_DF_CSV:
        return pandas.read_csv(reader)
    elif data_type_id == azureml.DataTypeIds.PANDAS_DF_JSON:
        return pandas.read_json(reader)
    elif data_type_id == azureml.DataTypeIds.PANDAS_DF_PARQUET:
        return pandas.read_parquet(reader)
    elif data_type_id == azureml.DataTypeIds.PANDAS
----------
Solution: 

    _not_none('reader', reader)
    _not_none_or_empty('data_type_id', data_type_id)

    serializer = _SERIALIZERS.get(data_type_id)
    if serializer is None:
        raise UnsupportedDatasetTypeError(data_type_id)
    return serializer[1](reader=reader)
--------------------
Prompt: 
def asin(wave):
    r"""
    Return the arc sine of a waveform's dependent variable vector.

    :param wave: Waveform
    :type  wave: :py:class:`peng.eng.Waveform`

    :rtype: :py:class:`peng.eng.Waveform`

    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]
    .. Auto-generated exceptions documentation for
    .. peng.wave_functions.asin

    :raises:
     * RuntimeError (Argument \`wave\` is not valid)

     * ValueError (Math domain error)

    .. [[[end]]]
    """
----------
Output: 
math.asin.
    .. [exobj_eng.get_sphinx_autodoc()]
    .. [exobj_eng.get_sphinx_autodoc()]
    .. [[[end]]]
    """
    return wave.copy(func=np.arcsin)


def acos(wave):
    r"""
    Return the arc cosine of a waveform's dependent variable vector.

    :param wave: Waveform
    :type  wave: :py:class:`peng.eng
----------
Solution: 

    pexdoc.exh.addex(
        ValueError,
        "Math domain error",
        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),
    )
    return _operation(wave, "asin", "rad", np.arcsin)
--------------------
Prompt: 
def _on_parent_event_dependent(self):
        """
        After parrent statement become event dependent
        propagate event dependency flag to child statements
        """
----------
Output: 

        if self.parent_statement.event_dependent:
            self.event_dependent = True

    def _on_parent_event_independent(self):
        """
        After parrent statement become event independent
        propagate event independence flag to child statements
        """
        if not self.parent_statement.event_dependent:
            self.event_dependent = False

    def _on_parent_event_independent_and_dependent(self):
        """
        After parrent statement become event independent and dependent
        propagate event independence and dependency
----------
Solution: 

        if not self._is_completly_event_dependent:
            self._is_completly_event_dependent = True
            for stm in self._iter_stms():
                stm._on_parent_event_dependent()
--------------------
Prompt: 
def allclose(a, b):
    """
    Test that a and b are close and match in shape.

    Parameters
    ----------
    a : ndarray
        First array to check

    b : ndarray
        First array to check
    """
----------
Output: 

    return np.allclose(a, b) and a.shape == b.shape


def assert_allclose(a, b, rtol=1e-7, atol=0):
    """
    Test that a and b are close and match in shape.

    Parameters
    ----------
    a : ndarray
        First array to check

    b : ndarray
        First array to check

    rtol : float
        Relative tolerance

    atol : float
        Absolute tolerance
   
----------
Solution: 

    from numpy import allclose
    return (a.shape == b.shape) and allclose(a, b)
--------------------
Prompt: 
def signFix(val, width):
    """
    Convert negative int to positive int which has same bits set
    """
----------
Output: 

    if val < 0:
        val = val + (1 << width)
    return val


def signFix_int(val, width):
    """
    Convert negative int to positive int which has same bits set
    """
    if val < 0:
        val = val + (1 << width)
    return val


def signFix_float(val, width):
    """
    Convert negative int to positive int which has same bits set
    """
    if val < 0:
        val = val + (1
----------
Solution: 

    if val > 0:
        msb = 1 << (width - 1)
        if val & msb:
            val -= mask(width) + 1
    return val
--------------------
Prompt: 
def list(self):
        """
        Get all current labels

        :return: The Logentries API response
        :rtype: list of dict

        :raises: This will raise a
            :class:`ServerException<logentries_api.exceptions.ServerException>`
            if there is an error from Logentries
        """
----------
Output: 

        return self.get(self.url)

    def create(self, name):
        """
        Create a new label

        :param name: The name of the label
        :type name: str

        :return: The Logentries API response
        :rtype: dict

        :raises: This will raise a
            :class:`ServerException<logentries_api.exceptions.ServerException>`
            if there is an error from Logentries
        """
        return self.post(self.url, {'name': name})


----------
Solution: 

        return self._post(
            request='list',
            uri=ApiUri.TAGS.value,
        ).get('tags')
--------------------
Prompt: 
def add_menu_action(self, menu, action, defer_shortcut=False):
        """Add action to menu as well as self
        
        So that when the menu bar is invisible, its actions are still available.
        
        If defer_shortcut is True, set the shortcut context to widget-only,
        where it will avoid conflict with shortcuts already bound to the
        widgets themselves.
        """
----------
Output: 

        menu.addAction(action)
        if defer_shortcut:
            action.setShortcutContext(QtCore.Qt.WidgetWithChildrenShortcut)
        self.addAction(action)

    def add_menu_actions(self, menu, actions, defer_shortcut=False):
        """Add actions to menu as well as self
        
        So that when the menu bar is invisible, its actions are still available.
        
        If defer_shortcut is True, set the shortcut context to widget-only,
        where it will
----------
Solution: 

        menu.addAction(action)
        self.addAction(action)

        if defer_shortcut:
            action.setShortcutContext(QtCore.Qt.WidgetShortcut)
--------------------
Prompt: 
def from_string(bnf: str, entry=None, *optional_inherit) -> Grammar:
    """
    Create a Grammar from a string
    """
----------
Output: 

    if entry is None:
        entry = bnf
    return Grammar(bnf, entry, *optional_inherit)


def from_file(filename: str, entry=None, *optional_inherit) -> Grammar:
    """
    Create a Grammar from a file
    """
    if entry is None:
        entry = filename
    return Grammar(filename, entry, *optional_inherit)


def from_grammar(grammar: Grammar, entry=None, *optional_inherit) -> Grammar
----------
Solution: 

    inherit = [Grammar] + list(optional_inherit)
    scope = {'grammar': bnf, 'entry': entry}
    return build_grammar(tuple(inherit), scope)
--------------------
Prompt: 
def parse_statement(self):
    """Parse a single statement.

    Returns:
      Either a `BindingStatement`, `ImportStatement`, `IncludeStatement`, or
      `None` if no more statements can be parsed (EOF reached).
    """
----------
Output: 

    # TODO: This is a bit of a hack. We should probably have a more
    # sophisticated way of parsing statements.
    if self.peek_token().type == TokenType.EOF:
      return None
    if self.peek_token().type == TokenType.IDENTIFIER:
      if self.peek_token().value == 'import':
        return self.parse_import_statement()
      if self.peek_token().value == 'include':
        return self.parse_include_statement()
     
----------
Solution: 

    self._skip_whitespace_and_comments()
    if self._current_token.kind == tokenize.ENDMARKER:
      return None

    # Save off location, but ignore char_num for any statement-level errors.
    stmt_loc = self._current_location(ignore_char_num=True)
    binding_key_or_keyword = self._parse_selector()
    statement = None
    if self._current_token.value != '=':
      if binding_key_or_keyword == 'import':
        module = self._parse_selector(scoped=False)
        statement = ImportStatement(module, stmt_loc)
      elif binding_key_or_keyword == 'include':
        str_loc = self._current_location()
        success, filename = self._maybe_parse_basic_type()
        if not success or not isinstance(filename, str):
          self._raise_syntax_error('Expected file path as string.', str_loc)
        statement = IncludeStatement(filename, stmt_loc)
      else:
        self._raise_syntax_error("Expected '='.")
    else:  # We saw an '='.
      self._advance_one_token()
      value = self.parse_value()
      scope, selector, arg_name = parse_binding_key(binding_key_or_keyword)
      statement = BindingStatement(scope, selector, arg_name, value, stmt_loc)

    assert statement, 'Internal parsing error.'

    if (self._current_token.kind != tokenize.NEWLINE and
        self._current_token.kind != tokenize.ENDMARKER):
      self._raise_syntax_error('Expected newline.')
    elif self._current_token.kind == tokenize.NEWLINE:
      self._advance_one_token()

    return statement
--------------------
Prompt: 
def add_xpaths_to_stream_item(si):
    """Mutably tag tokens with xpath offsets.

    Given some stream item, this will tag all tokens from all taggings
    in the document that contain character offsets. Note that some
    tokens may not have computable xpath offsets, so an xpath offset
    for those tokens will not be set. (See the documentation and
    comments for ``char_offsets_to_xpaths`` for what it means for a
    token to have a computable xpath.)

    If a token can have its xpath offset computed, it is added to its
    set of offsets with a ``OffsetType.XPATH_CHARS`` key.
    """
----------
Output: 
   :param si: The stream item to tag.
    :type si: :class:`stream_item.StreamItem`
    :returns: The stream item with tokens tagged with xpath offsets.
    :rtype: :class:`stream_item.StreamItem`
    """
    for tagging in si.taggings:
        for token in tagging.tokens:
            if token.char_offsets is not None:
                token.xpath_offsets = char_offsets_to_xpaths(
                    token.char_offsets
----------
Solution: 

    def sentences_to_xpaths(sentences):
        tokens = sentences_to_char_tokens(sentences)
        offsets = char_tokens_to_char_offsets(tokens)
        return char_offsets_to_xpaths(html, offsets)

    def xprange_to_offset(xprange):
        return Offset(type=OffsetType.XPATH_CHARS,
                      first=xprange.start_offset, length=0,
                      xpath=xprange.start_xpath,
                      content_form='clean_html', value=None,
                      xpath_end=xprange.end_xpath,
                      xpath_end_offset=xprange.end_offset)

    html = unicode(si.body.clean_html, 'utf-8')
    for sentences in si.body.sentences.itervalues():
        tokens = sentences_to_char_tokens(sentences)
        for token, xprange in izip(tokens, sentences_to_xpaths(sentences)):
            if xprange is None:
                continue
            offset = xprange_to_offset(xprange)
            token.offsets[OffsetType.XPATH_CHARS] = offset
--------------------
Prompt: 
def fill_parameters(self, path, blocks, exclude_free_params=False, check_parameters=False):
        """
        Load parameters from file to fill all blocks sequentially.
        :type blocks: list of deepy.layers.Block
        """
----------
Output: 

        if not os.path.exists(path):
            raise IOError("File not found: {}".format(path))
        with open(path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if line.startswith('['):
                    block_name = line[1:-1]
                    block = self.get_block(block_name)
                    if block is None:
                        raise ValueError("Block not found
----------
Solution: 

        if not os.path.exists(path):
            raise Exception("model {} does not exist".format(path))
        # Decide which parameters to load
        normal_params = sum([nn.parameters for nn in blocks], [])
        all_params = sum([nn.all_parameters for nn in blocks], [])
        # Load parameters
        if path.endswith(".gz"):
            opener = gzip.open if path.lower().endswith('.gz') else open
            handle = opener(path, 'rb')
            saved_params = pickle.load(handle)
            handle.close()
            # Write parameters
            if len(all_params) != len(saved_params):
                logging.warning(
                    "parameters in the network: {}, parameters in the dumped model: {}".format(len(all_params),
                                                                                               len(saved_params)))
            for target, source in zip(all_params, saved_params):
                if not exclude_free_params or target not in normal_params:
                    target.set_value(source)
        elif path.endswith(".npz"):
            arrs = np.load(path)
            # Write parameters
            if len(all_params) != len(arrs.keys()):
                logging.warning(
                    "parameters in the network: {}, parameters in the dumped model: {}".format(len(all_params),
                                                                                               len(arrs.keys())))
            for target, idx in zip(all_params, range(len(arrs.keys()))):
                if not exclude_free_params or target not in normal_params:
                    source = arrs['arr_%d' % idx]
                    target.set_value(source)
        else:
            raise Exception("File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'" % path)
--------------------
Prompt: 
def get_endpoint_path(self, endpoint_id):
    """return the first fullpath to a folder in the endpoint based on
       expanding the user's home from the globus config file. This
       function is fragile but I don't see any other way to do it.
    
       Parameters
       ==========
       endpoint_id: the endpoint id to look up the path for

    """
----------
Output: 

    endpoint_info = self.get_endpoint(endpoint_id)
    if endpoint_info is None:
        return None
    else:
        return endpoint_info['path']

def get_endpoint_path_from_name(self, endpoint_name):
    """return the first fullpath to a folder in the endpoint based on
       expanding the user's home from the globus config file. This
       function is fragile but I don't see any other way to do it.
    
       Parameters
       ==========
       endpoint_name
----------
Solution: 

    config = os.path.expanduser("~/.globusonline/lta/config-paths")
    if not os.path.exists(config):
        bot.error('%s not found for a local Globus endpoint.')
        sys.exit(1)

    path = None

    # Read in the config and get the root path

    config = [x.split(',')[0] for x in read_file(config)]
    for path in config:
        if os.path.exists(path):
            break

    # If we don't have an existing path, exit

    if path is None:
        bot.error('No path was found for a local Globus endpoint.')
        sys.exit(1)

    return path
--------------------
Prompt: 
def _request_activity_list(self, athlete):
        """Actually do the request for activity list
        This call is slow and therefore this method is memory cached.

        Keyword arguments:
        athlete -- Full name of athlete
        """
----------
Output: 

        if athlete not in self.activity_list_cache:
            self.activity_list_cache[athlete] = self._request_activity_list_raw(athlete)
        return self.activity_list_cache[athlete]

    def _request_activity_list_raw(self, athlete):
        """Actually do the request for activity list
        This call is slow and therefore this method is memory cached.

        Keyword arguments:
        athlete -- Full name of athlete
        """
        url = self.activity_list
----------
Solution: 

        response = self._get_request(self._athlete_endpoint(athlete))
        response_buffer = StringIO(response.text)

        activity_list = pd.read_csv(
            filepath_or_buffer=response_buffer,
            parse_dates={'datetime': ['date', 'time']},
            sep=',\s*',
            engine='python'
        )
        activity_list.rename(columns=lambda x: x.lower(), inplace=True)
        activity_list.rename(
            columns=lambda x: '_' + x if x[0].isdigit() else x, inplace=True)

        activity_list['has_hr'] = activity_list.average_heart_rate.map(bool)
        activity_list['has_spd'] = activity_list.average_speed.map(bool)
        activity_list['has_pwr'] = activity_list.average_power.map(bool)
        activity_list['has_cad'] = activity_list.average_heart_rate.map(bool)
        activity_list['data'] = pd.Series(dtype=np.dtype("object"))
        return activity_list
--------------------
Prompt: 
def reconnectUnitSignalsToModel(synthesisedUnitOrIntf, modelCls):
    """
    Reconnect model signals to unit to run simulation with simulation model
    but use original unit interfaces for communication

    :param synthesisedUnitOrIntf: interface where should be signals
        replaced from signals from modelCls
    :param modelCls: simulation model form where signals
        for synthesisedUnitOrIntf should be taken
    """
----------
Output: 

    if isinstance(synthesisedUnitOrIntf, int):
        synthesisedUnitOrIntf = modelCls.getUnit(synthesisedUnitOrIntf)
    for signal in synthesisedUnitOrIntf.getSignals():
        if signal.isInput():
            signal.setValue(modelCls.getInputSignal(signal.getName()))
        elif signal.isOutput():
            signal.setValue(modelCls.getOutputSignal(signal.getName()))


def reconnectUnitSignalsToModelWithoutReset
----------
Solution: 

    obj = synthesisedUnitOrIntf
    subInterfaces = obj._interfaces


    if subInterfaces:
        for intf in subInterfaces:
            # proxies are destroyed on original interfaces and only proxies on
            # array items will remain
            reconnectUnitSignalsToModel(intf, modelCls)
    else:
        # reconnect signal from model
        s = synthesisedUnitOrIntf
        s._sigInside = getattr(modelCls, s._sigInside.name)
--------------------
Prompt: 
def lines_without_stdlib(self):
        """Filters code from standard library from self.lines."""
----------
Output: 

        return [line for line in self.lines if not line.startswith('from ')]

    def lines_without_tests(self):
        """Filters code from tests from self.lines."""
        return [line for line in self.lines if not line.startswith('import ')]

    def lines_without_comments(self):
        """Filters code from comments from self.lines."""
        return [line for line in self.lines if not line.startswith('#')]

    def lines_without_docstrings(self):

----------
Solution: 

        prev_line = None
        current_module_path = inspect.getabsfile(inspect.currentframe())
        for module_path, lineno, runtime in self.lines:
            module_abspath = os.path.abspath(module_path)
            if not prev_line:
                prev_line = [module_abspath, lineno, runtime]
            else:
                if (not check_standard_dir(module_path) and
                        module_abspath != current_module_path):
                    yield prev_line
                    prev_line = [module_abspath, lineno, runtime]
                else:
                    prev_line[2] += runtime
        yield prev_line
--------------------
Prompt: 
def from_ascii(path, seperator=None, names=True, skip_lines=0, skip_after=0, **kwargs):
    """
    Create an in memory DataFrame from an ascii file (whitespace seperated by default).

    >>> ds = vx.from_ascii("table.asc")
    >>> ds = vx.from_ascii("table.csv", seperator=",", names=["x", "y", "z"])

    :param path: file path
    :param seperator: value seperator, by default whitespace, use "," for comma seperated values.
    :param names: If True, the first line is used for the column names, otherwise provide a list of strings with names
    :param skip_lines: skip lines at the start of the file
    :param skip_after: skip lines at the end of the file
    :param kwargs:
    :rtype: DataFrame
    """
----------
Output: 
: seperator for whitespace seperated files
    :param names: column names
    :param skip_lines: skip lines at the start of the file
    :param skip_after: skip lines at the end of the file
    :param kwargs:
    :return: DataFrame
    """
    if seperator is None:
        seperator = " "
    with open(path, "r") as f:
        lines = f.readlines()
    if skip_lines > 0:
        lines = lines[skip_lines:]
   
----------
Solution: 


    import vaex.ext.readcol as rc
    ds = vaex.dataframe.DataFrameArrays(path)
    if names not in [True, False]:
        namelist = names
        names = False
    else:
        namelist = None
    data = rc.readcol(path, fsep=seperator, asdict=namelist is None, names=names, skipline=skip_lines, skipafter=skip_after, **kwargs)
    if namelist:
        for name, array in zip(namelist, data.T):
            ds.add_column(name, array)
    else:
        for name, array in data.items():
            ds.add_column(name, array)
    return ds
--------------------
Prompt: 
def ndtr(x, name="ndtr"):
  """Normal distribution function.

  Returns the area under the Gaussian probability density function, integrated
  from minus infinity to x:

  ```
                    1       / x
     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt
                sqrt(2 pi)  /-inf

              = 0.5 (1 + erf(x / sqrt(2)))
              = 0.5 erfc(x / sqrt(2))
  ```

  Args:
    x: `Tensor` of type `float32`, `float64`.
    name: Python string. A name for the operation (default="ndtr").

  Returns:
    ndtr: `Tensor` with `dtype=x.dtype`.

  Raises:
    TypeError: if `x` is not floating-type.
  """
----------
Output: 

  ```

  Args:
    x: Floating-point `Tensor`. Must be broadcastable with `a`.
    name: Python `str`. The name prefixed to the ops created by this function.
      Default value: `None` (i.e., 'ndtr').

  Returns:
    norm_cdf: `Tensor` of the same shape as `x`.
  """
  with tf.name_scope(name or "ndtr"):
    x = tf.convert_to_tensor(x, name="x")
----------
Solution: 


  with tf.name_scope(name):
    x = tf.convert_to_tensor(value=x, name="x")
    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:
      raise TypeError(
          "x.dtype=%s is not handled, see docstring for supported types."
          % x.dtype)
    return _ndtr(x)
--------------------
Prompt: 
def _all_dirs(base_path):
        """
        Return all dirs in base_path, relative to base_path
        """
----------
Output: 

        return [os.path.relpath(x[0], base_path) for x in os.walk(base_path)]

    def _all_files(base_path):
        """
        Return all files in base_path, relative to base_path
        """
        return [os.path.relpath(x[0], base_path) for x in os.walk(base_path)]

    def _all_files_in_dir(base_path, dir_name):
        """
        Return all files in dir_name,
----------
Solution: 

        for root, dirs, files in os.walk(base_path, followlinks=True):
            for dir in dirs:
                yield os.path.relpath(os.path.join(root, dir), base_path)
--------------------
Prompt: 
def dump_encoding( file, encoding_name, encoding_list ):
  """dump a given encoding"""
----------
Output: 

  print( "Encoding: %s" % encoding_name )
  print( "  %s" % encoding_list )
  print( "" )

def dump_encodings( file, encodings ):
  """dump a list of encodings"""
  for encoding_name in encodings:
    encoding_list = encodings[ encoding_name ]
    dump_encoding( file, encoding_name, encoding_list )

def dump_encoding_list( file, encoding_list ):
  """dump a list of enc
----------
Solution: 


  write = file.write
  write( "  /* the following are indices into the SID name table */\n" )
  write( "  static const unsigned short  " + encoding_name +
         "[" + repr( len( encoding_list ) ) + "] =\n" )
  write( "  {\n" )

  line  = "    "
  comma = ""
  col   = 0
  for value in encoding_list:
    line += comma
    line += "%3d" % value
    comma = ","
    col  += 1
    if col == 16:
      col = 0
      comma = ",\n    "

  write( line + "\n  };\n\n\n" )
--------------------
Prompt: 
def _set_kernel_manager(self, kernel_manager):
        """ Disconnect from the current kernel manager (if any) and set a new
            kernel manager.
        """
----------
Output: 

        if self._kernel_manager is not None:
            self._kernel_manager.shutdown_kernel()
        self._kernel_manager = kernel_manager
        self._kernel_manager.start_kernel()
        self._kernel_manager.kernel.gui_name = 'qt'
        self._kernel_manager.kernel.gui_version = __version__
        self._kernel_manager.kernel.protocol_version = '5.0'
        self._kernel_manager.kernel.banner = 'Jupyter Qt Console'
        self._kernel_
----------
Solution: 

        # Disconnect the old kernel manager, if necessary.
        old_manager = self._kernel_manager
        if old_manager is not None:
            old_manager.started_kernel.disconnect(self._started_kernel)
            old_manager.started_channels.disconnect(self._started_channels)
            old_manager.stopped_channels.disconnect(self._stopped_channels)

            # Disconnect the old kernel manager's channels.
            old_manager.sub_channel.message_received.disconnect(self._dispatch)
            old_manager.shell_channel.message_received.disconnect(self._dispatch)
            old_manager.stdin_channel.message_received.disconnect(self._dispatch)
            old_manager.hb_channel.kernel_died.disconnect(
                self._handle_kernel_died)

            # Handle the case where the old kernel manager is still listening.
            if old_manager.channels_running:
                self._stopped_channels()

        # Set the new kernel manager.
        self._kernel_manager = kernel_manager
        if kernel_manager is None:
            return

        # Connect the new kernel manager.
        kernel_manager.started_kernel.connect(self._started_kernel)
        kernel_manager.started_channels.connect(self._started_channels)
        kernel_manager.stopped_channels.connect(self._stopped_channels)

        # Connect the new kernel manager's channels.
        kernel_manager.sub_channel.message_received.connect(self._dispatch)
        kernel_manager.shell_channel.message_received.connect(self._dispatch)
        kernel_manager.stdin_channel.message_received.connect(self._dispatch)
        kernel_manager.hb_channel.kernel_died.connect(self._handle_kernel_died)

        # Handle the case where the kernel manager started channels before
        # we connected.
        if kernel_manager.channels_running:
            self._started_channels()
--------------------
Prompt: 
def subscribe_to_trades(self, pair, **kwargs):
        """Subscribe to the passed pair's trades channel.

        :param pair: str, Symbol pair to request data for
        :param kwargs:
        :return:
        """
----------
Output: 

        return self.ws.subscribe_to_trades(pair, **kwargs)

    def subscribe_to_orderbook(self, pair, **kwargs):
        """Subscribe to the passed pair's orderbook channel.

        :param pair: str, Symbol pair to request data for
        :param kwargs:
        :return:
        """
        return self.ws.subscribe_to_orderbook(pair, **kwargs)

    def subscribe_to_orderbook_diffs(self, pair, **kwargs):
        """Sub
----------
Solution: 

        identifier = ('trades', pair)
        self._subscribe('trades', identifier, symbol=pair, **kwargs)
--------------------
Prompt: 
def interact(self, display_banner=None):
        """Closely emulate the interactive Python console."""
----------
Output: 

        if display_banner is None:
            display_banner = self.display_banner
        if display_banner:
            self.write(self.banner)
        more = 0
        while 1:
            try:
                if more:
                    prompt = '> '
                else:
                    prompt = '>>> '
                try:
                    line = self.raw_input(prompt)
                except EOFError:
                    self.write('\n')
                    break
                else:
                
----------
Solution: 


        # batch run -> do not interact
        if self.exit_now:
            return

        if display_banner is None:
            display_banner = self.display_banner

        if isinstance(display_banner, basestring):
            self.show_banner(display_banner)
        elif display_banner:
            self.show_banner()

        more = False

        if self.has_readline:
            self.readline_startup_hook(self.pre_readline)
            hlen_b4_cell = self.readline.get_current_history_length()
        else:
            hlen_b4_cell = 0
        # exit_now is set by a call to %Exit or %Quit, through the
        # ask_exit callback.

        while not self.exit_now:
            self.hooks.pre_prompt_hook()
            if more:
                try:
                    prompt = self.prompt_manager.render('in2')
                except:
                    self.showtraceback()
                if self.autoindent:
                    self.rl_do_indent = True

            else:
                try:
                    prompt = self.separate_in + self.prompt_manager.render('in')
                except:
                    self.showtraceback()
            try:
                line = self.raw_input(prompt)
                if self.exit_now:
                    # quick exit on sys.std[in|out] close
                    break
                if self.autoindent:
                    self.rl_do_indent = False

            except KeyboardInterrupt:
                #double-guard against keyboardinterrupts during kbdint handling
                try:
                    self.write('\nKeyboardInterrupt\n')
                    source_raw = self.input_splitter.source_raw_reset()[1]
                    hlen_b4_cell = \
                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)
                    more = False
                except KeyboardInterrupt:
                    pass
            except EOFError:
                if self.autoindent:
                    self.rl_do_indent = False
                    if self.has_readline:
                        self.readline_startup_hook(None)
                self.write('\n')
                self.exit()
            except bdb.BdbQuit:
                warn('The Python debugger has exited with a BdbQuit exception.\n'
                     'Because of how pdb handles the stack, it is impossible\n'
                     'for IPython to properly format this particular exception.\n'
                     'IPython will resume normal operation.')
            except:
                # exceptions here are VERY RARE, but they can be triggered
                # asynchronously by signal handlers, for example.
                self.showtraceback()
            else:
                self.input_splitter.push(line)
                more = self.input_splitter.push_accepts_more()
                if (self.SyntaxTB.last_syntax_error and
                    self.autoedit_syntax):
                    self.edit_syntax_error()
                if not more:
                    source_raw = self.input_splitter.source_raw_reset()[1]
                    self.run_cell(source_raw, store_history=True)
                    hlen_b4_cell = \
                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)

        # Turn off the exit flag, so the mainloop can be restarted if desired
        self.exit_now = False
--------------------
Prompt: 
async def add(self, dialog: Dialog):
        """
        Adds a new dialog to the set and returns the added dialog.
        :param dialog: The dialog to add.
        """
----------
Output: 

        self.dialogs.append(dialog)
        return dialog

    def get_dialog(self, name: str) -> Dialog:
        """
        Returns the dialog with the given name.
        :param name: The name of the dialog.
        """
        for dialog in self.dialogs:
            if dialog.name == name:
                return dialog
        raise ValueError("Dialog not found")

    def get_dialogs(self) -> List[Dialog]:
        """
        Returns the dialogs in the set.
        """
        return
----------
Solution: 

        if dialog is None or not isinstance(dialog, Dialog):
            raise TypeError('DialogSet.add(): dialog cannot be None and must be a Dialog or derived class.')

        if dialog.id in self._dialogs:
            raise TypeError("DialogSet.add(): A dialog with an id of '%s' already added." % dialog.id)

        # dialog.telemetry_client = this._telemetry_client;
        self._dialogs[dialog.id] = dialog

        return self
--------------------
Prompt: 
def cycle(self):
        """
        Request one batch of events from Skype, calling :meth:`onEvent` with each event in turn.

        Subclasses may override this method to alter loop functionality.
        """
----------
Output: 

        self.onEvent(self.skype.ReadEvent())

    def onEvent(self, event):
        """
        Called with each event from Skype.

        Subclasses may override this method to alter event handling.
        """
        if event.EventType == SKYPEEVENTTYPE.USER_STATE_CHANGED:
            if event.EventObject.UserState == SKYPEUSERSTATE.ONLINE:
                self.onUserOnline(event.EventObject.Handle)
            elif event.EventObject.UserState == SKYPEUSER
----------
Solution: 

        try:
            events = self.getEvents()
        except requests.ConnectionError:
            return
        for event in events:
            self.onEvent(event)
            if self.autoAck:
                event.ack()
--------------------
Prompt: 
def _maybe_validate_perm(perm, validate_args, name=None):
  """Checks that `perm` is valid."""
----------
Output: 

  with tf.name_scope(name, 'maybe_validate_perm', [perm]):
    if validate_args:
      assertions = [
          tf.assert_rank(perm, 2, message='perm must be a matrix.'),
          tf.assert_equal(
              tf.shape(perm)[0], tf.shape(perm)[1],
              message='perm must be a square matrix.')
      ]
      with tf.control_dependencies(assertions):
        return tf.identity(perm)
    else:
      return perm

----------
Solution: 

  with tf.name_scope(name or 'maybe_validate_perm'):
    assertions = []
    if not dtype_util.is_integer(perm.dtype):
      raise TypeError('`perm` must be integer type')

    msg = '`perm` must be a vector.'
    if tensorshape_util.rank(perm.shape) is not None:
      if tensorshape_util.rank(perm.shape) != 1:
        raise ValueError(
            msg[:-1] +
            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))
    elif validate_args:
      assertions += [assert_util.assert_rank(perm, 1, message=msg)]

    perm_ = tf.get_static_value(perm)
    msg = '`perm` must be a valid permutation vector.'
    if perm_ is not None:
      if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)):
        raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_))
    elif validate_args:
      assertions += [
          assert_util.assert_equal(
              tf.sort(perm), tf.range(tf.size(input=perm)), message=msg)
      ]

    return assertions
--------------------
Prompt: 
def create_cookie(self, delete=None):
        """
        Creates the value for ``Set-Cookie`` HTTP header.

        :param bool delete:
            If ``True`` the cookie value will be ``deleted`` and the
            Expires value will be ``Thu, 01-Jan-1970 00:00:01 GMT``.

        """
----------
Output: 

        if delete:
            self.value = ''
            self.expires = 'Thu, 01-Jan-1970 00:00:01 GMT'
        return '%s=%s; path=%s; domain=%s; expires=%s' % (
            self.name,
            self.value,
            self.path,
            self.domain,
            self.expires,
        )

    def __str__(self):
        return self.create_cookie()

    def __repr__(
----------
Solution: 

        value = 'deleted' if delete else self._serialize(self.data)
        split_url = parse.urlsplit(self.adapter.url)
        domain = split_url.netloc.split(':')[0]

        # Work-around for issue #11, failure of WebKit-based browsers to accept
        # cookies set as part of a redirect response in some circumstances.
        if '.' not in domain:
            template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}'
        else:
            template = ('{name}={value}; Domain={domain}; Path={path}; '
                        'HttpOnly{secure}{expires}')

        return template.format(
            name=self.name,
            value=value,
            domain=domain,
            path=split_url.path,
            secure='; Secure' if self.secure else '',
            expires='; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else ''
        )
--------------------
Prompt: 
def derivative(wave, indep_min=None, indep_max=None):
    r"""
    Return the numerical derivative of a waveform's dependent variable vector.

    The method used is the `backwards differences
    <https://en.wikipedia.org/wiki/
    Finite_difference#Forward.2C_backward.2C_and_central_differences>`_ method

    :param wave: Waveform
    :type  wave: :py:class:`peng.eng.Waveform`

    :param indep_min: Independent vector start point of computation
    :type  indep_min: integer or float

    :param indep_max: Independent vector stop point of computation
    :type  indep_max: integer or float

    :rtype: float

    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]
    .. Auto-generated exceptions documentation for
    .. peng.wave_functions.derivative

    :raises:
     * RuntimeError (Argument \`indep_max\` is not valid)

     * RuntimeError (Argument \`indep_min\` is not valid)

     * RuntimeError (Argument \`wave\` is not valid)

     * RuntimeError (Incongruent \`indep_min\` and \`indep_max\`
       arguments)

    .. [[[end]]]
    """
----------
Output: 
:param indep_min: Minimum independent variable value
    :type  indep_min: float

    :param indep_max: Maximum independent variable value
    :type  indep_max: float

    :return: Dependent variable vector derivative
    :rtype: numpy.ndarray
    """
    if indep_min is None:
        indep_min = wave.indep_min
    if indep_max is None:
        indep_max = wave.indep_max

    indep_step = (indep_max - indep_min)
----------
Solution: 

    ret = copy.copy(wave)
    _bound_waveform(ret, indep_min, indep_max)
    delta_indep = np.diff(ret._indep_vector)
    delta_dep = np.diff(ret._dep_vector)
    delta_indep = np.concatenate((np.array([delta_indep[0]]), delta_indep))
    delta_dep = np.concatenate((np.array([delta_dep[0]]), delta_dep))
    ret._dep_vector = np.divide(delta_dep, delta_indep)
    ret.dep_name = "derivative({0})".format(ret._dep_name)
    ret.dep_units = _build_units(ret.indep_units, ret.dep_units, "/")
    return ret
--------------------
Prompt: 
def publish_to_target(self, target_arn, message):
        """
        Publish a message to a topic or an endpoint.

        :param target_arn: either a TopicArn or an EndpointArn
        :type target_arn: str
        :param message: the default message you want to send
        :param message: str
        """
----------
Output: 

        if not target_arn:
            raise ValueError("target_arn is required")
        if not message:
            raise ValueError("message is required")

        self.client.publish(
            TargetArn=target_arn,
            Message=message
        )

    def publish_to_topic(self, topic_arn, message):
        """
        Publish a message to a topic.

        :param topic_arn: the topic arn
        :type topic_arn: str
        :param message: the default message you want to send
----------
Solution: 


        conn = self.get_conn()

        messages = {
            'default': message
        }

        return conn.publish(
            TargetArn=target_arn,
            Message=json.dumps(messages),
            MessageStructure='json'
        )
--------------------
Prompt: 
def has_delete_permission(self, request, obj=None):
        """
        Implement a lookup for object level permissions. Basically the same as
        ModelAdmin.has_delete_permission, but also passes the obj parameter in.
        """
----------
Output: 

        opts = self.opts
        codename = get_permission_codename('delete', opts)
        return request.user.has_perm("%s.%s" % (opts.app_label, codename))

    def has_change_permission(self, request, obj=None):
        """
        Implement a lookup for object level permissions. Basically the same as
        ModelAdmin.has_change_permission, but also passes the obj parameter in.
        """
        opts = self.opts
        codename = get_permission_codename('change
----------
Solution: 

        if settings.TREE_EDITOR_OBJECT_PERMISSIONS:
            opts = self.opts
            r = request.user.has_perm(opts.app_label + '.' + opts.get_delete_permission(), obj)
        else:
            r = True

        return r and super(TreeEditor, self).has_delete_permission(request, obj)
--------------------
Prompt: 
def _clean(self, rmConnetions=True, lockNonExternal=True):
        """
        Remove all signals from this interface (used after unit is synthesized
        and its parent is connecting its interface to this unit)
        """
----------
Output: 

        for signal in self.signals:
            if rmConnetions:
                if signal.isExternal:
                    signal.remove()
            else:
                if signal.isExternal:
                    signal.remove()
                else:
                    signal.remove()

    def _connect(self, other, name=None):
        """
        Connect this interface to another interface
        """
        if not isinstance(other, Interface):
            raise TypeError("Cannot connect to non-interface")
        if name is None:
            name = self.
----------
Solution: 


        if self._interfaces:
            for i in self._interfaces:
                i._clean(rmConnetions=rmConnetions,
                         lockNonExternal=lockNonExternal)
        else:
            self._sigInside = self._sig
            del self._sig

        if lockNonExternal and not self._isExtern:
            self._isAccessible = False
--------------------
Prompt: 
def bytesize(self, byteorder='@'):
        """Compute the byte size after serialization.

        Args:
            byteorder (str, optional): This is byte order of the serialized data. Use one
                of the `byte order characters
                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:
                ``@``, ``=``, ``<``, ``>``, and ``!``.
                Default is ``@`` -- the native order.

        Returns:
            int: Size in number of bytes after serialization.
        """
----------
Output: 
:
            int: The number of bytes needed to serialize the object.

        """
        return struct.calcsize(self._struct_format(byteorder))

    def serialize(self, byteorder='@'):
        """Serialize the object.

        Args:
            byteorder (str, optional): This is byte order of the serialized data. Use one
                of the `byte order characters
                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:
                ``@
----------
Solution: 

        # Use 8 bytes to store the seed integer
        seed_size = struct.calcsize(byteorder+'q')
        # Use 4 bytes to store the number of hash values
        length_size = struct.calcsize(byteorder+'i')
        # Use 4 bytes to store each hash value as we are using the lower 32 bit
        hashvalue_size = struct.calcsize(byteorder+'I')
        return seed_size + length_size + len(self) * hashvalue_size
--------------------
Prompt: 
def get_conn(self):
        """
        Returns a FTP connection object
        """
----------
Output: 

        return self.conn

    def get_file(self, file_path, local_path):
        """
        Downloads a file from the FTP server
        """
        with open(local_path, 'wb') as f:
            self.conn.retrbinary('RETR ' + file_path, f.write)

    def put_file(self, file_path, local_path):
        """
        Uploads a file to the FTP server
        """
        with open(local_path, 'rb') as f
----------
Solution: 

        if self.conn is None:
            params = self.get_connection(self.ftp_conn_id)
            pasv = params.extra_dejson.get("passive", True)
            self.conn = ftplib.FTP(params.host, params.login, params.password)
            self.conn.set_pasv(pasv)

        return self.conn
--------------------
Prompt: 
def commonprefix(items):
    """Get common prefix for completions

    Return the longest common prefix of a list of strings, but with special
    treatment of escape characters that might precede commands in IPython,
    such as %magic functions. Used in tab completion.

    For a more general function, see os.path.commonprefix
    """
----------
Output: 

    if not items:
        return ''
    if not isinstance(items[0], basestring):
        items = [str(item) for item in items]
    common = items[0]
    for item in items[1:]:
        for i in range(min(len(common), len(item))):
            if common[i] != item[i]:
                common = common[:i]
                break
        else:
            common = common[:i+1]
    return common


def _ipython_completer(self
----------
Solution: 

    # the last item will always have the least leading % symbol
    # min / max are first/last in alphabetical order
    first_match  = ESCAPE_RE.match(min(items))
    last_match  = ESCAPE_RE.match(max(items))
    # common suffix is (common prefix of reversed items) reversed
    if first_match and last_match:
        prefix = os.path.commonprefix((first_match.group(0)[::-1], last_match.group(0)[::-1]))[::-1]
    else:
        prefix = ''

    items = [s.lstrip(ESCAPE_CHARS) for s in items]
    return prefix+os.path.commonprefix(items)
--------------------
Prompt: 
def _graph_wrap(func, graph):
    """Constructs function encapsulated in the graph."""
----------
Output: 

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        with graph.as_default():
            return func(*args, **kwargs)
    return wrapper


def _get_graph(name):
    """Returns graph by name."""
    if name not in _GRAPHS:
        raise ValueError('Graph with name {} is not registered.'.format(name))
    return _GRAPHS[name]


def _register_graph(name, graph):
    """Registers graph."""
----------
Solution: 

    @wraps(func)
    def _wrapped(*args, **kwargs):
        with graph.as_default():
            return func(*args, **kwargs)
    return _wrapped
--------------------
Prompt: 
def avail_archs(self):
        """ Initialize the dictionary of architectures for disassembling via capstone"""
----------
Output: 

        self.archs = {
            'x86': capstone.CS_ARCH_X86,
            'x86_64': capstone.CS_ARCH_X86,
            'arm': capstone.CS_ARCH_ARM,
            'arm64': capstone.CS_ARCH_ARM64,
            'mips': capstone.CS_ARCH_MIPS,
            'mips64': capstone.CS_ARCH_MIPS,
            'ppc
----------
Solution: 


        return {
            ARM32:   (CS_ARCH_ARM,   CS_MODE_ARM),
            ARM64:   (CS_ARCH_ARM64, CS_MODE_LITTLE_ENDIAN),
            ARM_TB:  (CS_ARCH_ARM,   CS_MODE_THUMB),
            MIPS32:  (CS_ARCH_MIPS,  CS_MODE_MIPS32),
            MIPS64:  (CS_ARCH_MIPS,  CS_MODE_MIPS64),
            SPARC32: (CS_ARCH_SPARC, CS_MODE_BIG_ENDIAN),
            SPARC64: (CS_ARCH_SPARC, CS_MODE_V9),
            SYSTEMZ: (CS_ARCH_SYSZ,  CS_MODE_BIG_ENDIAN),
            X86_16:  (CS_ARCH_X86,   CS_MODE_16),
            X86_32:  (CS_ARCH_X86,   CS_MODE_32),
            X86_64:  (CS_ARCH_X86,   CS_MODE_64),
        }
--------------------
Prompt: 
def html_to_xhtml(html):
    """Convert all tags in an HTML tree to XHTML by moving them to the
    XHTML namespace.
    """
----------
Output: 

    def _move_to_xhtml(tag):
        if tag.namespace == 'http://www.w3.org/1999/xhtml':
            return
        tag.namespace = 'http://www.w3.org/1999/xhtml'

    for elem in html.iter():
        _move_to_xhtml(elem)

    return html


def html_to_xhtml_str(html):
    """Convert all tags in an HTML tree to XHTML by moving them to the
    XHTML namespace.
   
----------
Solution: 

    try:
        html = html.getroot()
    except AttributeError:
        pass
    prefix = "{%s}" % XHTML_NAMESPACE
    for el in html.iter(etree.Element):
        tag = el.tag
        if tag[0] != '{':
            el.tag = prefix + tag
--------------------
Prompt: 
def p_gate_op_2(self, program):
        """
        gate_op : id id_list ';'
        """
----------
Output: 

        program.append(('gate', program[-1], program[-2]))
        del program[-2:]

    def p_gate_op_3(self, program):
        """
        gate_op : id id_list ',' id_list ';'
        """
        program.append(('gate', program[-1], program[-2]))
        del program[-2:]

    def p_id_list(self, program):
        """
        id_list : id
        id_list : id_list ',' id
        """
       
----------
Solution: 

        program[0] = node.CustomUnitary([program[1], program[2]])
        # To verify:
        # 1. id is declared as a gate in global scope
        # 2. everything in the id_list is declared as a bit in local scope
        self.verify_as_gate(program[1], program[2])
        self.verify_bit_list(program[2])
        self.verify_distinct([program[2]])
--------------------
Prompt: 
def insert_and_get(self, **fields):
        """Creates a new record in the database and then gets
        the entire row.

        This allows specifying custom conflict behavior using .on_conflict().
        If no special behavior was specified, this uses the normal Django create(..)

        Arguments:
            fields:
                The fields of the row to create.

        Returns:
            The model instance representing the row that was created.
        """
----------
Output: 

        if self.on_conflict is not None:
            return self.on_conflict.insert_and_get(**fields)
        else:
            return self.model.objects.create(**fields)

    def insert_and_get_or_update(self, **fields):
        """Creates a new record in the database and then gets
        the entire row.

        This allows specifying custom conflict behavior using .on_conflict().
        If no special behavior was specified, this uses the normal Django create(..)

        Arguments
----------
Solution: 


        if not self.conflict_target and not self.conflict_action:
            # no special action required, use the standard Django create(..)
            return super().create(**fields)

        compiler = self._build_insert_compiler([fields])
        rows = compiler.execute_sql(return_id=False)

        columns = rows[0]

        # get a list of columns that are officially part of the model and preserve the fact that the attribute name
        # might be different than the database column name
        model_columns = {}
        for field in self.model._meta.local_concrete_fields:
            model_columns[field.column] = field.attname

        # strip out any columns/fields returned by the db that
        # are not present in the model
        model_init_fields = {}
        for column_name, column_value in columns.items():
            try:
                model_init_fields[model_columns[column_name]] = column_value
            except KeyError:
                pass

        return self.model(**model_init_fields)
--------------------
Prompt: 
def strip_email_quotes(text):
    """Strip leading email quotation characters ('>').

    Removes any combination of leading '>' interspersed with whitespace that
    appears *identically* in all lines of the input text.

    Parameters
    ----------
    text : str

    Examples
    --------

    Simple uses::

        In [2]: strip_email_quotes('> > text')
        Out[2]: 'text'

        In [3]: strip_email_quotes('> > text\\n> > more')
        Out[3]: 'text\\nmore'

    Note how only the common prefix that appears in all lines is stripped::

        In [4]: strip_email_quotes('> > text\\n> > more\\n> more...')
        Out[4]: '> text\\n> more\\nmore...'

    So if any line has no quote marks ('>') , then none are stripped from any
    of them ::
    
        In [5]: strip_email_quotes('> > text\\n> > more\\nlast different')
        Out[5]: '> > text\\n> > more\\nlast different'
    """
----------
Output: 
('> > text\n> > text')
        Out[3]: 'text\ntext'

        In [4]: strip_email_quotes('> > text\n> > text\n> > text')
        Out[4]: 'text\ntext\ntext'

    The following are all equivalent::

        In [5]: strip_email_quotes('> > text\n> > text\n> > text')
        Out[5]: 'text\ntext\ntext'

        In [6]: strip_email_
----------
Solution: 

    lines = text.splitlines()
    matches = set()
    for line in lines:
        prefix = re.match(r'^(\s*>[ >]*)', line)
        if prefix:
            matches.add(prefix.group(1))
        else:
            break
    else:
        prefix = long_substr(list(matches))
        if prefix:
            strip = len(prefix)
            text = '\n'.join([ ln[strip:] for ln in lines])
    return text
--------------------
Prompt: 
def delete_vm_image(self, vm_image_name, delete_vhd=False):
        """
        Deletes the specified VM Image from the image repository that is
        associated with the specified subscription.

        vm_image_name:
            The name of the image.
        delete_vhd:
            Deletes the underlying vhd blob in Azure storage.
        """
----------
Output: 

        if not vm_image_name:
            raise ValueError('vm_image_name is required.')

        vm_image = self.get_vm_image(vm_image_name)
        if not vm_image:
            raise WindowsAzureMissingResourceException(
                'The specified vm image does not exist.')

        if vm_image.is_custom_image:
            raise WindowsAzureError(
                'The specified vm image is a custom image and cannot be '
                'deleted.')

        if vm_image.is
----------
Solution: 

        _validate_not_none('vm_image_name', vm_image_name)
        path = self._get_vm_image_path(vm_image_name)
        if delete_vhd:
            path += '?comp=media'
        return self._perform_delete(path, as_async=True)
--------------------
Prompt: 
def _add_column_kwargs(self, kwargs, column):
        """Add keyword arguments to kwargs (in-place) based on the passed in
        `Column <sqlalchemy.schema.Column>`.
        """
----------
Output: 

        if column.primary_key:
            kwargs['primary_key'] = True
        if column.nullable:
            kwargs['nullable'] = True
        if column.default is not None:
            kwargs['default'] = column.default
        if column.autoincrement:
            kwargs['autoincrement'] = True
        if column.doc:
            kwargs['doc'] = column.doc

    def _add_column_kwargs(self, kwargs, column):
        """Add keyword arguments to kwargs (in-place) based
----------
Solution: 

        if column.nullable:
            kwargs["allow_none"] = True
        kwargs["required"] = not column.nullable and not _has_default(column)

        if hasattr(column.type, "enums"):
            kwargs["validate"].append(validate.OneOf(choices=column.type.enums))

        # Add a length validator if a max length is set on the column
        # Skip UUID columns
        # (see https://github.com/marshmallow-code/marshmallow-sqlalchemy/issues/54)
        if hasattr(column.type, "length"):
            try:
                python_type = column.type.python_type
            except (AttributeError, NotImplementedError):
                python_type = None
            if not python_type or not issubclass(python_type, uuid.UUID):
                kwargs["validate"].append(validate.Length(max=column.type.length))

        if hasattr(column.type, "scale"):
            kwargs["places"] = getattr(column.type, "scale", None)
--------------------
Prompt: 
def check_for_old_config(ipython_dir=None):
    """Check for old config files, and present a warning if they exist.

    A link to the docs of the new config is included in the message.

    This should mitigate confusion with the transition to the new
    config system in 0.11.
    """
----------
Output: 

    if ipython_dir is None:
        ipython_dir = get_ipython_dir()
    old_config_files = [
        os.path.join(ipython_dir, 'ipy_user_conf.py'),
        os.path.join(ipython_dir, 'ipy_user_conf.pyc'),
        os.path.join(ipython_dir, 'ipy_user_conf.pyo'),
        os.path.join(ipython_dir, 'ipy_user_conf.pyo.
----------
Solution: 

    if ipython_dir is None:
        ipython_dir = get_ipython_dir()

    old_configs = ['ipy_user_conf.py', 'ipythonrc', 'ipython_config.py']
    warned = False
    for cfg in old_configs:
        f = os.path.join(ipython_dir, cfg)
        if os.path.exists(f):
            if filehash(f) == old_config_md5.get(cfg, ''):
                os.unlink(f)
            else:
                warnings.warn("Found old IPython config file %r (modified by user)"%f)
                warned = True

    if warned:
        warnings.warn(
--------------------
Prompt: 
def getField(self, fld_name):
        """ Return :class:`~ekmmeters.Field` content, scaled and formatted.

        Args:
            fld_name (str): A :class:`~ekmmeters.Field` value which is on your meter.

        Returns:
            str: String value (scaled if numeric) for the field.
        """
----------
Output: 

        if fld_name in self.fields:
            fld = self.fields[fld_name]
            if fld.numeric:
                return fld.scaled_value
            else:
                return fld.value
        else:
            raise ValueError("Field not found: %s" % fld_name)

    def getFields(self):
        """ Return a dictionary of :class:`~ekmmeters.Field` content, scaled and formatted.

        Returns:
            dict: A dictionary of :class:`~
----------
Solution: 

        result = ""
        if fld_name in self.m_req:
            result = self.m_req[fld_name][MeterData.StringValue]
        else:
            ekm_log("Requested nonexistent field: " + fld_name)

        return result
--------------------
Prompt: 
def _mk_index(self):
        """create the index for this set"""
----------
Output: 

        self._index = {}
        for i, item in enumerate(self):
            self._index[item] = i

    def __contains__(self, item):
        """check if item is in this set"""
        return item in self._index

    def __iter__(self):
        """iterate over the items in this set"""
        return iter(self._items)

    def __len__(self):
        """return the number of items in this set"""
        return len(self._items)

    def __getitem
----------
Solution: 

        index = defaultdict(list)
        for line_no, line in enumerate(self._stripped_lines):
            if line:
                index[line].append(line_no)
        return index
--------------------
Prompt: 
def _find_blob_start(self):
        """Find first blob from selection.
        """
----------
Output: 

        for i in range(self.selection_start, len(self.data)):
            if self.data[i] == 0:
                return i
        return len(self.data)

    def _find_blob_end(self):
        """Find last blob from selection.
        """
        for i in range(self.selection_end, 0, -1):
            if self.data[i] == 0:
                return i
        return 0

    def _find_blob_start_in_selection(self):
----------
Solution: 


        # Convert input frequencies into what their corresponding channel number would be.
        self._setup_chans()

        # Check which is the blob time offset
        blob_time_start = self.t_start

        # Check which is the blob frequency offset (in channels)
        blob_freq_start = self.chan_start_idx

        blob_start = blob_time_start * self.n_channels_in_file + blob_freq_start

        return blob_start
--------------------
Prompt: 
def update(self, ava):
        """
        Implements the dict.update() method
        """
----------
Output: 

        self.update_from_dict(ava)

    def update_from_dict(self, ava):
        """
        Update the object from a dictionary
        """
        for key, value in ava.items():
            if key in self.keys():
                self[key] = value
            else:
                self.add_attribute(key, value)

    def update_from_xml(self, xml):
        """
        Update the object from an XML string
        """
        self.update_from_dict(xmltodict
----------
Solution: 

        for key, val in ava.items():
            self[key] = val
--------------------
Prompt: 
def multiqc(institute_id, case_name):
    """Load multiqc report for the case."""
----------
Output: 

    from snakemake.remote.S3 import S3RemoteProvider
    from snakemake.remote.FTP import FTPRemoteProvider
    from snakemake.remote.HTTP import HTTPRemoteProvider
    from snakemake.remote.SFTP import SFTPRemoteProvider
    from snakemake.remote.SCP import SCPRemoteProvider
    from snakemake.remote.Local import LocalRemoteProvider
    from snakemake.remote.Dropbox import DropboxRemoteProvider
    from snakemake.remote.GoogleCloud import
----------
Solution: 

    data = controllers.multiqc(store, institute_id, case_name)
    if data['case'].get('multiqc') is None:
        return abort(404)
    out_dir = os.path.abspath(os.path.dirname(data['case']['multiqc']))
    filename = os.path.basename(data['case']['multiqc'])
    return send_from_directory(out_dir, filename)
--------------------
Prompt: 
def get_user_token(user, purpose, minutes_valid):
    """Return login token info for given user."""
----------
Output: 

    token = user.get_token(purpose, minutes_valid)
    return {
        'token': token.decode('utf-8'),
        'minutes_valid': minutes_valid,
        'user_id': user.id,
        'username': user.username,
        'email': user.email,
        'first_name': user.first_name,
        'last_name': user.last_name,
        'is_admin': user.is_admin,
        'is_active': user.is_active,

----------
Solution: 

    token = ''.join(
        dumps([
            user.get_username(),
            get_auth_hash(user, purpose),
        ]).encode('base64').split('\n')
    )
    return {
        'id': get_meteor_id(user),
        'token': token,
        'tokenExpires': calc_expiry_time(minutes_valid),
    }
--------------------
Prompt: 
def write_meta(self):
        """ucds, descriptions and units are written as attributes in the hdf5 file, instead of a seperate file as
         the default :func:`Dataset.write_meta`.
         """
----------
Output: 

        for key in self.meta.keys():
            if key == 'ucds':
                for ucd in self.meta[key].keys():
                    self.h5_file.attrs[ucd] = self.meta[key][ucd]
            elif key == 'descriptions':
                for desc in self.meta[key].keys():
                    self.h5_file.attrs[desc] = self.meta[key][desc]
            elif key == 'units':
                for unit in self.meta[key].keys
----------
Solution: 

        with h5py.File(self.filename, "r+") as h5file_output:
            h5table_root = h5file_output[self.h5table_root_name]
            if self.description is not None:
                h5table_root.attrs["description"] = self.description
            h5columns = h5table_root if self._version == 1 else h5table_root['columns']
            for column_name in self.columns.keys():
                h5dataset = None
                if column_name in h5columns:
                    h5dataset = h5columns[column_name]
                else:
                    for group in h5columns.values():
                        if 'type' in group.attrs:
                            if group.attrs['type'] in ['csr_matrix']: 
                                for name, column in group.items():
                                    if name == column_name:
                                        h5dataset = column
                if h5dataset is None:
                    raise ValueError('column {} not found'.format(column_name))
                for name, values in [("ucd", self.ucds), ("unit", self.units), ("description", self.descriptions)]:
                    if column_name in values:
                        value = ensure_string(values[column_name], cast=True)
                        h5dataset.attrs[name] = value
                    else:
                        if name in h5columns.attrs:
                            del h5dataset.attrs[name]
--------------------
Prompt: 
def rename(self, columns=None):
        """
        Change names of columns in the frame.

        Dict key is an index or name of the column whose name is to be set.
        Dict value is the new name of the column.

        :param columns: dict-like transformations to apply to the column names
        """
----------
Output: 

        if columns is None:
            return self
        if not isinstance(columns, dict):
            raise TypeError("columns must be a dict-like object")
        if not all(isinstance(k, (int, str)) for k in columns.keys()):
            raise TypeError("columns must be a dict-like object with keys of type int or str")
        if not all(isinstance(v, str) for v in columns.values()):
            raise TypeError("columns must be a dict-like object with values of type str")
        if not all(is
----------
Solution: 

        assert_is_type(columns, None, dict)
        new_names = self.names
        ncols = self.ncols

        for col, name in columns.items():
            col_index = None
            if is_type(col, int) and (-ncols <= col < ncols):
                col_index = (col + ncols) % ncols  # handle negative indices
            elif is_type(col, str) and col in self.names:
                col_index = self.names.index(col)  # lookup the name

            if col_index is not None:
                new_names[col_index] = name

        return self.set_names(new_names)
--------------------
Prompt: 
def get_root_schema_dict(record):
    """Return a root jsonschema for a given record

    A root schema includes the $schema attribute and all sub-record
    schemas and definitions.
    """
----------
Output: 

    schema = record.schema
    definitions = record.definitions
    root_schema = {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "definitions": definitions,
        "type": "object",
        "properties": {
            "record": schema
        },
        "required": ["record"]
    }
    return root_schema


def get_record_schema_dict(record):
    """Return a record jsonschema for a given record

    A record schema includes
----------
Solution: 

    state = SchemaGeneratorState()
    schema = get_schema_dict(record, state)
    del state.record_schemas[record._schema_name]
    if state.record_schemas:
        schema['definitions'] = dict()
        for name, sub_schema in state.record_schemas.iteritems():
            schema['definitions'][name] = sub_schema
    return schema
--------------------
Prompt: 
def _check_multi_statement_line(self, node, line):
        """Check for lines containing multiple statements."""
----------
Output: 

        if node.type == 'function':
            return
        if node.type == 'class':
            return
        if node.type == 'if':
            return
        if node.type == 'for':
            return
        if node.type == 'while':
            return
        if node.type == 'try':
            return
        if node.type == 'with':
            return
        if node.type == 'except':
            return
        if node.type == 'finally':
            return
        if node.type == 'def':
----------
Solution: 

        # Do not warn about multiple nested context managers
        # in with statements.
        if isinstance(node, nodes.With):
            return
        # For try... except... finally..., the two nodes
        # appear to be on the same line due to how the AST is built.
        if isinstance(node, nodes.TryExcept) and isinstance(
            node.parent, nodes.TryFinally
        ):
            return
        if (
            isinstance(node.parent, nodes.If)
            and not node.parent.orelse
            and self.config.single_line_if_stmt
        ):
            return
        if (
            isinstance(node.parent, nodes.ClassDef)
            and len(node.parent.body) == 1
            and self.config.single_line_class_stmt
        ):
            return
        self.add_message("multiple-statements", node=node)
        self._visited_lines[line] = 2
--------------------
Prompt: 
def _validate(value, optdict, name=""):
    """return a validated value for an option according to its type

    optional argument name is only used for error message formatting
    """
----------
Output: 

    if value is None:
        return None
    if isinstance(value, optdict["type"]):
        return value
    if optdict["type"] == bool:
        if value in ("True", "true", "1"):
            return True
        elif value in ("False", "false", "0"):
            return False
        else:
            raise ValueError("invalid boolean value %r" % value)
    if optdict["type"] == int:
        try:
            return int(value)
        except ValueError:
            raise
----------
Solution: 

    try:
        _type = optdict["type"]
    except KeyError:
        # FIXME
        return value
    return _call_validator(_type, optdict, name, value)
--------------------
Prompt: 
def run(self, endpoint, data=None, headers=None, extra_options=None):
        """
        Performs the request

        :param endpoint: the endpoint to be called i.e. resource/v1/query?
        :type endpoint: str
        :param data: payload to be uploaded or request parameters
        :type data: dict
        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non
            2XX or 3XX status codes
        :type extra_options: dict
        """
----------
Output: 
_response': False}
        :type extra_options: dict
        :return: the response from the request
        :rtype: requests.Response
        """
        # Set the default headers
        headers = headers or {}
        headers.update({'Content-Type': 'application/json'})

        # Set the default options
        options = {'check_response': True}
        options.update(extra_options or {})

        # Set the default data
        data = data or {}

        # Set the default method
        method = 'GET'
----------
Solution: 

        extra_options = extra_options or {}

        session = self.get_conn(headers)

        if self.base_url and not self.base_url.endswith('/') and \
           endpoint and not endpoint.startswith('/'):
            url = self.base_url + '/' + endpoint
        else:
            url = (self.base_url or '') + (endpoint or '')

        req = None
        if self.method == 'GET':
            # GET uses params
            req = requests.Request(self.method,
                                   url,
                                   params=data,
                                   headers=headers)
        elif self.method == 'HEAD':
            # HEAD doesn't use params
            req = requests.Request(self.method,
                                   url,
                                   headers=headers)
        else:
            # Others use data
            req = requests.Request(self.method,
                                   url,
                                   data=data,
                                   headers=headers)

        prepped_request = session.prepare_request(req)
        self.log.info("Sending '%s' to url: %s", self.method, url)
        return self.run_and_check(session, prepped_request, extra_options)
--------------------
Prompt: 
def print_annotation(self):
        """Print annotation "key: value" pairs to standard output."""
----------
Output: 

        for key, value in self.items():
            print(f"{key}: {value}")

    def __str__(self):
        """Return a string representation of the annotation."""
        return self.print_annotation.__str__()

    def __repr__(self):
        """Return a string representation of the annotation."""
        return self.print_annotation.__repr__()

    def __getitem__(self, key):
        """Return the value of the annotation with the given key."""
        return self.annotations[key]
----------
Solution: 

        for path, ann in self.annotation.items():
            print("{}: {}".format(path, ann['value']))
--------------------
Prompt: 
def info(self, **kwargs):
        """
        Get the detailed information about a particular credit record. This is 
        currently only supported with the new credit model found in TV. These 
        ids can be found from any TV credit response as well as the tv_credits 
        and combined_credits methods for people.

        The episodes object returns a list of episodes and are generally going 
        to be guest stars. The season array will return a list of season 
        numbers.  Season credits are credits that were marked with the 
        "add to every season" option in the editing interface and are 
        assumed to be "season regulars".

        Args:
            language: (optional) ISO 639-1 code.

        Returns:
            A dict respresentation of the JSON returned from the API.
        """
----------
Output: 
itional" flag.

        :param credit_id: The id of the credit record.
        :type credit_id: int
        :returns: A dictionary containing the following information:

            - **cast** (*list*): A list of dictionaries containing the following information:

                - **cast_id** (*int*): The id of the cast member.
                - **character** (*str*): The character name.
                - **credit_id** (*str*): The id of the credit.
                - **gender** (*int*): The gender of
----------
Solution: 

        path = self._get_credit_id_path('info')

        response = self._GET(path, kwargs)
        self._set_attrs_to_values(response)
        return response
--------------------
Prompt: 
def parse_options():
    """
    Commandline options arguments parsing.

    :return: parsed commandline arguments.
    :rtype: optparse.Values.
    """
----------
Output: 

    parser = optparse.OptionParser()
    parser.add_option("-d", "--debug",
                      action="store_true", dest="debug", default=False,
                      help="Enable debug mode.")
    parser.add_option("-v", "--verbose",
                      action="store_true", dest="verbose", default=False,
                      help="Enable verbose mode.")
    parser.add_option("-f", "--file",
                      action="store", dest="file", default=None,
                      help
----------
Solution: 


    version = "%%prog {version}".format(version=__version__)
    parser = OptionParser(version=version)
    parser.add_option(
        "-s", "--server", action="store", dest="server",
        type="string", default="", metavar="SERVER",
        help="server name, IP address or unix socket path"
    )
    parser.add_option(
        "-p", "--port", action="store", type="int", dest="port",
        default=9001, metavar="PORT", help="port number"
    )
    parser.add_option(
        "-P", "--programs", action="store", dest="programs", type="string", default="",
        metavar="PROGRAMS", help="comma separated programs list, or empty for all programs in supervisord response"
    )
    parser.add_option(
        "-u", "--username", action="store", dest="username", type="string", default="",
        metavar="USERNAME", help="supervisord user"
    )
    parser.add_option(
        "-S", "--password", action="store", dest="password", type="string", default="",
        metavar="PASSWORD", help="supervisord user password"
    )
    parser.add_option(
        "-q", "--quiet", metavar="QUIET", action="store_true", default=False, dest="quiet", help="be quiet"
    )
    parser.add_option(
        "--stopped-state", action="store", dest="stopped_state", type="choice", choices=EXIT_CODES.keys(), default=EXIT_CODE_OK,
        metavar="STOPPED_STATE", help="stopped state"
    )
    parser.add_option(
        "--network-errors-exit-code", action="store", dest="network_errors_exit_code", type="choice", choices=EXIT_CODES.keys(), default=EXIT_CODE_UNKNOWN,
        metavar="NETWORK_ERRORS_EXIT_CODE", help="network errors exit code"
    )

    options = parser.parse_args(sys.argv)[0]
    STATE2TEMPLATE[STATE_STOPPED] = options.stopped_state  # update stopped state value from command line argument

    # check mandatory command line options supplied
    if not options.server:
        parser.error("Required server address option missing")
    if options.username and not options.password:
        parser.error("Required supervisord user password")

    return options
--------------------
Prompt: 
def _create_hstore_unique(self, model, field, keys):
        """Creates a UNIQUE constraint for the specified hstore keys."""
----------
Output: 

        return self._create_hstore_index(model, field, keys, True)

    def _create_hstore_index(self, model, field, keys, unique=False):
        """Creates an index for the specified hstore keys."""
        if not keys:
            return

        if isinstance(keys, basestring):
            keys = [keys]

        if not isinstance(keys, (list, tuple)):
            raise ValueError('keys must be a list or tuple')

        if not isinstance(field, H
----------
Solution: 


        name = self._unique_constraint_name(
            model._meta.db_table, field, keys)
        columns = [
            '(%s->\'%s\')' % (field.column, key)
            for key in keys
        ]
        sql = self.sql_hstore_unique_create.format(
            name=self.quote_name(name),
            table=self.quote_name(model._meta.db_table),
            columns=','.join(columns)
        )
        self.execute(sql)
--------------------
Prompt: 
def create_trace(
        turn_activity: Activity,
        name: str,
        value: object = None,
        value_type: str = None,
        label: str = None,
    ) -> Activity:
        """Creates a trace activity based on this activity.

        :param turn_activity:
        :type turn_activity: Activity
        :param name: The value to assign to the trace activity's <see cref="Activity.name"/> property.
        :type name: str
        :param value: The value to assign to the trace activity's <see cref="Activity.value"/> property., defaults to None
        :param value: object, optional
        :param value_type: The value to assign to the trace activity's <see cref="Activity.value_type"/> property, defaults to None
        :param value_type: str, optional
        :param label: The value to assign to the trace activity's <see cref="Activity.label"/> property, defaults to None
        :param label: str, optional
        :return: The created trace activity.
        :rtype: Activity
        """
----------
Output: 
the trace activity's <see cref="Activity.value"/> property.
        :type value: object
        :param value_type: The value to assign to the trace activity's <see cref="Activity.valueType"/> property.
        :type value_type: str
        :param label: The value to assign to the trace activity's <see cref="Activity.label"/> property.
        :type label: str
        :return: A trace activity based on this activity.
        :rtype: Activity
        """
        trace_activity = Activity(

----------
Solution: 


        from_property = (
            ChannelAccount(
                id=turn_activity.recipient.id, name=turn_activity.recipient.name
            )
            if turn_activity.recipient is not None
            else ChannelAccount()
        )
        if value_type is None and value is not None:
            value_type = type(value).__name__

        reply = Activity(
            type=ActivityTypes.trace,
            timestamp=datetime.utcnow(),
            from_property=from_property,
            recipient=ChannelAccount(
                id=turn_activity.from_property.id, name=turn_activity.from_property.name
            ),
            reply_to_id=turn_activity.id,
            service_url=turn_activity.service_url,
            channel_id=turn_activity.channel_id,
            conversation=ConversationAccount(
                is_group=turn_activity.conversation.is_group,
                id=turn_activity.conversation.id,
                name=turn_activity.conversation.name,
            ),
            name=name,
            label=label,
            value_type=value_type,
            value=value,
        )
        return reply
--------------------
Prompt: 
def voucher_code(request):
    """ A view *just* for entering a voucher form. """
----------
Output: 

    return render(request, 'voucher/voucher_code.html')


def voucher_code_result(request):
    """ A view *just* for displaying the result of a voucher form. """
    return render(request, 'voucher/voucher_code_result.html')


def voucher_code_form(request):
    """ A view *just* for displaying the form for a voucher form. """
    return render(request, 'voucher/voucher_code_
----------
Solution: 


    VOUCHERS_FORM_PREFIX = "vouchers"

    # Handle the voucher form *before* listing products.
    # Products can change as vouchers are entered.
    v = _handle_voucher(request, VOUCHERS_FORM_PREFIX)
    voucher_form, voucher_handled = v

    if voucher_handled:
        messages.success(request, "Your voucher code was accepted.")
        return redirect("dashboard")

    data = {
        "voucher_form": voucher_form,
    }

    return render(request, "registrasion/voucher_code.html", data)
--------------------
Prompt: 
def create(cls, name_value, name_type):
        """
            Returns a Name object, populated with the given value and type
        """
----------
Output: 

        return cls(name_value, name_type)

    def __init__(self, name_value, name_type):
        """
            Initializes a Name object, populated with the given value and type
        """
        self.value = name_value
        self.type = name_type

    def __str__(self):
        """
            Returns a string representation of the Name object
        """
        return self.value

    def __repr__(self):
        """
            Returns a string representation of the Name object
        """

----------
Solution: 

        if isinstance(name_value, Name.NameValue):
            value = name_value
        elif isinstance(name_value, str):
            value = cls.NameValue(name_value)
        else:
            name = 'Name'
            msg = exceptions.ErrorStrings.BAD_EXP_RECV
            member = 'name_value'
            raise TypeError(msg.format('{0}.{1}'.format(name, member),
                                       'name_value', type(Name.NameValue),
                                       type(name_value)))

        if isinstance(name_type, Name.NameType):
            n_type = name_type
        elif isinstance(name_type, Enum):
            n_type = cls.NameType(name_type)
        else:
            name = 'Name'
            msg = exceptions.ErrorStrings.BAD_EXP_RECV
            member = 'name_type'
            raise TypeError(msg.format('{0}.{1}'.format(name, member),
                                       'name_type', type(Name.NameType),
                                       type(name_type)))

        return Name(name_value=value,
                    name_type=n_type)
--------------------
Prompt: 
def loop_gtk(kernel):
    """Start the kernel, coordinating with the GTK event loop"""
----------
Output: 

    kernel.start()

    # This is a bit of a hack.  We want to be able to run the kernel
    # in a separate thread, but we also want to be able to run the
    # kernel in a separate process.  The problem is that the GTK
    # event loop is not thread-safe, and the process-level event loop
    # is not thread-safe.  So we need to run the kernel in a thread
    # and then run the event loop in a thread.  But then we need to
    # be able to communicate between
----------
Solution: 

    from .gui.gtkembed import GTKEmbed

    gtk_kernel = GTKEmbed(kernel)
    gtk_kernel.start()
--------------------
Prompt: 
def single_request_timeout(self, value):
        """The timeout (seconds) for a single HTTP REST API request."""
----------
Output: 

        self._single_request_timeout = value

    @property
    def max_retries(self):
        """The maximum number of retries for a single HTTP REST API request."""
        return self._max_retries

    @max_retries.setter
    def max_retries(self, value):
        """The maximum number of retries for a single HTTP REST API request."""
        self._max_retries = value

    @property
    def retry_delay(self):
        """The delay (seconds) between
----------
Solution: 

        check_type(value, int)
        assert value is None or value > 0
        self._single_request_timeout = value
--------------------
Prompt: 
def search(self, pattern="*", raw=True, search_raw=True,
                                                        output=False):
        """Search the database using unix glob-style matching (wildcards
        * and ?).

        Parameters
        ----------
        pattern : str
          The wildcarded pattern to match when searching
        search_raw : bool
          If True, search the raw input, otherwise, the parsed input
        raw, output : bool
          See :meth:`get_range`

        Returns
        -------
        Tuples as :meth:`get_range`
        """
----------
Output: 
ple[List[str], List[str]]
          A tuple of two lists, the first containing the matching
          input lines, and the second containing the matching output
          lines.
        """
        if search_raw:
            search_input = self.raw_input
            search_output = self.raw_output
        else:
            search_input = self.input
            search_output = self.output

        input_matches = []
        output_matches = []
        for i, line in enumerate(search_input):
            if fnmatch.fn
----------
Solution: 

        tosearch = "source_raw" if search_raw else "source"
        if output:
            tosearch = "history." + tosearch
        self.writeout_cache()
        return self._run_sql("WHERE %s GLOB ?" % tosearch, (pattern,),
                                    raw=raw, output=output)
--------------------
Prompt: 
async def get_bots(self, limit, offset):
        """Gets an object of bots on DBL"""
----------
Output: 

        return await self.http.get_bots(limit, offset)

    async def get_bot(self, bot_id):
        """Gets an object of a bot on DBL"""
        return await self.http.get_bot(bot_id)

    async def get_bot_stats(self, bot_id):
        """Gets an object of a bot's stats on DBL"""
        return await self.http.get_bot_stats(bot_id)

    async def get_bot_vote(self
----------
Solution: 

        if limit > 500:
            limit = 50
        return await self.request('GET', '{}/bots?limit={}&offset={}'.format(self.BASE, limit, offset))
--------------------
Prompt: 
def group_transactions(self):
        # type: () -> List[List[Transaction]]
        """
        Groups transactions in the bundle by address.
        """
----------
Output: 

        transactions = self.transactions
        grouped_transactions = []
        for address in transactions:
            grouped_transactions.append([])
            for transaction in transactions[address]:
                grouped_transactions[-1].append(transaction)
        return grouped_transactions

    def get_transaction_hashes(self):
        # type: () -> List[bytes]
        """
        Returns the hashes of all transactions in the bundle.
        """
        return [transaction.hash for transaction in self.transactions.values()]

    def get
----------
Solution: 

        groups = []

        if self:
            last_txn = self.tail_transaction
            current_group = [last_txn]
            for current_txn in self.transactions[1:]:
                # Transactions are grouped by address, so as long as the
                # address stays consistent from one transaction to
                # another, we are still in the same group.
                if current_txn.address == last_txn.address:
                    current_group.append(current_txn)
                else:
                    groups.append(current_group)
                    current_group = [current_txn]

                last_txn = current_txn

            if current_group:
                groups.append(current_group)

        return groups
--------------------
Prompt: 
def authenticate_redirect(
        self, callback_uri=None, ax_attrs=["name", "email", "language",
                                           "username"]):

        """Returns the authentication URL for this service.

        After authentication, the service will redirect back to the given
        callback URI.

        We request the given attributes for the authenticated user by
        default (name, email, language, and username). If you don't need
        all those attributes for your app, you can request fewer with
        the ax_attrs keyword argument.
        """
----------
Output: 


        :param callback_uri: The URI to redirect to after authentication.
        :param ax_attrs: The attributes to request from the service.
        :returns: The URL to redirect to.
        """

        # If we don't have a callback URI, we can't authenticate.
        if callback_uri is None:
            raise ValueError("No callback URI specified.")

        # If we don't have a client ID, we can't authenticate.
        if self.client_id is None:
            raise ValueError("No
----------
Solution: 

        callback_uri = callback_uri or self.request.uri
        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)
        self.redirect(self._OPENID_ENDPOINT + "?" + urllib.urlencode(args))
--------------------
Prompt: 
def write_byte_data(self, addr, cmd, val):
        """Write a byte of data to the specified cmd register of the device.
        """
----------
Output: 

        self._write_cmd(addr, cmd, val)

    def write_word_data(self, addr, cmd, val):
        """Write a word of data to the specified cmd register of the device.
        """
        self._write_cmd(addr, cmd, val)

    def write_block_data(self, addr, cmd, val):
        """Write a block of data to the specified cmd register of the device.
        """
        self._write_cmd(addr, cmd, val)

    def read_byte_data(
----------
Solution: 

        assert self._device is not None, 'Bus must be opened before operations are made against it!'
        # Construct a string of data to send with the command register and byte value.
        data = bytearray(2)
        data[0] = cmd & 0xFF
        data[1] = val & 0xFF
        # Send the data to the device.
        self._select_device(addr)
        self._device.write(data)
--------------------
Prompt: 
def remove_client(self, client):
        # type: (object) -> None
        """Remove the client from the users of the socket.

        If there are no more clients for the socket, it
        will close automatically.
        """
----------
Output: 

        self.users.remove(client)
        if not self.users:
            self.close()

    def close(self):
        # type: () -> None
        """Close the socket."""
        self.socket.close()

    def send(self, message):
        # type: (str) -> None
        """Send a message to the socket."""
        self.socket.send(message)

    def receive(self):
        # type: () -> str
        """Receive a message from the socket."""
        return self
----------
Solution: 


        try:
            self._clients.remove(id(client))
        except ValueError:
            pass

        if len(self._clients) < 1:
            self.close()
--------------------
Prompt: 
def send(self, data):
        """
        Send encoded instructions to Guacamole guacd server.
        """
----------
Output: 


        # Write data to socket
        self.socket.sendall(data)

        # Read response
        response = self.socket.recv(4096)

        # Parse response
        return self.parse_response(response)

    def parse_response(self, response):
        """
        Parse a response from the Guacamole guacd server.
        """

        # Parse response
        opcode = ord(response[0])
        length = ord(response[1])
        data = response[2:2
----------
Solution: 

        self.logger.debug('Sending data: %s' % data)
        self.client.sendall(data.encode())
--------------------
Prompt: 
def strsplit(self, pattern):
        """
        Split the strings in the target column on the given regular expression pattern.

        :param str pattern: The split pattern.
        :returns: H2OFrame containing columns of the split strings.
        """
----------
Output: 

        return H2OFrame._expr(expr=ExprNode("strsplit", self, pattern))

    def str_detect(self, pattern):
        """
        Detect the given regular expression pattern in the target column.

        :param str pattern: The pattern to detect.
        :returns: H2OFrame containing columns of the detected patterns.
        """
        return H2OFrame._expr(expr=ExprNode("str_detect", self, pattern))

    def str_replace(self, pattern, replacement):
        """
        Replace
----------
Solution: 

        fr = H2OFrame._expr(expr=ExprNode("strsplit", self, pattern))
        fr._ex._cache.nrows = self.nrow
        return fr
--------------------
Prompt: 
def report(self, output_file=sys.stdout):
        """Report gathered analysis data in human readable form."""
----------
Output: 

        if self.data is None:
            raise ValueError("No data to report.")

        if self.data.get("errors"):
            print("Errors:", file=output_file)
            for error in self.data["errors"]:
                print(error, file=output_file)
            print(file=output_file)

        print("Analysis of %s" % self.data["filename"], file=output_file)
        print("--------------------------------------------------------------------------------", file=output_file)
        print("", file=output
----------
Solution: 

        if self.verbose > 1:
            with pprint_nosort():
                pprint.pprint(self.results)

        if self.verbose > 0:
            print('Runtime (per repetition): {:.2g} s'.format(
                self.results['Runtime (per repetition) [s]']),
                file=output_file)
        if self.verbose > 0:
            print('Iterations per repetition: {!s}'.format(
                self.results['Iterations per repetition']),
                file=output_file)
        print('Runtime (per cacheline update): {:.2f} cy/CL'.format(
            self.results['Runtime (per cacheline update) [cy/CL]']),
            file=output_file)
        print('MEM volume (per repetition): {:.0f} Byte'.format(
            self.results['MEM volume (per repetition) [B]']),
            file=output_file)
        print('Performance: {:.2f} MFLOP/s'.format(self.results['Performance [MFLOP/s]']),
              file=output_file)
        print('Performance: {:.2f} MLUP/s'.format(self.results['Performance [MLUP/s]']),
              file=output_file)
        print('Performance: {:.2f} It/s'.format(self.results['Performance [MIt/s]']),
              file=output_file)
        if self.verbose > 0:
            print('MEM bandwidth: {:.2f} MByte/s'.format(self.results['MEM BW [MByte/s]']),
                  file=output_file)
        print('', file=output_file)

        if not self.no_phenoecm:
            print("Data Transfers:")
            print("{:^8} |".format("cache"), end='')
            for metrics in self.results['data transfers'].values():
                for metric_name in sorted(metrics):
                    print(" {:^14}".format(metric_name), end='')
                print()
                break
            for cache, metrics in sorted(self.results['data transfers'].items()):
                print("{!s:^8} |".format(cache), end='')
                for k, v in sorted(metrics.items()):
                    print(" {!s:^14}".format(v), end='')
                print()
            print()

            print('Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | '
                  '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL'.format(
                **{k: float(v) for k, v in self.results['ECM'].items()}),
                file=output_file)
            print('T_OL assumes that two loads per cycle may be retiered, which is true for '
                  '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, '
                  'BDW, SKL and SKX, but it also depends on AGU availability.',
                  file=output_file)
--------------------
Prompt: 
def wait_for_kernel(self, timeout=None):
        """method to wait for a kernel to be ready"""
----------
Output: 

        if self.kernel is None:
            raise RuntimeError("No kernel")
        if self.kernel.is_alive():
            return
        if timeout is None:
            timeout = self.timeout
        if timeout is None:
            timeout = 10
        start = time.time()
        while time.time() - start < timeout:
            if self.kernel.is_alive():
                return
            time.sleep(0.1)
        raise RuntimeError("Kernel not alive")

    def wait_for_idle(
----------
Solution: 

        tic = time.time()
        self.km.hb_channel.unpause()
        while True:
            self.run_cell('1', False)
            if self.km.hb_channel.is_beating():
                # heart failure was not the reason this returned
                break
            else:
                # heart failed
                if timeout is not None and (time.time() - tic) > timeout:
                    return False
        return True
--------------------
Prompt: 
def load(self, pkcs11dll_filename=None, *init_string):
        """
        load a PKCS#11 library

        :type pkcs11dll_filename: string
        :param pkcs11dll_filename: the library name.
          If this parameter is not set then the environment variable
          `PYKCS11LIB` is used instead
        :returns: a :class:`PyKCS11Lib` object
        :raises: :class:`PyKCS11Error` (-1): when the load fails
        """
----------
Output: 
)
        """
        if pkcs11dll_filename is None:
            pkcs11dll_filename = os.environ.get('PYKCS11LIB')
        if pkcs11dll_filename is None:
            raise PyKCS11Error('PKCS#11 library filename not specified')
        if not os.path.exists(pkcs11dll_filename):
            raise PyKCS11Error('PKCS#11 library not found')
        self._lib = CDLL(pkcs11dll
----------
Solution: 

        if pkcs11dll_filename is None:
            pkcs11dll_filename = os.getenv("PYKCS11LIB")
            if pkcs11dll_filename is None:
                raise PyKCS11Error(-1, "No PKCS11 library specified (set PYKCS11LIB env variable)")
        rv = self.lib.Load(pkcs11dll_filename)
        if rv == 0:
            raise PyKCS11Error(-1, pkcs11dll_filename)
--------------------
Prompt: 
def load_publickey(type, buffer):
    """
    Load a public key from a buffer.

    :param type: The file type (one of :data:`FILETYPE_PEM`,
        :data:`FILETYPE_ASN1`).
    :param buffer: The buffer the key is stored in.
    :type buffer: A Python string object, either unicode or bytestring.
    :return: The PKey object.
    :rtype: :class:`PKey`
    """
----------
Output: 

    if type == FILETYPE_PEM:
        return PEM_read_bio_PUBKEY(buffer)
    elif type == FILETYPE_ASN1:
        return ASN1_read_bio_PUBKEY(buffer)
    else:
        raise ValueError("Unknown key type")


def load_privatekey(type, buffer):
    """
    Load a private key from a buffer.

    :param type: The file type (one of :data:`FILETYPE_PEM`,
        :data:`FILETYPE_ASN
----------
Solution: 

    if isinstance(buffer, _text_type):
        buffer = buffer.encode("ascii")

    bio = _new_mem_buf(buffer)

    if type == FILETYPE_PEM:
        evp_pkey = _lib.PEM_read_bio_PUBKEY(
            bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)
    elif type == FILETYPE_ASN1:
        evp_pkey = _lib.d2i_PUBKEY_bio(bio, _ffi.NULL)
    else:
        raise ValueError("type argument must be FILETYPE_PEM or FILETYPE_ASN1")

    if evp_pkey == _ffi.NULL:
        _raise_current_error()

    pkey = PKey.__new__(PKey)
    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)
    pkey._only_public = True
    return pkey
--------------------
Prompt: 
def words(self):
        """
        Возвращает итератор по словам, содержащимся в боре
        """
----------
Output: 

        return self.words_dict.keys()

    def __iter__(self):
        """
        Возвращает итератор по словам, содержащимся в боре
        """
        return self.words_dict.keys()

    def __getitem__(self, word):
        """
        Возвращает список слов, содержащихся в боре
        """
        return self.words_dict[word]

    def __len__(self):
        """
        Возвращает количество
----------
Solution: 

        branch, word, indexes = [self.root], [], [0]
        letters_with_children = [self._get_children_and_letters(self.root)]
        while len(branch) > 0:
            if self.is_final(branch[-1]):
                yield "".join(word)
            while indexes[-1] == len(letters_with_children[-1]):
                indexes.pop()
                letters_with_children.pop()
                branch.pop()
                if len(indexes) == 0:
                    raise StopIteration()
                word.pop()
            next_letter, next_child = letters_with_children[-1][indexes[-1]]
            indexes[-1] += 1
            indexes.append(0)
            word.append(next_letter)
            branch.append(next_child)
            letters_with_children.append(self._get_children_and_letters(branch[-1]))
--------------------
Prompt: 
def parse_forensic_report(feedback_report, sample, msg_date,
                          nameservers=None, dns_timeout=2.0,
                          strip_attachment_payloads=False,
                          parallel=False):
    """
    Converts a DMARC forensic report and sample to a ``OrderedDict``

    Args:
        feedback_report (str): A message's feedback report as a string
        sample (str): The RFC 822 headers or RFC 822 message sample
        msg_date (str): The message's date header
        nameservers (list): A list of one or more nameservers to use
        (Cloudflare's public DNS resolvers by default)
        dns_timeout (float): Sets the DNS timeout in seconds
        strip_attachment_payloads (bool): Remove attachment payloads from
        forensic report results
        parallel (bool): Parallel processing

    Returns:
        OrderedDict: A parsed report and sample
    """
----------
Output: 
.1 headers and body
        msg_date (datetime.datetime): The date the message was sent
        nameservers (list): A list of nameservers to use when resolving
            domain names
        dns_timeout (float): The timeout to use when resolving domain names
        strip_attachment_payloads (bool): Whether or not to strip the
            payloads of attachments
        parallel (bool): Whether or not to use parallel processing

    Returns:
        OrderedDict: A ``OrderedDict`` of the parsed report
    """
    if not
----------
Solution: 

    delivery_results = ["delivered", "spam", "policy", "reject", "other"]

    try:
        parsed_report = OrderedDict()
        report_values = feedback_report_regex.findall(feedback_report)
        for report_value in report_values:
            key = report_value[0].lower().replace("-", "_")
            parsed_report[key] = report_value[1]

        if "arrival_date" not in parsed_report:
            if msg_date is None:
                raise InvalidForensicReport(
                    "Forensic sample is not a valid email")
            parsed_report["arrival_date"] = msg_date.isoformat()

        if "version" not in parsed_report:
            parsed_report["version"] = 1

        if "user_agent" not in parsed_report:
            parsed_report["user_agent"] = None

        if "delivery_result" not in parsed_report:
            parsed_report["delivery_result"] = None
        else:
            for delivery_result in delivery_results:
                if delivery_result in parsed_report["delivery_result"].lower():
                    parsed_report["delivery_result"] = delivery_result
                    break
        if parsed_report["delivery_result"] not in delivery_results:
            parsed_report["delivery_result"] = "other"

        arrival_utc = human_timestamp_to_datetime(
            parsed_report["arrival_date"], to_utc=True)
        arrival_utc = arrival_utc.strftime("%Y-%m-%d %H:%M:%S")
        parsed_report["arrival_date_utc"] = arrival_utc

        ip_address = parsed_report["source_ip"]
        parsed_report_source = get_ip_address_info(ip_address,
                                                   nameservers=nameservers,
                                                   timeout=dns_timeout,
                                                   parallel=parallel)
        parsed_report["source"] = parsed_report_source
        del parsed_report["source_ip"]

        if "identity_alignment" not in parsed_report:
            parsed_report["authentication_mechanisms"] = []
        elif parsed_report["identity_alignment"] == "none":
            parsed_report["authentication_mechanisms"] = []
            del parsed_report["identity_alignment"]
        else:
            auth_mechanisms = parsed_report["identity_alignment"]
            auth_mechanisms = auth_mechanisms.split(",")
            parsed_report["authentication_mechanisms"] = auth_mechanisms
            del parsed_report["identity_alignment"]

        if "auth_failure" not in parsed_report:
            parsed_report["auth_failure"] = "dmarc"
        auth_failure = parsed_report["auth_failure"].split(",")
        parsed_report["auth_failure"] = auth_failure

        optional_fields = ["original_envelope_id", "dkim_domain",
                           "original_mail_from", "original_rcpt_to"]
        for optional_field in optional_fields:
            if optional_field not in parsed_report:
                parsed_report[optional_field] = None

        parsed_sample = parse_email(
            sample,
            strip_attachment_payloads=strip_attachment_payloads)

        if "reported_domain" not in parsed_report:
            parsed_report["reported_domain"] = parsed_sample["from"]["domain"]

        sample_headers_only = False
        number_of_attachments = len(parsed_sample["attachments"])
        if number_of_attachments < 1 and parsed_sample["body"] is None:
            sample_headers_only = True
        if sample_headers_only and parsed_sample["has_defects"]:
            del parsed_sample["defects"]
            del parsed_sample["defects_categories"]
            del parsed_sample["has_defects"]
        parsed_report["sample_headers_only"] = sample_headers_only
        parsed_report["sample"] = sample
        parsed_report["parsed_sample"] = parsed_sample

        return parsed_report

    except KeyError as error:
        raise InvalidForensicReport("Missing value: {0}".format(
            error.__str__()))

    except Exception as error:
        raise InvalidForensicReport(
            "Unexpected error: {0}".format(error.__str__()))
--------------------
Prompt: 
def map_peaks_to_image(peaks, r=4, vox_dims=(2, 2, 2), dims=(91, 109, 91),
                       header=None):
    """ Take a set of discrete foci (i.e., 2-D array of xyz coordinates)
    and generate a corresponding image, convolving each focus with a
    hard sphere of radius r."""
----------
Output: 

    # Create a blank image
    if header is None:
        image = np.zeros(dims, dtype=np.float32)
    else:
        image = nib.Nifti1Image(np.zeros(dims, dtype=np.float32),
                                affine=header.get_best_affine())

    # Iterate over each focus
    for x, y, z in peaks:
        # Convolve each focus with a hard sphere
        image.get_data()[x - r:
----------
Solution: 

    data = np.zeros(dims)
    for p in peaks:
        valid = get_sphere(p, r, vox_dims, dims)
        valid = valid[:, ::-1]
        data[tuple(valid.T)] = 1
    return nifti1.Nifti1Image(data, None, header=header)
--------------------
Prompt: 
def iter_parse(fiql_str):
    """Iterate through the FIQL string. Yield a tuple containing the
    following FIQL components for each iteration:

      - preamble: Any operator or opening/closing paranthesis preceding a
        constraint or at the very end of the FIQL string.
      - selector: The selector portion of a FIQL constraint or ``None`` if
        yielding the last portion of the string.
      - comparison: The comparison portion of a FIQL constraint or ``None``
        if yielding the last portion of the string.
      - argument: The argument portion of a FIQL constraint or ``None`` if
        yielding the last portion of the string.

    For usage see :func:`parse_str_to_expression`.

    Args:
        fiql_str (string): The FIQL formatted string we want to parse.

    Yields:
        tuple: Preamble, selector, comparison, argument.
    """
----------
Output: 
string.
      - value: The value portion of a FIQL constraint or ``None`` if
        yielding the last portion of the string.
      - postamble: Any operator or opening/closing paranthesis following a
        constraint or at the very end of the FIQL string.

    :param fiql_str: The FIQL string to parse.
    :type fiql_str: str
    """
    # The FIQL string is parsed in three stages:
    #
    # 1. The FIQL string is split into a list of tokens.
----------
Solution: 

    while len(fiql_str):
        constraint_match = CONSTRAINT_COMP.split(fiql_str, 1)
        if len(constraint_match) < 2:
            yield (constraint_match[0], None, None, None)
            break
        yield (
            constraint_match[0],
            unquote_plus(constraint_match[1]),
            constraint_match[4],
            unquote_plus(constraint_match[6]) \
                    if constraint_match[6] else None
        )
        fiql_str = constraint_match[8]
--------------------
Prompt: 
def shape(self):
        """Returns the shape of the data."""
----------
Output: 

        return self.data.shape

    def __len__(self):
        """Returns the length of the data."""
        return len(self.data)

    def __getitem__(self, idx):
        """Returns the data at index idx."""
        return self.data[idx]

    def __setitem__(self, idx, value):
        """Sets the data at index idx to value."""
        self.data[idx] = value

    def __delitem__(self, idx):
        """Deletes the
----------
Solution: 

        # TODO cache
        first = self.first().shape
        shape = self._rdd.map(lambda x: x.shape[0]).sum()
        return (shape,) + first[1:]
--------------------
Prompt: 
def _wait_job_completion(self):
        """Wait for the cache to be empty before resizing the pool."""
----------
Output: 

        while self.cache_size > 0:
            time.sleep(1)

    def _resize_pool(self, new_size):
        """Resize the pool to the new size."""
        self.pool.resize(new_size)
        self.pool.wait_workers()

    def _resize_cache(self, new_size):
        """Resize the cache to the new size."""
        self.cache.resize(new_size)

    def _resize_pool_and_cache(self, new_
----------
Solution: 

        # Issue a warning to the user about the bad effect of this usage.
        if len(self._pending_work_items) > 0:
            warnings.warn("Trying to resize an executor with running jobs: "
                          "waiting for jobs completion before resizing.",
                          UserWarning)
            mp.util.debug("Executor {} waiting for jobs completion before"
                          " resizing".format(self.executor_id))
        # Wait for the completion of the jobs
        while len(self._pending_work_items) > 0:
            time.sleep(1e-3)
--------------------
Prompt: 
def energy(self, state=None):
        """Calculates the length of the route."""
----------
Output: 

        if state is None:
            state = self.state
        return np.sum(state.position ** 2)

    def energy_gradient(self, state=None):
        """Calculates the gradient of the energy."""
        if state is None:
            state = self.state
        return 2 * state.position

    def energy_hessian(self, state=None):
        """Calculates the hessian of the energy."""
        if state is None:
            state = self.state
        return np.diag
----------
Solution: 

        state = self.state if state is None else state
        route = state
        e = 0
        if self.distance_matrix:
            for i in range(len(route)):
                e += self.distance_matrix["{},{}".format(route[i-1], route[i])]
        else:
            for i in range(len(route)):
                e += distance(self.cities[route[i-1]], self.cities[route[i]])
        return e
--------------------
Prompt: 
def recommendations(self, **kwargs):
        """
        Get a list of recommended movies for a movie.

        Args:
            language: (optional) ISO 639-1 code.
            page: (optional) Minimum value of 1.  Expected value is an integer.

        Returns:
            A dict representation of the JSON returned from the API.
        """
----------
Output: 

        return self._get(
            'movie/{0}/recommendations'.format(self.id),
            language=kwargs.get('language'),
            page=kwargs.get('page')
        )

    def reviews(self, **kwargs):
        """
        Get the reviews for a movie.

        Args:
            language: (optional) ISO 639-1 code.
            page: (optional) Minimum value of 1.  Expected value is an integer.

        Returns:
            A dict representation of the JSON
----------
Solution: 

        path = self._get_id_path('recommendations')

        response = self._GET(path, kwargs)
        self._set_attrs_to_values(response)
        return response
--------------------
Prompt: 
def get_clinvar_id(self, submission_id):
        """Returns the official Clinvar submission ID for a submission object

            Args:
                submission_id(str): submission_id(str) : id of the submission

            Returns:
                clinvar_subm_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission

        """
----------
Output: 

        clinvar_subm_id = None
        if submission_id:
            try:
                clinvar_subm_id = self.get_clinvar_submission_id(submission_id)
            except Exception as e:
                logger.error("Error getting clinvar submission id for submission_id: %s" % submission_id)
                logger.error(e)
        return clinvar_subm_id

    def get_clinvar_submission_id(self, submission_id):
        """Returns the official
----------
Solution: 

        submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})
        clinvar_subm_id = submission_obj.get('clinvar_subm_id') # This key does not exist if it was not previously provided by user
        return clinvar_subm_id
--------------------
Prompt: 
def dict_factory(self, cursor, row):
        """ Sqlite callback accepting the cursor and the original row as a tuple.

        Simple return of JSON safe types.

        Args:
            cursor (sqlite cursor):  Original cursory
            row (sqlite row tuple): Original row.

        Returns:
            dict: modified row.
        """
----------
Output: 

        d = {}
        for idx, col in enumerate(cursor.description):
            d[col[0]] = self._json_safe(row[idx])
        return d

    def _json_safe(self, value):
        """ Convert value to a JSON safe type.

        Args:
            value (any): value to convert.

        Returns:
            any: JSON safe value.
        """
        if isinstance(value, datetime.datetime):
            return value.isoformat()
        elif isinstance(value, datetime.date
----------
Solution: 

        d = {}
        for idx, col in enumerate(cursor.description):
            val = row[idx]
            name = col[0]
            if name == Field.Time_Stamp:
                d[col[0]] = str(val)
                continue
            if name == "Raw_A" or name == "Raw_B":  # or name == Field.Meter_Time:
                continue
            if name not in self.m_all_fields:
                continue
            if (str(val) != "None") and ((val > 0) or (val < 0)):
                d[name] = str(val)
        return d
--------------------
Prompt: 
def streams(self, url, **params):
        """Attempts to find a plugin and extract streams from the *url*.

        *params* are passed to :func:`Plugin.streams`.

        Raises :exc:`NoPluginError` if no plugin is found.
        """
----------
Output: 

        plugin = self.resolve(url)
        return plugin.streams(url, **params)

    def resolve(self, url):
        """Attempts to find a plugin that can handle the *url*.

        Raises :exc:`NoPluginError` if no plugin is found.
        """
        for plugin in self.plugins:
            if plugin.can_handle_url(url):
                return plugin
        raise NoPluginError('No plugin can handle %s' % url)

    def __iter__(self):
        return
----------
Solution: 


        plugin = self.resolve_url(url)
        return plugin.streams(**params)
--------------------
Prompt: 
def map(self, func):
        """
        Process all data with given function.
        The scheme of function should be x,y -> x,y.
        """
----------
Output: 

        for i in range(self.size):
            self.data[i] = func(self.data[i])

    def __getitem__(self, key):
        return self.data[key]

    def __setitem__(self, key, value):
        self.data[key] = value

    def __len__(self):
        return self.size

    def __iter__(self):
        for i in range(self.size):
            yield self.data[i]

    def __repr__(self):

----------
Solution: 

        if self._train_set:
            self._train_set = map(func, self._train_set)
        if self._valid_set:
            self._valid_set = map(func, self._valid_set)
        if self._test_set:
            self._test_set = map(func, self._test_set)
--------------------
Prompt: 
def positions_func(inputs, pad=0):
    """
    A layer filling i-th column of a 2D tensor with
    1+ln(1+i) when it contains a meaningful symbol
    and with 0 when it contains PAD
    """
----------
Output: 

    return K.expand_dims(K.expand_dims(K.expand_dims(K.arange(inputs.shape[1]) + 1 + K.log(1 + K.expand_dims(K.expand_dims(K.expand_dims(K.arange(inputs.shape[2]), 0), 0), 0)), 0), 0) * K.cast(K.equal(inputs, pad), K.floatx()), 0)


def positions_embedding(inputs,
----------
Solution: 

    position_inputs = kb.cumsum(kb.ones_like(inputs, dtype="float32"), axis=1)
    position_inputs *= kb.cast(kb.not_equal(inputs, pad), "float32")
    return kb.log(1.0 + position_inputs)
--------------------
Prompt: 
def _with_loc(f: W) -> W:
    """Wrap a reader function in a decorator to supply line and column
    information along with relevant forms."""
----------
Output: 

    def wrapper(self: 'Reader', *args: Any, **kwargs: Any) -> Any:
        try:
            return f(self, *args, **kwargs)
        except Exception as e:
            raise type(e)(str(e) + ' at line ' + str(self.line) +
                          ', column ' + str(self.column))
    return wrapper


class Reader:
    """A reader for the Lisp language."""

    def __init__(self, source: str) -> None:
        """Initialize
----------
Solution: 


    @functools.wraps(f)
    def with_lineno_and_col(ctx):
        meta = lmap.map(
            {READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col}
        )
        v = f(ctx)
        try:
            return v.with_meta(meta)  # type: ignore
        except AttributeError:
            return v

    return cast(W, with_lineno_and_col)
--------------------
Prompt: 
def p_creg_decl(self, program):
        """
           creg_decl : CREG indexed_id
        """
----------
Output: 

        self.add_creg(program[2])

    def p_gate_decl(self, program):
        """
           gate_decl : GATE id '(' id_list ')' gate_body
        """
        self.add_gate(program[2], program[4], program[6])

    def p_gate_body(self, program):
        """
           gate_body : '{' gate_body_item_list '}'
        """
        return program[2]

    def p_gate_body_item_
----------
Solution: 

        program[0] = node.Creg([program[2]])
        if program[2].name in self.external_functions:
            raise QasmError("CREG names cannot be reserved words. "
                            + "Received '" + program[2].name + "'")
        if program[2].index == 0:
            raise QasmError("CREG size must be positive")
        self.update_symtab(program[0])
--------------------
Prompt: 
def order_verification(self, institute, case, user, link, variant):
        """Create an event for a variant verification for a variant
        and an event for a variant verification for a case

        Arguments:
            institute (dict): A Institute object
            case (dict): Case object
            user (dict): A User object
            link (str): The url to be used in the event
            variant (dict): A variant object

        Returns:
            updated_variant(dict)
        """
----------
Output: 

        # Create an event for the variant
        event = self.event_objs.create_event(
            user_id=user['_id'],
            institute_id=institute['_id'],
            case_id=case['_id'],
            link=link,
            link_type='verification',
            link_text='Verification',
            category='verification',
            verb='verified',
            variant_id=variant['_id'],
            variant=variant,
            date=datetime.datetime.utcnow(),
           
----------
Solution: 

        LOG.info("Creating event for ordering validation for variant" \
                    " {0}".format(variant['display_name']))

        updated_variant = self.variant_collection.find_one_and_update(
            {'_id': variant['_id']},
            {'$set': {'sanger_ordered': True}},
            return_document=pymongo.ReturnDocument.AFTER
        )

        self.create_event(
            institute=institute,
            case=case,
            user=user,
            link=link,
            category='variant',
            verb='sanger',
            variant=variant,
            subject=variant['display_name'],
        )

        LOG.info("Creating event for ordering sanger for case" \
                    " {0}".format(case['display_name']))

        self.create_event(
            institute=institute,
            case=case,
            user=user,
            link=link,
            category='case',
            verb='sanger',
            variant=variant,
            subject=variant['display_name'],
        )
        return updated_variant
--------------------
Prompt: 
def update_product_set(
        self,
        product_set,
        location=None,
        product_set_id=None,
        update_mask=None,
        project_id=None,
        retry=None,
        timeout=None,
        metadata=None,
    ):
        """
        For the documentation see:
        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator`
        """
----------
Output: 

        if not product_set_id:
            product_set_id = product_set.name.split("/")[-1]
        if not location:
            location = product_set.name.split("/")[-3]
        if not project_id:
            project_id = product_set.name.split("/")[-5]
        return self.update_product_set_template(
            product_set=product_set,
            location=location,
            product_set_id=product_set_id,
            update_mask=update
----------
Solution: 

        client = self.get_conn()
        product_set = self.product_set_name_determiner.get_entity_with_name(
            product_set, product_set_id, location, project_id
        )
        self.log.info('Updating ProductSet: %s', product_set.name)
        response = client.update_product_set(
            product_set=product_set, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata
        )
        self.log.info('ProductSet updated: %s', response.name if response else '')
        self.log.debug('ProductSet updated:\n%s', response)
        return MessageToDict(response)
--------------------
Prompt: 
def get_mic(self, message, qop_req=C.GSS_C_QOP_DEFAULT):
        """
        Calculates a cryptographic message integrity code (MIC) over an application message, and
        returns that MIC in a token. This is in contrast to :meth:`wrap` which calculates a MIC
        over a message, optionally encrypts it and returns the original message and the MIC packed
        into a single token. The peer application can then verify the MIC to ensure the associated
        message has not been changed in transit.

        :param message: The message to calculate a MIC for
        :type message: bytes
        :param qop_req: The quality of protection required. It is recommended to not change this
            from the default as most GSSAPI implementations do not support it.
        :returns: A MIC for the message calculated using this security context's cryptographic keys
        :rtype: bytes
        """
----------
Output: 
it.

        :param message: The message to calculate the MIC over.
        :type message: bytes
        :param qop_req: The quality of protection to use when calculating the MIC.
        :type qop_req: int
        :return: The MIC token.
        :rtype: bytes
        """
        mic_token = self._mic(message, qop_req)
        return mic_token

    def verify_mic(self, message, mic_token, qop_req=C.GSS_C_Q
----------
Solution: 

        if not (self.flags & C.GSS_C_INTEG_FLAG):
            raise GSSException("No integrity protection negotiated.")
        if not (self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)):
            raise GSSException("Protection not yet ready.")

        minor_status = ffi.new('OM_uint32[1]')
        output_token_buffer = ffi.new('gss_buffer_desc[1]')
        message_buffer = ffi.new('gss_buffer_desc[1]')
        message_buffer[0].length = len(message)
        c_str_message = ffi.new('char[]', message)
        message_buffer[0].value = c_str_message
        retval = C.gss_get_mic(
            minor_status,
            self._ctx[0],
            ffi.cast('gss_qop_t', qop_req),
            message_buffer,
            output_token_buffer
        )
        try:
            if GSS_ERROR(retval):
                if minor_status[0] and self.mech_type:
                    raise _exception_for_status(retval, minor_status[0], self.mech_type)
                else:
                    raise _exception_for_status(retval, minor_status[0])

            output_token = _buf_to_str(output_token_buffer[0])
            return output_token
        finally:
            if output_token_buffer[0].length != 0:
                C.gss_release_buffer(minor_status, output_token_buffer)
--------------------
Prompt: 
def extractHolidayDate(self, setting_holiday):
        """ Read a single holiday date from meter buffer.

        Args:
            setting_holiday (int):  Holiday from 0-19 or in range(Extents.Holidays)

        Returns:
            tuple: Holiday tuple, elements are strings.

            =============== ======================
            Holiday         Holiday 0-19 as string
            Day             Day 1-31 as string
            Month           Monty 1-12 as string
            =============== ======================

        """
----------
Output: 
string
            =============== ======================

        """
        holiday = self.read_int(setting_holiday)
        day = self.read_int(holiday + 1)
        month = self.read_int(holiday + 2)
        return (self.holiday_names[holiday], str(day), self.month_names[month])

    def extractHoliday(self, setting_holiday):
        """ Read a single holiday from meter buffer.

        Args:
            setting_holiday
----------
Solution: 

        ret = namedtuple("result", ["Holiday", "Month", "Day"])
        setting_holiday += 1
        ret.Holiday = str(setting_holiday)

        if (setting_holiday < 1) or (setting_holiday > Extents.Holidays):
            ekm_log("Out of bounds:  holiday " + str(setting_holiday))
            ret.Holiday = ret.Month = ret.Day = str(0)
            return ret

        idxday = "Holiday_" + str(setting_holiday) + "_Day"
        idxmon = "Holiday_" + str(setting_holiday) + "_Mon"
        if idxmon not in self.m_hldy:
            ret.Holiday = ret.Month = ret.Day = str(0)
            return ret
        if idxday not in self.m_hldy:
            ret.Holiday = ret.Month = ret.Day = str(0)
            return ret
        ret.Day = self.m_hldy[idxday][MeterData.StringValue]
        ret.Month = self.m_hldy[idxmon][MeterData.StringValue]
        return ret
--------------------
Prompt: 
def getMaxStmIdForStm(stm):
    """
    Get maximum _instId from all assigments in statement
    """
----------
Output: 

    maxStmId = 0
    for ass in stm.getAssignments():
        if ass.getId() > maxStmId:
            maxStmId = ass.getId()
    return maxStmId

def getMaxStmIdForStms(stms):
    """
    Get maximum _instId from all statements
    """
    maxStmId = 0
    for stm in stms:
        if getMaxStmIdForStm(stm) > maxStmId:
            max
----------
Solution: 

    maxId = 0
    if isinstance(stm, Assignment):
        return stm._instId
    elif isinstance(stm, WaitStm):
        return maxId
    else:
        for _stm in stm._iter_stms():
            maxId = max(maxId, getMaxStmIdForStm(_stm))
        return maxId
--------------------
Prompt: 
def delete(self, endpoint, headers):
        """
        Method to delete an item or all items

        headers['If-Match'] must contain the _etag identifier of the element to delete

        :param endpoint: endpoint (API URL)
        :type endpoint: str
        :param headers: headers (example: Content-Type)
        :type headers: dict
        :return: response (deletion information)
        :rtype: dict
        """
----------
Output: 

        response = requests.delete(endpoint, headers=headers)
        return response.json()

    def get_all(self, endpoint, headers):
        """
        Method to get all items

        :param endpoint: endpoint (API URL)
        :type endpoint: str
        :param headers: headers (example: Content-Type)
        :type headers: dict
        :return: response (all items)
        :rtype: dict
        """
        response = requests.get(endpoint, headers=headers)
        return response.json()


----------
Solution: 

        response = self.get_response(method='DELETE', endpoint=endpoint, headers=headers)

        logger.debug("delete, response: %s", response)
        if response.status_code != 204:  # pragma: no cover - should not happen ...
            resp = self.decode(response=response)

        resp = {"_status": "OK"}
        return resp
--------------------
Prompt: 
def _make_safe_pod_id(safe_dag_id, safe_task_id, safe_uuid):
        """
        Kubernetes pod names must be <= 253 chars and must pass the following regex for
        validation
        "^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$"

        :param safe_dag_id: a dag_id with only alphanumeric characters
        :param safe_task_id: a task_id with only alphanumeric characters
        :param random_uuid: a uuid
        :return: ``str`` valid Pod name of appropriate length
        """
----------
Output: 
alphanumeric characters and underscores
        :param safe_task_id: a task_id with only alphanumeric characters and underscores
        :param safe_uuid: a uuid with only alphanumeric characters and underscores
        :return: a string that is a safe pod_id
        """
        return '{dag_id}_{task_id}_{uuid}'.format(
            dag_id=safe_dag_id,
            task_id=safe_task_id,
            uuid=safe_uuid
        )

    def _make
----------
Solution: 

        MAX_POD_ID_LEN = 253

        safe_key = safe_dag_id + safe_task_id

        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + "-" + safe_uuid

        return safe_pod_id
--------------------
Prompt: 
def run(self, func):
        """Run 'func' under os sandboxing"""
----------
Output: 

        if not self.sandbox:
            return func()
        with self.sandbox:
            return func()

    def __enter__(self):
        """Context manager entry point"""
        if self.sandbox:
            self.sandbox.__enter__()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit point"""
        if self.sandbox:
            self.sandbox.__exit__(exc_type, exc_val, exc_tb)

----------
Solution: 

        try:
            self._copy(self)
            if _file:
                __builtin__.file = self._file
            __builtin__.open = self._open
            self._active = True
            return func()
        finally:
            self._active = False
            if _file:
                __builtin__.file = _file
            __builtin__.open = _open
            self._copy(_os)
--------------------
Prompt: 
def _isint(string):
    """
    >>> _isint("123")
    True
    >>> _isint("123.45")
    False
    """
----------
Output: 

    try:
        int(string)
        return True
    except ValueError:
        return False

def _isfloat(string):
    """
    >>> _isfloat("123")
    False
    >>> _isfloat("123.45")
    True
    """
    try:
        float(string)
        return True
    except ValueError:
        return False

def _isbool(string):
    """
    >>> _isbool("True")
    True
    >>> _isbool("False")

----------
Solution: 

    return type(string) is int or \
        (isinstance(string, _binary_type) or
         isinstance(string, string_types)) and \
        _isconvertible(int, string)
--------------------
Prompt: 
def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):
        """
        Read the data encoding the Get request payload and decode it into its
        constituent parts.

        Args:
            input_stream (stream): A data stream containing encoded object
                data, supporting a read method; usually a BytearrayStream
                object.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be decoded. Optional,
                defaults to KMIP 1.0.
        """
----------
Output: 
               defaults to KMIP 1.0.

        Raises:
            InvalidKmipEncoding: Raised if the data encoding is invalid.
        """
        super(GetRequestPayload, self).read(
            input_stream,
            kmip_version=kmip_version
        )
        local_stream = utils.BytearrayStream(input_stream.read(self.length))

        self.unique_identifier = primitives.TextString(
            enums.TextStringEncoding.UTF8
        )
        self.
----------
Solution: 

        super(GetRequestPayload, self).read(
            input_stream,
            kmip_version=kmip_version
        )
        local_stream = utils.BytearrayStream(input_stream.read(self.length))

        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):
            self._unique_identifier = primitives.TextString(
                tag=enums.Tags.UNIQUE_IDENTIFIER
            )
            self._unique_identifier.read(
                local_stream,
                kmip_version=kmip_version
            )

        if self.is_tag_next(enums.Tags.KEY_FORMAT_TYPE, local_stream):
            self._key_format_type = primitives.Enumeration(
                enum=enums.KeyFormatType,
                tag=enums.Tags.KEY_FORMAT_TYPE
            )
            self._key_format_type.read(
                local_stream,
                kmip_version=kmip_version
            )

        if self.is_tag_next(enums.Tags.KEY_COMPRESSION_TYPE, local_stream):
            self._key_compression_type = primitives.Enumeration(
                enum=enums.KeyCompressionType,
                tag=enums.Tags.KEY_COMPRESSION_TYPE
            )
            self._key_compression_type.read(
                local_stream,
                kmip_version=kmip_version
            )

        if self.is_tag_next(
                enums.Tags.KEY_WRAPPING_SPECIFICATION,
                local_stream
        ):
            self._key_wrapping_specification = \
                objects.KeyWrappingSpecification()
            self._key_wrapping_specification.read(
                local_stream,
                kmip_version=kmip_version
            )

        self.is_oversized(local_stream)
--------------------
Prompt: 
def get_latex_maybe_optional_arg(self, pos):
        """
        Parses the latex content given to the constructor (and stored in `self.s`),
        starting at position `pos`, to attempt to parse an optional argument.

        Attempts to parse an optional argument. If this is successful, we return
        a tuple `(node, pos, len)` if success where `node` is a
        :py:class:`LatexGroupNode`.  Otherwise, this method returns None.
        """
----------
Output: 

        # Check if we're at the start of an optional argument.
        if self.s[pos] != '[':
            return None

        # Parse the optional argument.
        pos += 1
        node, pos, len = self.get_latex_group(pos)
        if node is None:
            return None

        # Check if we're at the end of the optional argument.
        if self.s[pos] != ']':
            return None

        # Return the node.
        return node, pos + 1
----------
Solution: 


        tok = self.get_token(pos, brackets_are_chars=False, environments=False)
        if (tok.tok == 'brace_open' and tok.arg == '['):
            return self.get_latex_braced_group(pos, brace_type='[')

        return None
--------------------
Prompt: 
def _label__get(self):
        """
        Get or set any <label> element associated with this element.
        """
----------
Output: 

        return self._get_or_set(Label, 'label')

    @property
    def _label(self):
        """
        Get or set any <label> element associated with this element.
        """
        return self._get_or_set(Label, 'label')

    @property
    def _label_text(self):
        """
        Get or set the text of the <label> element associated with this element.
        """
        return self._get_or_set(Label, 'label_text')

    @property
----------
Solution: 

        id = self.get('id')
        if not id:
            return None
        result = _label_xpath(self, id=id)
        if not result:
            return None
        else:
            return result[0]
--------------------
Prompt: 
def _split_comma_separated(string):
    """Return a set of strings."""
----------
Output: 

    return set(string.split(','))


def _split_comma_separated_with_spaces(string):
    """Return a set of strings."""
    return set(string.split(', '))


def _split_semicolon_separated(string):
    """Return a set of strings."""
    return set(string.split(';'))


def _split_semicolon_separated_with_spaces(string):
    """Return a set of strings."""
    return set(string.split(';
----------
Solution: 

    return set(text.strip() for text in string.split(',') if text.strip())
--------------------
Prompt: 
def apply_next_patch(self, force=False, quiet=False):
        """ Apply next patch in series file """
----------
Output: 

        if not self.series_file:
            raise Exception("No series file found")
        if not self.patch_file:
            raise Exception("No patch file found")
        if not self.series_file_exists():
            raise Exception("Series file does not exist")
        if not self.patch_file_exists():
            raise Exception("Patch file does not exist")
        if not self.patch_file_is_patch():
            raise Exception("Patch file is not a patch")
        if not self.patch_file_is_applied
----------
Solution: 

        self._check()
        top = self.db.top_patch()
        if not top:
            patch = self.series.first_patch()
        else:
            patch = self.series.patch_after(top)

        if not patch:
            raise AllPatchesApplied(self.series, top)

        self.applying(patch)

        self._apply_patch(patch, force, quiet)

        self.db.save()

        self.applied(self.db.top_patch())
--------------------
Prompt: 
def _data_sanity_checks(self, explore_iterable):
        """Checks if data values are  valid.

        Checks if the data values are supported by the parameter and if the values are of the same
        type as the default value.

        """
----------
Output: 

        if not isinstance(explore_iterable, list):
            raise TypeError("Explore iterable must be a list")

        if not all(isinstance(x, str) for x in explore_iterable):
            raise TypeError("Explore iterable must be a list of strings")

        if not all(x in self.data_values for x in explore_iterable):
            raise ValueError("Explore iterable must contain valid data values")

        if not all(isinstance(x, type(self.data_values[
----------
Solution: 

        data_list = []

        for val in explore_iterable:

            if not self.f_supports(val):
                raise TypeError('%s is of not supported type %s.' % (repr(val), str(type(val))))

            if not self._values_of_same_type(val, self._default):
                raise TypeError(
                    'Data of `%s` is not of the same type as the original entry value, '
                    'new type is %s vs old type %s.' %
                    (self.v_full_name, str(type(val)), str(type(self._default))))

            data_list.append(val)

        if len(data_list) == 0:
            raise ValueError('Cannot explore an empty list!')

        return data_list
--------------------
Prompt: 
def _cursor_position_changed(self):
        """ Updates the tip based on user cursor movement.
        """
----------
Output: 

        if self.is_enabled():
            self.update_tip()

    def _cursor_position_changed_delayed(self, *args):
        """ Updates the tip based on user cursor movement.
        """
        if self.is_enabled():
            self.update_tip_delayed()

    def _cursor_position_changed_delayed_2(self, *args):
        """ Updates the tip based on user cursor movement.
        """
        if self.is_enabled():
            self.update_tip_delayed
----------
Solution: 

        cursor = self._text_edit.textCursor()
        if cursor.position() <= self._start_position:
            self.hide()
        else:
            position, commas = self._find_parenthesis(self._start_position + 1)
            if position != -1:
                self.hide()
--------------------
Prompt: 
def resources(self):
        """
        Returns list of unique `FragmentResource`s by order of first appearance.
        """
----------
Output: 

        return self._resources

    @property
    def fragments(self):
        """
        Returns list of unique `Fragment`s by order of first appearance.
        """
        return self._fragments

    @property
    def fragment_map(self):
        """
        Returns a map of `FragmentResource`s to `Fragment`s.
        """
        return self._fragment_map

    @property
    def fragment_resource_map(self):
        """
        Returns a map of `Fragment`s to `FragmentResource`s
----------
Solution: 

        seen = set()
        # seen.add always returns None, so 'not seen.add(x)' is always True,
        # but will only be called if the value is not already in seen (because
        # 'and' short-circuits)
        return [x for x in self._resources if x not in seen and not seen.add(x)]
--------------------
Prompt: 
def track_production(f):
    """
    decorator for adding positional information to returning nodes
    """
----------
Output: 

    def wrapper(*args, **kwargs):
        return f(*args, **kwargs)
    return wrapper


def track_production_with_pos(f):
    """
    decorator for adding positional information to returning nodes
    """
    def wrapper(*args, **kwargs):
        return f(*args, **kwargs)
    return wrapper


def track_production_with_pos_and_end(f):
    """
    decorator for adding positional information to returning nodes
    """
    def wrapper(*args, **kwargs):
       
----------
Solution: 

    @wraps(f)
    def wrapper(self, p):
        r = f(self, p)
        node = p[0]
        if isinstance(node, Node) and len(p) > 1:
            set_positional_info(node, p)
        return r
    
    return wrapper
--------------------
Prompt: 
def s2p(self):
        """Return 2 proton separation energy"""
----------
Output: 

        return self.s2p

    @property
    def s2n(self):
        """Return 2 neutron separation energy"""
        return self.s2n

    @property
    def s2d(self):
        """Return 2 deuteron separation energy"""
        return self.s2d

    @property
    def s2t(self):
        """Return 2 triton separation energy"""
        return self.s2t

    @property
    def s2he(self):
       
----------
Solution: 

        M_P = 7.28897050         # proton mass excess in MeV
        f = lambda parent, daugther: -parent + daugther + 2 * M_P
        return self.derived('s2p', (-2, 0), f)
--------------------
Prompt: 
def clear_xcom_data(self, session=None):
        """
        Clears all XCom data from the database for the task instance
        """
----------
Output: 

        session = session or settings.Session()
        self.log.info('Clearing xcom data for %s', self)
        session.query(XCom).filter(
            XCom.task_id == self.task_id,
            XCom.dag_id == self.dag_id,
            XCom.run_id == self.run_id,
        ).delete()
        session.commit()

    def get_xcom_dict(self, key=None):
        """
        Returns the value of the XCom associated to the
----------
Solution: 

        session.query(XCom).filter(
            XCom.dag_id == self.dag_id,
            XCom.task_id == self.task_id,
            XCom.execution_date == self.execution_date
        ).delete()
        session.commit()
--------------------
Prompt: 
def as_property_description(self):
        """
        Get the property description.

        Returns a dictionary describing the property.
        """
----------
Output: 

        return self._property_description

    def as_property_value(self):
        """
        Get the property value.

        Returns the value of the property.
        """
        return self._property_value

    def as_property_type(self):
        """
        Get the property type.

        Returns the type of the property.
        """
        return self._property_type

    def as_property_name(self):
        """
        Get the property name.

        Returns the name of the property.
       
----------
Solution: 

        description = deepcopy(self.metadata)

        if 'links' not in description:
            description['links'] = []

        description['links'].append(
            {
                'rel': 'property',
                'href': self.href_prefix + self.href,
            }
        )
        return description
--------------------
Prompt: 
def save(self, fname=None):
        """
        Download the chart from the URL into a filename as a PNG

        The filename defaults to the chart title (chtt) if any
        """
----------
Output: 

        if fname is None:
            fname = self.title
        if fname is None:
            raise ValueError("No filename given")
        if not fname.endswith(".png"):
            fname += ".png"
        url = self.url
        if url is None:
            raise ValueError("No URL given")
        if not url.startswith("http"):
            url = "http://" + url
        try:
            import urllib
            urllib.urlretrieve(url, fname)
        except
----------
Solution: 

        if not fname:
            fname = self.getname()
        assert fname != None, 'You must specify a filename to save to'
        if not fname.endswith('.png'):
            fname += '.png'
        try:
            urlretrieve(self.url, fname)
        except Exception:
            raise IOError('Problem saving %s to file'%fname)
        return fname
--------------------
Prompt: 
def opt(parser: Union[Parser, Sequence[Input]]) -> OptionalParser:
    """Optionally match a parser.

    An ``OptionalParser`` attempts to match ``parser``. If it succeeds, it
    returns a list of length one with the value returned by the parser as the
    only element. If it fails, it returns an empty list.

    Args:
        parser: Parser or literal
    """
----------
Output: 

    if isinstance(parser, Parser):
        return OptionalParser(parser)
    else:
        return OptionalParser(LiteralParser(parser))


class OptionalParser(Parser):
    """Parser that optionally matches a parser.

    Attributes:
        parser: Parser to optionally match
    """

    def __init__(self, parser: Parser):
        self.parser = parser

    def __repr__(self) -> str:
        return f"opt({self.parser!r})"

    def parse(
----------
Solution: 

    if isinstance(parser, str):
        parser = lit(parser)
    return OptionalParser(parser)
--------------------
Prompt: 
def log(self):
        """
        Print on screen and on file the percentages for each status.
        """
----------
Output: 

        print('----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------
Solution: 


        if (
            PyFunceble.CONFIGURATION["show_percentage"]
            and PyFunceble.INTERN["counter"]["number"]["tested"] > 0
        ):
            # * We are allowed to show the percentage on screen.
            # and
            # * The number of tested is greater than 0.

            # We initiate the output file.
            output = (
                PyFunceble.OUTPUT_DIRECTORY
                + PyFunceble.OUTPUTS["parent_directory"]
                + PyFunceble.OUTPUTS["logs"]["directories"]["parent"]
                + PyFunceble.OUTPUTS["logs"]["directories"]["percentage"]
                + PyFunceble.OUTPUTS["logs"]["filenames"]["percentage"]
            )

            # We delete the output file if it does exist.
            File(output).delete()

            # We calculate the percentage of each statuses.
            self._calculate()

            if not PyFunceble.CONFIGURATION["quiet"]:
                # The quiet mode is activated.

                # We print a new line.
                print("\n")

                # We print the percentage header on file and screen.
                Prints(None, "Percentage", output).header()

                # We construct the different lines/data to print on screen and file.
                lines_to_print = [
                    [
                        PyFunceble.STATUS["official"]["up"],
                        str(PyFunceble.INTERN["counter"]["percentage"]["up"]) + "%",
                        PyFunceble.INTERN["counter"]["number"]["up"],
                    ],
                    [
                        PyFunceble.STATUS["official"]["down"],
                        str(PyFunceble.INTERN["counter"]["percentage"]["down"]) + "%",
                        PyFunceble.INTERN["counter"]["number"]["down"],
                    ],
                    [
                        PyFunceble.STATUS["official"]["invalid"],
                        str(PyFunceble.INTERN["counter"]["percentage"]["invalid"])
                        + "%",
                        PyFunceble.INTERN["counter"]["number"]["invalid"],
                    ],
                ]

                if PyFunceble.CONFIGURATION["syntax"]:
                    # We are checking for syntax.

                    # We update the denomination of the UP.
                    lines_to_print[0][0] = PyFunceble.STATUS["official"]["valid"]

                    # And we unset the INACTIVE line.
                    del lines_to_print[1]

                for to_print in lines_to_print:
                    # We loop throught the different line to print.
                    # (one line for each status.)

                    # And we print the current status line on file and screen.
                    Prints(to_print, "Percentage", output).data()

        elif PyFunceble.INTERN["counter"]["number"]["tested"] > 0:
            # * We are not allowed to show the percentage on screen.
            # but
            # * The number of tested is greater than 0.

            # We run the calculation.
            # Note: The following is needed, because all counter calculation are
            # done by this class.
            self._calculate()
--------------------
Prompt: 
def grid(self, *args):
        """
        Apply a grid to your chart
        args are of the form::
            <x axis step size>,
            <y axis step size>,
            <length of line segment>,
            <length of blank segment>
            <x offset>,
            <y offset>
        APIPARAM: chg
        """
----------
Output: 

        self.chart.grid(*args)
        return self

    def grid_color(self, *args):
        """
        Set the color of the grid
        args are of the form::
            <color>
        APIPARAM: chg
        """
        self.chart.grid_color(*args)
        return self

    def grid_line_style(self, *args):
        """
        Set the line style of the grid
        args are of the form::
            <line style>
        APIPARAM: chg

----------
Solution: 

        grids =  map(str,map(float,args))
        self['chg'] = ','.join(grids).replace('None','')
        return self
--------------------
Prompt: 
def delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):
        """
        Deletes a single document in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one

        :param mongo_collection: The name of the collection to delete from.
        :type mongo_collection: str
        :param filter_doc: A query that matches the document to delete.
        :type filter_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str

        """
----------
Output: 
dict
        :param mongo_db: The name of the database to delete from.
        :type mongo_db: str
        :param kwargs: Any additional arguments to pass to the delete_one method.
        :return: The result of the delete_one method.
        :rtype: pymongo.results.DeleteResult
        """
        return self.get_mongo_collection(mongo_collection, mongo_db).delete_one(filter_doc, **kwargs)

    def delete_many(self, mongo_collection, filter_doc, mongo
----------
Solution: 

        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        return collection.delete_one(filter_doc, **kwargs)
--------------------
Prompt: 
def _check_relative_import(
        self, modnode, importnode, importedmodnode, importedasname
    ):
        """check relative import. node is either an Import or From node, modname
        the imported module name.
        """
----------
Output: 

        if modnode.modname == importedmodnode.modname:
            # relative import to the same module
            return
        if modnode.modname == "__main__":
            # relative import to the main module
            return
        if modnode.modname == "__init__":
            # relative import to an __init__ module
            return
        if modnode.modname == "builtins":
            # relative import to the builtins module
            return
        if modnode.modname == "sys":
            # relative import to the sys
----------
Solution: 

        if not self.linter.is_message_enabled("relative-import"):
            return None
        if importedmodnode.file is None:
            return False  # built-in module
        if modnode is importedmodnode:
            return False  # module importing itself
        if modnode.absolute_import_activated() or getattr(importnode, "level", None):
            return False
        if importedmodnode.name != importedasname:
            # this must be a relative import...
            self.add_message(
                "relative-import",
                args=(importedasname, importedmodnode.name),
                node=importnode,
            )
            return None
        return None
--------------------
Prompt: 
def download(self, bucket_name, object_name, filename=None):
        """
        Get a file from Google Cloud Storage.

        :param bucket_name: The bucket to fetch from.
        :type bucket_name: str
        :param object_name: The object to fetch.
        :type object_name: str
        :param filename: If set, a local file path where the file should be written to.
        :type filename: str
        """
----------
Output: 

        if filename is None:
            filename = object_name

        self.logger.info('Downloading %s from %s', object_name, bucket_name)
        self.logger.debug('Downloading %s to %s', object_name, filename)

        try:
            self.client.get_bucket(bucket_name).blob(object_name).download_to_filename(filename)
        except Exception as e:
            self.logger.error('Failed to download %s from %s: %s', object_name, bucket_name
----------
Solution: 

        client = self.get_conn()
        bucket = client.get_bucket(bucket_name)
        blob = bucket.blob(blob_name=object_name)

        if filename:
            blob.download_to_filename(filename)
            self.log.info('File downloaded to %s', filename)

        return blob.download_as_string()
--------------------
Prompt: 
def parse(self, s):
        """Parses the input string, and returns a reference to the created AST's root"""
----------
Output: 

        self.lexer.input(s)
        self.lexer.lineno = 1
        self.lexer.lexpos = 0
        self.lexer.filename = "<string>"
        return self.parser.parse(self.lexer)

    def parse_file(self, fname):
        """Parses the input file, and returns a reference to the created AST's root"""
        with open(fname, "r") as f:
            self.lexer.input(f.read())
        self
----------
Solution: 

        with self.lock:
            try:
                return self.parser.parse(s, lexer=self.lexer)
            except InvalidIEMLObjectArgument as e:
                raise CannotParse(s, str(e))
            except CannotParse as e:
                e.s = s
                raise e
--------------------
Prompt: 
def allocate_ids(self, partial_keys):
        """
        Allocate IDs for incomplete keys.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds

        :param partial_keys: a list of partial keys.
        :type partial_keys: list
        :return: a list of full keys.
        :rtype: list
        """
----------
Output: 

        if not partial_keys:
            return []

        # TODO: support partial_keys with incomplete keys.
        if any(not key.is_partial for key in partial_keys):
            raise ValueError('partial_keys must be a list of partial keys.')

        # TODO: support multiple partial keys.
        if len(partial_keys) > 1:
            raise ValueError('partial_keys must be a list of one partial key.')

        partial_key = partial_keys[0]

        # TODO: support partial_key
----------
Solution: 

        conn = self.get_conn()

        resp = (conn
                .projects()
                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})
                .execute(num_retries=self.num_retries))

        return resp['keys']
--------------------
Prompt: 
def _insert_plain_text_into_buffer(self, cursor, text):
        """ Inserts text into the input buffer using the specified cursor (which
            must be in the input buffer), ensuring that continuation prompts are
            inserted as necessary.
        """
----------
Output: 

        if text is None:
            return

        # If the cursor is at the end of the buffer, just insert the text.
        if cursor.at_end():
            cursor.insert_text(text)
            return

        # Otherwise, we need to insert the text at the cursor, and then insert
        # continuation prompts as necessary.
        cursor.insert_text(text)
        self._insert_continuation_prompts(cursor)

    def _insert_continuation_prompts(self, cursor):
        """ Inserts
----------
Solution: 

        lines = text.splitlines(True)
        if lines:
            cursor.beginEditBlock()
            cursor.insertText(lines[0])
            for line in lines[1:]:
                if self._continuation_prompt_html is None:
                    cursor.insertText(self._continuation_prompt)
                else:
                    self._continuation_prompt = \
                        self._insert_html_fetching_plain_text(
                            cursor, self._continuation_prompt_html)
                cursor.insertText(line)
            cursor.endEditBlock()
--------------------
Prompt: 
def get_total_contributors(self, repo):
        """
        Retrieves the number of contributors to a repo in the organization.
        Also adds to unique contributor list.
        """
----------
Output: 

        try:
            contributors = self.get_contributors(repo)
            total_contributors = len(contributors)
            self.total_contributors += total_contributors
            self.unique_contributors.update(contributors)
            return total_contributors
        except Exception as e:
            print(e)
            return 0

    def get_contributors(self, repo):
        """
        Retrieves the contributors to a repo in the organization.
        """
        try:
            contributors = []
            for
----------
Solution: 

        repo_contributors = 0
        for contributor in repo.iter_contributors():
            repo_contributors += 1
            self.unique_contributors[contributor.id].append(repo.name)
            self.contributors_json[repo.name].append(contributor.to_json())
        return repo_contributors
--------------------
Prompt: 
def _unreg_event(self, event_list, event):
        """ Tries to remove a registered event without triggering it """
----------
Output: 

        if event in event_list:
            event_list.remove(event)
            return True
        return False

    def _reg_event(self, event_list, event):
        """ Tries to register an event without triggering it """
        if event not in event_list:
            event_list.append(event)
            return True
        return False

    def _trigger_event(self, event_list, event):
        """ Triggers an event if it is registered """
        if event in event_list:
            for
----------
Solution: 

        try:
            self.log.debug("Removing event {0}({1},{2})".format(event['function'].__name__, event['args'], event['kwargs']))
        except AttributeError:
            self.log.debug("Removing event {0}".format(str(event)))

        try:
            event_list.remove(event)
        except ValueError:
            try:
                self.log.warn("Unable to remove event {0}({1},{2}) , not found in list: {3}".format(event['function'].__name__, event['args'], event['kwargs'], event_list))
            except AttributeError:
                self.log.debug("Unable to remove event {0}".format(str(event)))
            raise KeyError('Unable to unregister the specified event from the signals specified')
--------------------
Prompt: 
def get_system_per_cpu_times():
    """Return system per-CPU times as a list of named tuples."""
----------
Output: 

    with open("/proc/stat", "r") as f:
        data = f.read()
    data = data.splitlines()
    data = [x.split() for x in data]
    data = [x for x in data if x[0] == "cpu"]
    data = [x for x in data if len(x) == 10]
    data = [x for x in data if x[2] != "guest"]
    data = [x for x in data if x[2] != "guest_nice"]

----------
Solution: 

    ret = []
    for cpu_t in _psutil_mswindows.get_system_cpu_times():
        user, system, idle = cpu_t
        item = _cputimes_ntuple(user, system, idle)
        ret.append(item)
    return ret
--------------------
Prompt: 
def queue_startfile(self, cmdfile):
        """Arrange for file of debugger commands to get read in the
        process-command loop."""
----------
Output: 

        self.cmdqueue.append(cmdfile)

    def queue_command(self, cmd):
        """Arrange for a string of debugger commands to get read in the
        process-command loop."""
        self.cmdqueue.append(cmd)

    def run(self, cmd, *args, **kwargs):
        """Run a command in the process-command loop.  This is a
        convenience function for running a command in the process-command
        loop.  It is equivalent to:

        >>> from pdb import Pdb
        >>>
----------
Solution: 

        expanded_cmdfile = os.path.expanduser(cmdfile)
        is_readable = Mfile.readable(expanded_cmdfile)
        if is_readable:
            self.cmd_queue.append('source ' + expanded_cmdfile)
        elif is_readable is None:
            self.errmsg("source file '%s' doesn't exist" % expanded_cmdfile)
        else:
            self.errmsg("source file '%s' is not readable" %
                        expanded_cmdfile)
            pass
        return
--------------------
Prompt: 
def read_infile(infile: Union[Path, str], from_words=False,
                word_column: int = WORD_COLUMN, pos_column: int = POS_COLUMN,
                tag_column: int = TAG_COLUMN, max_sents: int = -1,
                read_only_words: bool = False) -> List[Tuple[List, Union[List, None]]]:
    """Reads input file in CONLL-U format

    Args:
        infile: a path to a file
        word_column: column containing words (default=1)
        pos_column: column containing part-of-speech labels (default=3)
        tag_column: column containing fine-grained tags (default=5)
        max_sents: maximal number of sents to read
        read_only_words: whether to read only words

    Returns:
        a list of sentences. Each item contains a word sequence and a tag sequence, which is ``None``
        in case ``read_only_words = True``
    """
----------
Output: 
in CONLL-U format
        from_words: if True, the input file is assumed to be in words format
        word_column: the column number of the word in the input file
        pos_column: the column number of the POS tag in the input file
        tag_column: the column number of the tag in the input file
        max_sents: the maximum number of sentences to read
        read_only_words: if True, only the words are read

    Returns:
        a list of tuples of the form (words, tags)
    """
----------
Solution: 

    answer, curr_word_sent, curr_tag_sent = [], [], []
    if from_words:
        word_column, read_only_words = 0, True
    with open(infile, "r", encoding="utf8") as fin:
        for line in fin:
            line = line.strip()
            if line.startswith("#"):
                continue
            if line == "":
                if len(curr_word_sent) > 0:
                    if read_only_words:
                        curr_tag_sent = None
                    answer.append((curr_word_sent, curr_tag_sent))
                curr_tag_sent, curr_word_sent = [], []
                if len(answer) == max_sents:
                    break
                continue
            splitted = line.split("\t")
            index = splitted[0]
            if not from_words and not index.isdigit():
                continue
            curr_word_sent.append(splitted[word_column])
            if not read_only_words:
                pos, tag = splitted[pos_column], splitted[tag_column]
                tag = pos if tag == "_" else "{},{}".format(pos, tag)
                curr_tag_sent.append(tag)
        if len(curr_word_sent) > 0:
            if read_only_words:
                curr_tag_sent = None
            answer.append((curr_word_sent, curr_tag_sent))
    return answer
--------------------
Prompt: 
def fuzzed(self):
        """
        Get a printable fuzzed object
        """
----------
Output: 

        return self.fuzzed_object

    def __str__(self):
        """
        Get a printable fuzzed object
        """
        return self.fuzzed_object

    def __repr__(self):
        """
        Get a printable fuzzed object
        """
        return self.fuzzed_object

    def __eq__(self, other):
        """
        Compare two fuzzed objects
        """
        if isinstance(other, Fuzzed):
            return self.fuzzed_
----------
Solution: 

        try:
            if self.config.strong_fuzz:
                fuzzer = PJFMutators(self.config)
                if self.config.url_encode:
                    if sys.version_info >= (3, 0):
                        return urllib.parse.quote(fuzzer.fuzz(json.dumps(self.config.json)))
                    else:
                        return urllib.quote(fuzzer.fuzz(json.dumps(self.config.json)))
                else:
                    if type(self.config.json) in [list, dict]:
                        return fuzzer.fuzz(json.dumps(self.config.json))
                    else:
                        return fuzzer.fuzz(self.config.json)
            else:
                if self.config.url_encode:
                    if sys.version_info >= (3, 0):
                        return urllib.parse.quote(self.get_fuzzed(self.config.indent, self.config.utf8))
                    else:
                        return urllib.quote(self.get_fuzzed(self.config.indent, self.config.utf8))
                else:
                    return self.get_fuzzed(self.config.indent, self.config.utf8)
        except Exception as e:
            raise PJFBaseException(e.message if hasattr(e, "message") else str(e))
--------------------
Prompt: 
def validate(self, request):
        """Validate JSON-RPC request.

        :param request: RPC request object
        :type request: dict

        """
----------
Output: 

        if not isinstance(request, dict):
            raise InvalidRequest('Request must be a JSON object')

        if 'jsonrpc' not in request:
            raise InvalidRequest('Request must contain "jsonrpc" member')

        if request['jsonrpc'] != '2.0':
            raise InvalidRequest('Request must contain "jsonrpc" member with value "2.0"')

        if 'method' not in request:
            raise InvalidRequest('Request must contain "method" member')

        if not isinstance(request['method
----------
Solution: 


        try:
            validate_version(request)
            validate_method(request)
            validate_params(request)
            validate_id(request)
        except (AssertionError, KeyError) as error:
            invalid_request(error)
--------------------
Prompt: 
def action_notify(self, action):
        """
        Notify all subscribers of an action status change.

        action -- the action whose status changed
        """
----------
Output: 

        if action.status == action.STATUS_SUCCESS:
            self.notify_success(action)
        elif action.status == action.STATUS_FAILURE:
            self.notify_failure(action)
        elif action.status == action.STATUS_RUNNING:
            self.notify_running(action)

    def notify_success(self, action):
        """
        Notify all subscribers of an action success.

        action -- the action that succeeded
        """
        for subscriber in self.subscribers:

----------
Solution: 

        message = json.dumps({
            'messageType': 'actionStatus',
            'data': action.as_action_description(),
        })

        for subscriber in list(self.subscribers):
            try:
                subscriber.write_message(message)
            except tornado.websocket.WebSocketClosedError:
                pass
--------------------
Prompt: 
def update_mongo_compound_variants(self, bulk):
        """Update the compound information for a bulk of variants in the database

            Args:
                bulk(dict): {'_id': scout.models.Variant}

        """
----------
Output: 

        for variant_obj in bulk.values():
            self.update_mongo_compound_variant(variant_obj)

    def update_mongo_compound_variant(self, variant_obj):
        """Update the compound information for a variant in the database

            Args:
                variant_obj(scout.models.Variant)

        """
        if not variant_obj.compound_recessive:
            return

        # Get the compound information for the variant
        compound_info = self.get_
----------
Solution: 

        requests = []
        for var_id in bulk:
            var_obj = bulk[var_id]
            if not var_obj.get('compounds'):
                continue
            # Add a request to update compounds
            operation = pymongo.UpdateOne(
                {'_id': var_obj['_id']},
                {
                    '$set': {
                        'compounds': var_obj['compounds']
                    }
                })
            requests.append(operation)

        if not requests:
            return

        try:
            self.variant_collection.bulk_write(requests, ordered=False)
        except BulkWriteError as err:
            LOG.warning("Updating compounds failed")
            raise err
--------------------
Prompt: 
def module_list(path):
    """
    Return the list containing the names of the modules available in the given
    folder.
    """
----------
Output: 

    return [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]


def module_path(module):
    """
    Return the path of the given module.
    """
    return os.path.join(MODULES_PATH, module)


def module_name(module):
    """
    Return the name of the given module.
    """
    return os.path.basename(module)


def module_exists(module):
    """

----------
Solution: 

    # sys.path has the cwd as an empty string, but isdir/listdir need it as '.'
    if path == '':
        path = '.'

    if os.path.isdir(path):
        folder_list = os.listdir(path)
    elif path.endswith('.egg'):
        try:
            folder_list = [f for f in zipimporter(path)._files]
        except:
            folder_list = []
    else:
        folder_list = []

    if not folder_list:
        return []

    # A few local constants to be used in loops below
    isfile = os.path.isfile
    pjoin = os.path.join
    basename = os.path.basename

    def is_importable_file(path):
        
--------------------
Prompt: 
def modify_conf():
    """
    pip install redbaron
    """
----------
Output: 

    from redbaron import RedBaron
    from redbaron.exceptions import RedBaronError

    def modify_conf_file(file_path):
        try:
            with open(file_path, "r") as f:
                red = RedBaron(f.read())
                for i in red.find_all("variable"):
                    if i.name == "conf":
                        i.value = "conf = '{}'".format(file_path)
                        break
                with open(file_path, "
----------
Solution: 

    import redbaron
    import ubelt as ub
    conf_path = 'docs/conf.py'

    source = ub.readfrom(conf_path)
    red = redbaron.RedBaron(source)

    # Insert custom extensions
    extra_extensions = [
        '"sphinxcontrib.napoleon"'
    ]

    ext_node = red.find('name', value='extensions').parent
    ext_node.value.value.extend(extra_extensions)

    # Overwrite theme to read-the-docs
    theme_node = red.find('name', value='html_theme').parent
    theme_node.value.value = '"sphinx_rtd_theme"'

    ub.writeto(conf_path, red.dumps())
--------------------
Prompt: 
def show(self, *args, **kwargs):
        """
        Shows the chart URL in a webbrowser

        Other arguments passed to webbrowser.open
        """
----------
Output: 

        import webbrowser
        webbrowser.open(self.url, *args, **kwargs)

    def save(self, filename, *args, **kwargs):
        """
        Saves the chart to a file

        Other arguments passed to urllib2.urlopen
        """
        import urllib2
        urllib2.urlopen(self.url, *args, **kwargs).read()
        with open(filename, 'w') as f:
            f.write(urllib2.urlopen(self.url,
----------
Solution: 

        from webbrowser import open as webopen
        return webopen(str(self), *args, **kwargs)
--------------------
Prompt: 
def getView(self,name):
        """
        Returns the view with name ``name``\ .
        
        Raises a :py:exc:`ValueError` if the view does not exist.
        """
----------
Output: 

        return self._views[name]
    
    def getViews(self):
        """
        Returns a list of all views in the database.
        """
        return self._views.values()
    
    def getViewNames(self):
        """
        Returns a list of all view names in the database.
        """
        return self._views.keys()
    
    def getViewNamesAndTypes(self):
        """
        Returns a list of all view names and types in the database.
        """
        return [(name,view.get
----------
Solution: 

        if name not in self.views:
            raise ValueError("Unknown world view")
        return self.views[name]
--------------------
Prompt: 
def _process_execute_error(self, msg):
        """ Process a reply for an execution request that resulted in an error.
        """
----------
Output: 

        self._process_error(msg)

    def _process_execute_reply(self, msg):
        """ Process a reply for an execution request.
        """
        self._process_reply(msg)

    def _process_error(self, msg):
        """ Process an error message.
        """
        self._process_reply(msg)

    def _process_reply(self, msg):
        """ Process a reply message.
        """
        if msg.header.msg_type == 'status' and msg.
----------
Solution: 

        content = msg['content']
        # If a SystemExit is passed along, this means exit() was called - also
        # all the ipython %exit magic syntax of '-k' to be used to keep
        # the kernel running
        if content['ename']=='SystemExit':
            keepkernel = content['evalue']=='-k' or content['evalue']=='True'
            self._keep_kernel_on_exit = keepkernel
            self.exit_requested.emit(self)
        else:
            traceback = ''.join(content['traceback'])
            self._append_plain_text(traceback)
--------------------
Prompt: 
def s_add(self, path, function, method=None, type_cast=None):
        """Function for registering a simple path.

        Args:
            path (str): Path to be matched.
            function (function): Function to associate with this path.
            method (str, optional): Usually used to define one of GET, POST,
                PUT, DELETE. You may use whatever fits your situation though.
                Defaults to None.
            type_cast (dict, optional): Mapping between the param name and
                one of `int`, `float` or `bool`. The value reflected by the
                provided param name will than be casted to the given type.
                Defaults to None.
        """
----------
Output: 
`, `float`, `str`, `bool`. Defaults to None.

        Returns:
            None
        """
        self.add(path, function, method, type_cast)

    def add(self, path, function, method=None, type_cast=None):
        """Function for registering a path.

        Args:
            path (str): Path to be matched.
            function (function): Function to associate with this path.
            method (str, optional): Usually used to define one of GET, POST,
                PUT, DE
----------
Solution: 

        with self._lock:
            try:
                path = '^/{}'.format(path.lstrip('/'))
                path = '{}/$'.format(path.rstrip('/'))
                path = path.replace('<', '(?P<')
                path = path.replace('>', '>[^/]*)')

                self.add(path, function, method, type_cast)
            except Exception:
                pass
--------------------
Prompt: 
def login(request, template_name='ci/login.html',
          redirect_field_name=REDIRECT_FIELD_NAME,
          authentication_form=AuthenticationForm):
    """
    Displays the login form and handles the login action.
    """
----------
Output: 

    redirect_to = request.REQUEST.get(redirect_field_name, '')

    if request.method == 'POST':
        form = authentication_form(request, data=request.POST)
        if form.is_valid():
            # Ensure the user-originating redirection url is safe.
            if not is_safe_url(url=redirect_to, host=request.get_host()):
                redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)

            # Okay, security check complete. Log
----------
Solution: 

    redirect_to = request.POST.get(redirect_field_name,
                                   request.GET.get(redirect_field_name, ''))

    if request.method == "POST":
        form = authentication_form(request, data=request.POST)
        if form.is_valid():

            # Ensure the user-originating redirection url is safe.
            if not is_safe_url(url=redirect_to, host=request.get_host()):
                redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)

            # Okay, security check complete. Get the user object from auth api.
            user = form.get_user()
            request.session['user_token'] = user["token"]
            request.session['user_email'] = user["email"]
            request.session['user_permissions'] = user["permissions"]
            request.session['user_id'] = user["id"]
            request.session['user_list'] = user["user_list"]

            if not settings.HIDE_DASHBOARDS:
                # Set user dashboards because they are slow to change
                dashboards = ciApi.get_user_dashboards(user["id"])
                dashboard_list = list(dashboards['results'])
                if len(dashboard_list) > 0:
                    request.session['user_dashboards'] = \
                        dashboard_list[0]["dashboards"]
                    request.session['user_default_dashboard'] = \
                        dashboard_list[0]["default_dashboard"]["id"]
                else:
                    request.session['user_dashboards'] = []
                    request.session['user_default_dashboard'] = None

            # Get the user access tokens too and format for easy access
            tokens = ciApi.get_user_service_tokens(
                params={"user_id": user["id"]})
            token_list = list(tokens['results'])
            user_tokens = {}
            if len(token_list) > 0:
                for token in token_list:
                    user_tokens[token["service"]["name"]] = {
                        "token": token["token"],
                        "url": token["service"]["url"] + "/api/v1"
                    }
            request.session['user_tokens'] = user_tokens

            return HttpResponseRedirect(redirect_to)
    else:
        form = authentication_form(request)

    current_site = get_current_site(request)

    context = {
        'form': form,
        redirect_field_name: redirect_to,
        'site': current_site,
        'site_name': current_site.name,
    }

    return TemplateResponse(request, template_name, context)
--------------------
Prompt: 
def get_io_write_task(self, fileobj, data, offset):
        """Get an IO write task for the requested set of data

        This task can be ran immediately or be submitted to the IO executor
        for it to run.

        :type fileobj: file-like object
        :param fileobj: The file-like object to write to

        :type data: bytes
        :param data: The data to write out

        :type offset: integer
        :param offset: The offset to write the data to in the file-like object

        :returns: An IO task to be used to write data to a file-like object
        """
----------
Output: 
like object

        :rtype: :class:`~.IOWriteTask`
        :returns: An IO write task for the requested set of data
        """
        return IOWriteTask(fileobj, data, offset)

    def get_io_read_task(self, fileobj, length, offset):
        """Get an IO read task for the requested set of data

        This task can be ran immediately or be submitted to the IO executor
        for it to run.

        :type fileobj: file-like object
        :param fileobj:
----------
Solution: 

        return IOWriteTask(
            self._transfer_coordinator,
            main_kwargs={
                'fileobj': fileobj,
                'data': data,
                'offset': offset,
            }
        )
--------------------
Prompt: 
def __notify_listeners(self, data, stats):
        """ notify all listeners about aggregate data and stats """
----------
Output: 

        for listener in self.listeners:
            listener.update(data, stats)

    def __get_data_from_listener(self, listener):
        """ get data from a listener """
        return listener.get_data()

    def __get_stats_from_listener(self, listener):
        """ get stats from a listener """
        return listener.get_stats()

    def __get_data_from_listeners(self):
        """ get data from all listeners """
        data = {}
        for listener in self.
----------
Solution: 

        for listener in self.listeners:
            listener.on_aggregated_data(data, stats)
--------------------
Prompt: 
def utf8(unicode_str):
    """
    Return a utf-8 encoded string from a valid unicode string.

    :param unicode_str: Unicode string.

    :return: str
    """
----------
Output: 

    if isinstance(unicode_str, str):
        return unicode_str
    else:
        return unicode_str.encode('utf-8')


def utf8_to_str(utf8_str):
    """
    Return a str from a valid utf-8 encoded string.

    :param utf8_str: UTF-8 encoded string.

    :return: str
    """
    if isinstance(utf8_str, str):
        return utf8_str
    else:
        return utf8_str.decode
----------
Solution: 

    if six.PY2 and isinstance(unicode_str, __unicode__):
        return unicode_str.encode('utf-8')

    return unicode_str
--------------------
Prompt: 
def csoftmax_for_slice(input):
    """ It is a implementation of the constrained softmax (csoftmax) for slice.
        Based on the paper:
        https://andre-martins.github.io/docs/emnlp2017_final.pdf "Learning What's Easy: Fully Differentiable Neural Easy-First Taggers" (page 4)
    Args:
        input: A list of [input tensor, cumulative attention].
    Returns:
        output: A list of [csoftmax results, masks]
    """
----------
Output: 
[output tensor, cumulative attention].
    """
    input_tensor, cumulative_attention = input
    input_tensor = tf.nn.softmax(input_tensor)
    cumulative_attention = tf.nn.softmax(cumulative_attention)
    output_tensor = tf.multiply(input_tensor, cumulative_attention)
    output_tensor = tf.reduce_sum(output_tensor, axis=1)
    output_tensor = tf.nn.softmax(output_tensor)
    return [output_tensor
----------
Solution: 


    [ten, u] = input

    shape_t = ten.shape
    shape_u = u.shape

    ten -= tf.reduce_mean(ten)
    q = tf.exp(ten)
    active = tf.ones_like(u, dtype=tf.int32)
    mass = tf.constant(0, dtype=tf.float32)
    found = tf.constant(True, dtype=tf.bool)

    def loop(q_, mask, mass_, found_):
        q_list = tf.dynamic_partition(q_, mask, 2)
        condition_indices = tf.dynamic_partition(tf.range(tf.shape(q_)[0]), mask, 2)  # 0 element it False,
        #  1 element if true

        p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])
        p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])

        # condition verification and mask modification
        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p
        condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,
                                                 2)  # 0 when u is bigger than p, 1 when u is less than p

        split_p_new = tf.dynamic_partition(p_new, less_mask, 2)
        split_u = tf.dynamic_partition(u, less_mask, 2)

        alpha = tf.dynamic_stitch(condition_indices, [split_p_new[0], split_u[1]])
        mass_ += tf.reduce_sum(split_u[1])

        mask = mask * (tf.ones_like(less_mask) - less_mask)

        found_ = tf.cond(tf.equal(tf.reduce_sum(less_mask), 0),
                         lambda: False,
                         lambda: True)

        alpha = tf.reshape(alpha, q_.shape)

        return alpha, mask, mass_, found_

    (csoft, mask_, _, _) = tf.while_loop(cond=lambda _0, _1, _2, f: f,
                                         body=loop,
                                         loop_vars=(q, active, mass, found))

    return [csoft, mask_]
--------------------
Prompt: 
def lazy(function, *args, **kwargs):
    """ Produces a callable so that functions can be lazily evaluated in
    templates.

    Arguments:

        function (callable): The function to call at evaluation time.

        args: Positional arguments, passed directly to ``function``.

        kwargs: Keyword arguments, passed directly to ``function``.

    Return:

        callable: A callable that will evaluate a call to ``function`` with
            the specified arguments.

    """
----------
Output: 

    def callable():
        return function(*args, **kwargs)
    return callable


def lazy_property(function):
    """ Produces a property so that functions can be lazily evaluated in
    templates.

    Arguments:

        function (callable): The function to call at evaluation time.

    Return:

        property: A property that will evaluate a call to ``function``.

    """
    return property(lazy(function))


def lazy_attribute(function):
    """ Produces an attribute so
----------
Solution: 


    NOT_EVALUATED = object()
    retval = [NOT_EVALUATED]

    def evaluate():
        if retval[0] is NOT_EVALUATED:
            retval[0] = function(*args, **kwargs)
        return retval[0]

    return evaluate
--------------------
Prompt: 
def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:
  """Maybe broadcasts `from_structure` to `to_structure`.

  If `from_structure` is a singleton, it is tiled to match the structure of
  `to_structure`. Note that the elements in `from_structure` are not copied if
  this tiling occurs.

  Args:
    from_structure: A structure.
    to_structure: A structure.

  Returns:
    new_from_structure: Same structure as `to_structure`.
  """
----------
Output: 
: A structure.
  """
  if is_sequence(from_structure):
    if is_sequence(to_structure):
      return type(from_structure)(
          maybe_broadcast_structure(s, t) for s, t in zip(from_structure,
                                                         to_structure))
    else:
      return type(from_structure)(
          maybe_broadcast_structure(s, to_structure) for s in from_structure)
  else:
    if is_sequence(to_structure):
      return type(
----------
Solution: 

  flat_from = tf.nest.flatten(from_structure)
  flat_to = tf.nest.flatten(to_structure)
  if len(flat_from) == 1:
    flat_from *= len(flat_to)
  return tf.nest.pack_sequence_as(to_structure, flat_from)
--------------------
Prompt: 
def main():
    """Initialize and run command line interface."""
----------
Output: 

    import argparse
    import sys

    from . import __version__
    from . import cli

    parser = argparse.ArgumentParser(
        description="A command line interface for the OpenAI API."
    )
    parser.add_argument(
        "--version",
        action="version",
        version="%(prog)s {version}".format(version=__version__),
    )
    subparsers = parser.add_subparsers(dest="command")

    # Add subparsers
    cli.add_
----------
Solution: 

    # Create and populate parser
    parser = create_parser()

    # Parse given arguments
    args = parser.parse_args()

    # Checking arguments
    check_arguments(args, parser)

    # BUSINESS LOGIC IS FOLLOWING
    run(parser, args)
--------------------
Prompt: 
def from_tuplelist(tuple_list):
        """
        Populates a Layout from a list containing virtual
        qubits---(QuantumRegister, int) tuples---, or None.

        Args:
            tuple_list (list):
                e.g.: [qr[0], None, qr[2], qr[3]]
        Returns:
            Layout: the corresponding Layout object
        Raises:
            LayoutError: If the elements are not (Register, integer) or None
        """
----------
Output: 

        layout = Layout()
        for element in tuple_list:
            if element is None:
                layout.add_register(None)
            elif isinstance(element, tuple):
                if len(element) != 2:
                    raise LayoutError('Tuple must contain two elements.')
                if not isinstance(element[0], QuantumRegister):
                    raise LayoutError('First element of tuple must be a '
                                      'QuantumRegister.')
                if not isinstance(element[1], int):
                    raise LayoutError('
----------
Solution: 

        out = Layout()
        for physical, virtual in enumerate(tuple_list):
            if virtual is None:
                continue
            elif Layout.is_virtual(virtual):
                if virtual in out._v2p:
                    raise LayoutError('Duplicate values not permitted; Layout is bijective.')
                out[virtual] = physical
            else:
                raise LayoutError("The list should contain elements of the form"
                                  " (Register, integer) or None")
        return out
--------------------
Prompt: 
def reject_connection(self, reason=None):
        """
        Reject a pending connection.
        """
----------
Output: 

        if self._state != self.CONNECTING:
            raise RuntimeError("Cannot reject connection in state %s" % self._state)
        self._state = self.DISCONNECTED
        self._close_connection(reason)

    def _close_connection(self, reason=None):
        """
        Close the connection.
        """
        if self._state == self.DISCONNECTED:
            return
        self._state = self.DISCONNECTED
        if self._connection is not None:
            self._
----------
Solution: 

        assert self.pending, "Connection is not pending."
        if reason is None:
            reason = ConnectionRefusedError()
        self._accept_d.errback(reason)
--------------------
Prompt: 
def signature(frame):
    """return suitable frame signature to key display expressions off of."""
----------
Output: 

    return frame.get('signature', frame.get('name', frame.get('filename')))


def _get_frame_info(frame):
    """return frame info as a dict."""
    return {
        'filename': frame.f_code.co_filename,
        'name': frame.f_code.co_name,
        'lineno': frame.f_lineno,
        'signature': signature(frame),
    }


def _get_frame_info_list(frames):
    """return a list
----------
Solution: 

    if not frame: return None
    code = frame.f_code
    return (code.co_name, code.co_filename, code.co_firstlineno)
--------------------
Prompt: 
async def parse_tag_results(soup):
    """
    Parse a page of tag or trait results. Same format.

    :param soup: BS4 Class Object
    :return: A list of tags, Nothing else really useful there
    """
----------
Output: 

    tags = []
    for tag in soup.find_all('div', class_='tag-container'):
        tags.append(tag.text)
    return tags


def parse_trait_results(soup):
    """
    Parse a page of trait results. Same format.

    :param soup: BS4 Class Object
    :return: A list of traits, Nothing else really useful there
    """
    traits = []
    for trait in soup.find_all('div', class_='trait-container'):
       
----------
Solution: 

    soup = soup.find_all('td', class_='tc3')
    tags = []
    for item in soup:
        tags.append(item.a.string)
    return tags
--------------------
Prompt: 
def mean(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):
        """Calculate the mean for expression, possibly on a grid defined by binby.

        Example:

        >>> df.mean("x")
        -0.067131491264005971
        >>> df.mean("(x**2+y**2)**0.5", binby="E", shape=4)
        array([  2.43483742,   4.41840721,   8.26742458,  15.53846476])

        :param expression: {expression}
        :param binby: {binby}
        :param limits: {limits}
        :param shape: {shape}
        :param selection: {selection}
        :param delay: {delay}
        :param progress: {progress}
        :return: {return_stat_scalar}
        """
----------
Output: 
([0.00017396, 0.00020902, 0.00023811, 0.00026353])

        Args:
            expression (str): The expression to calculate the mean for.
            binby (list of str, optional): The columns to bin by.
            limits (list of tuple, optional): The limits to use for the bins.
            shape (tuple of int, optional): The shape of the grid.
            selection (bool, optional):
----------
Solution: 

        return self._compute_agg('mean', expression, binby, limits, shape, selection, delay, edges, progress)
        logger.debug("mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r", expression, binby, limits, shape, selection, delay)
        expression = _ensure_strings_from_expressions(expression)
        selection = _ensure_strings_from_expressions(selection)
        binby = _ensure_strings_from_expressions(binby)

        @delayed
        def calculate(expression, limits):
            task = tasks.TaskStatistic(self, binby, shape, limits, weight=expression, op=tasks.OP_ADD_WEIGHT_MOMENTS_01, selection=selection)
            self.executor.schedule(task)
            progressbar.add_task(task, "mean for %s" % expression)
            return task

        @delayed
        def finish(*stats_args):
            stats = np.array(stats_args)
            counts = stats[..., 0]
            with np.errstate(divide='ignore', invalid='ignore'):
                mean = stats[..., 1] / counts
            return vaex.utils.unlistify(waslist, mean)
        waslist, [expressions, ] = vaex.utils.listify(expression)
        progressbar = vaex.utils.progressbars(progress)
        limits = self.limits(binby, limits, delay=True)
        stats = [calculate(expression, limits) for expression in expressions]
        var = finish(*stats)
        return self._delay(delay, var)
--------------------
Prompt: 
def build_fake_data(num_examples=10):
  """Build fake MNIST-style data for unit testing."""
----------
Output: 

  # Create a fake dataset
  fake_data = np.zeros((num_examples, 28, 28, 1), dtype=np.float32)
  fake_labels = np.zeros((num_examples, 10), dtype=np.float32)
  for i in range(num_examples):
    label = i % 10
    fake_data[i, :, :, 0] = (label + 1) * np.ones((28, 28), dtype=np.float3
----------
Solution: 


  class Dummy(object):
    pass

  num_examples = 10
  mnist_data = Dummy()
  mnist_data.train = Dummy()
  mnist_data.train.images = np.float32(np.random.randn(
      num_examples, *IMAGE_SHAPE))
  mnist_data.train.labels = np.int32(np.random.permutation(
      np.arange(num_examples)))
  mnist_data.train.num_examples = num_examples
  mnist_data.validation = Dummy()
  mnist_data.validation.images = np.float32(np.random.randn(
      num_examples, *IMAGE_SHAPE))
  mnist_data.validation.labels = np.int32(np.random.permutation(
      np.arange(num_examples)))
  mnist_data.validation.num_examples = num_examples
  return mnist_data
--------------------
Prompt: 
def min(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):
        """Calculate the minimum for given expressions, possibly on a grid defined by binby.


        Example:

        >>> df.min("x")
        array(-128.293991)
        >>> df.min(["x", "y"])
        array([-128.293991 ,  -71.5523682])
        >>> df.min("x", binby="x", shape=5, limits=[-10, 10])
        array([-9.99919128, -5.99972439, -1.99991322,  2.0000093 ,  6.0004878 ])

        :param expression: {expression}
        :param binby: {binby}
        :param limits: {limits}
        :param shape: {shape}
        :param selection: {selection}
        :param delay: {delay}
        :param progress: {progress}
        :return: {return_stat_scalar}, the last dimension is of shape (2)
        """
----------
Output: 
24])

        Args:
            expression (str or list): The expression or expressions to calculate the minimum for.
            binby (str or list): The column(s) to bin by.
            limits (list): The limits to use for the bins.
            shape (tuple): The shape of the output array.
            selection (bool): If True, only the selected rows will be used.
            delay (bool): If True, the calculation will be delayed until the ``compute`` method is called.
            progress (bool): If True, progress messages will be printed.
----------
Solution: 

        return self._compute_agg('min', expression, binby, limits, shape, selection, delay, edges, progress)
        @delayed
        def finish(result):
            return result[..., 0]
        return self._delay(delay, finish(self.minmax(expression, binby=binby, limits=limits, shape=shape, selection=selection, delay=delay, progress=progress)))
--------------------
Prompt: 
def _create_value(self, *args, **kwargs):
        """
        Lowest value generator.

        Separated from __call__, because it seems that python
        cache __call__ reference on module import
        """
----------
Output: 

        return self.value(*args, **kwargs)

    def __call__(self, *args, **kwargs):
        """
        Highest value generator.

        Separated from __call__, because it seems that python
        cache __call__ reference on module import
        """
        return self.value(*args, **kwargs)

    def __getattr__(self, name):
        """
        Get attribute from value.

        :param name: attribute name
        :return: attribute value
        """
        return getattr(self.value
----------
Solution: 

        if not len(args):
            raise TypeError('Object instance is not provided')

        if self.by_instance:
            field_type = args[0]
        else:
            field_type = args[0].__class__

        function = self.registry.get(field_type, self.default)

        if function is None:
            raise TypeError("no match %s" % field_type)

        return function(*args, **kwargs)
--------------------
Prompt: 
def join_lines(iterator):
    """
    Joins a line ending in '\' with the previous line.
    """
----------
Output: 

    for line in iterator:
        if line.endswith('\\'):
            yield line[:-1]
        else:
            yield line


def parse_line(line):
    """
    Parses a line of the form:
        <key> <value>
    """
    key, value = line.split(None, 1)
    return key, value


def parse_lines(iterator):
    """
    Parses a line of the form:
        <key> <value>
    """
    for line
----------
Solution: 

    lines = []
    for line in iterator:
        if not line.endswith('\\'):
            if lines:
                lines.append(line)
                yield ''.join(lines)
                lines = []
            else:
                yield line
        else:
            lines.append(line.strip('\\'))
--------------------
Prompt: 
def handle(self):
        """
        Handle the backend of the given status.
        """
----------
Output: 

        if self.status == 'success':
            self.handle_success()
        elif self.status == 'failure':
            self.handle_failure()
        elif self.status == 'error':
            self.handle_error()
        elif self.status == 'skipped':
            self.handle_skipped()
        elif self.status == 'cancelled':
            self.handle_cancelled()
        elif self.status == 'pending':
            self.handle_pending()
        elif self.status == 'running':
           
----------
Solution: 


        # We initiate the source we are going to parse to the Generate class.
        source = "URL"

        if self.catched.lower() not in PyFunceble.STATUS["list"]["invalid"]:
            # The parsed status is not in the list of invalid.

            # We generate the status file with the catched status.
            Generate(self.catched, source).status_file()
        else:
            # The parsed status is in the list of invalid.

            # We generate the status file with the parsed status.
            Generate(self.catched, "SYNTAX").status_file()

        # We return the parsed status.
        return self.catched
--------------------
Prompt: 
def _segmentPath(self, path):
        """Return list of valid *path* segments."""
----------
Output: 

        return [s for s in path.split('/') if s]

    def _isValidPath(self, path):
        """Return True if *path* is valid."""
        return self._segmentPath(path) == self._segmentPath(self.path)

    def _isValidSegment(self, segment):
        """Return True if *segment* is valid."""
        return segment in self.segments

    def _isValidSegments(self, segments):
        """Return True if *segments* is valid."""
       
----------
Solution: 

        parts = []
        model = self._filesystemWidget.model()

        # Separate root path from remainder.
        remainder = path

        while True:
            if remainder == model.root.path:
                break

            if remainder:
                parts.append(remainder)

            head, tail = os.path.split(remainder)
            if head == remainder:
                break

            remainder = head

        parts.append(model.root.path)
        return parts
--------------------
Prompt: 
def p_gate_op_5(self, program):
        """
        gate_op : BARRIER id_list ';'
        """
----------
Output: 

        program.append(GateOp(BARRIER, program[2]))

    def p_gate_op_6(self, program):
        """
        gate_op : MEASURE id_list '[' id_list ']' ';'
        """
        program.append(GateOp(MEASURE, program[2], program[4]))

    def p_gate_op_7(self, program):
        """
        gate_op : MEASURE id_list ';'
        """
        program.append(G
----------
Solution: 

        program[0] = node.Barrier([program[2]])
        self.verify_bit_list(program[2])
        self.verify_distinct([program[2]])
--------------------
Prompt: 
def luhn_check(card_number):
    """ checks to make sure that the card passes a luhn mod-10 checksum """
----------
Output: 

    if len(card_number) < 13:
        return False
    else:
        checksum = 0
        for i in range(len(card_number) - 1, -1, -1):
            digit = int(card_number[i])
            if (i % 2) == 0:
                digit *= 2
                if digit > 9:
                    digit -= 9
            checksum += digit
        return checksum % 10 == 0


def get_card_type(card
----------
Solution: 

    sum = 0
    num_digits = len(card_number)
    oddeven = num_digits & 1

    for count in range(0, num_digits):
        digit = int(card_number[count])

        if not ((count & 1) ^ oddeven):
            digit *= 2
        if digit > 9:
            digit -= 9

        sum += digit

    return (sum % 10) == 0
--------------------
Prompt: 
def _make_empty_queue_for(k, element):
  """Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.

  For example:

  ```python
    element = tf.constant([[0., 1., 2., 3., 4.],
                           [5., 6., 7., 8., 9.]])

    # A queue capable of holding 3 elements.
    _make_empty_queue_for(3, element)
    # => [[[ 0.,  0.,  0.,  0.,  0.],
    #      [ 0.,  0.,  0.,  0.,  0.]],
    #
    #     [[ 0.,  0.,  0.,  0.,  0.],
    #      [ 0.,  0.,  0.,  0.,  0.]],
    #
    #     [[ 0.,  0.,  0.,  0.,  0.],
    #      [ 0.,  0.,  0.,  0.,  0.]]]
  ```

  Args:
    k: A positive scalar integer, number of elements that each queue will hold.
    element: A `tf.Tensor`, only its shape and dtype information are relevant.

  Returns:
    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype
    as `element`.
  """
----------
Output: 
0., 1., 2., 3., 4.],
           [5., 6., 7., 8., 9.],
           [0., 0., 0., 0., 0.]],
          [[0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.]],
          [[0., 0., 0., 0., 0.],
           [0.,
----------
Solution: 

  queue_shape = tf.concat(
      [[k], distribution_util.prefer_static_shape(element)], axis=0)
  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)
--------------------
Prompt: 
def join(self):
        """Waits until all there are no processes enqueued."""
----------
Output: 

        self._queue.join()

    def _put(self, item):
        """Put an item into the queue."""
        self._queue.put(item)

    def _get(self):
        """Get an item from the queue."""
        return self._queue.get()

    def _put_nowait(self, item):
        """Put an item into the queue without blocking."""
        self._queue.put_nowait(item)

    def _get_nowait(self):
        """Get an item from
----------
Solution: 

        while True:
            with self._processes_lock:
                if not self._processes and self._processing_semaphore.locked():
                    return True
            gevent.sleep(0.1)
--------------------
Prompt: 
def phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype='l',**kwargs):
    """
    Calculates phase difference between X and Y feeds given U and V (U and Q for circular basis)
    data from a noise diode measurement on the target
    """
----------
Output: 

    # Calculate phase difference between X and Y feeds
    # This is done by calculating the phase difference between the U and V data
    # and then subtracting the phase difference between the U and Q data
    # The phase difference between U and Q is calculated by taking the difference
    # between the phase of the U and V data and then subtracting the phase of the
    # U and I data.
    # The phase difference between U and V is calculated by taking the difference
    # between the phase of the U and V data and then subtracting the phase of the
   
----------
Solution: 

    #Fold noise diode data and calculate ON OFF diferences for U and V
    if feedtype=='l':
        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)
        V_OFF,V_ON = foldcal(Vdat,tsamp,**kwargs)
        Udiff = U_ON-U_OFF
        Vdiff = V_ON-V_OFF
        poffset = np.arctan2(-1*Vdiff,Udiff)

    if feedtype=='c':
        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)
        Q_OFF,Q_ON = foldcal(Qdat,tsamp,**kwargs)
        Udiff = U_ON-U_OFF
        Qdiff = Q_ON-Q_OFF
        poffset = np.arctan2(Udiff,Qdiff)

    coarse_p =  convert_to_coarse(poffset,chan_per_coarse)

    #Correct for problems created by discontinuity in arctan
    #Find whether phase offsets have increasing or decreasing slope
    y = coarse_p[:6]
    x = np.arange(y.size)
    m = np.polyfit(x,y,1)[0]

    for i in range(coarse_p.size-3):
        if (m>0 and coarse_p[i+1]<coarse_p[i]) or (m<0 and coarse_p[i+1]>coarse_p[i]):
            coarse_p[i+1] = 2*coarse_p[i+2]-coarse_p[i+3]    #Move problem point near the next

    return coarse_p
--------------------
Prompt: 
def filter_on(self, filt=None, analyte=None, samples=None, subset=None, show_status=False):
        """
        Turns data filters on for particular analytes and samples.

        Parameters
        ----------
        filt : optional, str or array_like
            Name, partial name or list of names of filters. Supports
            partial matching. i.e. if 'cluster' is specified, all
            filters with 'cluster' in the name are activated.
            Defaults to all filters.
        analyte : optional, str or array_like
            Name or list of names of analytes. Defaults to all analytes.
        samples : optional, array_like or None
            Which samples to apply this filter to. If None, applies to all
            samples.

        Returns
        -------
        None
        """
----------
Output: 
te : optional, str or array_like
            Name, partial name or list of names of analytes. Supports
            partial matching. i.e. if 'cluster' is specified, all
            analytes with 'cluster' in the name are activated.
            Defaults to all analytes.
        samples : optional, str or array_like
            Name, partial name or list of names of samples. Supports
            partial matching. i.e. if 'cluster' is specified, all
            samples with 'cluster' in the name are activated.
            Defaults to
----------
Solution: 

        if samples is not None:
            subset = self.make_subset(samples)

        samples = self._get_samples(subset)

        for s in samples:
            try:
                self.data[s].filt.on(analyte, filt)
            except:
                warnings.warn("filt.on failure in sample " + s)

        if show_status:
            self.filter_status(subset=subset)
        return
--------------------
Prompt: 
def _add_https(self, q):
        """for push, pull, and other api interactions, the user can optionally
           define a custom registry. If the registry name doesn't include http
           or https, add it.
 
           Parameters
           ==========
           q: the parsed image query (names), including the original
        """
----------
Output: 

        if not q.registry:
            return
        if not q.registry.startswith("http"):
            q.registry = "https://" + q.registry

    def _add_tag(self, q):
        """for push, pull, and other api interactions, the user can optionally
           define a custom tag. If the tag is not defined, add it.
 
           Parameters
           ==========
           q: the parsed image query (names), including the original
        """
        if not q.tag:
           
----------
Solution: 


        # If image uses http or https, add back
        if not q['registry'].startswith('http'):

            if q['original'].startswith('http:'):
                q['registry'] = 'http://%s' % q['registry']

            elif q['original'].startswith('https:'):
                q['registry'] = 'https://%s' % q['registry']

            # Otherwise, guess from the user's environment
            else:

                prefix = 'https://'

                # The user can set an environment variable to specify nohttps
                nohttps = os.environ.get('SREGISTRY_REGISTRY_NOHTTPS')
                if nohttps != None:
                    prefix = 'http://'
                q['registry'] = '%s%s' %(prefix, q['registry'])

        return q
--------------------
Prompt: 
def clean_tempfiles():
  """Clean up temp files"""
----------
Output: 

  import glob
  import os
  import shutil
  import tempfile

  temp_dir = tempfile.gettempdir()
  for f in glob.glob(os.path.join(temp_dir, '*')):
    if os.path.isfile(f):
      os.remove(f)
    elif os.path.isdir(f):
      shutil.rmtree(f)


def get_temp_dir():
  """Get the temp dir"""
  import tempfile
  return tempfile.get
----------
Solution: 

  for fn in TEMP_FILES:
    if os.path.exists(fn):
      os.unlink(fn)
--------------------
Prompt: 
def get_section_by_sis_id(self, sis_section_id, params={}):
        """
        Return section resource for given sis id.
        """
----------
Output: 

        return self._get_section_by_id(sis_section_id, params=params)

    def get_section_by_id(self, section_id, params={}):
        """
        Return section resource for given id.
        """
        return self._get_section_by_id(section_id, params=params)

    def _get_section_by_id(self, section_id, params={}):
        """
        Return section resource for given id.
        """
        return self._get_section
----------
Solution: 

        return self.get_section(
            self._sis_id(sis_section_id, sis_field="section"), params)
--------------------
Prompt: 
async def get_tracks(self, *, limit=20, offset=0) -> List[Track]:
        """Get a list of the songs saved in the current Spotify user’s ‘Your Music’ library.

        Parameters
        ----------
        limit : Optional[int]
            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.
        offset : Optional[int]
            The index of the first item to return. Default: 0
        """
----------
Output: 

        return self.http.get_tracks(limit=limit, offset=offset)

    def get_track(self, track_id: str) -> Track:
        """Get Spotify catalog information for a single track identified by its unique Spotify ID.

        Parameters
        ----------
        track_id : str
            The Spotify ID for the track.
        """
        return self.http.get_track(track_id)

    def get_albums(self, *, limit=20, offset=0
----------
Solution: 

        data = await self.user.http.saved_tracks(limit=limit, offset=offset)

        return [Track(self.__client, item['track']) for item in data['items']]
--------------------
Prompt: 
def get_direct(self):
        """
        Allows direct access to the buffer element.
        Blocks until there is data that can be read.

        :return: A guard object that returns the buffer element.
        """
----------
Output: 

        return self.buffer.get()

    def get_direct_nowait(self):
        """
        Allows direct access to the buffer element.
        Returns None if there is no data that can be read.

        :return: A guard object that returns the buffer element.
        """
        return self.buffer.get_nowait()

    def get_all(self):
        """
        Allows direct access to the buffer elements.
        Blocks until there is data that can be read.

        :return: A guard object that returns
----------
Solution: 


        read_idx = self.__get_idx()

        if read_idx is QueueClosed:
            return QueueClosed

        # Once the guard is released, read_idx will be placed into write_queue.
        return self.Guard(self.write_queue, self.arys, lambda: read_idx)
--------------------
Prompt: 
def _get_format_from_style(self, token, style):
        """ Returns a QTextCharFormat for token by reading a Pygments style.
        """
----------
Output: 

        format = QTextCharFormat()
        format.setFont(self.font)
        format.setForeground(self.color_scheme.get_style_property(
            style, 'color', token))
        format.setBackground(self.color_scheme.get_style_property(
            style, 'bgcolor', token))
        format.setFontWeight(self.color_scheme.get_style_property(
            style, 'bold', token))
        format.setFontItalic(self.color_scheme.get_
----------
Solution: 

        result = QtGui.QTextCharFormat()
        for key, value in style.style_for_token(token).items():
            if value:
                if key == 'color':
                    result.setForeground(self._get_brush(value))
                elif key == 'bgcolor':
                    result.setBackground(self._get_brush(value))
                elif key == 'bold':
                    result.setFontWeight(QtGui.QFont.Bold)
                elif key == 'italic':
                    result.setFontItalic(True)
                elif key == 'underline':
                    result.setUnderlineStyle(
                        QtGui.QTextCharFormat.SingleUnderline)
                elif key == 'sans':
                    result.setFontStyleHint(QtGui.QFont.SansSerif)
                elif key == 'roman':
                    result.setFontStyleHint(QtGui.QFont.Times)
                elif key == 'mono':
                    result.setFontStyleHint(QtGui.QFont.TypeWriter)
        return result
--------------------
Prompt: 
def p_closed_proposition_list(self, p):
        """ closed_proposition_list :  closed_proposition_list SLASH SLASH closed_proposition
                                    | closed_proposition"""
----------
Output: 

        if len(p) == 5:
            p[0] = p[1] + [p[4]]
        else:
            p[0] = [p[1]]

    def p_closed_proposition(self, p):
        """ closed_proposition :  atom
                                | negation
                                | conjunction
                                | disjunction
                                | implication
                                | equivalence
                                | universal_quantification
                                | existential_quantification
                                | negated_existential_quantification
----------
Solution: 

        if len(p) == 2:
            p[0] = [p[1]]
        else:
            p[0] = p[1] + [p[4]]
--------------------
Prompt: 
def docker(self, method, *args, **kwargs):
        """Call a docker method in a background thread

        returns a Future
        """
----------
Output: 

        return self.docker_client.call(method, *args, **kwargs)

    def _get_container_name(self, container_id):
        """Get the container name from the container id

        returns a Future
        """
        return self.docker('inspect_container', container_id)

    def _get_container_image(self, container_id):
        """Get the container image from the container id

        returns a Future
        """
        return self.docker('inspect_container', container_id)

   
----------
Solution: 

        return self.executor.submit(self._docker, method, *args, **kwargs)
--------------------
Prompt: 
def restore_context(self) -> bool:
        """Rollback to previous saved position."""
----------
Output: 

        if self.context_stack:
            self.context = self.context_stack.pop()
            return True
        return False

    def save_context(self) -> bool:
        """Save current position."""
        if self.context:
            self.context_stack.append(self.context)
            return True
        return False

    def get_context(self) -> Optional[Context]:
        """Get current context."""
        return self.context

    def get_context_stack(self) -> List[Context]:

----------
Solution: 

        self._cursor.position = self._contexts.pop()
        return False
--------------------
Prompt: 
def wave_interp_option(obj):
    r"""
    Validate if an object is a :ref:`WaveInterpOption` pseudo-type object.

    :param obj: Object
    :type  obj: any

    :raises: RuntimeError (Argument \`*[argument_name]*\` is not valid). The
     token \*[argument_name]\* is replaced by the name of the argument the
     contract is attached to

    :rtype: None
    """
----------
Output: 

    if not isinstance(obj, WaveInterpOption):
        raise RuntimeError(
            "Argument `{}` is not valid".format(
                obj.__class__.__name__
            )
        )


def wave_interp_options(obj):
    r"""
    Validate if an object is a :ref:`WaveInterpOptions` pseudo-type object.

    :param obj: Object
    :type  obj: any

    :raises: RuntimeError (Argument \`*[argument_name]*\` is
----------
Solution: 

    exdesc = pexdoc.pcontracts.get_exdesc()
    if not isinstance(obj, str):
        raise ValueError(exdesc)
    if obj.upper() in ["CONTINUOUS", "STAIRCASE"]:
        return None
    raise ValueError(exdesc)
--------------------
Prompt: 
def run(self, dag):
        """
        Pick a layout by assigning n circuit qubits to device qubits 0, .., n-1.

        Args:
            dag (DAGCircuit): DAG to find layout for.

        Raises:
            TranspilerError: if dag wider than self.coupling_map
        """
----------
Output: 

        if len(dag.qubits) > len(self.coupling_map.get_all_qubits()):
            raise TranspilerError("Layout pass requires a coupling map with at least as many qubits as the "
                                  "DAG has qubits.")

        # initialize empty layout
        layout = Layout()

        # assign qubits to qubits in coupling map
        for qubit in dag.qubits:
            layout[qubit] = self.coupling_map.get_qubit_by_index(qubit.index
----------
Solution: 

        num_dag_qubits = sum([qreg.size for qreg in dag.qregs.values()])
        if num_dag_qubits > self.coupling_map.size():
            raise TranspilerError('Number of qubits greater than device.')
        self.property_set['layout'] = Layout.generate_trivial_layout(*dag.qregs.values())
--------------------
Prompt: 
def update(self,dt):
        """
        Should be called regularly to move the actor.
        
        This method does nothing if the :py:attr:`enabled` property is set to False.
        
        This method is called automatically and should not be called manually.
        """
----------
Output: 

        if not self.enabled:
            return
        
        # update position
        self.position += self.velocity*dt
        
        # update rotation
        self.rotation += self.angular_velocity*dt
        
        # update scale
        self.scale += self.scale_velocity*dt
        
        # update color
        self.color += self.color_velocity*dt
        
        # update alpha
        self.alpha += self.alpha_velocity*dt
        
        # update alpha
        self.alpha +=
----------
Solution: 

        if not self.enabled:
            return
        dy = self.speed * dt * self.move
        x,y,z = self.actor._pos
        newpos = x,dy+y,z
        self.actor.pos = newpos
--------------------
Prompt: 
def _start_webserver(self, authorize_url=None):
		"""
		Start the webserver that will receive the code
		"""
----------
Output: 

		self.webserver = WebServer(self.config, authorize_url)
		self.webserver.start()

	def _stop_webserver(self):
		"""
		Stop the webserver
		"""
		self.webserver.stop()

	def _get_authorize_url(self):
		"""
		Get the authorize url
		"""
		return self.webserver.get_authorize_url()

	def _get_access_token(
----------
Solution: 

		server_address = (SERVER_URL, SERVER_PORT)
		self.server = HTTPServer(server_address, OAuth2UtilRequestHandler)
		self.server.response_code = None
		self.server.authorize_url = authorize_url
		t = Thread(target=self.server.serve_forever)
		t.daemon = True
		t.start()
--------------------
Prompt: 
def transcode_to_stream(input_filename, date_format=None):
    """
    Read a JSON or CSV file and convert it into a JSON stream, which will
    be saved in an anonymous temp file.
    """
----------
Output: 

    if input_filename.endswith('.json'):
        return transcode_json_to_stream(input_filename, date_format)
    elif input_filename.endswith('.csv'):
        return transcode_csv_to_stream(input_filename, date_format)
    else:
        raise ValueError('Unknown file extension: %s' % input_filename)


def transcode_json_to_stream(input_filename, date_format=None):
    """
    Read a JSON file and convert it into a JSON stream, which will
----------
Solution: 

    tmp = tempfile.TemporaryFile()
    for entry in open_json_or_csv_somehow(input_filename,
                                          date_format=date_format):
        tmp.write(json.dumps(entry, ensure_ascii=False).encode('utf-8'))
        tmp.write(b'\n')
    tmp.seek(0)
    return tmp
--------------------
Prompt: 
def visit_classdef(self, node):
        """visit an astroid.Class node

         * set the locals_type and instance_attrs_type mappings
         * set the implements list and build it
         * optionally tag the node with a unique id
        """
----------
Output: 

        self.locals_type = self.locals_type.copy()
        self.instance_attrs_type = self.instance_attrs_type.copy()
        self.implements = []
        self.class_name = node.name
        if node.parent:
            self.class_name = node.parent.name + '.' + node.name
        if node.parent and isinstance(node.parent, astroid.Class):
            self.implements.append(node.parent.name)
        if node.parent and is
----------
Solution: 

        if hasattr(node, "locals_type"):
            return
        node.locals_type = collections.defaultdict(list)
        if self.tag:
            node.uid = self.generate_id()
        # resolve ancestors
        for baseobj in node.ancestors(recurs=False):
            specializations = getattr(baseobj, "specializations", [])
            specializations.append(node)
            baseobj.specializations = specializations
        # resolve instance attributes
        node.instance_attrs_type = collections.defaultdict(list)
        for assignattrs in node.instance_attrs.values():
            for assignattr in assignattrs:
                self.handle_assignattr_type(assignattr, node)
        # resolve implemented interface
        try:
            node.implements = list(interfaces(node, self.inherited_interfaces))
        except astroid.InferenceError:
            node.implements = ()
--------------------
Prompt: 
def _create_input_transactions(self, addy):
        # type: (Address) -> None
        """
        Creates transactions for the specified input address.
        """
----------
Output: 

        # Create a transaction for each output address.
        for output_address in self.output_addresses:
            # Create a transaction for each output address.
            tx = Transaction(
                inputs=[
                    TxInput(
                        address=addy,
                        value=self.output_addresses[output_address],
                        script=self.output_addresses[output_address].script,
                    )
                ],
                outputs=[
                    TxOutput(
                        address=output_address,
                        value=self
----------
Solution: 

        self._transactions.append(ProposedTransaction(
            address=addy,
            tag=self.tag,

            # Spend the entire address balance; if necessary, we will
            # add a change transaction to the bundle.
            value=-addy.balance,
        ))

        # Signatures require additional transactions to store, due to
        # transaction length limit.
        # Subtract 1 to account for the transaction we just added.
        for _ in range(addy.security_level - 1):
            self._transactions.append(ProposedTransaction(
                address=addy,
                tag=self.tag,

                # Note zero value; this is a meta transaction.
                value=0,
            ))
--------------------
Prompt: 
def _define(self):
        """Calculate a subcircuit that implements this initialization

        Implements a recursive initialization algorithm, including optimizations,
        from "Synthesis of Quantum Logic Circuits" Shende, Bullock, Markov
        https://arxiv.org/abs/quant-ph/0406176v5

        Additionally implements some extra optimizations: remove zero rotations and
        double cnots.
        """
----------
Output: 

        # pylint: disable=too-many-branches
        # pylint: disable=too-many-statements
        # pylint: disable=too-many-locals
        # pylint: disable=too-many-nested-blocks
        # pylint: disable=too-many-return-statements
        # pylint: disable=too-many-arguments
        # pylint: disable=too-many-lines
        # pylint: disable=too-many-
----------
Solution: 

        # call to generate the circuit that takes the desired vector to zero
        disentangling_circuit = self.gates_to_uncompute()

        # invert the circuit to create the desired vector from zero (assuming
        # the qubits are in the zero state)
        initialize_instr = disentangling_circuit.to_instruction().inverse()

        q = QuantumRegister(self.num_qubits, 'q')
        initialize_circuit = QuantumCircuit(q, name='init_def')
        for qubit in q:
            initialize_circuit.append(Reset(), [qubit])
        initialize_circuit.append(initialize_instr, q[:])

        self.definition = initialize_circuit.data
--------------------
Prompt: 
def generate_query(command):
    """Add header, checksum and footer to command data."""
----------
Output: 

    command_data = command.get_data()
    header = command.get_header()
    footer = command.get_footer()
    checksum = command.get_checksum()
    return header + command_data + checksum + footer


def parse_query(query):
    """Parse query data and return command object."""
    header = query[:2]
    command_data = query[2:-2]
    checksum = query[-2:]
    footer = query[-1]
    command = Command(header, command_data,
----------
Solution: 

    data = bytearray(command)
    c = checksum(data)
    data.append(c >> 8)
    data.append(c & 0xFF)
    data.replace(b'\xFE', b'\xFE\xF0')

    data = bytearray.fromhex("FEFE") + data + bytearray.fromhex("FE0D")
    return data
--------------------
Prompt: 
def copy(self):
        """
        :returns: datasketch.MinHash -- A copy of this MinHash by exporting its state.
        """
----------
Output: 

        return MinHash(self.seed, self.hash_type, self.hash_set)

    def __eq__(self, other):
        """
        :param other: datasketch.MinHash -- The other MinHash to compare with.
        :returns: bool -- True if the two MinHashes are equal, False otherwise.
        """
        if not isinstance(other, MinHash):
            return False
        return self.hash_set == other.hash_set

    def __ne__(self, other):
        """
        :param
----------
Solution: 

        return MinHash(seed=self.seed, hashfunc=self.hashfunc,
                hashvalues=self.digest(),
                permutations=self.permutations)
--------------------
Prompt: 
def publish(
            self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):
        """Publish runbook draft.

        :param resource_group_name: Name of an Azure Resource group.
        :type resource_group_name: str
        :param automation_account_name: The name of the automation account.
        :type automation_account_name: str
        :param runbook_name: The parameters supplied to the publish runbook
         operation.
        :type runbook_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: The poller return type is ClientRawResponse, the
         direct response alongside the deserialized response
        :param polling: True for ARMPolling, False for no polling, or a
         polling object for personal polling strategy
        :return: An instance of LROPoller that returns None or
         ClientRawResponse<None> if raw==True
        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or
         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]
        :raises:
         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`
        """
----------
Output: 
the publish runbook
         operation.
        :type runbook_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: The poller return type is ClientRawResponse, the
         direct response alongside the deserialized response
        :param polling: True for ARMPolling, False for no polling, or a
         polling object for personal polling strategy
        :return: An instance of LROPoller that returns None or
         ClientRawResponse<None> if raw==True
        :rtype
----------
Solution: 

        raw_result = self._publish_initial(
            resource_group_name=resource_group_name,
            automation_account_name=automation_account_name,
            runbook_name=runbook_name,
            custom_headers=custom_headers,
            raw=True,
            **operation_config
        )

        def get_long_running_output(response):
            if raw:
                client_raw_response = ClientRawResponse(None, response)
                client_raw_response.add_headers({
                    'location': 'str',
                })
                return client_raw_response

        lro_delay = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)
        elif polling is False: polling_method = NoPolling()
        else: polling_method = polling
        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)
--------------------
Prompt: 
def _is_async(o: IMeta) -> bool:
    """Return True if the meta contains :async keyword."""
----------
Output: 

    return o.meta.get("async", False)


def _is_sync(o: IMeta) -> bool:
    """Return True if the meta contains :sync keyword."""
    return o.meta.get("sync", False)


def _is_async_or_sync(o: IMeta) -> bool:
    """Return True if the meta contains :async or :sync keyword."""
    return _is_async(o) or _is_sync(o)


def _is_async_or_sync_or_
----------
Solution: 

    return (  # type: ignore
        Maybe(o.meta)
        .map(lambda m: m.entry(SYM_ASYNC_META_KEY, None))
        .or_else_get(False)
    )
--------------------
Prompt: 
def add_value(self, name, value):
        """
        Add a new value to the list.

        :param str name: name of the value that is being parsed
        :param str value: value that is being parsed
        :raises ietfparse.errors.MalformedLinkValue:
            if *strict mode* is enabled and a validation error
            is detected

        This method implements most of the validation mentioned in
        sections 5.3 and 5.4 of :rfc:`5988`.  The ``_rfc_values``
        dictionary contains the appropriate values for the attributes
        that get special handling.  If *strict mode* is enabled, then
        only values that are acceptable will be added to ``_values``.

        """
----------
Output: 
5988_validate``
        method is called to validate the value.  If the value is
        valid, it is added to the list of values.

        """
        if self._strict and not self._rfc5988_validate(name, value):
            raise errors.MalformedLinkValue(
                'Invalid value for {0}: {1}'.format(name, value))
        self._values.append(value)

    def _rfc5988_validate(self, name, value):
        """
        Valid
----------
Solution: 

        try:
            if self._rfc_values[name] is None:
                self._rfc_values[name] = value
            elif self.strict:
                if name in ('media', 'type'):
                    raise errors.MalformedLinkValue(
                        'More than one {} parameter present'.format(name))
                return
        except KeyError:
            pass

        if self.strict and name in ('title', 'title*'):
            return

        self._values.append((name, value))
--------------------
Prompt: 
def sp_search_query(query):
    """Translate a Mopidy search query to a Spotify search query"""
----------
Output: 

    if query.get('any'):
        return '*'
    if query.get('artist'):
        return ' '.join(query['artist'])
    if query.get('album'):
        return ' '.join(query['album'])
    if query.get('track'):
        return ' '.join(query['track'])
    if query.get('genre'):
        return ' '.join(query['genre'])
    if query.get('composer'):
        return ' '.join(query['composer'])
    if query.get('
----------
Solution: 


    result = []

    for (field, values) in query.items():
        field = SEARCH_FIELD_MAP.get(field, field)
        if field is None:
            continue

        for value in values:
            if field == 'year':
                value = _transform_year(value)
                if value is not None:
                    result.append('%s:%d' % (field, value))
            elif field == 'any':
                result.append('"%s"' % value)
            else:
                result.append('%s:"%s"' % (field, value))

    return ' '.join(result)
--------------------
Prompt: 
def summary(self, fullpath=False):
        """Return a dict summarizing the coverage data.

        Keys are based on the filenames, and values are the number of executed
        lines.  If `fullpath` is true, then the keys are the full pathnames of
        the files, otherwise they are the basenames of the files.

        """
----------
Output: 

        if fullpath:
            return self._data
        else:
            return {os.path.basename(f): v for f, v in self._data.items()}

    def get_coverage_data(self, fullpath=False):
        """Return a dict summarizing the coverage data.

        Keys are based on the filenames, and values are the number of executed
        lines.  If `fullpath` is true, then the keys are the full pathnames of
        the files, otherwise they are the basenames of the files
----------
Solution: 

        summ = {}
        if fullpath:
            filename_fn = lambda f: f
        else:
            filename_fn = os.path.basename
        for filename, lines in iitems(self.lines):
            summ[filename_fn(filename)] = len(lines)
        return summ
--------------------
Prompt: 
def read_hex_integer(self) -> bool:
    """
    read a hexadecimal number
    Read the following BNF rule else return False::

        readHexInteger = [
            [ '0'..'9' | 'a'..'f' | 'A'..'F' ]+
        ]
    """
----------
Output: 

    if self.current_char == '0':
        self.advance()
        if self.current_char in 'xX':
            self.advance()
            while self.current_char in '0123456789abcdefABCDEF':
                self.advance()
            return True
        else:
            return False
    else:
        return False


def read_octal_integer(self) -> bool:
    """
    read an octal number
    Read the following BNF rule else return False
----------
Solution: 

    if self.read_eof():
        return False
    self._stream.save_context()
    c = self._stream.peek_char
    if c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f'):
        self._stream.incpos()
        while not self.read_eof():
            c = self._stream.peek_char
            if not (c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f')):
                break
            self._stream.incpos()
        return self._stream.validate_context()
    return self._stream.restore_context()
--------------------
Prompt: 
def init_app(self, app):
        """
        Initializes the Flask-Allows object against the provided application
        """
----------
Output: 

        self.app = app
        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE', 'default')
        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_ID', 1)
        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_NAME', 'default')
        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_DESCRIPTION', 'default')
        self.app.config.setdefault('ALLOWS_DEFAULT_ROLE_
----------
Solution: 

        if not hasattr(app, "extensions"):  # pragma: no cover
            app.extensions = {}
        app.extensions["allows"] = self

        @app.before_request
        def start_context(*a, **k):
            self.overrides.push(Override())
            self.additional.push(Additional())

        @app.after_request
        def cleanup(response):
            self.clear_all_overrides()
            self.clear_all_additional()
            return response
--------------------
Prompt: 
def raw_dict_factory(cursor, row):
        """ Sqlite callback accepting the cursor and the original row as a tuple.

        Simple return of JSON safe types, including raw read hex strings.

        Args:
            cursor (sqlite cursor):  Original cursory
            row (sqlite row tuple): Original row.

        Returns:
            dict: modified row.
        """
----------
Output: 

        d = {}
        for idx, col in enumerate(cursor.description):
            d[col[0]] = row[idx]
        return d


def raw_list_factory(cursor, row):
    """ Sqlite callback accepting the cursor and the original row as a tuple.

    Simple return of JSON safe types, including raw read hex strings.

    Args:
        cursor (sqlite cursor):  Original cursory
        row (sqlite row tuple): Original row.

    Returns:
        list: modified row.
    """
    return
----------
Solution: 

        d = {}
        for idx, col in enumerate(cursor.description):
            val = row[idx]
            name = col[0]
            if name == Field.Time_Stamp or name == Field.Meter_Address:
                d[name] = str(val)
                continue
            if name == "Raw_A" or name == "Raw_B":
                d[name] = str(val)
                continue
        return d
--------------------
Prompt: 
def _onSelectItem(self, selection, previousSelection):
        """Handle selection of item in listing."""
----------
Output: 

        if selection is None:
            return
        if selection == previousSelection:
            return
        if selection.isFolder:
            self._onSelectFolder(selection)
        else:
            self._onSelectFile(selection)

    def _onSelectFolder(self, folder):
        """Handle selection of folder in listing."""
        self.folder = folder
        self.refresh()

    def _onSelectFile(self, file):
        """Handle selection of file in listing."""
        self.file = file
        self.refresh()

----------
Solution: 

        self._acceptButton.setEnabled(True)
        del self._selected[:]
        item = self._filesystemWidget.model().item(selection)
        self._selected.append(item.path)
--------------------
Prompt: 
def _srvc_set_config(self, trajectory):
        """Sets a config value to the Trajectory or changes it if the trajectory was loaded
        a the settings no longer match"""
----------
Output: 

        if self.trajectory is None:
            self.trajectory = trajectory
        elif self.trajectory.settings != trajectory.settings:
            self.trajectory = trajectory

    def _srvc_get_config(self):
        """Returns the config value of the Trajectory"""
        return self.trajectory.settings

    def _srvc_get_trajectory(self):
        """Returns the Trajectory"""
        return self.trajectory

    def _srvc
----------
Solution: 

        def _set_config(name, value, comment):
            if not trajectory.f_contains('config.'+name, shortcuts=False):
                trajectory.f_add_config(Parameter, name, value, comment=comment)

        for attr_name in HDF5StorageService.NAME_TABLE_MAPPING:
            table_name = HDF5StorageService.NAME_TABLE_MAPPING[attr_name]
            value = getattr(self, attr_name)
            _set_config('hdf5.overview.' + table_name,
                         value,
                         comment='Whether or not to have an overview '
                                 'table with that name')

        _set_config('hdf5.purge_duplicate_comments',
                    self._purge_duplicate_comments,
                    comment='Whether comments of results and'
                            ' derived parameters should only'
                            ' be stored for the very first instance.'
                            ' Works only if the summary tables are'
                            ' active.')

        _set_config('hdf5.results_per_run', self._results_per_run,
                    comment='Expected number of results per run,'
                            ' a good guess can increase storage performance')

        _set_config('hdf5.derived_parameters_per_run',
                    self._derived_parameters_per_run,
                    comment='Expected number of derived parameters per run,'
                            ' a good guess can increase storage performance')

        _set_config('hdf5.complevel', self._complevel,
                    comment='Compression Level (0 no compression '
                            'to 9 highest compression)')

        _set_config('hdf5.complib', self._complib,
                    comment='Compression Algorithm')

        _set_config('hdf5.encoding', self._encoding,
                    comment='Encoding for unicode characters')

        _set_config('hdf5.fletcher32', self._fletcher32,
                    comment='Whether to use fletcher 32 checksum')

        _set_config('hdf5.shuffle', self._shuffle,
                    comment='Whether to use shuffle filtering.')

        _set_config('hdf5.pandas_format', self._pandas_format,
                    comment=
--------------------
Prompt: 
def draw(self):
        """
        Draws all layers of this LayeredWidget.
        
        This should normally be unneccessary, since it is recommended that layers use Vertex Lists instead of OpenGL Immediate Mode.
        """
----------
Output: 

        for layer in self.layers:
            layer.draw()

    def add_layer(self, layer):
        """
        Adds a layer to this LayeredWidget.
        
        The layer will be drawn after all other layers.
        """
        self.layers.append(layer)

    def remove_layer(self, layer):
        """
        Removes a layer from this LayeredWidget.
        
        The layer will no longer be drawn.
        """
        self.layers.remove(layer)

    def
----------
Solution: 

        super(LayeredWidget,self).draw()
        for layer,_ in self.layers:
            layer._draw()
--------------------
Prompt: 
def get_exitcodes_terminated_worker(processes):
    """Return a formated string with the exitcodes of terminated workers.

    If necessary, wait (up to .25s) for the system to correctly set the
    exitcode of one terminated worker.
    """
----------
Output: 

    exitcodes = []
    for p in processes:
        if p.exitcode is None:
            p.wait(.25)
        exitcodes.append(p.exitcode)
    return "exitcodes: {}".format(exitcodes)


def get_exitcodes_terminated_workers(processes):
    """Return a formated string with the exitcodes of terminated workers.

    If necessary, wait (up to .25s) for the system to correctly set the
    exitcode of one terminated worker.
    """
   
----------
Solution: 

    patience = 5

    # Catch the exitcode of the terminated workers. There should at least be
    # one. If not, wait a bit for the system to correctly set the exitcode of
    # the terminated worker.
    exitcodes = [p.exitcode for p in list(processes.values())
                 if p.exitcode is not None]
    while len(exitcodes) == 0 and patience > 0:
        patience -= 1
        exitcodes = [p.exitcode for p in list(processes.values())
                     if p.exitcode is not None]
        time.sleep(.05)

    return _format_exitcodes(exitcodes)
--------------------
Prompt: 
def eintr_retry(exc_type, f, *args, **kwargs):
    """Calls a function.  If an error of the given exception type with
    interrupted system call (EINTR) occurs calls the function again.
    """
----------
Output: 

    while True:
        try:
            return f(*args, **kwargs)
        except exc_type, e:
            if e.errno != errno.EINTR:
                raise

def get_file_size(path):
    """Returns the size of the file in bytes."""
    return os.stat(path).st_size

def get_file_mtime(path):
    """Returns the modification time of the file."""
    return os.stat(path).st_mtime

def get_file_
----------
Solution: 

    while True:
        try:
            return f(*args, **kwargs)
        except exc_type as exc:
            if exc.errno != EINTR:
                raise
        else:
            break
--------------------
Prompt: 
def _colorify(self, data):
        """
        Retun colored string.

        :param data: The string to colorify.
        :type data: str

        :return: A colored string.
        :rtype: str
        """
----------
Output: 

        return self.color + data + self.reset

    def _colorify_list(self, data):
        """
        Colorify a list of strings.

        :param data: The list of strings to colorify.
        :type data: list

        :return: A list of colored strings.
        :rtype: list
        """
        return [self._colorify(item) for item in data]

    def _colorify_dict(self, data):
        """
        Colorify a dictionary of strings.

        :
----------
Solution: 


        if self.template in ["Generic", "Less"]:
            # The template is in the list of template that need the coloration.

            if (
                self.data_to_print[1].lower() in PyFunceble.STATUS["list"]["up"]
                or self.data_to_print[1].lower() in PyFunceble.STATUS["list"]["valid"]
            ):
                # The status is in the list of up status.

                # We print the data with a green background.
                data = PyFunceble.Fore.BLACK + PyFunceble.Back.GREEN + data
            elif self.data_to_print[1].lower() in PyFunceble.STATUS["list"]["down"]:
                # The status is in the list of down status.

                # We print the data with a red background.
                data = PyFunceble.Fore.BLACK + PyFunceble.Back.RED + data
            else:
                # The status is not in the list of up and down status.

                # We print the data with a cyan background.
                data = PyFunceble.Fore.BLACK + PyFunceble.Back.CYAN + data

        # We return the data.
        return data
--------------------
Prompt: 
def __eof_qubit(rho):
    """
    Compute the Entanglement of Formation of a 2-qubit density matrix.

    Args:
        rho ((array_like): (4,4) array_like, input density matrix.

    Returns:
        float: The entanglement of formation.
    """
----------
Output: 

    rho = np.asarray(rho, dtype=complex)
    if rho.shape != (4, 4):
        raise ValueError("Input density matrix must be 4x4.")
    if not is_density_matrix(rho):
        raise ValueError("Input density matrix is not a valid density matrix.")
    if not is_trace_preserving(rho):
        raise ValueError("Input density matrix is not trace preserving.")

    # Compute the entanglement of formation
    eof = 0.0
    for i
----------
Solution: 

    c = concurrence(rho)
    c = 0.5 + 0.5 * np.sqrt(1 - c * c)
    return shannon_entropy([c, 1 - c])
--------------------
Prompt: 
def compile_sympy_accesses(self, sources=True, destinations=True):
        """
        Return a dictionary of lists of sympy accesses, for each variable.

        Use *source* and *destination* to filter output
        """
----------
Output: 

        accesses = {}
        for var in self.variables:
            accesses[var] = []
        for access in self.accesses:
            if sources and access.source not in accesses:
                continue
            if destinations and access.destination not in accesses:
                continue
            accesses[access.source].append(access)
            accesses[access.destination].append(access)
        return accesses

    def compile_sympy_accesses_by_source(self):
        """
        Return a dictionary of lists of sym
----------
Solution: 

        sympy_accesses = defaultdict(list)
        # Compile sympy accesses
        for var_name in self.variables:
            if sources:
                for r in self.sources.get(var_name, []):
                    if r is None:
                        continue
                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))
            if destinations:
                for w in self.destinations.get(var_name, []):
                    if w is None:
                        continue
                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))

        return sympy_accesses
--------------------
Prompt: 
def deserialize(bstr):
    """
    Attempts to deserialize a bytestring into an audiosegment.

    :param bstr: The bytestring serialized via an audiosegment's serialize() method.
    :returns: An AudioSegment object deserialized from `bstr`.
    """
----------
Output: 

    import io
    import wave
    import struct
    import numpy as np
    import soundfile as sf

    # Create a wave object from the bytestring.
    w = wave.open(io.BytesIO(bstr), 'rb')

    # Get the format of the wave object.
    nchannels, sampwidth, framerate, nframes, comptype, compname = w.getparams()

    # Read the wave object into a numpy array.
    data = np.frombuffer(w.readframes(nframes),
----------
Solution: 

    d = pickle.loads(bstr)
    seg = pickle.loads(d['seg'])
    return AudioSegment(seg, d['name'])
--------------------
Prompt: 
def create_validator(data_struct_dict, name=None):
    """
    create a Validator instance from data_struct_dict

    :param data_struct_dict: a dict describe validator's fields, like the dict `to_dict()` method returned.
    :param name: name of Validator class 

    :return: Validator instance
    """
----------
Output: 

    if name is None:
        name = data_struct_dict.get('name', None)
    if name is None:
        raise ValueError('name is required')
    validator_class = globals()[name]
    return validator_class(**data_struct_dict)


def create_validators(data_struct_dicts, name=None):
    """
    create a list of Validator instances from data_struct_dicts

    :param data_struct_dicts: a list of dict describe validator's
----------
Solution: 


    if name is None:
        name = 'FromDictValidator'
    attrs = {}
    for field_name, field_info in six.iteritems(data_struct_dict):
        field_type = field_info['type']
        if field_type == DictField.FIELD_TYPE_NAME and isinstance(field_info.get('validator'), dict):
            field_info['validator'] = create_validator(field_info['validator'])
        attrs[field_name] = create_field(field_info)
    name = force_str(name)
    return type(name, (Validator, ), attrs)
--------------------
Prompt: 
def _store(self, lines, buffer=None, store='source'):
        """Store one or more lines of input.

        If input lines are not newline-terminated, a newline is automatically
        appended."""
----------
Output: 

        if buffer is None:
            buffer = self.buffer
        if not lines:
            return
        if not isinstance(lines, (list, tuple)):
            lines = [lines]
        if not lines[-1].endswith('\n'):
            lines[-1] += '\n'
        buffer.write(''.join(lines))
        buffer.seek(0)
        self.input_lines.append(lines)
        self.input_line_no += len(lines)
        if store == 'source':
            self.source_lines
----------
Solution: 

        
        if buffer is None:
            buffer = self._buffer
            
        if lines.endswith('\n'):
            buffer.append(lines)
        else:
            buffer.append(lines+'\n')
        setattr(self, store, self._set_source(buffer))
--------------------
Prompt: 
def selection_redo(self, name="default", executor=None):
        """Redo selection, for the name."""
----------
Output: 

        return self._redo_selection(name, executor)

    def _redo_selection(self, name="default", executor=None):
        """Redo selection, for the name."""
        if executor is None:
            executor = self.executor
        if name not in self.selection_history:
            return
        if self.selection_history[name] is None:
            return
        self.selection_history[name] = self.selection_history[name].redo(executor)
        self.selection_
----------
Solution: 

        logger.debug("redo")
        executor = executor or self.executor
        assert self.selection_can_redo(name=name)
        selection_history = self.selection_histories[name]
        index = self.selection_history_indices[name]
        next = selection_history[index + 1]
        self.selection_history_indices[name] += 1
        self.signal_selection_changed.emit(self)
        logger.debug("redo: selection history is %r, index is %r", selection_history, index)
--------------------
Prompt: 
def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET):
        """Fetch questions from the Kitsune url.

        :param category: the category of items to fetch
        :offset: obtain questions after offset
        :returns: a generator of questions
        """
----------
Output: 

        if category not in CATEGORIES:
            raise ValueError("Invalid category: %s" % category)

        url = self.url + category
        if offset:
            url += "?offset=%s" % offset

        response = self.session.get(url)
        response.raise_for_status()
        return self._parse_questions(response.text)

    def _parse_questions(self, html):
        """Parse questions from the html.

        :param html: the html to parse
        :returns:
----------
Solution: 

        if not offset:
            offset = DEFAULT_OFFSET

        kwargs = {"offset": offset}
        items = super().fetch(category, **kwargs)

        return items
--------------------
Prompt: 
def make_reports(self, stats, old_stats):
        """render registered reports"""
----------
Output: 

        for report in self.reports:
            report.render(stats, old_stats)

    def make_report(self, report_name, stats, old_stats):
        """render a specific report"""
        for report in self.reports:
            if report.name == report_name:
                report.render(stats, old_stats)

    def make_report_for_test(self, report_name, test_name, stats, old_stats):
        """render a specific report for a specific test"""
        for
----------
Solution: 

        sect = Section("Report", "%s statements analysed." % (self.stats["statement"]))
        for checker in self.report_order():
            for reportid, r_title, r_cb in self._reports[checker]:
                if not self.report_is_enabled(reportid):
                    continue
                report_sect = Section(r_title)
                try:
                    r_cb(report_sect, stats, old_stats)
                except EmptyReportError:
                    continue
                report_sect.report_id = reportid
                sect.append(report_sect)
        return sect
--------------------
Prompt: 
def format_lines(statements, lines):
    """Nicely format a list of line numbers.

    Format a list of line numbers for printing by coalescing groups of lines as
    long as the lines represent consecutive statements.  This will coalesce
    even if there are gaps between statements.

    For example, if `statements` is [1,2,3,4,5,10,11,12,13,14] and
    `lines` is [1,2,5,10,11,13,14] then the result will be "1-2, 5-11, 13-14".

    """
----------
Output: 
12,13,14], then the result will be
    [1-3,4,10-14].

    Args:
      statements: A list of line numbers.
      lines: A list of line numbers.

    Returns:
      A list of strings.
    """
    result = []
    i = 0
    j = 0
    while i < len(statements):
        if statements[i] == lines[j]:
            start = statements[i]
            while i < len(statements) and
----------
Solution: 

    pairs = []
    i = 0
    j = 0
    start = None
    statements = sorted(statements)
    lines = sorted(lines)
    while i < len(statements) and j < len(lines):
        if statements[i] == lines[j]:
            if start == None:
                start = lines[j]
            end = lines[j]
            j += 1
        elif start:
            pairs.append((start, end))
            start = None
        i += 1
    if start:
        pairs.append((start, end))
    ret = ', '.join(map(nice_pair, pairs))
    return ret
--------------------
Prompt: 
def _check_function(self):
        """ make some basic checks on the function to make sure it is valid"""
----------
Output: 

        if not self.function:
            raise ValueError("No function specified")
        if not callable(self.function):
            raise ValueError("Function is not callable")
        if not self.function.__name__:
            raise ValueError("Function has no name")

    def _check_arguments(self):
        """ make some basic checks on the arguments to make sure they are valid"""
        if not self.arguments:
            raise ValueError("No arguments specified")
        if not isinstance(self.arguments, list):
            raise ValueError
----------
Solution: 

        # note, callable is valid for Python 2 and Python 3.2 onwards but
        # not inbetween
        if not callable(self._function):
            raise RuntimeError(
                "provided function '{0}' is not callable".
                format(str(self._function)))
        from inspect import getargspec
        arg_info = getargspec(self._function)
        if len(arg_info.args) != 1:
            print str(arg_info)
            raise RuntimeError(
                "provided function should have one argument but found "
                "{0}".format(len(arg_info.args)))
--------------------
Prompt: 
def span(self, index):
        """Give the range of possible values in a tuple
        Useful for mnemonic and explanation
        """
----------
Output: 

        return self.tuples[index][1] - self.tuples[index][0]

    def __getitem__(self, index):
        """Get the value of a tuple
        """
        return self.tuples[index][0]

    def __setitem__(self, index, value):
        """Set the value of a tuple
        """
        self.tuples[index][0] = value

    def __len__(self):
        """Get the number of tuples
        """
        return len(self.tuples)
----------
Solution: 

        lower = self.value0+sum(1<<x for x in self.extraTable[:index])
        upper = lower+(1<<self.extraTable[index])
        return lower, upper-1
--------------------
Prompt: 
def paid_invoices_by_date(request, form):
    """ Shows the number of paid invoices containing given products or
    categories per day. """
----------
Output: 

    if form.is_valid():
        data = form.cleaned_data
        if data['start_date'] > data['end_date']:
            messages.error(request, _('Start date must be before end date.'))
            return redirect('paid_invoices_by_date')
        if data['start_date'] == data['end_date']:
            messages.error(request, _('Start date and end date must be different.'))
            return redirect('paid_invoices_by_date')
        if data
----------
Solution: 


    products = form.cleaned_data["product"]
    categories = form.cleaned_data["category"]

    invoices = commerce.Invoice.objects.filter(
        (
            Q(lineitem__product__in=products) |
            Q(lineitem__product__category__in=categories)
        ),
        status=commerce.Invoice.STATUS_PAID,
    )

    # Invoices with payments will be paid at the time of their latest payment
    payments = commerce.PaymentBase.objects.all()
    payments = payments.filter(
        invoice__in=invoices,
    )
    payments = payments.order_by("invoice")
    invoice_max_time = payments.values("invoice").annotate(
        max_time=Max("time")
    )

    # Zero-value invoices will have no payments, so they're paid at issue time
    zero_value_invoices = invoices.filter(value=0)

    times = itertools.chain(
        (line["max_time"] for line in invoice_max_time),
        (invoice.issue_time for invoice in zero_value_invoices),
    )

    by_date = collections.defaultdict(int)
    for time in times:
        date = datetime.datetime(
            year=time.year, month=time.month, day=time.day
        )
        by_date[date] += 1

    data = [(date_, count) for date_, count in sorted(by_date.items())]
    data = [(date_.strftime("%Y-%m-%d"), count) for date_, count in data]

    return ListReport(
        "Paid Invoices By Date",
        ["date", "count"],
        data,
    )
--------------------
Prompt: 
def get(m, k, default=None):
    """Return the value of k in m. Return default if k not found in m."""
----------
Output: 

    return m.get(k, default)


def setdefault(m, k, default=None):
    """Set m[k] to default if k not in m; return m[k] either way."""
    return m.setdefault(k, default)


def update(m, other):
    """Update m from other.

    For each (k, v) in other, set m[k] = v.
    """
    m.update(other)


def pop(m, k, default=None):

----------
Solution: 

    if isinstance(m, IAssociative):
        return m.entry(k, default=default)

    try:
        return m[k]
    except (KeyError, IndexError, TypeError) as e:
        logger.debug("Ignored %s: %s", type(e).__name__, e)
        return default
--------------------
Prompt: 
def _send_coroutine():
    """
    Creates a running coroutine to receive message instances and send
    them in a futures executor.
    """
----------
Output: 

    while True:
        message = yield
        if message is None:
            break
        else:
            message.send()


class Message:
    """
    A message is a wrapper around a message instance.
    """

    def __init__(self, message, future=None):
        """
        Initializes a message.
        """
        self.message = message
        self.future = future

    def send(self):
        """
        Sends the message.
        """
        if self.future is not None
----------
Solution: 

    with PoolExecutor() as executor:
        while True:
            msg = yield
            future = executor.submit(msg.send)
            future.add_done_callback(_exception_handler)
--------------------
Prompt: 
def cb_list_groups(self, *args, **kwargs):
        """List all the check groups that pylint knows about

        These should be useful to know what check groups someone can disable
        or enable.
        """
----------
Output: 

        return self.linter.list_groups()

    def cb_list_checkers(self, *args, **kwargs):
        """List all the checkers that pylint knows about

        These should be useful to know what checkers someone can disable
        or enable.
        """
        return self.linter.list_checkers()

    def cb_list_msgs(self, *args, **kwargs):
        """List all the messages that pylint knows about

        These should be useful to know what messages someone can disable

----------
Solution: 

        for check in self.linter.get_checker_names():
            print(check)
        sys.exit(0)
--------------------
Prompt: 
def add_column(self, name, data):
        """Add a column to the DataFrame

        :param str name: name of column
        :param data: numpy array with the data
        """
----------
Output: 

        self.data[name] = data

    def add_row(self, data):
        """Add a row to the DataFrame

        :param data: numpy array with the data
        """
        self.data = np.vstack((self.data, data))

    def get_column(self, name):
        """Get a column from the DataFrame

        :param str name: name of column
        :return: numpy array with the data
        """
        return self.data[name]

    def get_row(self
----------
Solution: 

        # assert _is_array_type_ok(data), "dtype not supported: %r, %r" % (data.dtype, data.dtype.type)
        # self._length = len(data)
        # if self._length_unfiltered is None:
        #     self._length_unfiltered = len(data)
        #     self._length_original = len(data)
        #     self._index_end = self._length_unfiltered
        super(DataFrameArrays, self).add_column(name, data)
        self._length_unfiltered = int(round(self._length_original * self._active_fraction))
--------------------
Prompt: 
def get_objects_from_form(variant_ids, form_fields, object_type):
    """Extract the objects to be saved in the clinvar database collection.
       object_type param specifies if these objects are variant or casedata objects

       Args:
        variant_ids(list): list of database variant ids
        form_fields(dict): it's the submission form dictionary. Keys have the same names as CLINVAR_HEADER and CASEDATA_HEADER
        object_type(str): either 'variant' or 'case_data'

       Returns:
        submission_objects(list): list of submission objects of either type 'variant' or 'casedata'
    """
----------
Output: 
'

       Returns:
        list: list of objects to be saved in the clinvar database collection
    """
    objects = []
    for variant_id in variant_ids:
        object = {}
        object['variant_id'] = variant_id
        object['object_type'] = object_type
        object['submission_date'] = form_fields['submission_date']
        object['submission_id'] = form_fields['submission_id']
        object['clinvar_id'] = form_fields['clinvar_id
----------
Solution: 

    submission_fields = []
    if object_type == 'variant':
        submission_fields = CLINVAR_HEADER
    else: #collect casedata objects
        submission_fields = CASEDATA_HEADER

    # A list of objects (variants of casedata info) to be saved into clinvar database collection
    submission_objects = []

    # Loop over the form fields and collect the data:
    for variant_id in variant_ids: # loop over the variants

        subm_obj = {} # A new submission object for each

        # Don't included casedata for a variant unless specified by user
        if object_type == 'casedata' and 'casedata_'+variant_id not in form_fields:
            continue

        subm_obj['csv_type'] = object_type
        subm_obj['case_id'] = form_fields.get('case_id')
        subm_obj['category'] = form_fields.get('category@'+variant_id)

        for key, values in submission_fields.items(): # loop over the form info fields
            field_value = form_fields.get(key+'@'+variant_id)
            if field_value and not field_value == '-':
                if key == 'ref_seq': # split this field into
                    refseq_raw = field_value.split('|')
                    subm_obj['ref_seq'] = refseq_raw[0]
                    subm_obj['hgvs'] = refseq_raw[1]
                else:
                    subm_obj[key] = field_value

        # Create a unique ID for the database
        # For casedata : = caseID_sampleID_variantID
        # For variants : ID = caseID_variantID
        if object_type == 'casedata':
            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id + '_' + str(subm_obj['individual_id'])
        else:
            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id

        submission_objects.append(subm_obj)

    return submission_objects
--------------------
Prompt: 
def evaluate(self, x, y=None, batch_size=32):
        """
        Evaluate a model on a given dataset in distributed mode.

        # Arguments
        x: Input data. A Numpy array or RDD of Sample.
        y: Labels. A Numpy array. Default is None if x is already RDD of Sample.
        batch_size: Number of samples per gradient update.
        """
----------
Output: 

        if y is None:
            x, y = x
        if not isinstance(x, RDD):
            x = RDD(x)
        if not isinstance(y, RDD):
            y = RDD(y)
        if not x.is_distributed():
            raise ValueError("x must be distributed")
        if not y.is_distributed():
            raise ValueError("y must be distributed")
        if x.partitioner() != y.partitioner():
            raise ValueError("x and y must have the same partition
----------
Solution: 

        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):
            evaluation_data = to_sample_rdd(x, y)
        elif isinstance(x, RDD) and not y:
            evaluation_data = x
        else:
            raise TypeError("Unsupported evaluation data type: %s" % type(x))
        return callBigDlFunc(self.bigdl_type, "evaluate",
                             self.value,
                             evaluation_data,
                             batch_size)
--------------------
Prompt: 
def _loop_to_py_ast(ctx: GeneratorContext, node: Loop) -> GeneratedPyAST:
    """Return a Python AST Node for a `loop*` expression."""
----------
Output: 

    # TODO: This is a hack.
    #       We should be able to get the type of the loop variable
    #       from the type of the loop expression.
    #       This is not possible at the moment.
    #       We should also be able to get the type of the loop variable
    #       from the type of the loop body.
    #       This is not possible at the moment.
    #       We should also be able to get the type of the loop variable
    #       from the type of the loop condition.
    #       This is not possible
----------
Solution: 

    assert node.op == NodeOp.LOOP

    with ctx.new_symbol_table("loop"):
        binding_names = []
        init_bindings: List[ast.AST] = []
        for binding in node.bindings:
            init_node = binding.init
            assert init_node is not None
            init_ast = gen_py_ast(ctx, init_node)
            init_bindings.extend(init_ast.dependencies)
            binding_name = genname(munge(binding.name))
            binding_names.append(binding_name)
            init_bindings.append(
                ast.Assign(
                    targets=[ast.Name(id=binding_name, ctx=ast.Store())],
                    value=init_ast.node,
                )
            )
            ctx.symbol_table.new_symbol(
                sym.symbol(binding.name), binding_name, LocalType.LOOP
            )

        loop_result_name = genname("loop")
        with ctx.new_recur_point(
            node.loop_id, RecurType.LOOP, binding_names=binding_names
        ):
            loop_body_ast: List[ast.AST] = []
            body_ast = _synthetic_do_to_py_ast(ctx, node.body)
            loop_body_ast.extend(body_ast.dependencies)
            loop_body_ast.append(
                ast.Assign(
                    targets=[ast.Name(id=loop_result_name, ctx=ast.Store())],
                    value=body_ast.node,
                )
            )
            loop_body_ast.append(ast.Break())

            return GeneratedPyAST(
                node=_load_attr(loop_result_name),
                dependencies=list(
                    chain(
                        [
                            ast.Assign(
                                targets=[
                                    ast.Name(id=loop_result_name, ctx=ast.Store())
                                ],
                                value=ast.NameConstant(None),
                            )
                        ],
                        init_bindings,
                        [
                            ast.While(
                                test=ast.NameConstant(True),
                                body=loop_body_ast,
                                orelse=[],
                            )
                        ],
                    )
                ),
            )
--------------------
Prompt: 
def get_model(self, model_name):
        """
        TODO: Need to validate model name has 2x '.' chars
        """
----------
Output: 

        model_name = model_name.replace('.', '/')
        model_path = os.path.join(self.model_dir, model_name)
        model = tf.saved_model.load(model_path)
        return model

    def get_model_names(self):
        """
        TODO: Need to validate model name has 2x '.' chars
        """
        model_names = []
        for model_name in os.listdir(self.model_dir):
            if os.path.isdir(os.
----------
Solution: 

        klass = None
        try:
            module_name, class_name = model_name.rsplit('.', 1)
            mod = __import__(module_name, fromlist=[class_name])
            klass = getattr(mod, class_name)
        except ImportError, e:
            self.error('Cannot find app %s %s' % (model_name, e))

        return klass
--------------------
Prompt: 
def compare(self, cost_map):
        """
        Compare to previous records and return whether the given cost is a new best.
        :return: True if the given cost is a new best
        """
----------
Output: 

        if self.best_cost is None:
            self.best_cost = cost_map
            return True
        else:
            for key, value in cost_map.items():
                if value < self.best_cost[key]:
                    self.best_cost = cost_map
                    return True
            return False

    def get_best_cost(self):
        """
        Return the best cost map.
        :return: best cost map
        """
        return self.best_cost

    def get_best_cost
----------
Solution: 

        cri_val = cost_map[self._criteria]
        if self._best_criteria is None:
            self._best_criteria = cri_val
            return True
        else:
            if self._smaller_is_better and cri_val < self._best_criteria:
                self._best_criteria = cri_val
                return True
            elif not self._smaller_is_better and cri_val > self._best_criteria:
                self._best_criteria = cri_val
                return True
            else:
                return False
--------------------
Prompt: 
def execute(self):
        """
        Execute R script
        """
----------
Output: 

        self.log.info("Executing R script")
        self.log.info("R script: %s", self.script)
        self.log.info("R script arguments: %s", self.arguments)
        self.log.info("R script working directory: %s", self.working_directory)

        # Create R script
        r_script = self.create_r_script()

        # Execute R script
        self.execute_r_script(r_script)

        # Remove R script
        self.remove_
----------
Solution: 

        rprocess = OrderedDict()
        commands = OrderedDict([
            (self.file, ['Rscript', self.file] + self.cmd),
        ])
        for cmd_name, cmd in commands.items():
            rprocess[cmd_name] = self.run_command_under_r_root(cmd)
        
        return self.decode_cmd_out(completed_cmd=rprocess[self.file])
--------------------
Prompt: 
def _duplicated_isinstance_types(node):
        """Get the duplicated types from the underlying isinstance calls.

        :param astroid.BoolOp node: Node which should contain a bunch of isinstance calls.
        :returns: Dictionary of the comparison objects from the isinstance calls,
                  to duplicate values from consecutive calls.
        :rtype: dict
        """
----------
Output: 

        duplicated_types = {}
        for child in node.values:
            if isinstance(child, astroid.BoolOp) and child.op == 'and':
                duplicated_types.update(_duplicated_isinstance_types(child))
            elif isinstance(child, astroid.Call) and child.func.attrname == 'isinstance':
                for arg in child.args:
                    if isinstance(arg, astroid.Const):
                        duplicated_types[arg.value] = True
        return duplicated_types

----------
Solution: 

        duplicated_objects = set()
        all_types = collections.defaultdict(set)

        for call in node.values:
            if not isinstance(call, astroid.Call) or len(call.args) != 2:
                continue

            inferred = utils.safe_infer(call.func)
            if not inferred or not utils.is_builtin_object(inferred):
                continue

            if inferred.name != "isinstance":
                continue

            isinstance_object = call.args[0].as_string()
            isinstance_types = call.args[1]

            if isinstance_object in all_types:
                duplicated_objects.add(isinstance_object)

            if isinstance(isinstance_types, astroid.Tuple):
                elems = [
                    class_type.as_string() for class_type in isinstance_types.itered()
                ]
            else:
                elems = [isinstance_types.as_string()]
            all_types[isinstance_object].update(elems)

        # Remove all keys which not duplicated
        return {
            key: value for key, value in all_types.items() if key in duplicated_objects
        }
--------------------
Prompt: 
def digest(self, data, mecha=MechanismSHA1):
        """
        C_DigestInit/C_Digest

        :param data: the data to be digested
        :type data:  (binary) sring or list/tuple of bytes
        :param mecha: the digesting mechanism to be used
          (use `MechanismSHA1` for `CKM_SHA_1`)
        :type mecha: :class:`Mechanism`
        :return: the computed digest
        :rtype: list of bytes

        :note: the returned value is an istance of :class:`ckbytelist`.
          You can easly convert it to a binary string with:
          ``bytes(ckbytelistDigest)``
          or, for Python 2:
          ``''.join(chr(i) for i in ckbytelistDigest)``

        """
----------
Output: 
 (binary) string
        """
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError("data must be a (binary) string")
        if not isinstance(mecha, Mechanism):
            raise TypeError("mecha must be a Mechanism")
        if not mecha.is_digesting():
            raise ValueError("mecha must be a digesting mechanism")
        if not isinstance(mecha.mecha_type, CK_MECHANISM_TYPE):
            raise TypeError("mecha.mecha
----------
Solution: 

        digest = ckbytelist()
        m = mecha.to_native()
        data1 = ckbytelist(data)
        rv = self.lib.C_DigestInit(self.session, m)
        if rv != CKR_OK:
            raise PyKCS11Error(rv)
        # first call get digest size
        rv = self.lib.C_Digest(self.session, data1, digest)
        if rv != CKR_OK:
            raise PyKCS11Error(rv)
        # second call get actual digest data
        rv = self.lib.C_Digest(self.session, data1, digest)
        if rv != CKR_OK:
            raise PyKCS11Error(rv)
        return digest
--------------------
Prompt: 
def file_hash(self, source, cu):
        """Compute a hash that changes if the file needs to be re-reported."""
----------
Output: 

        # TODO: This is a very simple hash. We should probably use a
        #       more sophisticated one.
        return source.path + str(source.mtime)

    def report(self, source, cu):
        """Report the given compilation unit."""
        # TODO: This is a very simple report. We should probably use a
        #       more sophisticated one.
        self.report_file(source.path, cu.name, cu.line, cu.column, cu.message)

    def report_file(
----------
Solution: 

        m = Hasher()
        m.update(source)
        self.coverage.data.add_to_hash(cu.filename, m)
        return m.digest()
--------------------
Prompt: 
def voronoi(script, hole_num=50, target_layer=None, sample_layer=None, thickness=0.5, backward=True):
    """ Turn a model into a surface with Voronoi style holes in it

    References:
    http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html
    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html

    Requires FilterScript object

    Args:
        script: the FilterScript object to write the filter to. Does not
            work with a script filename.

    Layer stack:
        No impacts

    MeshLab versions:
        2016.12
        1.3.4BETA
    """
----------
Output: 
2.html
    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-3.html
    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-4.html
    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-5.html
    http://meshlabstuff.blogspot.com/20
----------
Solution: 


    if target_layer is None:
        target_layer = script.current_layer()
    if sample_layer is None:
        # Current layer is currently not changed after poisson_disk is run
        sampling.poisson_disk(script, sample_num=hole_num)
        sample_layer = script.last_layer()

    vert_color.voronoi(script, target_layer=target_layer, source_layer=sample_layer, backward=backward)
    select.vert_quality(script, min_quality=0.0, max_quality=thickness)
    if backward:
        select.invert(script)
    delete.selected(script)
    smooth.laplacian(script, iterations=3)

    return None
--------------------
Prompt: 
def indices_to_global_iterator(self, indices):
        """
        Transform a dictionary of indices to a global iterator integer.

        Inverse of global_iterator_to_indices().
        """
----------
Output: 

        return sum([self.n_nodes[i] * indices[i] for i in range(self.dim)])

    def global_iterator_to_indices(self, global_iterator):
        """
        Transform a global iterator integer to a dictionary of indices.

        Inverse of indices_to_global_iterator().
        """
        indices = {}
        for i in range(self.dim):
            indices[i] = global_iterator // self.n_nodes[i]
            global_iterator -= self.n_nodes[
----------
Solution: 

        global_iterator = self.subs_consts(self.global_iterator().subs(indices))
        return global_iterator
--------------------
Prompt: 
def _value(self, dtype=None, name=None, as_ref=False):  # pylint: disable=g-doc-args
  """Get the value returned by `tf.convert_to_tensor(distribution)`.

  Note: this function may mutate the distribution instance state by caching
  the concretized `Tensor` value.

  Args:
    dtype: Must return a `Tensor` with the given `dtype` if specified.
    name: If the conversion function creates a new `Tensor`, it should use the
      given `name` if specified.
    as_ref: `as_ref` is true, the function must return a `Tensor` reference,
      such as a `Variable`.
  Returns:
    concretized_distribution_value: `Tensor` identical to
    `tf.convert_to_tensor(distribution)`.

  #### Examples

  ```python
  tfd = tfp.distributions
  x = tfd.Normal(0.5, 1).set_tensor_conversion(tfd.Distribution.mean)

  x._value()
  # ==> tf.convert_to_tensor(x) ==> 0.5

  x._value() + 2
  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5

  x + 2
  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5
  ```

  """
----------
Output: 
     given name.
    as_ref: If True, the result is returned as a `tf.Tensor` reference.

  Returns:
    value: A `Tensor` or `tf.Tensor` reference.
  """
  if dtype is None:
    dtype = self.dtype
  if name is None:
    name = self.name
  if as_ref:
    return self._as_ref()
  if self._concrete_value is None:
    self._concrete_value = tf.convert_to_tensor(

----------
Solution: 

  # pylint: disable=protected-access
  if as_ref:
    raise NotImplementedError(
        'Cannot convert a `Distribution` to a reference '
        '(e.g., `tf.Variable`).')
  if self._concrete_value is None:
    if self._convert_to_tensor_fn is None:
      raise NotImplementedError(
          'Failed to convert object of type {} to Tensor. Contents: {}. '
          'Call `distribution.set_tensor_conversion(lambda self: ...)` to '
          'enable `tf.convert_to_tensor` capability. For example: '
          '`x = tfd.Normal(0,1).set_tensor_conversion(tfd.Distribution.mean)`'
          ' results in `tf.convert_to_tensor(x)` being identical to '
          '`x.mean()`.'.format(type(self), self))
    with self._name_scope('value'):
      self._concrete_value = (self._convert_to_tensor_fn(self)
                              if callable(self._convert_to_tensor_fn)
                              else self._convert_to_tensor_fn)
      if not tf.is_tensor(self._concrete_value):
        self._concrete_value = tfd._convert_to_tensor(
            value=self._concrete_value,
            name=name or 'concrete_value',
            dtype=dtype,
            dtype_hint=self.dtype)
  return self._concrete_value
--------------------
Prompt: 
def url_equal(first, second, ignore_scheme=False, ignore_netloc=False, ignore_path=False, ignore_params=False,
              ignore_query=False, ignore_fragment=False):
    """
    Compare two URLs and return True if they are equal, some parts of the URLs can be ignored
    :param first: URL
    :param second: URL
    :param ignore_scheme: ignore the scheme
    :param ignore_netloc: ignore the netloc
    :param ignore_path: ignore the path
    :param ignore_params: ignore the params
    :param ignore_query: ignore the query string
    :param ignore_fragment: ignore the fragment
    :return: result of comparison
    """
----------
Output: 
param ignore_params: ignore the params
    :param ignore_query: ignore the query
    :param ignore_fragment: ignore the fragment
    :return: True if the URLs are equal
    """
    if ignore_scheme:
        first = urlparse(first)
        second = urlparse(second)
        first = first._replace(scheme=None)
        second = second._replace(scheme=None)
    if ignore_netloc:
        first = urlparse(first)
        second = urlparse(second)
        first = first._
----------
Solution: 

    # <scheme>://<netloc>/<path>;<params>?<query>#<fragment>

    firstp = urlparse(first)
    secondp = urlparse(second)

    return ((firstp.scheme == secondp.scheme or ignore_scheme) and
            (firstp.netloc == secondp.netloc or ignore_netloc) and
            (firstp.path == secondp.path or ignore_path) and
            (firstp.params == secondp.params or ignore_params) and
            (firstp.query == secondp.query or ignore_query) and
            (firstp.fragment == secondp.fragment or ignore_fragment))
--------------------
Prompt: 
def get_users_for_course(self, course_id, params={}):
        """
        Returns a list of users for the given course id.
        """
----------
Output: 

        return self._get_users_for_course(course_id, params)

    def get_users_for_course_by_sis_id(self, sis_course_id, params={}):
        """
        Returns a list of users for the given course sis id.
        """
        return self._get_users_for_course(sis_course_id, params, True)

    def _get_users_for_course(self, course_id, params={}, sis_id=False):
        """
----------
Solution: 

        url = COURSES_API.format(course_id) + "/users"
        data = self._get_paged_resource(url, params=params)
        users = []
        for datum in data:
            users.append(CanvasUser(data=datum))
        return users
--------------------
Prompt: 
def make_signing_service(config, entity_id):
    """
    Given configuration initiate a SigningService instance

    :param config: The signing service configuration
    :param entity_id: The entity identifier
    :return: A SigningService instance
    """
----------
Output: 

    if config.get('signing_service_type') == 'internal':
        return InternalSigningService(config, entity_id)
    elif config.get('signing_service_type') == 'external':
        return ExternalSigningService(config, entity_id)
    else:
        raise Exception('Unknown signing service type: {}'.format(config.get('signing_service_type')))


class SigningService(object):
    """
    A signing service is responsible for signing and verifying signatures.
    """


----------
Solution: 


    _args = dict([(k, v) for k, v in config.items() if k in KJ_SPECS])
    _kj = init_key_jar(**_args)

    if config['type'] == 'internal':
        signer = InternalSigningService(entity_id, _kj)
    elif config['type'] == 'web':
        _kj.issuer_keys[config['iss']] = _kj.issuer_keys['']
        del _kj.issuer_keys['']
        signer = WebSigningServiceClient(config['iss'], config['url'],
                                         entity_id, _kj)
    else:
        raise ValueError('Unknown signer type: {}'.format(config['type']))

    return signer
--------------------
Prompt: 
def delete(filename, delete_v1=True, delete_v2=True):
    """Remove tags from a file.

    Keyword arguments:

    * delete_v1 -- delete any ID3v1 tag
    * delete_v2 -- delete any ID3v2 tag
    """
----------
Output: 

    if not filename:
        raise ValueError("Filename must be specified")

    try:
        f = open(filename, "rb+")
    except IOError:
        raise IOError("Could not open file for writing")

    try:
        f.seek(0)
        data = f.read()
        f.seek(0)
        f.truncate()
        f.write(data)
    finally:
        f.close()

    if delete_v1:
        delete_v1(filename
----------
Solution: 


    f = open(filename, 'rb+')

    if delete_v1:
        try:
            f.seek(-128, 2)
        except IOError:
            pass
        else:
            if f.read(3) == b'TAG':
                f.seek(-128, 2)
                f.truncate()

    # technically an insize=0 tag is invalid, but we delete it anyway
    # (primarily because we used to write it)
    if delete_v2:
        f.seek(0, 0)
        idata = f.read(10)
        try:
            id3, vmaj, vrev, flags, insize = unpack('>3sBBB4s', idata)
        except struct.error:
            id3, insize = b'', -1
        insize = BitPaddedInt(insize)
        if id3 == b'ID3' and insize >= 0:
            delete_bytes(f, insize + 10, 0)
--------------------
Prompt: 
def state(self):
        """Return internal state, useful for testing."""
----------
Output: 

        return self._state

    def _set_state(self, state):
        """Set internal state, useful for testing."""
        self._state = state

    def _get_state(self):
        """Get internal state, useful for testing."""
        return self._state

    def _set_state_from_file(self, state_file):
        """Set internal state from a file, useful for testing."""
        with open(state_file, 'r') as f:
            self._state = json.load(f)
----------
Solution: 

        return {'c': self.c, 's0': self.s0, 's1': self.s1, 's2': self.s2}
--------------------
Prompt: 
def get_mint_tree(tokens_stream):
    """
    This function is wrapper to normal parsers (tag_parser, block_parser, etc.).
    Returns mint tree.
    """
----------
Output: 

    mint_tree = []
    while tokens_stream:
        mint_tree.append(tag_parser(tokens_stream))
    return mint_tree


def tag_parser(tokens_stream):
    """
    This function parses tokens stream and returns tag.
    """
    tag = tokens_stream.pop(0)
    if tag == '(':
        return block_parser(tokens_stream)
    elif tag == '[':
        return list_parser(tokens_stream)
    elif tag == '{':
----------
Solution: 

    smart_stack = RecursiveStack()
    block_parser.parse(tokens_stream, smart_stack)
    return MintTemplate(body=smart_stack.stack)
--------------------
Prompt: 
def from_yaml(cls, defaults, **kwargs):
        """Creates a new instance of a rule by merging two dictionaries.

        This allows for independant configuration files to be merged
        into the defaults."""
----------
Output: 

        return cls(**dict(defaults, **kwargs))

    @classmethod
    def from_yaml_file(cls, filename, defaults=None):
        """Creates a new instance of a rule by merging two dictionaries.

        This allows for independant configuration files to be merged
        into the defaults."""
        with open(filename, 'r') as f:
            return cls.from_yaml(defaults, **yaml.load(f))

    def __init__(self, **kwargs):
        """Creates a new
----------
Solution: 

        # TODO: I hate myself for this. Fix it later mmkay?
        if "token" not in defaults:
            kwargs["token"] = None

        defaults = copy.deepcopy(defaults)
        return cls(
            defaults=defaults,
            token=kwargs.pop("token"),
            directory=kwargs.pop("directory"),
            **kwargs
        )
--------------------
Prompt: 
def getPhysicalInterface(self, physicalInterfaceId, draft=False):
        """
        Get a physical interface.
        Parameters:
          - physicalInterfaceId (string)
          - draft (boolean)
        Throws APIException on failure.
        """
----------
Output: 

        return self.getPhysicalInterfaces(draft=draft)[physicalInterfaceId]

    def getPhysicalInterfaces(self, draft=False):
        """
        Get all physical interfaces.
        Parameters:
          - draft (boolean)
        Throws APIException on failure.
        """
        return self.getPhysicalInterfacesByNode(None, draft=draft)

    def getPhysicalInterfacesByNode(self, nodeId, draft=False):
        """
        Get all physical interfaces for a node.
       
----------
Solution: 

        if draft:
            req = ApiClient.onePhysicalInterfaceUrl % (self.host, "/draft", physicalInterfaceId)
        else:
            req = ApiClient.onePhysicalInterfaceUrl % (self.host, "", physicalInterfaceId)

        resp = requests.get(req, auth=self.credentials, verify=self.verify)
        if resp.status_code == 200:
            self.logger.debug("physical interface retrieved")
        else:
            raise ibmiotf.APIException(resp.status_code, "HTTP error getting a physical interface", resp)
        return resp.json()
--------------------
Prompt: 
def get(cls):
        """
        Get the status while testing for an IP or domain.

        .. note::
            We consider that the domain or IP we are currently testing
            is into :code:`PyFunceble.INTERN["to_test"]`.
        """
----------
Output: 


        if PyFunceble.INTERN["to_test"]:
            if PyFunceble.INTERN["to_test"] in PyFunceble.INTERN["ipv4_list"]:
                return PyFunceble.STATUS["ipv4"]
            elif PyFunceble.INTERN["to_test"] in PyFunceble.INTERN["ipv6_list"]:
                return PyFunceble.STATUS["ipv6"]
            else:
                return PyFunceble.STATUS["domain"]
        else:

----------
Solution: 


        if "to_test" in PyFunceble.INTERN and PyFunceble.INTERN["to_test"]:
            expiration_date = ExpirationDate().get()

            if expiration_date is False:
                return cls.handle(status="invalid")

            if expiration_date == PyFunceble.STATUS["official"]["up"]:
                return expiration_date, "WHOIS"

            return cls.handle(status="inactive")

        raise NotImplementedError("We expect `INTERN['to_test']` to be set.")
--------------------
Prompt: 
def execute(option):
    """A script that melody calls with each valid set of options. This
    script runs the required code and returns the results."""
----------
Output: 

    import os
    import sys
    import melody
    import melody.config
    import melody.util
    import melody.util.log
    import melody.util.shell
    import melody.util.shell.bash
    import melody.util.shell.bash.bash_command
    import melody.util.shell.bash.bash_command.bash_command_result
    import melody.util.shell.bash.bash_command.bash_command_result.bash_command_result_status
    import melody.util
----------
Solution: 


    namelist_option = []
    makefile_option = []
    flags = ""
    for entry in option:
        key = entry.keys()[0]
        if key == "Problem Size":
            namelist_option.append({"SIZE": entry[key]})
        elif key == "F90":
            makefile_option.append(entry)
        else:
            flags += entry[key] + " "
    makefile_option.append({"F90FLAGS": flags})

    namelist = create_input(namelist_option, "namelist",
                            template_location="templates")

    makefile_include = create_input(makefile_option, "Makefile.include",
                                    template_location="templates")

    benchmark_base = "shallow"

    # save the input files in the appropriate place
    location = benchmark_base + "/original/namelist"
    my_file = open(location, 'w')
    my_file.write(namelist)
    my_file.flush()

    location = benchmark_base + "/common/Makefile.include"
    my_file = open(location, 'w')
    my_file.write(makefile_include)
    my_file.flush()

    # compile shallow if required
    base_path = benchmark_base + "/original"
    import subprocess
    make_process = subprocess.Popen(["make", "clean"], cwd=base_path,
                                    stderr=subprocess.PIPE,
                                    stdout=subprocess.PIPE)
    if make_process.wait() != 0:
        return False, []

    make_process = subprocess.Popen(["make"], cwd=base_path,
                                    stderr=subprocess.PIPE,
                                    stdout=subprocess.PIPE)
    if make_process.wait() != 0:
        return False, []

    # run shallow
    make_process = subprocess.Popen(["./shallow_base"], cwd=base_path,
                                    stderr=subprocess.PIPE,
                                    stdout=subprocess.PIPE)
    if make_process.wait() != 0:
        return False, []
    # _ = make_process.stderr.read()
    stdout = make_process.stdout.read()

    # determine if the results are correct. We will need to look at
    # the results from stdout but for the moment we assume they are
    # correct

    # extract the required outputs
    for line in stdout.split("\n"):
        if "Time-stepping" in line:
            total_time = line.split()[2]

    return True, total_time
--------------------
Prompt: 
def _read_next(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901
    """Read the next full form from the input stream."""
----------
Output: 

    # Read the first character.
    ch = ctx.read_char()

    # If we're at the end of the stream, return EOF.
    if ch is None:
        return LispReaderForm(LispReaderEOF())

    # If it's a whitespace character, skip it and read the next character.
    if ch in WHITESPACE:
        return _read_next(ctx)

    # If it's a comment, skip it and read the next character.
    if ch == COMMENT_CHAR
----------
Solution: 

    reader = ctx.reader
    token = reader.peek()
    if token == "(":
        return _read_list(ctx)
    elif token == "[":
        return _read_vector(ctx)
    elif token == "{":
        return _read_map(ctx)
    elif begin_num_chars.match(token):
        return _read_num(ctx)
    elif whitespace_chars.match(token):
        reader.next_token()
        return _read_next(ctx)
    elif token == ":":
        return _read_kw(ctx)
    elif token == '"':
        return _read_str(ctx)
    elif token == "'":
        return _read_quoted(ctx)
    elif token == "\\":
        return _read_character(ctx)
    elif ns_name_chars.match(token):
        return _read_sym(ctx)
    elif token == "#":
        return _read_reader_macro(ctx)
    elif token == "^":
        return _read_meta(ctx)  # type: ignore
    elif token == ";":
        return _read_comment(ctx)
    elif token == "`":
        return _read_syntax_quoted(ctx)
    elif token == "~":
        return _read_unquote(ctx)
    elif token == "@":
        return _read_deref(ctx)
    elif token == "":
        return ctx.eof
    else:
        raise SyntaxError("Unexpected token '{token}'".format(token=token))
--------------------
Prompt: 
def __tomo_linear_inv(freqs, ops, weights=None, trace=None):
    """
    Reconstruct a matrix through linear inversion.

    Args:
        freqs (list[float]): list of observed frequences.
        ops (list[np.array]): list of corresponding projectors.
        weights (list[float] or array_like):
            weights to be used for weighted fitting.
        trace (float or None): trace of returned operator.

    Returns:
        numpy.array: A numpy array of the reconstructed operator.
    """
----------
Output: 
reconstructed operator.
    """
    if weights is None:
        weights = np.ones(len(freqs))
    else:
        weights = np.asarray(weights)
    if trace is None:
        trace = np.sum(weights)
    else:
        trace = float(trace)
    if len(freqs) != len(ops):
        raise ValueError("freqs and ops must have the same length.")
    if len(freqs) != len(weights):
        raise ValueError("
----------
Solution: 

    # get weights matrix
    if weights is not None:
        W = np.array(weights)
        if W.ndim == 1:
            W = np.diag(W)

    # Get basis S matrix
    S = np.array([vectorize(m).conj()
                  for m in ops]).reshape(len(ops), ops[0].size)
    if weights is not None:
        S = np.dot(W, S)  # W.S

    # get frequencies vec
    v = np.array(freqs)  # |f>
    if weights is not None:
        v = np.dot(W, freqs)  # W.|f>
    Sdg = S.T.conj()  # S^*.W^*
    inv = np.linalg.pinv(np.dot(Sdg, S))  # (S^*.W^*.W.S)^-1

    # linear inversion of freqs
    ret = devectorize(np.dot(inv, np.dot(Sdg, v)))
    # renormalize to input trace value
    if trace is not None:
        ret = trace * ret / np.trace(ret)
    return ret
--------------------
Prompt: 
def check_upload_status(self, video_id):
        """
        Checks the video upload status
        Newly uploaded videos may be in the processing state

        Authentication is required

        Returns:
            True if video is available
            otherwise a dict containes upload_state and detailed message
            i.e. {"upload_state": "processing", "detailed_message": ""}
        """
----------
Output: 

        url = "https://www.googleapis.com/youtube/v3/videos"
        params = {
            "part": "status",
            "id": video_id,
            "key": self.api_key,
        }
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        if data["pageInfo"]["totalResults"] == 0:
            raise ValueError("Video not found")
        elif data["items"][0]["status"]
----------
Solution: 

        # Raise ApiError if not authenticated
        if not self.authenticated:
            raise ApiError(_("Authentication is required"))

        entry = self.fetch_video(video_id)
        upload_status = Api.yt_service.CheckUploadStatus(entry)

        if upload_status is not None:
            video_upload_state = upload_status[0]
            detailed_message = upload_status[1]
            return {"upload_state": video_upload_state, "detailed_message": detailed_message}
        else:
            return True
--------------------
Prompt: 
def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':
        """Merge the failure message from another status into this one.

        Whichever status represents parsing that has gone the farthest is
        retained. If both statuses have gone the same distance, then the
        expected values from both are retained.

        Args:
            status: The status to merge into this one.

        Returns:
            This ``Status`` which may have ``farthest`` and ``expected``
            updated accordingly.
        """
----------
Output: 

        """
        if status.farthest is None:
            return self
        if self.farthest is None:
            return status
        if status.farthest > self.farthest:
            self.farthest = status.farthest
            self.expected = status.expected
        elif status.farthest == self.farthest:
            self.expected.update(status.expected)
        return self

    def __str__(self) -> str:
        """Return a string representation of this status.

        Returns
----------
Solution: 

        if status is None or status.farthest is None:
            # No new message; simply return unchanged
            pass
        elif self.farthest is None:
            # No current message to compare to; use the message from status
            self.farthest = status.farthest
            self.expected = status.expected
        elif status.farthest.position < self.farthest.position:
            # New message is not farther; keep current message
            pass
        elif status.farthest.position > self.farthest.position:
            # New message is farther than current message; replace with new message
            self.farthest = status.farthest
            self.expected = status.expected
        else:
            # New message and current message are equally far; merge messages
            self.expected = status.expected + self.expected

        return self
--------------------
Prompt: 
def stream(self, id, task, type, follow=False, offset=0, origin="start", plain=False):
        """ This endpoint streams a task's stderr/stdout logs.

            https://www.nomadproject.io/api/client.html#stream-logs

            arguments:
              - id: (str) allocation_id required
              - task: (str) name of the task inside the allocation to stream logs from
              - type: (str) Specifies the stream to stream. Either "stderr|stdout"
              - follow: (bool) default false
              - offset: (int) default 0
              - origin: (str) either start|end, default "start"
              - plain: (bool) Return just the plain text without framing. default False
            returns: (str) text
            raises:
              - nomad.api.exceptions.BaseNomadException
              - nomad.api.exceptions.BadRequestNomadException
        """
----------
Output: 
: (bool) Specifies if the connection should be kept open and
                        logs streamed as they arrive.
              - offset: (int) Offset is the byte offset to stream from.
              - origin: (str) Specifies the origin of the stream. Either "start" or "end"
              - plain: (bool) Specifies if the output should be plain text.

            returns:
              - (dict)
        """
        params = {
            "type": type,
            "follow": follow,
            "offset": offset,
            "origin
----------
Solution: 

        params = {
            "task": task,
            "type": type,
            "follow": follow,
            "offset": offset,
            "origin": origin,
            "plain": plain
        }
        return self.request(id, params=params, method="get").text
--------------------
Prompt: 
def _map_segmentation_mask_to_stft_domain(mask, times, frequencies, stft_times, stft_frequencies):
    """
    Maps the given `mask`, which is in domain (`frequencies`, `times`) to the new domain (`stft_frequencies`, `stft_times`)
    and returns the result.
    """
----------
Output: 

    # Map the mask to the STFT domain
    mask = mask[frequencies, times]
    mask = np.flip(mask, axis=0)
    mask = np.flip(mask, axis=1)

    # Reshape the mask to the STFT shape
    mask = np.reshape(mask, (len(stft_frequencies), len(stft_times)))

    return mask


def _map_stft_to_segmentation_mask_domain(stft, stft_times, stft
----------
Solution: 

    assert mask.shape == (frequencies.shape[0], times.shape[0]), "Times is shape {} and frequencies is shape {}, but mask is shaped {}".format(
        times.shape, frequencies.shape, mask.shape
    )
    result = np.zeros((stft_frequencies.shape[0], stft_times.shape[0]))

    if len(stft_times) > len(times):
        all_j = [j for j in range(len(stft_times))]
        idxs  = [int(i) for i in np.linspace(0, len(times) - 1, num=len(stft_times))]
        all_i = [all_j[idx] for idx in idxs]
    else:
        all_i = [i for i in range(len(times))]
        idxs  = [int(i) for i in np.linspace(0, len(stft_times) - 1, num=len(times))]
        all_j = [all_i[idx] for idx in idxs]

    for i, j in zip(all_i, all_j):
        result[:, j] = np.interp(stft_frequencies, frequencies, mask[:, i])

    return result
--------------------
Prompt: 
def sync_labels(self, repo):
        """Creates a local map of github labels/milestones to asana tags."""
----------
Output: 

        # TODO: Add a way to sync tags
        # TODO: Add a way to sync milestones
        # TODO: Add a way to sync issues
        # TODO: Add a way to sync pull requests
        # TODO: Add a way to sync comments
        # TODO: Add a way to sync commits
        # TODO: Add a way to sync releases
        # TODO: Add a way to sync releases
        # TODO: Add a way to sync releases
        # TODO: Add a way to sync releases
        # TODO: Add a way to sync releases
        # TODO
----------
Solution: 


        logging.info("syncing new github.com labels to tags")

        # create label tag map
        ltm = self.app.data.get("label-tag-map", {})

        # loop over labels, if they don't have tags, make them
        for label in repo.get_labels():
            tag_id = ltm.get(label.name, None)
            if tag_id is None:

                tag = self.app.asana.tags.create(name=label.name,
                                      workspace=self.asana_ws_id,
                                      notes="gh: %s" % label.url
                                      )

                logging.info("\t%s => tag %d", label.name, tag['id'])
                ltm[label.name] = tag['id']

        # loop over milestones, if they don't have tags, make them
        for ms in repo.get_milestones(state="all"):
            tag_id = ltm.get(_ms_label(ms.id), None)
            if tag_id is None:

                tag = self.app.asana.tags.create(name=ms.title,
                                      workspace=self.asana_ws_id,
                                      notes="gh: %s" % ms.url
                                      )

                logging.info("\t%s => tag %d", ms.title, tag['id'])
                ltm[_ms_label(ms.id)] = tag['id']

        self.app.data['label-tag-map'] = ltm
        return ltm
--------------------
Prompt: 
def recurse(self, k, wait=False, wait_index=None, timeout='5m'):
        """Recursively get the tree below the given key"""
----------
Output: 

        if not wait:
            return self.get(k, recurse=True)
        else:
            return self.get(k, recurse=True, wait=wait, wait_index=wait_index, timeout=timeout)

    def delete(self, k, recurse=False, **kwargs):
        """Delete a key"""
        if recurse:
            return self.delete(k, recurse=True, **kwargs)
        else:
            return self.delete(k, **kwargs)

    def
----------
Solution: 

        k = k.lstrip('/')
        url = '{}/{}'.format(self.endpoint, k)
        params = {}
        params['recurse'] = 'true'
        if wait:
            params['wait'] = timeout
            if not wait_index:
                params['index'] = self.index(k, recursive=True)
            else:
                params['index'] = wait_index
        r = requests.get(url, params=params)
        if r.status_code == 404:
            raise KeyDoesNotExist("Key " + k + " does not exist")
        if r.status_code != 200:
            raise KVStoreError('GET returned {}'.format(r.status_code))
        entries = {} 
        for e in r.json():
            if e['Value']:
                entries[e['Key']] = base64.b64decode(e['Value'])
            else:
                entries[e['Key']] = ''
        return entries
--------------------
Prompt: 
def delayed(f):
    """Decorator to transparantly accept delayed computation.

    Example:

    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,
    >>>                   shape=4, delay=True)
    >>> @vaex.delayed
    >>> def total_sum(sums):
    >>>     return sums.sum()
    >>> sum_of_sums = total_sum(delayed_sum)
    >>> ds.execute()
    >>> sum_of_sums.get()
    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations
    """
----------
Output: 
s.get()
    """
    def delayed_f(*args, **kwargs):
        if isinstance(args[0], vaex.delayed.Delayed):
            return args[0].map(f, *args[1:], **kwargs)
        else:
            return f(*args, **kwargs)
    return delayed_f


def delayed_map(f, *args, **kwargs):
    """Map a function over a list of delayed computations.

    Example:

    >>> delayed_sum = ds.sum(ds.E
----------
Solution: 


    def wrapped(*args, **kwargs):
        # print "calling", f, "with", kwargs
        # key_values = kwargs.items()
        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])
        # key_promise = [(key, promisify(value)) for key, value in key_values]
        arg_promises = list([promisify(value) for value in args])
        kwarg_promises = list([promise for key, promise in key_promise])
        promises = arg_promises + kwarg_promises
        for promise in promises:
            def echo_error(exc, promise=promise):
                print("error with ", promise, "exception is", exc)
                # raise exc

            def echo(value, promise=promise):
                print("done with ", repr(promise), "value is", value)
            # promise.then(echo, echo_error)

        # print promises
        allarguments = aplus.listPromise(*promises)

        def call(_):
            kwargs_real = {key: promise.get() for key, promise in key_promise}
            args_real = list([promise.get() for promise in arg_promises])
            return f(*args_real, **kwargs_real)

        def error(exc):
            print("error", exc)
            raise exc
        return allarguments.then(call, error)
    return wrapped
--------------------
Prompt: 
def read(cls, path, sc=None, min_partitions=1, bigdl_type="float"):
        """
        Read images as Image Frame
        if sc is defined, Read image as DistributedImageFrame from local file system or HDFS
        if sc is null, Read image as LocalImageFrame from local file system
        :param path path to read images
        if sc is defined, path can be local or HDFS. Wildcard character are supported.
        if sc is null, path is local directory/image file/image file with wildcard character
        :param sc SparkContext
        :param min_partitions A suggestion value of the minimal splitting number for input data.
        :return ImageFrame
        """
----------
Output: 
.
        :param sc SparkContext
        :param min_partitions min partitions
        :param bigdl_type bigdl_type to create the frame
        :return: ImageFrame
        """
        if sc is None:
            jimage_frame = callBigDlFunc(bigdl_type, "readImageFrame", path, min_partitions)
            return ImageFrame(jimage_frame, bigdl_type)
        else:
            jimage_frame = callBigDlFunc(bigdl_type, "readDistributedImageFrame", path,
----------
Solution: 

        return ImageFrame(jvalue=callBigDlFunc(bigdl_type, "read", path, sc, min_partitions))
--------------------
Prompt: 
def _is_package(path):
    """
    判断传入的路径是否为一个 Python 模块包

    :param str path: 待判断的路径
    :return: 返回是，则传入 path 为一个 Python 包，否则不是
    :rtype: bool
    """
----------
Output: 

    return os.path.isdir(path) and os.path.exists(os.path.join(path, '__init__.py'))


def _is_module(path):
    """
    判断传入的路径是否为一个 Python 模块

    :param str path: 待判断的路径
    :return: 返回是，则传入 path 为一个 Python 模块，否则不是
    :rtype: bool
    """

----------
Solution: 

    def _exists(s):
        return os.path.exists(os.path.join(path, s))

    return (
        os.path.isdir(path) and
        (_exists('__init__.py') or _exists('__init__.pyc'))
    )
--------------------
Prompt: 
def get_settings(self, client_name=None):
    """get all settings, either for a particular client if a name is provided,
       or across clients.

       Parameters
       ==========
       client_name: the client name to return settings for (optional)

    """
----------
Output: 

    if client_name is None:
        return self.settings
    else:
        return self.settings[client_name]


def get_setting(self, setting_name, client_name=None):
    """get a particular setting, either for a particular client if a name is provided,
       or across clients.

       Parameters
       ==========
       setting_name: the name of the setting to return
       client_name: the client name to return settings for (optional)

    """
    if client_name is None:
       
----------
Solution: 

    settings = read_client_secrets()
    if client_name is not None and client_name in settings:
        return settings[client_name]           
    return settings
--------------------
Prompt: 
def _evolve(self, state, qargs=None):
        """Evolve a quantum state by the operator.

        Args:
            state (QuantumState): The input statevector or density matrix.
            qargs (list): a list of QuantumState subsystem positions to apply
                           the operator on.

        Returns:
            QuantumState: the output quantum state.

        Raises:
            QiskitError: if the operator dimension does not match the
            specified QuantumState subsystem dimensions.
        """
----------
Output: 

        if qargs is None:
            qargs = getattr(state, 'qargs', None)
        if qargs is None:
            raise QiskitError('Evolution requires QuantumState subsystem '
                              'specification.')
        if self._evo_type == 'unitary':
            return self._evolve_unitary(state, qargs)
        elif self._evo_type == 'super':
            return self._evolve_super(state, qargs)
        elif self._evo_type == '
----------
Solution: 

        state = self._format_state(state)
        if qargs is None:
            if state.shape[0] != self._input_dim:
                raise QiskitError(
                    "Operator input dimension is not equal to state dimension."
                )
            if state.ndim == 1:
                # Return evolved statevector
                return np.dot(self.data, state)
            # Return evolved density matrix
            return np.dot(
                np.dot(self.data, state), np.transpose(np.conj(self.data)))
        # Subsystem evolution
        return self._evolve_subsystem(state, qargs)
--------------------
Prompt: 
def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):
        """
        Read the data encoding the CreateKeyPair response payload and decode it
        into its constituent parts.

        Args:
            input_buffer (stream): A data buffer containing encoded object
                data, supporting a read method.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be decoded. Optional,
                defaults to KMIP 1.0.

        Raises:
            InvalidKmipEncoding: Raised if the private key unique identifier or
                the public key unique identifier is missing from the encoded
                payload.
        """
----------
Output: 
1.0.

        Raises:
            InvalidKmipEncoding: Raised if the data in the input buffer is
                not a valid encoding.
        """
        super(CreateKeyPairResponsePayload, self).read(
            input_buffer,
            kmip_version=kmip_version
        )
        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))

        if self.is_oversized(local_buffer):
            raise exceptions.InvalidKmipEncoding(
                "
----------
Solution: 

        super(CreateKeyPairResponsePayload, self).read(
            input_buffer,
            kmip_version=kmip_version
        )
        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))

        if self.is_tag_next(
                enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER,
                local_buffer
        ):
            self._private_key_unique_identifier = primitives.TextString(
                tag=enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER
            )
            self._private_key_unique_identifier.read(
                local_buffer,
                kmip_version=kmip_version
            )
        else:
            raise exceptions.InvalidKmipEncoding(
                "The CreateKeyPair response payload encoding is missing the "
                "private key unique identifier."
            )

        if self.is_tag_next(
                enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER,
                local_buffer
        ):
            self._public_key_unique_identifier = primitives.TextString(
                tag=enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER
            )
            self._public_key_unique_identifier.read(
                local_buffer,
                kmip_version=kmip_version
            )
        else:
            raise exceptions.InvalidKmipEncoding(
                "The CreateKeyPair response payload encoding is missing the "
                "public key unique identifier."
            )

        if kmip_version < enums.KMIPVersion.KMIP_2_0:
            if self.is_tag_next(
                    enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE,
                    local_buffer
            ):
                self._private_key_template_attribute = \
                    objects.TemplateAttribute(
                        tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE
                    )
                self._private_key_template_attribute.read(
                    local_buffer,
                    kmip_version=kmip_version
                )

            if self.is_tag_next(
                    enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE,
                    local_buffer
            ):
                self._public_key_template_attribute = \
                    objects.TemplateAttribute(
                        tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE
                    )
                self._public_key_template_attribute.read(
                    local_buffer,
                    kmip_version=kmip_version
                )

        self.is_oversized(local_buffer)
--------------------
Prompt: 
def validate(self):
        """
        Error check the attributes of the ActivateRequestPayload object.
        """
----------
Output: 

        self.logger.debug('validate()')

        self.validate_type(self.activation_code, six.string_types)
        self.validate_type(self.activation_id, six.string_types)
        self.validate_type(self.activation_url, six.string_types)
        self.validate_type(self.device_id, six.string_types)
        self.validate_type(self.device_type, six.string_types)
        self.validate_type(self.device_token
----------
Solution: 

        if self.unique_identifier is not None:
            if not isinstance(self.unique_identifier,
                              attributes.UniqueIdentifier):
                msg = "invalid unique identifier"
                raise TypeError(msg)
        if self.compromise_occurrence_date is not None:
            if not isinstance(self.compromise_occurrence_date,
                              primitives.DateTime):
                msg = "invalid compromise time"
                raise TypeError(msg)
        if not isinstance(self.revocation_reason, objects.RevocationReason):
            msg = "invalid revocation reason"
            raise TypeError(msg)
--------------------
Prompt: 
def receive_ack_renewing(self, pkt):
        """Receive ACK in RENEWING state."""
----------
Output: 

        if pkt.ack_nr != self.seq_nr:
            self.seq_nr = pkt.ack_nr
            self.seq_nr_ack = pkt.ack_nr
            self.seq_nr_ack_sent = pkt.ack_nr
            self.seq_nr_ack_sent_time = pkt.time
            self.seq_nr_ack_sent_time_ack = pkt.time
            self.seq_nr_ack_sent_time_ack_ack = pkt.time
            self.
----------
Solution: 

        logger.debug("C3. Received ACK?, in RENEWING state.")
        if self.process_received_ack(pkt):
            logger.debug("C3: T. Received ACK, in RENEWING state, "
                         "raise BOUND.")
            raise self.BOUND()
--------------------
Prompt: 
def _get_server_info_from_logs(self):
        """
        Check server's output log, and determine its scheme / IP / port (helper method for `._launch_server()`).

        This method is polled during process startup. It looks at the server output log and checks for a presence of
        a particular string ("INFO: Open H2O Flow in your web browser:") which indicates that the server is
        up-and-running. If the method detects this string, it extracts the server's scheme, ip and port and returns
        them; otherwise it returns None.

        :returns: (scheme, ip, port) tuple if the server has already started, None otherwise.
        """
----------
Output: 
them.

        :return: scheme, ip and port of the server.
        """
        # check if the server is up and running
        if self._server_process is None:
            raise H2OServerError("Server is not running.")

        # check if the server is up and running
        if self._server_process.poll() is not None:
            raise H2OServerError("Server is not running.")

        # check if the server is up and running
        if self._server_process.stdout is None:
            raise H2
----------
Solution: 

        searchstr = "INFO: Open H2O Flow in your web browser:"
        with open(self._stdout, "rt") as f:
            for line in f:
                if searchstr in line:
                    url = line[line.index(searchstr) + len(searchstr):].strip().rstrip("/")
                    parts = url.split(":")
                    assert len(parts) == 3 and (parts[0] == "http" or parts[1] == "https") and parts[2].isdigit(), \
                        "Unexpected URL: %s" % url
                    return parts[0], parts[1][2:], int(parts[2])
        return None
--------------------
Prompt: 
def track_production(f):
    """
    decorator for adding positional information to returning nodes
    """
----------
Output: 

    def wrapper(self, *args, **kwargs):
        result = f(self, *args, **kwargs)
        if isinstance(result, list):
            for i, r in enumerate(result):
                r.position = i
        else:
            result.position = 0
        return result
    return wrapper


def track_consumption(f):
    """
    decorator for adding positional information to returning nodes
    """
    def wrapper(self, *args, **kwargs):
        result = f(self, *args,
----------
Solution: 

    @wraps(f)
    def wrapper(self, p):
        r = f(self, p)
        node = p[0]
        if isinstance(node, Node) and len(p) > 1:
            set_positional_info(node, p)
        return r
    
    return wrapper
--------------------
Prompt: 
def comparison_stats(df, els=['Mg', 'Sr', 'Ba', 'Al', 'Mn']):
    """
    Compute comparison stats for test and LAtools data.
    
    Population-level similarity assessed by a Kolmogorov-Smirnov test.
    
    Individual similarity assessed by a pairwise Wilcoxon signed rank test.
    
    Trends in residuals assessed by regression analysis, where significance of
    the slope and intercept is determined by t-tests (both relative to zero).
    
    Parameters
    ----------
    df : pandas.DataFrame
        A dataframe containing reference ('X/Ca_r'), test user 
        ('X/Ca_t') and LAtools ('X123') data.
    els : list
        list of elements (names only) to plot.
    
    Returns
    -------
    pandas.DataFrame
    
    """
----------
Output: 

    Parameters
    ----------
    df : pandas.DataFrame
        DataFrame containing the data to be compared.
    els : list
        Elements to be compared.
    
    Returns
    -------
    pandas.DataFrame
        DataFrame containing the comparison stats.
    """
    # Check if elements are in df
    for el in els:
        if el not in df.columns:
            raise ValueError(f'{el} not in df')
    
    # Check if df has a 'test' column
    if 'test' not
----------
Solution: 

    
    # get corresponding analyte and ratio names
    As = []
    Rs = []
    analytes = [c for c in df.columns if ('_r' not in c) and ('_t' not in c)]
    ratios = [c for c in df.columns if ('_r' in c)]

    for e in els:
        if e == 'Sr':
            As.append('Sr88')
        elif e == 'Mg':
            As.append('Mg24')
        else:
            As.append([a for a in analytes if e in a][0])
        Rs.append([r for r in ratios if e in r][0][:-2])
        
    yt_stats = []
    yl_stats = []
    
    for i, (e, a) in enumerate(zip(Rs, As)):
        if a == 'Ba138':
            m = 1e3
            u = '$\mu$mol/mol'
        else:
            m = 1
            u = 'mmol/mol'
        
        x = df.loc[:, e + '_r'].values * m
        yt = df.loc[:, e + '_t'].values * m
        yl = df.loc[:, a].values * m
        
        yt_stats.append(summary_stats(x, yt, e))
        yl_stats.append(summary_stats(x, yl, e))
    
    yt_stats = pd.concat(yt_stats).T
    yl_stats = pd.concat(yl_stats).T
    
    return pd.concat([yt_stats, yl_stats], keys=['Test User', 'LAtools']).T
--------------------
Prompt: 
def get_pk_value_on_save(self, instance):
        """Generate ID if required."""
----------
Output: 

        if instance.pk is None:
            instance.pk = self.generate_id()
        return instance

    def generate_id(self):
        """Generate a random ID."""
        return uuid.uuid4().hex

    def get_or_create(self, defaults=None, **kwargs):
        """Get or create an object with the given kwargs."""
        if defaults is None:
            defaults = {}
        defaults['id'] = self.generate_id()
        return super(UUIDModel, self).get_or
----------
Solution: 

        value = super(AleaIdField, self).get_pk_value_on_save(instance)
        if not value:
            value = self.get_seeded_value(instance)
        return value
--------------------
Prompt: 
def _send(self, data, content_type):
        """
        Sends a GCM message with the given content type
        """
----------
Output: 

        headers = {
            'Authorization': 'key=%s' % self.api_key,
            'Content-Type': content_type
        }
        response = requests.post(
            self.url,
            headers=headers,
            data=data
        )
        return response

    def send_message(self, registration_ids, data, collapse_key=None,
                     delay_while_idle=False, time_to_live=None,
                     restricted_package_name=None, dry_run=False,
                
----------
Solution: 


        headers = {
            "Content-Type": content_type,
            "Authorization": "key=%s" % (self.api_key),
            "Content-Length": str(len(data))
        }

        request = Request(self.api_url, data, headers)
        return urlopen(request).read().decode(self.encoding)
--------------------
Prompt: 
def make_clean_visible(_html, tag_replacement_char=' '):
    """
    Takes an HTML-like Unicode string as input and returns a UTF-8
    encoded string with all tags replaced by whitespace. In particular,
    all Unicode characters inside HTML are replaced with a single
    whitespace character.

    This does not detect comments, style, script, link.  It also does
    do anything with HTML-escaped characters.  All of these are
    handled by the clean_html pre-cursor step.

    Pre-existing whitespace of any kind (newlines, tabs) is converted
    to single spaces ' ', which has the same byte length (and
    character length).

    This is a simple state machine iterator without regexes
    """
----------
Output: 
is preserved.

    :param _html: The HTML-like string to be cleaned.
    :param tag_replacement_char: The character to replace tags with.
    :return: The cleaned string.
    """
    # Replace all tags with a single whitespace character.
    # This is a very simple approach, but it works for our purposes.
    # We could do something more sophisticated, but it's not worth it.
    #
    # We use a list of tuples to avoid having to escape the replacement
    # character.

----------
Solution: 

    def non_tag_chars(html):
        n = 0
        while n < len(html):
            angle = html.find('<', n)
            if angle == -1:
                yield html[n:]
                n = len(html)
                break
            yield html[n:angle]
            n = angle

            while n < len(html):
                nl = html.find('\n', n)
                angle = html.find('>', n)
                if angle == -1:
                    yield ' ' * (len(html) - n)
                    n = len(html)
                    break
                elif nl == -1 or angle < nl:
                    yield ' ' * (angle + 1 - n)
                    n = angle + 1
                    break
                else:
                    yield ' ' * (nl - n) + '\n'
                    n = nl + 1
                    # do not break

    if not isinstance(_html, unicode):
        _html = unicode(_html, 'utf-8')

    # Protect emails by substituting with unique key
    _html = fix_emails(_html)

    #Strip tags with previous logic
    non_tag = ''.join(non_tag_chars(_html))

    return non_tag.encode('utf-8')
--------------------
Prompt: 
def match(self, request):
        """
        Matches a given Request instance contract against the registered mocks.

        If a mock passes all the matchers, its response will be returned.

        Arguments:
            request (pook.Request): Request contract to match.

        Raises:
            pook.PookNoMatches: if networking is disabled and no mock matches
                with the given request contract.

        Returns:
            pook.Response: the mock response to be used by the interceptor.
        """
----------
Output: 

        # If networking is disabled, raise an exception
        if not self.enabled:
            raise PookNoMatches(
                'No mocks found for request: {}'.format(request))

        # If the request is a mock, return it
        if request.is_mock:
            return request.response

        # If the request is a mock, return it
        if request.is_mock:
            return request.response

        # If the request is a mock, return it
        if request.is_mock:
            return request.response
----------
Solution: 

        # Trigger engine-level request filters
        for test in self.filters:
            if not test(request, self):
                return False

        # Trigger engine-level request mappers
        for mapper in self.mappers:
            request = mapper(request, self)
            if not request:
                raise ValueError('map function must return a request object')

        # Store list of mock matching errors for further debugging
        match_errors = []

        # Try to match the request against registered mock definitions
        for mock in self.mocks[:]:
            try:
                # Return the first matched HTTP request mock
                matches, errors = mock.match(request.copy())
                if len(errors):
                    match_errors += errors
                if matches:
                    return mock
            except PookExpiredMock:
                # Remove the mock if already expired
                self.mocks.remove(mock)

        # Validate that we have a mock
        if not self.should_use_network(request):
            msg = 'pook error!\n\n'

            msg += (
                '=> Cannot match any mock for the '
                'following request:\n{}'.format(request)
            )

            # Compose unmatch error details, if debug mode is enabled
            if self.debug:
                err = '\n\n'.join([str(err) for err in match_errors])
                if err:
                    msg += '\n\n=> Detailed matching errors:\n{}\n'.format(err)

            # Raise no matches exception
            raise PookNoMatches(msg)

        # Register unmatched request
        self.unmatched_reqs.append(request)
--------------------
Prompt: 
def discrete_best_alpha(data, alpharangemults=(0.9,1.1), n_alpha=201, approximate=True, verbose=True):
    """
    Use the maximum L to determine the most likely value of alpha

    *alpharangemults* [ 2-tuple ]
        Pair of values indicating multiplicative factors above and below the
        approximate alpha from the MLE alpha to use when determining the
        "exact" alpha (by directly maximizing the likelihood function)
    """
----------
Output: 

    if approximate:
        alpha = discrete_alpha(data, n_alpha=n_alpha, verbose=verbose)
    else:
        alpha = discrete_alpha(data, n_alpha=n_alpha, verbose=verbose,
                               approximate=False)

    # Get the range of alpha values to try
    alpharange = np.linspace(alpharangemults[0]*alpha,
                             alpharangemults[1]*alpha,
                             n_alpha)

    # Get the likelihoods
----------
Solution: 


    xmins = np.unique(data)
    if approximate:
        alpha_of_xmin = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]
    else:
        alpha_approx = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]
        alpharanges = [(0.9*a,1.1*a) for a in alpha_approx]
        alpha_of_xmin = [ most_likely_alpha(data,xmin,alpharange=ar,n_alpha=n_alpha) for xmin,ar in zip(xmins,alpharanges) ]
    ksvalues = [ discrete_ksD(data, xmin, alpha) for xmin,alpha in zip(xmins,alpha_of_xmin) ]

    best_index = argmin(ksvalues)
    best_alpha = alpha_of_xmin[best_index]
    best_xmin = xmins[best_index]
    best_ks = ksvalues[best_index]
    best_likelihood = discrete_likelihood(data, best_xmin, best_alpha)

    if verbose:
        print("alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i" % (
                best_alpha, best_xmin, best_ks, best_likelihood,
                (data<best_xmin).sum(), (data>=best_xmin).sum()))

    return best_alpha,best_xmin,best_ks,best_likelihood
--------------------
Prompt: 
def _convert_date(date_string, date_format):
    """
    Convert a date in a given format to epoch time. Mostly a wrapper for
    datetime's strptime.
    """
----------
Output: 

    return int(time.mktime(datetime.datetime.strptime(date_string, date_format).timetuple()))


def _convert_time(time_string, time_format):
    """
    Convert a time in a given format to epoch time. Mostly a wrapper for
    datetime's strptime.
    """
    return int(time.mktime(datetime.datetime.strptime(time_string, time_format).timetuple()))


def _convert_datetime(datetime_string, datetime_
----------
Solution: 

    if date_format != 'epoch':
        return datetime.strptime(date_string, date_format).timestamp()
    else:
        return float(date_string)
--------------------
Prompt: 
def create_session(token=None):
    """
    Create a github3.py session connected to GitHub.com

    If token is not provided, will attempt to use the GITHUB_API_TOKEN
    environment variable if present.
    """
----------
Output: 

    if token is None:
        token = os.environ.get('GITHUB_API_TOKEN')
    if token is None:
        raise ValueError('No token provided')
    return session.Session(token)


def create_session_from_config(config):
    """
    Create a github3.py session connected to GitHub.com

    If token is not provided, will attempt to use the GITHUB_API_TOKEN
    environment variable if present.
    """
    if config.token is None:
       
----------
Solution: 

    if token is None:
        token = os.environ.get('GITHUB_API_TOKEN', None)

    gh_session = github3.login(token=token)

    if gh_session is None:
        raise RuntimeError('Invalid or missing GITHUB_API_TOKEN')

    return gh_session
--------------------
Prompt: 
def load(self, source):
        """
        Opens the source file.
        """
----------
Output: 

        self.source = source
        self.file = open(source, 'r')

    def close(self):
        """
        Closes the source file.
        """
        self.file.close()

    def read(self):
        """
        Reads the source file.
        """
        return self.file.read()

    def write(self, data):
        """
        Writes the data to the source file.
        """
        self.file.write(data)

    def seek(self, position):
----------
Solution: 

        self.source = open(self.source, 'rb')    
        self.loaded = True
--------------------
Prompt: 
def unregisterObserver(self, observer):
        """ Remove an observer from the meter update() chain.

        Args:
            observer (MeterObserver): Subclassed MeterObserver.
        """
----------
Output: 

        self._observers.remove(observer)

    def update(self, value):
        """ Update the meter's value.

        Args:
            value (float): New value for the meter.
        """
        self._value = value
        for observer in self._observers:
            observer.update(self)

    def getValue(self):
        """ Get the meter's current value.

        Returns:
            float: Current value of the meter.
        """
        return self._value

    def
----------
Solution: 

        if observer in self.m_observers:
            self.m_observers.remove(observer)
        pass
--------------------
Prompt: 
def query(self, minhash, size):
        """
        Giving the MinHash and size of the query set, retrieve
        keys that references sets with containment with respect to
        the query set greater than the threshold.

        Args:
            minhash (datasketch.MinHash): The MinHash of the query set.
            size (int): The size (number of unique items) of the query set.

        Returns:
            `iterator` of keys.
        """
----------
Output: 

        return self._query(minhash, size)

    def _query(self, minhash, size):
        """
        Giving the MinHash and size of the query set, retrieve
        keys that references sets with containment with respect to
        the query set greater than the threshold.

        Args:
            minhash (datasketch.MinHash): The MinHash of the query set.
            size (int): The size (number of unique items) of the query set.

        Returns:
            `iterator` of keys.
        """

----------
Solution: 

        for i, index in enumerate(self.indexes):
            u = self.uppers[i]
            if u is None:
                continue
            b, r = self._get_optimal_param(u, size)
            for key in index[r]._query_b(minhash, b):
                yield key
--------------------
Prompt: 
def get_security_repository(self):
        """ Security repository """
----------
Output: 

        return self._security_repository

    @property
    def security_repository(self):
        """ Security repository """
        return self._security_repository

    @security_repository.setter
    def security_repository(self, value):
        """ Security repository """
        self._security_repository = value

    @property
    def security_repository_id(self):
        """ Security repository id """
        return self._security_repository_id

    @security_repository_id.setter
    def security_repository_id(self,
----------
Solution: 

        from .repositories import SecurityRepository

        if not self.security_repo:
            self.security_repo = SecurityRepository(self.session)
        return self.security_repo
--------------------
Prompt: 
def Counter(a, b, delta):
    """Counter derivative
    """
----------
Output: 

    return a + delta * b


def Counter_deriv(a, b, delta):
    """Counter derivative
    """
    return delta * b


def Counter_deriv2(a, b, delta):
    """Counter derivative
    """
    return 0


def Counter_deriv3(a, b, delta):
    """Counter derivative
    """
    return 0


def Counter_deriv4(a, b, delta):
    """Counter derivative
    """
    return 0
----------
Solution: 

    if b < a:
        return None 

    return (b - a) / float(delta)
--------------------
Prompt: 
def cli_command_restart(self, msg):
        """\
        restart the subprocess
        i. we set our state to RESTARTING - on restarting we still send heartbeat
        ii. we kill the subprocess
        iii. we start again
        iv. if its started we set our state to RUNNING, else we set it to WAITING

        :param msg:
        :return:
        """
----------
Output: 

        self.logger.info("restarting")
        self.state = self.STATE_RESTARTING
        self.kill_subprocess()
        self.start_subprocess()
        if self.subprocess_is_running():
            self.state = self.STATE_RUNNING
        else:
            self.state = self.STATE_WAITING

    def cli_command_stop(self, msg):
        """\
        stop the subprocess
        i. we set our state to STOPPING - on stopping we still
----------
Solution: 

        info = ''
        if self.state == State.RUNNING and self.sprocess and self.sprocess.proc:
            self.state = State.RESTARTING
            self.sprocess.set_exit_callback(self.proc_exit_cb_restart)
            self.sprocess.proc.kill()
            info = 'killed'
            # TODO: check if process is really dead etc.
        return info
--------------------
Prompt: 
def delete_dir(bucket_name, root_path,
               aws_access_key_id=None, aws_secret_access_key=None,
               aws_profile=None):
    """Delete all objects in the S3 bucket named ``bucket_name`` that are
    found in the ``root_path`` directory.

    Parameters
    ----------
    bucket_name : `str`
        Name of an S3 bucket.
    root_path : `str`
        Directory in the S3 bucket that will be deleted.
    aws_access_key_id : `str`
        The access key for your AWS account. Also set
        ``aws_secret_access_key``.
    aws_secret_access_key : `str`
        The secret key for your AWS account.
    aws_profile : `str`, optional
        Name of AWS profile in :file:`~/.aws/credentials`. Use this instead
        of ``aws_access_key_id`` and ``aws_secret_access_key`` for file-based
        credentials.

    Raises
    ------
    ltdconveyor.s3.S3Error
        Thrown by any unexpected faults from the S3 API.
    """
----------
Output: 
_access_key_id : `str`, optional
        AWS access key ID.
    aws_secret_access_key : `str`, optional
        AWS secret access key.
    aws_profile : `str`, optional
        AWS profile name.

    """
    s3 = get_s3_resource(aws_access_key_id, aws_secret_access_key, aws_profile)
    bucket = s3.Bucket(bucket_name)
    for obj in bucket.objects.filter(Prefix=root_path):
        obj.delete
----------
Solution: 

    logger = logging.getLogger(__name__)

    session = boto3.session.Session(
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key)
    s3 = session.resource('s3')
    client = s3.meta.client

    # Normalize directory path for searching patch prefixes of objects
    if not root_path.endswith('/'):
        root_path.rstrip('/')

    paginator = client.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket_name, Prefix=root_path)

    keys = dict(Objects=[])
    for item in pages.search('Contents'):
        try:
            keys['Objects'].append({'Key': item['Key']})
        except TypeError:  # item is None; nothing to delete
            continue
        # Delete immediately when 1000 objects are listed
        # the delete_objects method can only take a maximum of 1000 keys
        if len(keys['Objects']) >= 1000:
            try:
                client.delete_objects(Bucket=bucket_name, Delete=keys)
            except Exception:
                message = 'Error deleting objects from %r' % root_path
                logger.exception(message)
                raise S3Error(message)
            keys = dict(Objects=[])

    # Delete remaining keys
    if len(keys['Objects']) > 0:
        try:
            client.delete_objects(Bucket=bucket_name, Delete=keys)
        except Exception:
            message = 'Error deleting objects from %r' % root_path
            logger.exception(message)
            raise S3Error(message)
--------------------
Prompt: 
def find_launcher_class(clsname, kind):
    """Return a launcher for a given clsname and kind.

    Parameters
    ==========
    clsname : str
        The full name of the launcher class, either with or without the
        module path, or an abbreviation (MPI, SSH, SGE, PBS, LSF,
        WindowsHPC).
    kind : str
        Either 'EngineSet' or 'Controller'.
    """
----------
Output: 

    if clsname in _launcher_classes:
        return _launcher_classes[clsname]
    if clsname in _launcher_abbreviations:
        clsname = _launcher_abbreviations[clsname]
        if clsname in _launcher_classes:
            return _launcher_classes[clsname]
    if kind == 'EngineSet':
        if clsname in _launcher_classes:
            return _launcher_classes[clsname]
        if clsname in _launcher
----------
Solution: 

    if '.' not in clsname:
        # not a module, presume it's the raw name in apps.launcher
        if kind and kind not in clsname:
            # doesn't match necessary full class name, assume it's
            # just 'PBS' or 'MPI' prefix:
            clsname = clsname + kind + 'Launcher'
        clsname = 'IPython.parallel.apps.launcher.'+clsname
    klass = import_item(clsname)
    return klass
--------------------
Prompt: 
def get_end_of_day(timestamp):
    """
    Given a date or a datetime, return a datetime at 23:59:59 on that day
    """
----------
Output: 

    if isinstance(timestamp, datetime):
        return datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)
    else:
        return datetime.combine(timestamp, time(23, 59, 59))


def get_start_of_day(timestamp):
    """
    Given a date or a datetime, return a datetime at 00:00:00 on that day
    """
    if isinstance(timestamp, datetime):
        return datetime(timestamp.
----------
Solution: 

    return datetime.datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)
--------------------
Prompt: 
def find_modules(rootpath, skip):
    """
    Look for every file in the directory tree and return a dict
    Hacked from sphinx.autodoc
    """
----------
Output: 

    modules = {}
    for dirpath, dirnames, filenames in os.walk(rootpath):
        # Strip the rootpath from the dirpath
        dirpath = dirpath[len(rootpath):].lstrip(os.path.sep)
        if dirpath and dirpath not in skip:
            # If we are not in the root path, add the directory to the
            # modules dictionary
            modules[dirpath] = []
        for filename in filenames:
            if filename.endswith
----------
Solution: 


    INITPY = '__init__.py'

    rootpath = os.path.normpath(os.path.abspath(rootpath))
    if INITPY in os.listdir(rootpath):
        root_package = rootpath.split(os.path.sep)[-1]
        print "Searching modules in", rootpath
    else:
        print "No modules in", rootpath
        return

    def makename(package, module):
        
--------------------
Prompt: 
def institutes(self, institute_ids=None):
        """Fetch all institutes.
        
        Args:
            institute_ids(list(str))
        
        Returns:
            res(pymongo.Cursor)
        """
----------
Output: 

        if institute_ids is None:
            institute_ids = []
        return self.institutes.find({"_id": {"$in": institute_ids}})

    def institute(self, institute_id):
        """Fetch an institute.
        
        Args:
            institute_id(str)
        
        Returns:
            res(pymongo.Cursor)
        """
        return self.institutes.find_one({"_id": institute_id})

    def institute_
----------
Solution: 

        query = {}
        if institute_ids:
            query['_id'] = {'$in': institute_ids}
        LOG.debug("Fetching all institutes")
        return self.institute_collection.find(query)
--------------------
Prompt: 
def send(self, node, message):
        """
        Send a message to a node. Returns False if the connection appears to be dead either before or after actually trying to send the message.

        :param node: target node
        :type node: Node
        :param message: message
        :param message: any
        :returns success
        :rtype bool
        """
----------
Output: 

        if self.is_dead():
            return False
        try:
            self.send_message(node, message)
        except Exception:
            return False
        return True

    def send_message(self, node, message):
        """
        Send a message to a node.

        :param node: target node
        :type node: Node
        :param message: message
        :param message: any
        """
        raise NotImplementedError

    def is_dead(self):
        """
        Check if
----------
Solution: 


        if node not in self._connections or self._connections[node].state != CONNECTION_STATE.CONNECTED:
            return False
        self._connections[node].send(message)
        if self._connections[node].state != CONNECTION_STATE.CONNECTED:
            return False
        return True
--------------------
Prompt: 
def create_model(self, project_id, model):
        """
        Create a Model. Blocks until finished.
        """
----------
Output: 

        operation = self.client.create_model(
            project_id, model, retry=self.retry, timeout=self.timeout
        )
        operation.result(timeout=self.timeout)

    def delete_model(self, project_id, model_id):
        """
        Delete a Model. Blocks until finished.
        """
        operation = self.client.delete_model(
            project_id, model_id, retry=self.retry, timeout=self.timeout
        )
        operation.result(timeout
----------
Solution: 

        if not model['name']:
            raise ValueError("Model name must be provided and "
                             "could not be an empty string")
        project = 'projects/{}'.format(project_id)

        request = self._mlengine.projects().models().create(
            parent=project, body=model)
        return request.execute()
--------------------
Prompt: 
def read_msg(self):
        """Read one message unit. It's possible however that
        more than one message will be set in a receive, so we will
        have to buffer that for the next read.
        EOFError will be raised on EOF.
        """
----------
Output: 

        if self.buffer:
            return self.buffer.pop(0)
        if self.closed:
            raise EOFError
        try:
            msg = self.socket.recv(self.max_msg_size)
        except socket.error as e:
            if e.errno == errno.EAGAIN:
                return None
            raise
        if not msg:
            self.closed = True
            raise EOFError
        return msg

    def read(self, n=-1):
        """Read up to n
----------
Solution: 

        if self.state == 'connected':
            if 0 == len(self.buf):
                self.buf = self.inout.recv(Mtcpfns.TCP_MAX_PACKET)
                if 0 == (self.buf):
                    self.state = 'disconnected'
                    raise EOFError
                pass
            self.buf, data = Mtcpfns.unpack_msg(self.buf)
            return data.decode('utf-8')
        else:
            raise IOError("read_msg called in state: %s." % self.state)
--------------------
Prompt: 
def _wakeup(self):
        """
        issue wakeup command to device to take out of standby mode.
        """
----------
Output: 

        self.log.debug("wakeup")
        self.write("wakeup")

    def _sleep(self):
        """
        issue sleep command to device to put into standby mode.
        """
        self.log.debug("sleep")
        self.write("sleep")

    def _reset(self):
        """
        issue reset command to device to reset.
        """
        self.log.debug("reset")
        self.write("reset")

    def _set_power(self, power):
        """
----------
Solution: 

        log.info("send: WAKEUP")
        for i in xrange(3):
            self.port.write('\n')  # wakeup device
            ack = self.port.read(len(self.WAKE_ACK))  # read wakeup string
            log_raw('read', ack)
            if ack == self.WAKE_ACK:
                return
        raise NoDeviceException('Can not access weather station')
--------------------
Prompt: 
def get_home_dir(require_writable=False):
    """Return the 'home' directory, as a unicode string.

    * First, check for frozen env in case of py2exe
    * Otherwise, defer to os.path.expanduser('~')
    
    See stdlib docs for how this is determined.
    $HOME is first priority on *ALL* platforms.
    
    Parameters
    ----------
    
    require_writable : bool [default: False]
        if True:
            guarantees the return value is a writable directory, otherwise
            raises HomeDirError
        if False:
            The path is resolved, but it is not guaranteed to exist or be writable.
    """
----------
Output: 
value is a writable directory
            (by checking if the directory is writable)

    Returns
    -------
    unicode
        the home directory

    Raises
    ------
    IOError
        if the home directory is not writable
    """
    if getattr(sys, 'frozen', False):
        # If the application is run as a bundle, the pyInstaller bootloader
        # extends the sys module by a flag frozen=True and sets the app 
        # path into variable _MEIPASS'.
        return os
----------
Solution: 


    # first, check py2exe distribution root directory for _ipython.
    # This overrides all. Normally does not exist.

    if hasattr(sys, "frozen"): #Is frozen by py2exe
        if '\\library.zip\\' in IPython.__file__.lower():#libraries compressed to zip-file
            root, rest = IPython.__file__.lower().split('library.zip')
        else:
            root=os.path.join(os.path.split(IPython.__file__)[0],"../../")
        root=os.path.abspath(root).rstrip('\\')
        if _writable_dir(os.path.join(root, '_ipython')):
            os.environ["IPYKITROOT"] = root
        return py3compat.cast_unicode(root, fs_encoding)
    
    homedir = os.path.expanduser('~')
    # Next line will make things work even when /home/ is a symlink to
    # /usr/home as it is on FreeBSD, for example
    homedir = os.path.realpath(homedir)
    
    if not _writable_dir(homedir) and os.name == 'nt':
        # expanduser failed, use the registry to get the 'My Documents' folder.
        try:
            import _winreg as wreg
            key = wreg.OpenKey(
                wreg.HKEY_CURRENT_USER,
                "Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders"
            )
            homedir = wreg.QueryValueEx(key,'Personal')[0]
            key.Close()
        except:
            pass
    
    if (not require_writable) or _writable_dir(homedir):
        return py3compat.cast_unicode(homedir, fs_encoding)
    else:
        raise HomeDirError('%s is not a writable dir, '
                'set $HOME environment variable to override' % homedir)
--------------------
Prompt: 
def run_eplus(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None):
    """
    Parameters
    ----------
    epm_or_idf_path:
    weather_data_or_epw_path
    simulation_dir_path
    stdout: default sys.stdout
    stderr: default sys.stderr
    beat_freq: if not none, stdout will be used at least every beat_freq (in seconds)
    """
----------
Output: 
every beat_freq seconds
    """
    if isinstance(epm_or_idf_path, str):
        epm_or_idf_path = Path(epm_or_idf_path)
    if isinstance(weather_data_or_epw_path, str):
        weather_data_or_epw_path = Path(weather_data_or_epw_path)
    if isinstance(simulation_dir_path, str):
        simulation_dir_path = Path(simulation_dir_
----------
Solution: 

    # work with absolute paths
    simulation_dir_path = os.path.abspath(simulation_dir_path)

    # check dir path
    if not os.path.isdir(simulation_dir_path):
        raise NotADirectoryError("Simulation directory does not exist: '%s'." % simulation_dir_path)

    # epm
    if not isinstance(epm_or_idf_path, Epm):
        # we don't copy file directly because we want to manage it's external files
        # could be optimized (use _copy_without_read_only)
        epm = Epm.from_idf(epm_or_idf_path)
    else:
        epm = epm_or_idf_path

    # create idf
    simulation_idf_path = os.path.join(simulation_dir_path, CONF.default_model_name + ".idf")
    epm.to_idf(simulation_idf_path)

    # weather data
    simulation_epw_path = os.path.join(simulation_dir_path, CONF.default_model_name + ".epw")
    if isinstance(weather_data_or_epw_path, WeatherData):
        weather_data_or_epw_path.to_epw(simulation_epw_path)
    else:
        # no need to load: we copy directly
        _copy_without_read_only(weather_data_or_epw_path, simulation_epw_path)

    # copy epw if needed (depends on os/eplus version)
    temp_epw_path = get_simulated_epw_path()
    if temp_epw_path is not None:
        _copy_without_read_only(simulation_epw_path, temp_epw_path)

    # prepare command
    eplus_relative_cmd = get_simulation_base_command()
    eplus_cmd = os.path.join(CONF.eplus_base_dir_path, eplus_relative_cmd)

    # idf
    idf_command_style = get_simulation_input_command_style("idf")
    if idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:
        idf_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)
    elif idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:
        idf_file_cmd = simulation_idf_path
    else:
        raise AssertionError("should not be here")

    # epw
    epw_command_style = get_simulation_input_command_style("epw")
    if epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:
        epw_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)
    elif epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:
        epw_file_cmd = simulation_epw_path
    else:
        raise AssertionError("should not be here")

    # command list
    simulation_command_style = get_simulation_command_style()
    if simulation_command_style == SIMULATION_COMMAND_STYLES.args:
        cmd_l = [eplus_cmd, idf_file_cmd, epw_file_cmd]
    elif simulation_command_style == SIMULATION_COMMAND_STYLES.kwargs:
        cmd_l = [eplus_cmd, "-w", epw_file_cmd, "-r", idf_file_cmd]
    else:
        raise RuntimeError("should not be here")

    # launch calculation
    run_subprocess(
        cmd_l,
        cwd=simulation_dir_path,
        stdout=stdout,
        stderr=stderr,
        beat_freq=beat_freq
    )

    # if needed, we delete temp weather data (only on Windows, see above)
    if (temp_epw_path is not None) and os.path.isfile(temp_epw_path):
        os.remove(os.path.join(temp_epw_path))
--------------------
Prompt: 
def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:
    """Generate recursive Python Attribute AST nodes for resolving nested
    names."""
----------
Output: 

    return ast.Attribute(
        value=_load_attr(name.split(".")[0], ctx) if "." in name else ctx,
        attr=name.split(".")[-1],
        ctx=ctx,
    )


def _load_name(name: str, ctx: ast.AST = ast.Load()) -> ast.Name:
    """Generate recursive Python Name AST nodes for resolving nested names."""
    return ast.Name(id=name.split(".")[-1], ctx=ctx) if "." in name else
----------
Solution: 

    attrs = name.split(".")

    def attr_node(node, idx):
        if idx >= len(attrs):
            node.ctx = ctx
            return node
        return attr_node(
            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1
        )

    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)
--------------------
Prompt: 
def _prepare_query_values(self):
        """Extra prep on query values by converting
        dictionaries into :see:HStoreValue expressions.

        This allows putting expressions in a dictionary.
        The :see:HStoreValue will take care of resolving
        the expressions inside the dictionary."""
----------
Output: 

        for i, value in enumerate(self.values):
            if isinstance(value, dict):
                self.values[i] = HStoreValue(value)

    def _prepare_query_values_for_insert(self):
        """Extra prep on query values by converting
        dictionaries into :see:HStoreValue expressions.

        This allows putting expressions in a dictionary.
        The :see:HStoreValue will take care of resolving
        the expressions inside the dictionary."""
        for i, value in enumerate(self.values):
           
----------
Solution: 


        new_query_values = []
        for field, model, val in self.query.values:
            if isinstance(val, dict):
                val = HStoreValue(val)

            new_query_values.append((
                field,
                model,
                val
            ))

        self.query.values = new_query_values
--------------------
Prompt: 
def get_handler(level: str, fmt: str) -> logging.Handler:
    """Get the default logging handler for Basilisp."""
----------
Output: 

    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter(fmt))
    handler.setLevel(level)
    return handler


def get_logger(name: str, level: str, fmt: str) -> logging.Logger:
    """Get the default logging handler for Basilisp."""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(get_handler(level, fmt))
    return logger


def get_logger_from_config(config: dict
----------
Solution: 

    handler: logging.Handler = logging.NullHandler()
    if os.getenv("BASILISP_USE_DEV_LOGGER") == "true":
        handler = logging.StreamHandler()

    handler.setFormatter(logging.Formatter(fmt))
    handler.setLevel(level)
    return handler
--------------------
Prompt: 
def _get_course_content_from_ecommerce(course_id, site_code=None):
    """
    Get course information using the Ecommerce course api.

    In case of error returns empty response.
    Arguments:
        course_id (str): course key of the course
        site_code (str): site code

    Returns:
        course information from Ecommerce
    """
----------
Output: 

    try:
        if site_code:
            return get_course_content_from_ecommerce(course_id, site_code)
        else:
            return get_course_content_from_ecommerce(course_id)
    except Exception:
        return {}


def _get_course_content_from_edx(course_id, site_code=None):
    """
    Get course information using the edX course api.

    In case of error returns empty response.
    Arguments:
        course_id (str
----------
Solution: 

    api = get_ecommerce_client(site_code=site_code)
    try:
        api_response = api.courses(course_id).get()
    except Exception:  # pylint: disable=broad-except
        logger.exception(
            'An error occurred while retrieving data for course run [%s] from the Catalog API.',
            course_id,
            exc_info=True
        )
        return {}

    return {
        'title': api_response.get('name'),
        'verification_deadline': api_response.get('verification_deadline')
    }
--------------------
Prompt: 
def container_services(self):
        """Instance depends on the API version:

           * 2017-07-01: :class:`ContainerServicesOperations<azure.mgmt.containerservice.v2017_07_01.operations.ContainerServicesOperations>`
        """
----------
Output: 

        api_version = self._get_api_version('container_services')
        if api_version == '2017-07-01':
            from .v2017_07_01.operations import ContainerServicesOperations as OperationClass
        else:
            raise ValueError("API version {} does not have operation group 'container_services'".format(api_version))
        return OperationClass(self._client, self._config, Serializer(self._models_dict(api_version)), Deserializer(self._models
----------
Solution: 

        api_version = self._get_api_version('container_services')
        if api_version == '2017-07-01':
            from .v2017_07_01.operations import ContainerServicesOperations as OperationClass
        else:
            raise NotImplementedError("APIVersion {} is not available".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))
--------------------
Prompt: 
def doc_from_xml(document_element_name, inner_xml,
                     xmlns='http://schemas.microsoft.com/windowsazure'):
        """Wraps the specified xml in an xml root element with default azure
        namespaces"""
----------
Output: 

        return '<{0} xmlns="{1}">{2}</{0}>'.format(document_element_name,
                                                    xmlns, inner_xml)


def get_xml_element(xml_doc, element_name):
    """Returns the specified xml element from the xml document"""
    return xml_doc.find(element_name)


def get_xml_elements(xml_doc, element_name):
    """Returns all the specified xml elements from the xml document"""
    return xml_doc.findall(
----------
Solution: 

        xml = ''.join(['<', document_element_name,
                      ' xmlns="{0}">'.format(xmlns)])
        xml += inner_xml
        xml += ''.join(['</', document_element_name, '>'])
        return xml
--------------------
Prompt: 
def lessThan(self, left, right):
        """Return ordering of *left* vs *right*."""
----------
Output: 

        return left < right

    def lessThanEqual(self, left, right):
        """Return ordering of *left* vs *right*."""
        return left <= right

    def greaterThan(self, left, right):
        """Return ordering of *left* vs *right*."""
        return left > right

    def greaterThanEqual(self, left, right):
        """Return ordering of *left* vs *right*."""
        return left >= right

    def equal(self, left, right):
        """
----------
Solution: 

        sourceModel = self.sourceModel()
        if sourceModel:
            leftItem = sourceModel.item(left)
            rightItem = sourceModel.item(right)

            if (isinstance(leftItem, Directory)
                and not isinstance(rightItem, Directory)):
                return self.sortOrder() == Qt.AscendingOrder

            elif (not isinstance(leftItem, Directory)
                and isinstance(rightItem, Directory)):
                return self.sortOrder() == Qt.DescendingOrder

        return super(FilesystemSortProxy, self).lessThan(left, right)
--------------------
Prompt: 
def get_entry_properties_from_element(element, include_id, id_prefix_to_skip=None, use_title_as_id=False):
        """ get properties from element tree element """
----------
Output: 

        properties = {}
        if element.tag == 'entry':
            for child in element:
                if child.tag == 'id' and not include_id:
                    continue
                if child.tag == 'id' and id_prefix_to_skip and child.text.startswith(id_prefix_to_skip):
                    continue
                if child.tag == 'title' and use_title_as_id:
                    properties['id'] = child.text
                else:
                    properties[child.tag] = child.text

----------
Solution: 

        properties = {}

        etag = element.attrib.get(_make_etree_ns_attr_name(_etree_entity_feed_namespaces['m'], 'etag'), None)
        if etag is not None:
            properties['etag'] = etag

        updated = element.findtext('./atom:updated', '', _etree_entity_feed_namespaces)
        if updated:
            properties['updated'] = updated

        author_name = element.findtext('./atom:author/atom:name', '', _etree_entity_feed_namespaces)
        if author_name:
            properties['author'] = author_name

        if include_id:
            if use_title_as_id:
                title = element.findtext('./atom:title', '', _etree_entity_feed_namespaces)
                if title:
                    properties['name'] = title
            else:
                element_id = element.findtext('./atom:id', '', _etree_entity_feed_namespaces)
                if element_id:
                    properties['name'] = _get_readable_id(element_id, id_prefix_to_skip)

        return properties
--------------------
Prompt: 
def source_address(self):
        """Return the authorative source of the link."""
----------
Output: 

        return self._source_address

    @property
    def destination_address(self):
        """Return the authorative destination of the link."""
        return self._destination_address

    @property
    def source_port(self):
        """Return the authorative source port of the link."""
        return self._source_port

    @property
    def destination_port(self):
        """Return the authorative destination port of the link."""
        return self._destination_port

    @property
    def source_interface(self
----------
Solution: 

        # If link is a sender, source is determined by the local
        # value, else use the remote.
        if self._pn_link.is_sender:
            return self._pn_link.source.address
        else:
            return self._pn_link.remote_source.address
--------------------
Prompt: 
def _serialize(xp_ast):
    """Generate token strings which, when joined together, form a valid
    XPath serialization of the AST."""
----------
Output: 

    if isinstance(xp_ast, str):
        return xp_ast
    elif isinstance(xp_ast, list):
        return ''.join(_serialize(child) for child in xp_ast)
    elif isinstance(xp_ast, dict):
        return ''.join(
            _serialize(child) for child in xp_ast.values()
        )
    elif isinstance(xp_ast, tuple):
        return ''.join(
            _serialize(child) for child in xp_ast
        )
----------
Solution: 


    if hasattr(xp_ast, '_serialize'):
        for tok in xp_ast._serialize():
            yield(tok)
    elif isinstance(xp_ast, str):
        yield(repr(xp_ast))
--------------------
Prompt: 
def check_for_prefix(self, bucket_name, prefix, delimiter):
        """
        Checks that a prefix exists in a bucket

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        """
----------
Output: 

        if not prefix:
            return

        if delimiter:
            prefix = prefix.rstrip(delimiter)

        if prefix.endswith(delimiter):
            prefix = prefix[:-1]

        if not self.check_for_key(bucket_name, prefix):
            raise ValueError("Prefix %s does not exist in bucket %s" % (prefix, bucket_name))

    def check_for_key(self, bucket_name, key):
        """
        Checks that a key exists in a bucket

        :
----------
Solution: 

        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix
        prefix_split = re.split(r'(\w+[{d}])$'.format(d=delimiter), prefix, 1)
        previous_level = prefix_split[0]
        plist = self.list_prefixes(bucket_name, previous_level, delimiter)
        return False if plist is None else prefix in plist
--------------------
Prompt: 
def group(self, name):
        """GROUP command.
        """
----------
Output: 

        return self.send_command(b'GROUP', name)

    def id(self):
        """ID command.
        """
        return self.send_command(b'ID')

    def info(self):
        """INFO command.
        """
        return self.send_command(b'INFO')

    def lastsave(self):
        """LASTSAVE command.
        """
        return self.send_command(b'LASTSAVE')

    def monitor(self):
        """MONITOR
----------
Solution: 

        args = name

        code, message = self.command("GROUP", args)
        if code != 211:
            raise NNTPReplyError(code, message)

        parts = message.split(None, 4)
        try:
            total = int(parts[0])
            first = int(parts[1])
            last  = int(parts[2])
            group = parts[3]
        except (IndexError, ValueError):
            raise NNTPDataError("Invalid GROUP status '%s'" % message)

        return total, first, last, group
--------------------
Prompt: 
def get(self, thing_id='0', property_name=None):
        """
        Handle a GET request.

        thing_id -- ID of the thing this request is for
        property_name -- the name of the property from the URL path
        """
----------
Output: 

        if property_name is None:
            return self.get_thing(thing_id)
        else:
            return self.get_property(thing_id, property_name)

    def get_thing(self, thing_id):
        """
        Return the thing with the given ID.

        thing_id -- ID of the thing this request is for
        """
        thing = self.get_thing_by_id(thing_id)
        if thing is None:
            return self.not_found()
        else:
           
----------
Solution: 

        thing = self.get_thing(thing_id)
        if thing is None:
            self.set_status(404)
            return

        if thing.has_property(property_name):
            self.set_header('Content-Type', 'application/json')
            self.write(json.dumps({
                property_name: thing.get_property(property_name),
            }))
        else:
            self.set_status(404)
--------------------
Prompt: 
def plexp_inv(P, xmin, alpha, guess=1.):
    """
    Inverse CDF for a piecewise PDF as defined in eqn. 3.10
    of Clauset et al.

    (previous version was incorrect and lead to weird discontinuities in the
    distribution function)
    """
----------
Output: 

    if P < 0 or P > 1:
        raise ValueError("P must be in [0, 1]")
    if P == 0:
        return xmin
    if P == 1:
        return alpha
    if P < 1./3:
        return xmin + (alpha - xmin) * P**(1./3)
    if P < 2./3:
        return alpha - (alpha - xmin) * (1 - P)**(1./3)
    return alpha


def plex
----------
Solution: 

    def equation(x,prob):
        return plexp_cdf(x, xmin, alpha)-prob
    # http://stackoverflow.com/questions/19840425/scipy-optimize-faster-root-finding-over-2d-grid
    def solver(y, x0=guess):
        return scipy.optimize.fsolve(equation, guess, args=(y,))
    f = np.vectorize(solver)
    return f(P)
--------------------
Prompt: 
def start(self, context):
		"""Construct the SQLAlchemy engine and session factory."""
----------
Output: 

		self.engine = sqlalchemy.create_engine(
			self.config.get('database', 'url'),
			pool_size=self.config.getint('database', 'pool_size'),
			max_overflow=self.config.getint('database', 'max_overflow'),
			pool_recycle=self.config.getint('database', 'pool_recycle'),
			pool_timeout=self.config.getint('database', 'pool_timeout'),
			echo=self
----------
Solution: 

		
		if __debug__:
			log.info("Connecting SQLAlchemy database layer.", extra=dict(
					uri = redact_uri(self.uri),
					config = self.config,
					alias = self.alias,
				))
		
		# Construct the engine.
		engine = self.engine = create_engine(self.uri, **self.config)
		
		# Construct the session factory.
		self.Session = scoped_session(sessionmaker(bind=engine))
		
		# Test the connection.
		engine.connect().close()
		
		# Assign the engine to our database alias.
		context.db[self.alias] = engine
--------------------
Prompt: 
def _matches_filepath_pattern(self, filepath):
        """Given a filepath, and a list of regex patterns, this function returns true
        if filepath matches any one of those patterns"""
----------
Output: 

        for pattern in self.filepath_patterns:
            if re.match(pattern, filepath):
                return True
        return False

    def _matches_filepath_exclude_pattern(self, filepath):
        """Given a filepath, and a list of regex patterns, this function returns true
        if filepath matches any one of those patterns"""
        for pattern in self.filepath_exclude_patterns:
            if re.match(pattern, filepath):
                return True
        return False

    def _matches
----------
Solution: 

        if not self.only_blame_patterns:
            return True

        for pattern in self.only_blame_patterns:
            if pattern.match(filepath):
                return True
        return False
--------------------
Prompt: 
def iter_auth_hashes(user, purpose, minutes_valid):
    """
    Generate auth tokens tied to user and specified purpose.

    The hash expires at midnight on the minute of now + minutes_valid, such
    that when minutes_valid=1 you get *at least* 1 minute to use the token.
    """
----------
Output: 

    now = datetime.datetime.utcnow()
    expires = now + datetime.timedelta(minutes=minutes_valid)
    expires = expires.replace(minute=0, second=0, microsecond=0)
    if expires < now:
        expires = expires + datetime.timedelta(days=1)
    for i in range(10000):
        token = hashlib.sha256(
            str(uuid.uuid4()) + str(now) + str(i) +
----------
Solution: 

    now = timezone.now().replace(microsecond=0, second=0)
    for minute in range(minutes_valid + 1):
        yield hashlib.sha1(
            '%s:%s:%s:%s:%s' % (
                now - datetime.timedelta(minutes=minute),
                user.password,
                purpose,
                user.pk,
                settings.SECRET_KEY,
            ),
        ).hexdigest()
--------------------
Prompt: 
def _control_key_down(self, modifiers, include_command=False):
        """ Given a KeyboardModifiers flags object, return whether the Control
        key is down.

        Parameters:
        -----------
        include_command : bool, optional (default True)
            Whether to treat the Command key as a (mutually exclusive) synonym
            for Control when in Mac OS.
        """
----------
Output: 

        if self.platform == 'darwin':
            if include_command:
                return modifiers & KeyboardModifiers.COMMAND
            else:
                return modifiers & KeyboardModifiers.CONTROL
        else:
            return modifiers & KeyboardModifiers.CONTROL

    def _control_key_up(self, modifiers, include_command=False):
        """ Given a KeyboardModifiers flags object, return whether the Control
        key is up.

        Parameters:
        -----------
        include_command :
----------
Solution: 

        # Note that on Mac OS, ControlModifier corresponds to the Command key
        # while MetaModifier corresponds to the Control key.
        if sys.platform == 'darwin':
            down = include_command and (modifiers & QtCore.Qt.ControlModifier)
            return bool(down) ^ bool(modifiers & QtCore.Qt.MetaModifier)
        else:
            return bool(modifiers & QtCore.Qt.ControlModifier)
--------------------
Prompt: 
def compat_validate_token(cls, *args, **kwargs):
        """Multiple algorithm-compatible token validation."""
----------
Output: 

        try:
            return super(cls, cls).validate_token(*args, **kwargs)
        except (jwt.exceptions.InvalidSignatureError,
                jwt.exceptions.DecodeError):
            pass

        # Try to decode with all algorithms
        for algorithm in cls.ALGORITHMS:
            try:
                return super(cls, cls).decode_token(
                    *args,
                    algorithm=algorithm,
                    **kwargs
                )
            except (jwt.exceptions.InvalidSign
----------
Solution: 

        data = None
        for algorithm in SUPPORTED_DIGEST_ALGORITHMS:
            data = cls(algorithm_name=algorithm).validate_token(
                *args, **kwargs)
            if not data:  # move to next algorithm
                continue
        return data
--------------------
Prompt: 
def timestamp_YmdHMS(value):
    """Convert timestamp string to time in seconds since epoch.

    Timestamps strings like '20130618120000' are able to be converted by this
    function.

    Args:
        value: A timestamp string in the format '%Y%m%d%H%M%S'.

    Returns:
        The time in seconds since epoch as an integer.

    Raises:
        ValueError: If timestamp is invalid.

    Note: The timezone is assumed to be UTC/GMT.
    """
----------
Output: 
:
        This function is not used in the code, but is provided for
        completeness.
    """
    try:
        return int(time.mktime(time.strptime(value, '%Y%m%d%H%M%S')))
    except ValueError:
        raise ValueError('Invalid timestamp: %s' % value)


def timestamp_YmdHMS_to_datetime(value):
    """Convert timestamp string to datetime.

    Timestamps strings like '2013061812
----------
Solution: 

    i = int(value)
    S = i
    M = S//100
    H = M//100
    d = H//100
    m = d//100
    Y = m//100
    return int(calendar.timegm((
        Y % 10000, m % 100, d % 100, H % 100, M % 100, S % 100, 0, 0, 0)
    ))
--------------------
Prompt: 
def report_by_type_stats(sect, stats, _):
    """make a report of

    * percentage of different types documented
    * percentage of different types with a bad name
    """
----------
Output: 

    if sect.name == "type":
        return
    if sect.name == "function":
        return
    if sect.name == "variable":
        return
    if sect.name == "constant":
        return
    if sect.name == "class":
        return
    if sect.name == "method":
        return
    if sect.name == "attribute":
        return
    if sect.name == "module":
        return
    if sect.name == "exception":
        return
    if sect.name == "keyword":

----------
Solution: 

    # percentage of different types documented and/or with a bad name
    nice_stats = {}
    for node_type in ("module", "class", "method", "function"):
        try:
            total = stats[node_type]
        except KeyError:
            raise exceptions.EmptyReportError()
        nice_stats[node_type] = {}
        if total != 0:
            try:
                documented = total - stats["undocumented_" + node_type]
                percent = (documented * 100.0) / total
                nice_stats[node_type]["percent_documented"] = "%.2f" % percent
            except KeyError:
                nice_stats[node_type]["percent_documented"] = "NC"
            try:
                percent = (stats["badname_" + node_type] * 100.0) / total
                nice_stats[node_type]["percent_badname"] = "%.2f" % percent
            except KeyError:
                nice_stats[node_type]["percent_badname"] = "NC"
    lines = ("type", "number", "old number", "difference", "%documented", "%badname")
    for node_type in ("module", "class", "method", "function"):
        new = stats[node_type]
        lines += (
            node_type,
            str(new),
            "NC",
            "NC",
            nice_stats[node_type].get("percent_documented", "0"),
            nice_stats[node_type].get("percent_badname", "0"),
        )
    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))
--------------------
Prompt: 
def find_safe(ns_qualified_sym: sym.Symbol) -> "Var":
        """Return the Var currently bound to the name in the namespace specified
        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.

        This is a utility method to return useful debugging information when code
        refers to an invalid symbol at runtime."""
----------
Output: 

        if not isinstance(ns_qualified_sym, sym.Symbol):
            raise TypeError(f"{ns_qualified_sym} is not a Symbol")
        if ns_qualified_sym.ns is None:
            raise ValueError(f"{ns_qualified_sym} is not a qualified symbol")
        if ns_qualified_sym.name not in ns_qualified_sym.ns.symbols:
            raise ValueError(f"{ns_qualified_sym} is not bound in {ns_qualified_sym.ns
----------
Solution: 

        v = Var.find(ns_qualified_sym)
        if v is None:
            raise RuntimeException(
                f"Unable to resolve symbol {ns_qualified_sym} in this context"
            )
        return v
--------------------
Prompt: 
def start(self):
        """Start the logger background synchronization service. This allows you to not need to
        worry about syncing with ConnectorDB - you just insert into the Logger, and the Logger
        will by synced every syncperiod."""
----------
Output: 

        self.logger.info("Starting Logger")
        self.logger.info("Starting Logger Sync Service")
        self.sync_service = threading.Thread(target=self.sync_loop)
        self.sync_service.start()
        self.logger.info("Logger Sync Service Started")

    def stop(self):
        """Stop the logger background synchronization service."""
        self.logger.info("Stopping Logger")
        self.logger.info("Stopping Logger Sync Service")
        self.sync_service.
----------
Solution: 


        with self.synclock:
            if self.syncthread is not None:
                logging.warn(
                    "Logger: Start called on a syncer that is already running")
                return

        self.sync()  # Attempt a sync right away
        self.__setsync()
--------------------
Prompt: 
def make_path_relative(path, rel_to):
    """
    Make a filename relative, where the filename path, and it is
    relative to rel_to

        >>> make_path_relative('/usr/share/something/a-file.pth',
        ...                    '/usr/share/another-place/src/Directory')
        '../../../something/a-file.pth'
        >>> make_path_relative('/usr/share/something/a-file.pth',
        ...                    '/home/user/src/Directory')
        '../../../usr/share/something/a-file.pth'
        >>> make_path_relative('/usr/share/a-file.pth', '/usr/share/')
        'a-file.pth'
    """
----------
Output: 
Directory')
        '../../../share/something/a-file.pth'
        >>> make_path_relative('/usr/share/something/a-file.pth',
        ...                    '/usr/share/something/src/Directory')
        'src/Directory'
        >>> make_path_relative('/usr/share/something/a-file.pth',
        ...                    '/usr/share/something/src/Directory/')
        'src/Directory'
        >>> make_path_relative('/usr/share/something/a-file.
----------
Solution: 

    path_filename = os.path.basename(path)
    path = os.path.dirname(path)
    path = os.path.normpath(os.path.abspath(path))
    rel_to = os.path.normpath(os.path.abspath(rel_to))
    path_parts = path.strip(os.path.sep).split(os.path.sep)
    rel_to_parts = rel_to.strip(os.path.sep).split(os.path.sep)
    while path_parts and rel_to_parts and path_parts[0] == rel_to_parts[0]:
        path_parts.pop(0)
        rel_to_parts.pop(0)
    full_parts = ['..'] * len(rel_to_parts) + path_parts + [path_filename]
    if full_parts == ['']:
        return '.' + os.path.sep
    return os.path.sep.join(full_parts)
--------------------
Prompt: 
def _rotations_to_disentangle(local_param):
        """
        Static internal method to work out Ry and Rz rotation angles used
        to disentangle the LSB qubit.
        These rotations make up the block diagonal matrix U (i.e. multiplexor)
        that disentangles the LSB.

        [[Ry(theta_1).Rz(phi_1)  0   .   .   0],
         [0         Ry(theta_2).Rz(phi_2) .  0],
                                    .
                                        .
          0         0           Ry(theta_2^n).Rz(phi_2^n)]]
        """
----------
Output: 
        [.         .         Ry(theta_3).Rz(phi_3) .  0],
         [.         .         .         Ry(theta_4).Rz(phi_4) .],
         [0         .         .         .         Ry(theta_5).Rz(phi_5)]]

        :param local_param: The local parameters of the circuit.
        :type local_param: dict
        :return: The rotation angles.
        :rtype: list
        """
        theta_1 = local_param['theta
----------
Solution: 

        remaining_vector = []
        thetas = []
        phis = []

        param_len = len(local_param)

        for i in range(param_len // 2):
            # Ry and Rz rotations to move bloch vector from 0 to "imaginary"
            # qubit
            # (imagine a qubit state signified by the amplitudes at index 2*i
            # and 2*(i+1), corresponding to the select qubits of the
            # multiplexor being in state |i>)
            (remains,
             add_theta,
             add_phi) = Initialize._bloch_angles(local_param[2 * i: 2 * (i + 1)])

            remaining_vector.append(remains)

            # rotations for all imaginary qubits of the full vector
            # to move from where it is to zero, hence the negative sign
            thetas.append(-add_theta)
            phis.append(-add_phi)

        return remaining_vector, thetas, phis
--------------------
Prompt: 
def crcMeterRead(self, raw_read, def_buf):
        """ Internal read CRC wrapper.

        Args:
            raw_read (str): Bytes with implicit string cast from serial read
            def_buf (SerialBlock): Populated read buffer.

        Returns:
            bool:  True if passed CRC equals calculated CRC.
        """
----------
Output: 

        # Read the CRC from the buffer.
        crc_read = raw_read[len(raw_read) - 2:]

        # Calculate the CRC.
        crc_calc = self.crc.calculate(def_buf.to_bytes())

        # Compare the CRCs.
        if crc_read == crc_calc:
            return True
        else:
            return False

    def crcMeterWrite(self, raw_write, def_buf):
        """ Internal write C
----------
Solution: 

        try:
            if len(raw_read) == 0:
                ekm_log("(" + self.m_context + ") Empty return read.")
                return False
            sent_crc = self.calc_crc16(raw_read[1:-2])
            logstr = "(" + self.m_context + ")CRC sent = " + str(def_buf["crc16"][MeterData.StringValue])
            logstr += " CRC calc = " + sent_crc
            ekm_log(logstr)
            if int(def_buf["crc16"][MeterData.StringValue], 16) == int(sent_crc, 16):
                return True

        # A cross simple test lines on a USB serial adapter, these occur every
        # 1000 to 2000 reads, and they show up here as a bad unpack or
        # a bad crc type call.  In either case, we suppress them a log will
        # become quite large.  ekmcrc errors come through as type errors.
        # Failures of int type conversion in 16 bit conversion occur as value
        # errors.
        except struct.error:
            ekm_log(str(sys.exc_info()))
            for frame in traceback.extract_tb(sys.exc_info()[2]):
                fname, lineno, fn, text = frame
                ekm_log("Error in %s on line %d" % (fname, lineno))
            return False

        except TypeError:
            ekm_log(str(sys.exc_info()))
            for frame in traceback.extract_tb(sys.exc_info()[2]):
                fname, lineno, fn, text = frame
                ekm_log("Error in %s on line %d" % (fname, lineno))
            return False

        except ValueError:
            ekm_log(str(sys.exc_info()))
            for frame in traceback.extract_tb(sys.exc_info()[2]):
                fname, lineno, fn, text = frame
                ekm_log("Error in %s on line %d" % (fname, lineno))
            return False

        return False
--------------------
Prompt: 
def is_bare_exception(self, node):
        """
        Checks if the node is a bare exception name from an except block.

        """
----------
Output: 

        if isinstance(node, ast.Name):
            if node.id in self.exception_names:
                return True
        return False

    def is_bare_exception_handler(self, node):
        """
        Checks if the node is a bare exception handler from an except block.

        """
        if isinstance(node, ast.ExceptHandler):
            if self.is_bare_exception(node.type):
                return True
        return False

    def is_bare_exception_handler_with_name(self,
----------
Solution: 

        return isinstance(node, Name) and node.id in self.current_except_names
--------------------
Prompt: 
def add_gene_links(gene_obj, build=37):
    """Update a gene object with links

    Args:
        gene_obj(dict)
        build(int)

    Returns:
        gene_obj(dict): gene_obj updated with many links
    """
----------
Output: 

    # Add links to gene
    gene_obj['links'] = {
        'self': '/genes/{}'.format(gene_obj['id']),
        'exons': '/genes/{}/exons'.format(gene_obj['id']),
        'transcripts': '/genes/{}/transcripts'.format(gene_obj['id']),
        'variants': '/genes/{}/variants'.format(gene_obj['id']),
        'similar': '/genes/{}/similar'.format(gene
----------
Solution: 

    try:
        build = int(build)
    except ValueError:
        build = 37
    # Add links that use the hgnc_id
    hgnc_id = gene_obj['hgnc_id']

    gene_obj['hgnc_link'] = genenames(hgnc_id)
    gene_obj['omim_link'] = omim(hgnc_id)
    # Add links that use ensembl_id
    if not 'ensembl_id' in gene_obj:
        ensembl_id = gene_obj.get('common',{}).get('ensembl_id')
    else:
        ensembl_id = gene_obj['ensembl_id']
    ensembl_37_link = ensembl(ensembl_id, build=37)
    ensembl_38_link = ensembl(ensembl_id, build=38)
    gene_obj['ensembl_37_link'] = ensembl_37_link
    gene_obj['ensembl_38_link'] = ensembl_38_link
    gene_obj['ensembl_link'] = ensembl_37_link
    if build == 38:
        gene_obj['ensembl_link'] = ensembl_38_link
    gene_obj['hpa_link'] = hpa(ensembl_id)
    gene_obj['string_link'] = string(ensembl_id)
    gene_obj['reactome_link'] = reactome(ensembl_id)
    gene_obj['clingen_link'] = clingen(hgnc_id)
    gene_obj['expression_atlas_link'] = expression_atlas(ensembl_id)
    gene_obj['exac_link'] = exac(ensembl_id)
    # Add links that use entrez_id
    gene_obj['entrez_link'] = entrez(gene_obj.get('entrez_id'))
    # Add links that use omim id
    gene_obj['omim_link'] = omim(gene_obj.get('omim_id'))
    # Add links that use hgnc_symbol
    gene_obj['ppaint_link'] = ppaint(gene_obj['hgnc_symbol'])
    # Add links that use vega_id
    gene_obj['vega_link'] = vega(gene_obj.get('vega_id'))
    # Add links that use ucsc link
    gene_obj['ucsc_link'] = ucsc(gene_obj.get('ucsc_id'))
--------------------
Prompt: 
def cqt_frequencies(n_bins, fmin, bins_per_octave=12, tuning=0.0):
    """Compute the center frequencies of Constant-Q bins.

    Examples
    --------
    >>> # Get the CQT frequencies for 24 notes, starting at C2
    >>> librosa.cqt_frequencies(24, fmin=librosa.note_to_hz('C2'))
    array([  65.406,   69.296,   73.416,   77.782,   82.407,   87.307,
             92.499,   97.999,  103.826,  110.   ,  116.541,  123.471,
            130.813,  138.591,  146.832,  155.563,  164.814,  174.614,
            184.997,  195.998,  207.652,  220.   ,  233.082,  246.942])

    Parameters
    ----------
    n_bins  : int > 0 [scalar]
        Number of constant-Q bins

    fmin    : float > 0 [scalar]
        Minimum frequency

    bins_per_octave : int > 0 [scalar]
        Number of bins per octave

    tuning : float in `[-0.5, +0.5)`
        Deviation from A440 tuning in fractional bins (cents)

    Returns
    -------
    frequencies : np.ndarray [shape=(n_bins,)]
        Center frequency for each CQT bin
    """
----------
Output: 
.296,   73.416,   77.782,   82.407,   87.307,
             92.499,   97.999,  103.826,  109.855,  116.541,  123.471,
            130.813,  138.591,  146.832,  155.566,  
----------
Solution: 


    correction = 2.0**(float(tuning) / bins_per_octave)
    frequencies = 2.0**(np.arange(0, n_bins, dtype=float) / bins_per_octave)

    return correction * fmin * frequencies
--------------------
Prompt: 
def export_keying_material(self, label, olen, context=None):
        """
        Obtain keying material for application use.

        :param: label - a disambiguating label string as described in RFC 5705
        :param: olen - the length of the exported key material in bytes
        :param: context - a per-association context value
        :return: the exported key material bytes or None
        """
----------
Output: 

        raise NotImplementedError

    def export_keying_material_early(self, label, olen, context=None):
        """
        Obtain keying material for application use.

        :param: label - a disambiguating label string as described in RFC 5705
        :param: olen - the length of the exported key material in bytes
        :param: context - a per-association context value
        :return: the exported key material bytes or None
        """
        raise NotImplementedError

----------
Solution: 

        outp = _no_zero_allocator("unsigned char[]", olen)
        context_buf = _ffi.NULL
        context_len = 0
        use_context = 0
        if context is not None:
            context_buf = context
            context_len = len(context)
            use_context = 1
        success = _lib.SSL_export_keying_material(self._ssl, outp, olen,
                                                  label, len(label),
                                                  context_buf, context_len,
                                                  use_context)
        _openssl_assert(success == 1)
        return _ffi.buffer(outp, olen)[:]
--------------------
Prompt: 
def register_function(scope=None, as_property=False, name=None):
    """Decorator to register a new function with vaex.

    Example:

    >>> import vaex
    >>> df = vaex.example()
    >>> @vaex.register_function()
    >>> def invert(x):
    >>>     return 1/x
    >>> df.x.invert()


    >>> import numpy as np
    >>> df = vaex.from_arrays(departure=np.arange('2015-01-01', '2015-12-05', dtype='datetime64'))
    >>> @vaex.register_function(as_property=True, scope='dt')
    >>> def dt_relative_day(x):
    >>>     return vaex.functions.dt_dayofyear(x)/365.
    >>> df.departure.dt.relative_day
    """
----------
Output: 
16-01-01', '2016-01-02', dtype='datetime64[D]'))
    >>> @vaex.register_function(scope='df')
    >>> def day_of_week(departure):
    >>>     return departure.dayofweek()
    >>> df.day_of_week()

    >>> @vaex.register_function(scope='df')
    >>> def day_of_week(departure):
    >>>     return departure.dayofweek()
    >>> df.day
----------
Solution: 

    prefix = ''
    if scope:
        prefix = scope + "_"
        if scope not in scopes:
            raise KeyError("unknown scope")
    def wrapper(f, name=name):
        name = name or f.__name__
        # remove possible prefix
        if name.startswith(prefix):
            name = name[len(prefix):]
        full_name = prefix + name
        if scope:
            def closure(name=name, full_name=full_name, function=f):
                def wrapper(self, *args, **kwargs):
                    lazy_func = getattr(self.expression.ds.func, full_name)
                    args = (self.expression, ) + args
                    return lazy_func(*args, **kwargs)
                return functools.wraps(function)(wrapper)
            if as_property:
                setattr(scopes[scope], name, property(closure()))
            else:
                setattr(scopes[scope], name, closure())
        else:
            def closure(name=name, full_name=full_name, function=f):
                def wrapper(self, *args, **kwargs):
                    lazy_func = getattr(self.ds.func, full_name)
                    args = (self, ) + args
                    return lazy_func(*args, **kwargs)
                return functools.wraps(function)(wrapper)
            setattr(vaex.expression.Expression, name, closure())
        vaex.expression.expression_namespace[prefix + name] = f
        return f  # we leave the original function as is
    return wrapper
--------------------
Prompt: 
def iter_hierarchy(self, ontology, size=None, sleep=None):
        """Iterates over parent-child relations

        :param str ontology: The name of the ontology
        :param int size: The size of each page. Defaults to 500, which is the maximum allowed by the EBI.
        :param int sleep: The amount of time to sleep between pages. Defaults to 0 seconds.
        :rtype: iter[tuple[str,str]]
        """
----------
Output: 

        if size is None:
            size = 500
        if sleep is None:
            sleep = 0
        for page in self.iter_pages(ontology, size=size, sleep=sleep):
            for child in page:
                yield (child['parent'], child['child'])

    def iter_pages(self, ontology, size=None, sleep=None):
        """Iterates over pages of parent-child relations

        :param str ontology: The name of the ontology
        :param int size: The size of each
----------
Solution: 

        for term in self.iter_terms(ontology=ontology, size=size, sleep=sleep):
            try:
                hierarchy_children_link = term['_links'][HIERARCHICAL_CHILDREN]['href']
            except KeyError:  # there's no children for this one
                continue

            response = requests.get(hierarchy_children_link).json()

            for child_term in response['_embedded']['terms']:
                yield term['label'], child_term['label']
--------------------
Prompt: 
def closeAllSessions(self, slot):
        """
        C_CloseAllSessions

        :param slot: slot number
        :type slot: integer
        """
----------
Output: 

        self.checkSlotAvailability(slot)
        self.checkSessionAvailability(slot)
        self.checkSessionStatus(slot, self.SESSION_ACTIVE)
        self.checkSessionStatus(slot, self.SESSION_MODIFIED)
        self.checkSessionStatus(slot, self.SESSION_READ_ONLY)
        self.checkSessionStatus(slot, self.SESSION_READ_WRITE)
        self.checkSessionStatus(slot, self.SESSION_RW_PUBLIC)
       
----------
Solution: 

        rv = self.lib.C_CloseAllSessions(slot)
        if rv != CKR_OK:
            raise PyKCS11Error(rv)
--------------------
Prompt: 
def load_panel_app(adapter, panel_id=None, institute='cust000'):
    """Load PanelApp panels into scout database
    
    If no panel_id load all PanelApp panels 
    
    Args:
        adapter(scout.adapter.MongoAdapter)
        panel_id(str): The panel app panel id
    """
----------
Output: 

    # Load all panel app panels
    if panel_id is None:
        logger.info("Loading all panel app panels")
        panel_ids = adapter.panel_app_panels()
        logger.info("Found {} panel app panels".format(len(panel_ids)))
    else:
        logger.info("Loading panel app panel {}".format(panel_id))
        panel_ids = [panel_id]

    for panel_id in panel_ids:
        logger.info("Loading panel app panel {}".format(panel_
----------
Solution: 

    base_url = 'https://panelapp.genomicsengland.co.uk/WebServices/{0}/'
    
    hgnc_map = adapter.genes_by_alias()
    
    if panel_id:
        panel_ids = [panel_id]

    if not panel_id:
        
        LOG.info("Fetching all panel app panels")
        data = get_request(base_url.format('list_panels'))
    
        json_lines = json.loads(data)
        
        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]
    
    for panel_id in panel_ids:
        panel_data = get_request(base_url.format('get_panel') + panel_id)
        
        parsed_panel = parse_panel_app_panel(
            panel_info = json.loads(panel_data)['result'], 
            hgnc_map=hgnc_map,
            institute=institute
        )
        parsed_panel['panel_id'] = panel_id
        
        if len(parsed_panel['genes']) == 0:
            LOG.warning("Panel {} is missing genes. Skipping.".format(parsed_panel['display_name']))
            continue
        
        try:
            adapter.load_panel(parsed_panel=parsed_panel)
        except Exception as err:
            raise err
--------------------
Prompt: 
def reads_py(s, **kwargs):
    """Read a .py notebook from a string and return the NotebookNode object."""
----------
Output: 

    from nbformat.reader import reads
    return reads(s, **kwargs)


def reads_json(s, **kwargs):
    """Read a .ipynb notebook from a string and return the NotebookNode object."""
    from nbformat.reader import reads_json
    return reads_json(s, **kwargs)


def reads_nbjson(s, **kwargs):
    """Read a .ipynb notebook from a string and return the NotebookNode object."""
    from nbformat.reader import reads_
----------
Solution: 

    nbf, nbm, s = parse_py(s, **kwargs)
    if nbf == 2:
        nb = v2.to_notebook_py(s, **kwargs)
    elif nbf == 3:
        nb = v3.to_notebook_py(s, **kwargs)
    else:
        raise NBFormatError('Unsupported PY nbformat version: %i' % nbf)
    return nb
--------------------
Prompt: 
def mkdirs(path, mode):
    """
    Creates the directory specified by path, creating intermediate directories
    as necessary. If directory already exists, this is a no-op.

    :param path: The directory to create
    :type path: str
    :param mode: The mode to give to the directory e.g. 0o755, ignores umask
    :type mode: int
    """
----------
Output: 

    if not os.path.exists(path):
        os.makedirs(path, mode)


def mkdir(path, mode):
    """
    Creates the directory specified by path, creating intermediate directories
    as necessary. If directory already exists, this is a no-op.

    :param path: The directory to create
    :type path: str
    :param mode: The mode to give to the directory e.g. 0o755, ignores umask
    :type mode: int
    """
    if not os
----------
Solution: 

    try:
        o_umask = os.umask(0)
        os.makedirs(path, mode)
    except OSError:
        if not os.path.isdir(path):
            raise
    finally:
        os.umask(o_umask)
--------------------
Prompt: 
def _resolve_graph(self, distribution_names=None, leaf_name='x'):
    """Creates a `tuple` of `tuple`s of dependencies.

    This function is **experimental**. That said, we encourage its use
    and ask that you report problems to `tfprobability@tensorflow.org`.

    Args:
      distribution_names: `list` of `str` or `None` names corresponding to each
        of `model` elements. (`None`s are expanding into the
        appropriate `str`.)
      leaf_name: `str` used when no maker depends on a particular
        `model` element.

    Returns:
      graph: `tuple` of `(str tuple)` pairs representing the name of each
        distribution (maker) and the names of its dependencies.

    #### Example

    ```python
    d = tfd.JointDistributionSequential([
                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),
        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),
                     tfd.Normal(loc=0, scale=2.),
        lambda n, g: tfd.Normal(loc=n, scale=g),
    ])
    d._resolve_graph()
    # ==> (
    #       ('e', ()),
    #       ('g', ('e',)),
    #       ('n', ()),
    #       ('x', ('n', 'g')),
    #     )
    ```

    """
----------
Output: 
leaf_name: `str` name of the leaf distribution.

    Returns:
      `tuple` of `tuple`s of dependencies.
    """
    if distribution_names is None:
      distribution_names = [
          '{}'.format(i) for i in range(len(self.model))]

    if len(distribution_names) != len(self.model):
      raise ValueError(
          '`distribution_names` must have the same length as `model`.')

    def _resolve_graph_helper(
        distribution_names,
----------
Solution: 

    # This function additionally depends on:
    #   self._dist_fn_args
    #   self._dist_fn_wrapped
    # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,
    # ignore args prefixed with `_`.
    if distribution_names is None or any(self._dist_fn_args):
      distribution_names = _resolve_distribution_names(
          self._dist_fn_args, distribution_names, leaf_name)
    if len(set(distribution_names)) != len(distribution_names):
      raise ValueError('Distribution names must be unique: {}'.format(
          distribution_names))
    if len(distribution_names) != len(self._dist_fn_wrapped):
      raise ValueError('Distribution names must be 1:1 with `rvs`.')
    return tuple(zip(distribution_names,
                     tuple(() if a is None else a for a in self._dist_fn_args)))
--------------------
Prompt: 
def cli_encrypt(context, key):
    """
    Encrypts context.io_manager's stdin and sends that to
    context.io_manager's stdout.

    This can be useful to encrypt to disk before attempting to
    upload, allowing uploads retries and segmented encrypted objects.

    See :py:mod:`swiftly.cli.encrypt` for context usage information.

    See :py:class:`CLIEncrypt` for more information.
    """
----------
Output: 

    CLIEncrypt(context).encrypt(key)


def cli_decrypt(context, key):
    """
    Decrypts context.io_manager's stdin and sends that to
    context.io_manager's stdout.

    See :py:mod:`swiftly.cli.decrypt` for context usage information.

    See :py:class:`CLIDecrypt` for more information.
    """
    CLIDecrypt(context).decrypt(key)


def cli_encrypt_file(context,
----------
Solution: 

    with context.io_manager.with_stdout() as stdout:
        with context.io_manager.with_stdin() as stdin:
            for chunk in aes_encrypt(key, stdin, preamble=AES256CBC):
                stdout.write(chunk)
            stdout.flush()
--------------------
Prompt: 
def _confirm_exit(self):
        """confirm shutdown on ^C
        
        A second ^C, or answering 'y' within 5s will cause shutdown,
        otherwise original SIGINT handler will be restored.
        
        This doesn't work on Windows.
        """
----------
Output: 

        if os.name == 'nt':
            return
        try:
            sys.exit(0)
        except SystemExit:
            pass
        print('^C again to exit')
        self.set_raw_mode()
        try:
            rlist, _, _ = select.select([sys.stdin], [], [], 5)
        except select.error:
            pass
        else:
            if rlist:
                answer = sys.stdin.readline().strip()
                if answer.lower() in ('y', 'yes'):
----------
Solution: 

        # FIXME: remove this delay when pyzmq dependency is >= 2.1.11
        time.sleep(0.1)
        sys.stdout.write("Shutdown Notebook Server (y/[n])? ")
        sys.stdout.flush()
        r,w,x = select.select([sys.stdin], [], [], 5)
        if r:
            line = sys.stdin.readline()
            if line.lower().startswith('y'):
                self.log.critical("Shutdown confirmed")
                ioloop.IOLoop.instance().stop()
                return
        else:
            print "No answer for 5s:",
        print "resuming operation..."
        # no answer, or answer is no:
        # set it back to original SIGINT handler
        # use IOLoop.add_callback because signal.signal must be called
        # from main thread
        ioloop.IOLoop.instance().add_callback(self._restore_sigint_handler)
--------------------
Prompt: 
def send_raw(self, stream, msg_list, flags=0, copy=True, ident=None):
        """Send a raw message via ident path.

        This method is used to send a already serialized message.

        Parameters
        ----------
        stream : ZMQStream or Socket
            The ZMQ stream or socket to use for sending the message.
        msg_list : list
            The serialized list of messages to send. This only includes the
            [p_header,p_parent,p_content,buffer1,buffer2,...] portion of
            the message.
        ident : ident or list
            A single ident or a list of idents to use in sending.
        """
----------
Output: 
,...] part of the
            message.
        flags : int
            The flags to use for the send.
        copy : bool
            Whether to copy the message data before sending.
        ident : bytes
            The identity to use for the send.

        Returns
        -------
        None

        """
        if copy:
            msg_list = list(msg_list)
            for i in range(len(msg_list)):
                msg_list[i] = msg_list[i].copy()

        if ident is None:
            stream
----------
Solution: 

        to_send = []
        if isinstance(ident, bytes):
            ident = [ident]
        if ident is not None:
            to_send.extend(ident)

        to_send.append(DELIM)
        to_send.append(self.sign(msg_list))
        to_send.extend(msg_list)
        stream.send_multipart(msg_list, flags, copy=copy)
--------------------
Prompt: 
def resolve_expression(self, *args, **kwargs) -> HStoreColumn:
        """Resolves the expression into a :see:HStoreColumn expression."""
----------
Output: 

        return HStoreColumn(self.expression)

    def __str__(self) -> str:
        """Returns the string representation of the expression."""
        return str(self.expression)

    def __repr__(self) -> str:
        """Returns the representation of the expression."""
        return f"HStoreColumn({self.expression!r})"

    def __eq__(self, other: Any) -> bool:
        """Returns whether the expression is equal to the other."""
        return isinstance(other, HStoreColumn
----------
Solution: 


        original_expression = super().resolve_expression(*args, **kwargs)
        expression = HStoreColumn(
            original_expression.alias,
            original_expression.target,
            self.key
        )
        return expression
--------------------
Prompt: 
def insert_default_options():
    """insert default options to sys.argv
    """
----------
Output: 

    import sys
    import os
    import re
    from optparse import OptionParser
    from optparse import OptionGroup
    from optparse import Option
    from optparse import OptionValueError
    from optparse import SUPPRESS_HELP
    from optparse import SUPPRESS_USAGE
    from optparse import AmbiguousOptionError
    from optparse import OptionError
    from optparse import OptionValueError
    from optparse import OptionConflictError
    from optparse import OptionValueError
    from optparse import OptionValueError

----------
Solution: 

    options = get_default_options()
    options.reverse()
    for arg in options:
        sys.argv.insert(1, arg)
--------------------
Prompt: 
def get_components(self):
        """
        Returns the components of this name, as a sequence of 2-tuples.

        :return: The components of this name.
        :rtype: :py:class:`list` of ``name, value`` tuples.
        """
----------
Output: 

        return self._components

    def get_component(self, name):
        """
        Returns the value of the component with the given name.

        :param name: The name of the component to return.
        :type name: :py:class:`str`
        :return: The value of the component with the given name.
        :rtype: :py:class:`str`
        """
        for component in self._components:
            if component[0] == name:
                return component[1]
        return None

    def get
----------
Solution: 

        result = []
        for i in range(_lib.X509_NAME_entry_count(self._name)):
            ent = _lib.X509_NAME_get_entry(self._name, i)

            fname = _lib.X509_NAME_ENTRY_get_object(ent)
            fval = _lib.X509_NAME_ENTRY_get_data(ent)

            nid = _lib.OBJ_obj2nid(fname)
            name = _lib.OBJ_nid2sn(nid)

            # ffi.string does not handle strings containing NULL bytes
            # (which may have been generated by old, broken software)
            value = _ffi.buffer(_lib.ASN1_STRING_data(fval),
                                _lib.ASN1_STRING_length(fval))[:]
            result.append((_ffi.string(name), value))

        return result
--------------------
Prompt: 
def fetch_items(self, category, **kwargs):
        """Fetch the pages

        :param category: the category of items to fetch
        :param kwargs: backend arguments

        :returns: a generator of items
        """
----------
Output: 

        if category == 'all':
            category = None
        if category is None:
            return self._fetch_all_items(**kwargs)
        else:
            return self._fetch_category_items(category, **kwargs)

    def _fetch_all_items(self, **kwargs):
        """Fetch all items

        :param kwargs: backend arguments

        :returns: a generator of items
        """
        for page in self._fetch_pages(**kwargs):
            for item in page['items']:
                yield item

----------
Solution: 

        from_date = kwargs['from_date']
        reviews_api = kwargs['reviews_api']

        mediawiki_version = self.client.get_version()
        logger.info("MediaWiki version: %s", mediawiki_version)

        if reviews_api:
            if ((mediawiki_version[0] == 1 and mediawiki_version[1] >= 27) or mediawiki_version[0] > 1):
                fetcher = self.__fetch_1_27(from_date)
            else:
                logger.warning("Reviews API only available in MediaWiki >= 1.27")
                logger.warning("Using the Pages API instead")
                fetcher = self.__fetch_pre1_27(from_date)
        else:
            fetcher = self.__fetch_pre1_27(from_date)

        for page_reviews in fetcher:
            yield page_reviews
--------------------
Prompt: 
def create_session():
    """
    Contextmanager that will create and teardown a session.
    """
----------
Output: 

    session = Session()
    try:
        yield session
    finally:
        session.close()


def get_session():
    """
    Get a session from the current thread.
    """
    session = getattr(thread_local, 'session', None)
    if session is None:
        session = thread_local.session = Session()
    return session


def get_db_session():
    """
    Get a session from the current thread.
    """
    session = getattr(thread_local, 'db_session
----------
Solution: 

    session = settings.Session()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()
--------------------
Prompt: 
def disable_report(self, reportid):
        """disable the report of the given id"""
----------
Output: 

        return self.request('disable_report', reportid=reportid)

    def enable_report(self, reportid):
        """enable the report of the given id"""
        return self.request('enable_report', reportid=reportid)

    def get_report(self, reportid):
        """get the report of the given id"""
        return self.request('get_report', reportid=reportid)

    def get_report_list(self, report_type=None, report_status=None,
                       
----------
Solution: 

        reportid = reportid.upper()
        self._reports_state[reportid] = False
--------------------
Prompt: 
def chrono(ctx, app_id, sentence_file,
           json_flag, sentence, doc_time, request_id):
    # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None  # NOQA
    """Extract expression expressing date and time and normalize its value """
----------
Output: 

    # TODO: add support for other languages
    # TODO: add support for other time expressions
    # TODO: add support for other date expressions
    # TODO: add support for other timezones
    # TODO: add support for other languages
    # TODO: add support for other languages
    # TODO: add support for other languages
    # TODO: add support for other languages
    # TODO: add support for other languages
    # TODO: add support for other languages
    # TODO: add support for other languages
    # TODO: add support for other languages
    # TODO
----------
Solution: 


    app_id = clean_app_id(app_id)
    sentence = clean_sentence(sentence, sentence_file)

    api = GoolabsAPI(app_id)
    ret = api.chrono(
        sentence=sentence,
        doc_time=doc_time,
        request_id=request_id,
    )

    if json_flag:
        click.echo(format_json(api.response.json()))
        return

    for pair in ret['datetime_list']:
        click.echo(u'{0}: {1}'.format(text(pair[0]), pair[1]))
--------------------
Prompt: 
def _get_unpacking_extra_info(node, infered):
    """return extra information to add to the message for unpacking-non-sequence
    and unbalanced-tuple-unpacking errors
    """
----------
Output: 

    if isinstance(node, ast.Tuple):
        return " (%s)" % ", ".join(
            "%s=%s" % (name, infered[name]) for name in node.elts
        )
    return ""


def _get_unpacking_extra_info_for_unbalanced_tuple_unpacking(node, infered):
    """return extra information to add to the message for unbalanced-tuple-unpacking
    errors
    """
    if isinstance(node, ast.Tuple):
----------
Solution: 

    more = ""
    infered_module = infered.root().name
    if node.root().name == infered_module:
        if node.lineno == infered.lineno:
            more = " %s" % infered.as_string()
        elif infered.lineno:
            more = " defined at line %s" % infered.lineno
    elif infered.lineno:
        more = " defined at line %s of %s" % (infered.lineno, infered_module)
    return more
--------------------
Prompt: 
def godot_options(self, info):
        """ Handles display of the options menu. """
----------
Output: 

        self.options_menu.show()
        self.options_menu.set_title("Options")
        self.options_menu.set_description("")
        self.options_menu.set_icon(info["icon"])
        self.options_menu.set_position(info["position"])
        self.options_menu.set_size(info["size"])
        self.options_menu.set_color(info["color"])
        self.options_menu.set_background_color(info["background_color"])
        self.options_menu
----------
Solution: 


        if info.initialized:
            self.edit_traits( parent = info.ui.control,
                              kind   = "livemodal",
                              view   = "options_view" )
--------------------
Prompt: 
def getMappingsOnDeviceType(self, typeId, draft=False):
        """
        Get all the mappings for a device type.
        Parameters:
            - typeId (string) - the device type
            - draft (boolean) - draft or active
        Throws APIException on failure.
        """
----------
Output: 

        return self._getMappings(typeId, draft)

    def getMappingsOnDevice(self, deviceId, draft=False):
        """
        Get all the mappings for a device.
        Parameters:
            - deviceId (string) - the device id
            - draft (boolean) - draft or active
        Throws APIException on failure.
        """
        return self._getMappings(deviceId, draft)

    def getMappingsOnDeviceTypeAndDevice(self, typeId, deviceId, draft=False):
       
----------
Solution: 

        if draft:
            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, "/draft", typeId)
        else:
            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, "", typeId)

        resp = requests.get(req, auth=self.credentials, verify=self.verify)
        if resp.status_code == 200:
            self.logger.debug("All device type mappings retrieved")
        else:
            raise ibmiotf.APIException(resp.status_code, "HTTP error getting all device type mappings", resp)
        return resp.json()
--------------------
Prompt: 
def video_list(request, username=None):
    """
    list of videos of a user
    if username does not set, shows the currently logged in user
    """
----------
Output: 

    if username:
        user = get_object_or_404(User, username=username)
    else:
        user = request.user

    videos = Video.objects.filter(user=user)

    return render(request, 'video_list.html', {'videos': videos})


def video_detail(request, username, video_id):
    """
    show details of a video
    """
    video = get_object_or_404(Video, id=video_id)
    return render(
----------
Solution: 


    # If user is not authenticated and username is None, raise an error
    if username is None and not request.user.is_authenticated():
        from django.http import Http404
        raise Http404

    from django.contrib.auth.models import User
    user = User.objects.get(username=username) if username else request.user

    # loop through the videos of the user
    videos = Video.objects.filter(user=user).all()
    video_params = []
    for video in videos:
        video_params.append(_video_params(request, video.video_id))

    return render_to_response(
        "django_youtube/videos.html",
        {"video_params": video_params},
        context_instance=RequestContext(request)
    )
--------------------
Prompt: 
def weekday(year_or_num, month=None, day=None, full=False):
    """Simple tag - returns the weekday of the given (year, month, day) or of given (weekday_number).

    Usage (in template):

    {% weekday 2014 3 3 %}

    Result: Mon

    Return abbreviation by default. To return full name: pass full=True

    {% weekday 2014 3 3 full=True %}

    Result: Monday


    When only number of weekday is given then 0 is considered as "Monday"

    {% weekday 0 full=True %}

    Result: Monday

    """
----------
Output: 
   """
    if month is None and day is None:
        # weekday_number
        weekday_number = year_or_num
        if full:
            return calendar.day_name[weekday_number]
        else:
            return calendar.day_abbr[weekday_number]
    else:
        # year, month, day
        year = year_or_num
        month = month or 1
        day = day or 1
        return calendar.day_name[datetime.date(year, month, day).weekday
----------
Solution: 

    if any([month, day]) and not all([month, day]):
        raise TemplateSyntaxError("weekday accepts 1 or 3 arguments plus optional 'full' argument")

    try:
        if all([year_or_num, month, day]):
            weekday_num = date(*map(int, (year_or_num, month, day))).weekday()
        else:
            weekday_num = year_or_num
        if full:
            return WEEKDAYS[weekday_num]
        else:
            return WEEKDAYS_ABBR[weekday_num]
    except Exception:
        return
--------------------
Prompt: 
def clinvar_submissions(store, user_id, institute_id):
    """Get all Clinvar submissions for a user and an institute"""
----------
Output: 

    query = """
        query MyQuery {
            clinvar_submissions(user_id: "%s", institute_id: "%s") {
                id
                clinvar_id
                clinvar_submission_id
                clinvar_submission_version
                clinvar_submission_date
                clinvar_submission_status
                clinvar_submission_type
                clinvar_submission_title
                clinvar_submission_authors
                clinvar_submission_abstract
                clinvar_submission_journal
----------
Solution: 

    submissions = list(store.clinvar_submissions(user_id, institute_id))
    return submissions
--------------------
Prompt: 
def get_table(self, arch, pattern, colored=False, verbose=False):
        """
        This function is used in sys command (when user want to find a specific syscall)

        :param Architecture for syscall table;
        :param Searching pattern;
        :param Flag for verbose output
        :return Return a printable table of matched syscalls
        """
----------
Output: 

        if arch == "x86":
            table = self.x86_table
        elif arch == "x64":
            table = self.x64_table
        else:
            raise Exception("Unsupported architecture")

        if colored:
            print(colored("Syscall Table for %s" % arch, "green"))
        else:
            print("Syscall Table for %s" % arch)

        if pattern is None:
            print(table)
        else:
            for syscall in table
----------
Solution: 


        rawtable = self.search(arch, pattern)
        if len(rawtable) == 0:
            return None

        used_hd = self.__fetch_used_headers(rawtable, verbose)
        table   = [self.__make_colored_row(used_hd, 'yellow,bold', upper=True) if colored else used_hd]

        for command in rawtable:
            cur_tb_field = []
            for hd in used_hd:
                value = command[hd]
                cur_tb_field.append(self.__make_colored_field(value, hd, verbose=verbose))
            table.append(cur_tb_field)
        return DoubleTable(table)
--------------------
Prompt: 
def str_variant(store, institute_id, case_name, variant_id):
    """Pre-process an STR variant entry for detail page.

    Adds information to display variant

    Args:
        store(scout.adapter.MongoAdapter)
        institute_id(str)
        case_name(str)
        variant_id(str)

    Returns:
        detailed_information(dict): {
            'institute': <institute_obj>,
            'case': <case_obj>,
            'variant': <variant_obj>,
            'overlapping_snvs': <overlapping_snvs>,
            'manual_rank_options': MANUAL_RANK_OPTIONS,
            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS
        }
        """
----------
Output: 
': <variant_obj>,
            'genes': <gene_objs>,
            'transcripts': <transcript_objs>,
            'hgnc_symbols': <hgnc_symbols>,
            'clinvar_variants': <clinvar_variants>,
            'cosmic_variants': <cosmic_variants>,
            'transcript_consequences': <transcript_consequences>,
            'transcript_consequence_terms': <transcript_consequence_terms>,
            '
----------
Solution: 


    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)
    variant_obj =  store.variant(variant_id)

    # fill in information for pilup view
    variant_case(store, case_obj, variant_obj)

    variant_obj['callers'] = callers(variant_obj, category='str')

    # variant_obj['str_ru']
    # variant_obj['str_repid']
    # variant_obj['str_ref']

    variant_obj['comments'] = store.events(institute_obj, case=case_obj,
                                           variant_id=variant_obj['variant_id'], comments=True)

    return {
        'institute': institute_obj,
        'case': case_obj,
        'variant': variant_obj,
        'overlapping_snvs': overlapping_snvs,
        'manual_rank_options': MANUAL_RANK_OPTIONS,
        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS
    }
--------------------
Prompt: 
def index(context, update):
    """Create indexes for the database"""
----------
Output: 

    context.execute("CREATE INDEX IF NOT EXISTS idx_user_id ON user (id)")
    context.execute("CREATE INDEX IF NOT EXISTS idx_user_name ON user (name)")
    context.execute("CREATE INDEX IF NOT EXISTS idx_user_email ON user (email)")
    context.execute("CREATE INDEX IF NOT EXISTS idx_user_role ON user (role)")
    context.execute("CREATE INDEX IF NOT EXISTS idx_user_active ON user (active)")
    context.execute("CREATE INDEX IF NOT EXISTS idx_user_created_
----------
Solution: 

    LOG.info("Running scout index")
    adapter = context.obj['adapter']
    
    if update:
        adapter.update_indexes()
    else:
        adapter.load_indexes()
--------------------
Prompt: 
def remove_pid_file(self):
        """Remove the pid file.

        This should be called at shutdown by registering a callback with
        :func:`reactor.addSystemEventTrigger`. This needs to return
        ``None``.
        """
----------
Output: 

        if self.pid_file:
            try:
                os.remove(self.pid_file)
            except OSError:
                pass

    def _get_pid_file(self):
        """Get the pid file.

        This is a property that returns the pid file.
        """
        return self._pid_file

    def _set_pid_file(self, pid_file):
        """Set the pid file.

        This is a property that sets the pid file.
        """
        self._pid_file =
----------
Solution: 

        pid_file = os.path.join(self.profile_dir.pid_dir, self.name + u'.pid')
        if os.path.isfile(pid_file):
            try:
                self.log.info("Removing pid file: %s" % pid_file)
                os.remove(pid_file)
            except:
                self.log.warn("Error removing the pid file: %s" % pid_file)
--------------------
Prompt: 
def getoutput_pexpect(self, cmd):
        """Run a command and return its stdout/stderr as a string.

        Parameters
        ----------
        cmd : str
          A command to be executed in the system shell.

        Returns
        -------
        output : str
          A string containing the combination of stdout and stderr from the
        subprocess, in whatever order the subprocess originally wrote to its
        file descriptors (so the order of the information in this string is the
        correct order as would be seen if running the command in a terminal).
        """
----------
Output: 
in a shell).

        """
        if self.logfile is None:
            self.logfile = open(os.devnull, 'w')
        self.logfile.flush()
        self.child.sendline(cmd)
        self.child.expect(self.prompt)
        self.child.sendline('')
        self.child.expect(self.prompt)
        self.child.sendline('')
        self.child.expect(self.prompt)
        self.child.sendline('')
        self.
----------
Solution: 

        try:
            return pexpect.run(self.sh, args=['-c', cmd]).replace('\r\n', '\n')
        except KeyboardInterrupt:
            print('^C', file=sys.stderr, end='')
--------------------
Prompt: 
def get_signature(self, value):
        """Returns the signature for the given value"""
----------
Output: 

        return self.signatures[value]

    def get_signature_for_value(self, value):
        """Returns the signature for the given value"""
        return self.signatures[value]

    def get_signature_for_name(self, name):
        """Returns the signature for the given name"""
        return self.signatures[self.names[name]]

    def get_signature_for_type(self, type):
        """Returns the signature for the given type"""
        return self
----------
Solution: 

        value = want_bytes(value)
        key = self.derive_key()
        sig = self.algorithm.get_signature(key, value)
        return base64_encode(sig)
--------------------
Prompt: 
def generate_config_file(self):
        """generate default config file from Configurables"""
----------
Output: 

        config_file = open(self.config_file, 'w')
        config_file.write("[Configurables]\n")
        for key, value in self.configurables.items():
            config_file.write(key + "=" + str(value) + "\n")
        config_file.close()

    def load_config_file(self):
        """load config file to Configurables"""
        config_file = open(self.config_file, 'r')
        for line in config_file:
           
----------
Solution: 

        lines = ["# Configuration file for %s."%self.name]
        lines.append('')
        lines.append('c = get_config()')
        lines.append('')
        for cls in self.classes:
            lines.append(cls.class_config_section())
        return '\n'.join(lines)
--------------------
Prompt: 
def _republish_displaypub(self, content, eid):
        """republish individual displaypub content dicts"""
----------
Output: 

        if not content:
            return
        if not isinstance(content, list):
            content = [content]
        for c in content:
            if c['type'] == 'displaypub':
                self.republish_displaypub(c, eid)

    def _republish_displaypub_content(self, content, eid):
        """republish individual displaypub content dicts"""
        if not content:
            return
        if not isinstance(content, list):
            content = [content]
        for c
----------
Solution: 

        try:
            ip = get_ipython()
        except NameError:
            # displaypub is meaningless outside IPython
            return
        md = content['metadata'] or {}
        md['engine'] = eid
        ip.display_pub.publish(content['source'], content['data'], md)
--------------------
Prompt: 
def __make_points_for_label(self, ts, data, label, prefix, gun_stats):
        """x
        Make a set of points for `this` label

        overall_quantiles, overall_meta, net_codes, proto_codes, histograms
        """
----------
Output: 

        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms
        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms
        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms
        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms
        # overall_quantiles, overall_meta, net_codes, proto_codes, histograms
        # overall_quantiles, overall_meta, net
----------
Solution: 

        label_points = list()

        label_points.extend(
            (
                # overall quantiles for label
                self.__make_points(
                    prefix + "overall_quantiles",
                    {"label": label},
                    ts,
                    self.__make_quantile_fields(data)
                ),
                # overall meta (gun status) for label
                self.__make_points(
                    prefix + "overall_meta",
                    {"label": label},
                    ts,
                    self.__make_overall_meta_fields(data, gun_stats)
                ),
                # net codes for label
                self.__make_points(
                    prefix + "net_codes",
                    {"label": label},
                    ts,
                    self.__make_netcodes_fields(data)
                ),
                # proto codes for label
                self.__make_points(
                    prefix + "proto_codes",
                    {"label": label},
                    ts,
                    self.__make_protocodes_fields(data)
                )
            )
        )
        # histograms, one row for each bin
        if self.histograms:
            for bin_, count in zip(data["interval_real"]["hist"]["bins"],
                                   data["interval_real"]["hist"]["data"]):
                label_points.append(
                    self.__make_points(
                        prefix + "histograms",
                        {"label": label},
                        ts,
                        {"bin": bin_, "count": count}
                    )
                )
        return label_points
--------------------
Prompt: 
def app_url_defaults(self, f):
        """Same as :meth:`url_defaults` but application wide.
        """
----------
Output: 

        self.url_defaults = f
        return f

    def url_defaults(self, f):
        """Decorator to set the default values for the URL.

        This decorator is used to set the default values for the URL.
        The decorated function should take a single argument which is a
        :class:`~werkzeug.routing.Map` object and return a dictionary with
        the default values.

        Example::

            @app.url_defaults
            def add_default_language_code(endpoint, values
----------
Solution: 

        self.record_once(lambda s: s.app.url_default_functions
            .setdefault(None, []).append(f))
        return f
--------------------
Prompt: 
def bulk_dump(self, table, tmp_file):
        """
        Dumps a database table into a tab-delimited file
        """
----------
Output: 

        self.cursor.execute("SELECT * FROM %s" % table)
        self.cursor.copy_to(tmp_file, sep="\t")

    def bulk_load(self, table, tmp_file):
        """
        Loads a tab-delimited file into a database table
        """
        self.cursor.copy_from(tmp_file, table, sep="\t")

    def get_table_names(self):
        """
        Returns a list of table names in the database
        """
        self.cursor
----------
Solution: 

        self.copy_expert("COPY {table} TO STDOUT".format(table=table), tmp_file)
--------------------
Prompt: 
def  make_html_items( self, items ):
        """ convert a field's content into some valid HTML """
----------
Output: 

        html_items = []
        for item in items:
            html_items.append( self.make_html_item( item ) )
        return html_items

    def make_html_item( self, item ):
        """ convert a field's content into some valid HTML """
        if isinstance( item, str ):
            return item
        elif isinstance( item, int ):
            return str( item )
        elif isinstance( item, float ):
            return str( item )
        elif isinstance( item, bool
----------
Solution: 

        lines = []
        for item in items:
            if item.lines:
                lines.append( self.make_html_code( item.lines ) )
            else:
                lines.append( self.make_html_para( item.words ) )

        return string.join( lines, '\n' )
--------------------
Prompt: 
def p_relate_statement_2(self, p):
        """statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase"""
----------
Output: 

        p[0] = RelateStatement(p[2], p[4], p[6], p[7])

    def p_relate_statement_3(self, p):
        """statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase_list"""
        p[0] = RelateStatement(p[2], p[4], p[6], p[7])

    def p_relate_statement_4(self, p):
        """statement : RELATE instance_name TO
----------
Solution: 

        p[0] = RelateNode(from_variable_name=p[2],
                          to_variable_name=p[4],
                          rel_id=p[6],
                          phrase=p[8])
--------------------
Prompt: 
def load_exons(self, exons, genes=None, build='37'):
        """Create exon objects and insert them into the database
        
        Args:
            exons(iterable(dict))
        """
----------
Output: 

        for exon in exons:
            if exon['chrom'] not in self.chroms:
                self.chroms[exon['chrom']] = Chrom(exon['chrom'], build=build)
            self.chroms[exon['chrom']].add_exon(exon, genes)

    def load_genes(self, genes, build='37'):
        """Create gene objects and insert them into the database
        
        Args:
            genes(iterable(dict))
        """
        for
----------
Solution: 

        genes = genes or self.ensembl_genes(build)
        for exon in exons:
            exon_obj = build_exon(exon, genes)
            if not exon_obj:
                continue
            
            res = self.exon_collection.insert_one(exon_obj)
--------------------
Prompt: 
def enrich(self, gmt):
        """use local mode
         
        p = p-value computed using the Fisher exact test (Hypergeometric test)  

        Not implemented here:

            combine score = log(p)·z

        see here: http://amp.pharm.mssm.edu/Enrichr/help#background&q=4
        
        columns contain:
            
            Term Overlap P-value Adjusted_P-value Genes

        """
----------
Output: 

        if self.verbose:
            print("enrichment")
        if self.verbose:
            print("enrichment: use local mode")
        if self.verbose:
            print("enrichment: use local mode")
        if self.verbose:
            print("enrichment: use local mode")
        if self.verbose:
            print("enrichment: use local mode")
        if self.verbose:
            print("enrichment: use local mode")
        if self.verbose:
            print("
----------
Solution: 

        if isscalar(self.background):
            if isinstance(self.background, int) or self.background.isdigit():
                self._bg = int(self.background)
            elif isinstance(self.background, str):
                # self.background = set(reduce(lambda x,y: x+y, gmt.values(),[]))
                self._bg = self.get_background()
                self._logger.info("Background: found %s genes"%(len(self._bg)))
            else:
                raise Exception("Unsupported background data type")
        else:
            # handle array object: nd.array, list, tuple, set, Series
            try:
                it = iter(self.background)
                self._bg = set(self.background)
            except TypeError:
                self._logger.error("Unsupported background data type")
        # statistical testing
        hgtest = list(calc_pvalues(query=self._gls, gene_sets=gmt, 
                                   background=self._bg))
        if len(hgtest) > 0:
            terms, pvals, olsz, gsetsz, genes = hgtest
            fdrs, rej = multiple_testing_correction(ps = pvals, 
                                                    alpha=self.cutoff,
                                                    method='benjamini-hochberg')
            # save to a dataframe
            odict = OrderedDict()
            odict['Term'] = terms
            odict['Overlap'] = list(map(lambda h,g: "%s/%s"%(h, g), olsz, gsetsz))
            odict['P-value'] = pvals
            odict['Adjusted P-value'] = fdrs
            # odict['Reject (FDR< %s)'%self.cutoff ] = rej
            odict['Genes'] = [";".join(g) for g in genes]
            res = pd.DataFrame(odict)
            return res
        return
--------------------
Prompt: 
def populate_from_sequence(seq: list, r: ref(Edge), sr: state.StateRegister):
    """ function that connect each other one sequence of MatchExpr. """
----------
Output: 

    for i in range(len(seq) - 1):
        r.connect(seq[i], seq[i + 1])


def populate_from_sequence_with_state(seq: list, r: ref(Edge), sr: state.StateRegister):
    """ function that connect each other one sequence of MatchExpr. """
    for i in range(len(seq) - 1):
        r.connect(seq[i], seq[i + 1], sr.get_state(seq[i]))


def populate_from_
----------
Solution: 

    base_state = r
    # we need to detect the last state of the sequence
    idxlast = len(seq) - 1
    idx = 0
    for m in seq:
        # alternatives are represented by builtin list
        if isinstance(m, list):
            # so recursively connect all states of each alternative sequences.
            for item in m:
                populate_from_sequence(item, r, sr)
        elif isinstance(m, MatchExpr):
            # from the current state, have we a existing edge for this event?
            eX = r().get_next_edge(m)
            if eX is None:
                sX = None
                if idx != idxlast:
                    sX = state.State(sr)
                    sX.matchDefault(base_state().s)
                else:
                    # last state of sequence return to the base
                    sX = base_state().s
                eX = Edge(sX)
                r().next_edge[id(sX)] = eX
                m.attach(r().s, sX, sr)
            r = ref(eX)
        idx += 1
--------------------
Prompt: 
def mr_reader(job, input_stream, loads=core.loads):
    """ Converts a file object with json serialised pyschema records
        to a stream of pyschema objects

    Can be used as job.reader in luigi.hadoop.JobTask
    """
----------
Output: 

    for line in input_stream:
        yield loads(line)


def mr_writer(job, output_stream, dumps=core.dumps):
    """ Converts a stream of pyschema objects to a file object with
        json serialised pyschema records

    Can be used as job.writer in luigi.hadoop.JobTask
    """
    for record in output_stream:
        yield dumps(record)


def mr_mapper(job, input_stream, output_stream, loads=core.loads):
----------
Solution: 

    for line in input_stream:
        yield loads(line),
--------------------
Prompt: 
def add_from_raw_data(self, raw_data, data_type_id, name, description):
        """
        Upload already serialized raw data as a new dataset.

        Parameters
        ----------
        raw_data: bytes
            Dataset contents to upload.
        data_type_id : str
            Serialization format of the raw data.
            Supported formats are:
                'PlainText'
                'GenericCSV'
                'GenericTSV'
                'GenericCSVNoHeader'
                'GenericTSVNoHeader'
                'ARFF'
            See the azureml.DataTypeIds class for constants.
        name : str
            Name for the new dataset.
        description : str
            Description for the new dataset.

        Returns
        -------
        SourceDataset
            Dataset that was just created.
            Use open(), read_as_binary(), read_as_text() or to_dataframe() on
            the dataset object to get its contents as a stream, bytes, str or
            pandas DataFrame.
        """
----------
Output: 
TSVNoHeader'
                'GenericJSON'
                'GenericXML'
                'GenericExcel'
                'GenericExcel2007'
                'GenericExcel2010'
                'GenericExcel2013'
                'GenericExcel2016'
                'GenericExcel2019'
                'GenericPDF'
                'GenericRTF'
                'GenericWord'
                'GenericPowerPoint'
                'GenericVisio'
                'GenericOutlook'
                'GenericOpenDocumentSpreadsheet'
                'GenericOpenDocument
----------
Solution: 

        _not_none('raw_data', raw_data)
        _not_none_or_empty('data_type_id', data_type_id)
        _not_none_or_empty('name', name)
        _not_none_or_empty('description', description)

        return self._upload(raw_data, data_type_id, name, description)
--------------------
Prompt: 
def write_bit(self, value, bit_num, address):
        """Writes the value given to the bit in the address specified.

        :param value: The value to write.
        :type value: int
        :param bit_num: The bit number to write to.
        :type bit_num: int
        :param address: The address to write to.
        :type address: int
        """
----------
Output: 

        self.write_byte(value, address)

    def write_byte(self, value, address):
        """Writes the value given to the address specified.

        :param value: The value to write.
        :type value: int
        :param address: The address to write to.
        :type address: int
        """
        self.write_bytes([value], address)

    def write_bytes(self, values, address):
        """Writes the values given to the address specified.

        :param values:
----------
Solution: 

        bit_mask = get_bit_mask(bit_num)
        old_byte = self.read(address)
         # generate the new byte
        if value:
            new_byte = old_byte | bit_mask
        else:
            new_byte = old_byte & ~bit_mask
        self.write(new_byte, address)
--------------------
Prompt: 
def filter(self, *filters, **kwargs):
        """
        Returns this Query instance with the query args combined with
        existing set with AND.

        kwargs are simply passed to a new Filter object and combined to any
        other filters with AND.

        By default, everything is combined using AND. If you provide
        multiple filters in a single filter call, those are ANDed
        together. If you provide multiple filters in multiple filter
        calls, those are ANDed together.

        If you want something different, use the F class which supports
        ``&`` (and), ``|`` (or) and ``~`` (not) operators. Then call
        filter once with the resulting Filter instance.
        """
----------
Output: 
`` and ``|`` operators.

        :param filters:
            Any number of Filter objects.
        :param kwargs:
            Any number of keyword arguments to be passed to the Filter
            constructor.
        :return:
            This Query instance.
        """
        if kwargs:
            filters = (F(**kwargs),) + filters

        if not filters:
            return self

        if len(filters) == 1:
            self._filters = filters[0]
        else:
            self._filters = F(filters)


----------
Solution: 

        f = list(filters)

        if kwargs:
            f += [Filter(**kwargs)]

        return self._clone(filters=f)
--------------------
Prompt: 
def topological_nodes(self):
        """
        Yield nodes in topological order.

        Returns:
            generator(DAGNode): node in topological order
        """
----------
Output: 

        for node in self.topological_sort():
            yield node

    def topological_sort(self):
        """
        Return nodes in topological order.

        Returns:
            list(DAGNode): nodes in topological order
        """
        return list(self._topological_sort())

    def _topological_sort(self):
        """
        Return nodes in topological order.

        Returns:
            iterator(DAGNode): nodes in topological order
        """
        visited = set()
        stack = []
        for node in
----------
Solution: 

        return nx.lexicographical_topological_sort(self._multi_graph,
                                                   key=lambda x: str(x.qargs))
--------------------
Prompt: 
def peng_power(snum):
    r"""
    Return engineering suffix and its floating point equivalent of a number.

    :py:func:`peng.peng` lists the correspondence between suffix and floating
    point exponent.

    :param snum: Number
    :type  snum: :ref:`EngineeringNotationNumber`

    :rtype: named tuple in which the first item is the engineering suffix and
            the second item is the floating point equivalent of the suffix
            when the number is represented in engineering notation.


    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]
    .. Auto-generated exceptions documentation for
    .. peng.functions.peng_power

    :raises: RuntimeError (Argument \`snum\` is not valid)

    .. [[[end]]]

    For example:

        >>> import peng
        >>> peng.peng_power(peng.peng(1235.6789E3, 3, False))
        EngPower(suffix='M', exp=1000000.0)
    """
----------
Output: 
   .. versionadded:: 1.0.0

    """
    return peng(snum)


def peng(snum):
    r"""
    Return engineering suffix and its floating point equivalent of a number.

    :param snum: Number
    :type  snum: :ref:`EngineeringNotationNumber`

    :rtype: named tuple in which the first item is the engineering suffix and
            the second item is the floating point equivalent of the suffix
            when the number is represented in engineering notation.


    .. version
----------
Solution: 

    suffix = " " if snum[-1].isdigit() else snum[-1]
    return EngPower(suffix, _SUFFIX_POWER_DICT[suffix])
--------------------
Prompt: 
def get_next_proto_negotiated(self):
        """
        Get the protocol that was negotiated by NPN.

        :returns: A bytestring of the protocol name.  If no protocol has been
            negotiated yet, returns an empty string.

        .. versionadded:: 0.15
        """
----------
Output: 

        return self._get_next_proto_negotiated()

    def _get_next_proto_negotiated(self):
        """
        Get the protocol that was negotiated by NPN.

        :returns: A bytestring of the protocol name.  If no protocol has been
            negotiated yet, returns an empty string.

        .. versionadded:: 0.15
        """
        return self._next_proto_negotiated

    def set_next_proto_negotiated(self, proto):
        """
----------
Solution: 

        _warn_npn()
        data = _ffi.new("unsigned char **")
        data_len = _ffi.new("unsigned int *")

        _lib.SSL_get0_next_proto_negotiated(self._ssl, data, data_len)

        return _ffi.buffer(data[0], data_len[0])[:]
--------------------
Prompt: 
def update_virtual_meta(self):
        """Will read back the virtual column etc, written by :func:`DataFrame.write_virtual_meta`. This will be done when opening a DataFrame."""
----------
Output: 

        if self._virtual_meta is not None:
            return
        self._virtual_meta = {}
        if not self._virtual_meta_path.exists():
            return
        with self._virtual_meta_path.open('r') as f:
            for line in f:
                if line.startswith('#'):
                    continue
                try:
                    key, value = line.strip().split('=', 1)
                except ValueError:
                    continue
                self._virtual_meta[key] = value

    def write
----------
Solution: 

        import astropy.units
        try:
            path = os.path.join(self.get_private_dir(create=False), "virtual_meta.yaml")
            if os.path.exists(path):
                meta_info = vaex.utils.read_json_or_yaml(path)
                if 'virtual_columns' not in meta_info:
                    return
                self.virtual_columns.update(meta_info["virtual_columns"])
                self.variables.update(meta_info["variables"])
                self.ucds.update(meta_info["ucds"])
                self.descriptions.update(meta_info["descriptions"])
                units = {key: astropy.units.Unit(value) for key, value in meta_info["units"].items()}
                self.units.update(units)
        except:
            logger.exception("non fatal error")
--------------------
Prompt: 
def delete(self, teamId):
        """Delete a team.

        Args:
            teamId(basestring): The ID of the team to be deleted.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError: If the Webex Teams cloud returns an error.

        """
----------
Output: 

        check_type(teamId, basestring)
        return self.delete_path('/teams/{}'.format(teamId))

    def get(self, teamId):
        """Get a team.

        Args:
            teamId(basestring): The ID of the team to be retrieved.

        Returns:
            Team: A Team object.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError: If the Webex Teams cloud returns an error.

        """
        check_
----------
Solution: 

        check_type(teamId, basestring, may_be_none=False)

        # API request
        self._session.delete(API_ENDPOINT + '/' + teamId)
--------------------
Prompt: 
def per_triangle(script, sidedim=0, textdim=1024, border=2, method=1):
    """Trivial Per-Triangle parameterization

    """
----------
Output: 

    script.add_command('per_triangle')
    script.add_arg('-sidedim', sidedim)
    script.add_arg('-textdim', textdim)
    script.add_arg('-border', border)
    script.add_arg('-method', method)


def per_vertex(script, sidedim=0, textdim=1024, border=2, method=1):
    """Trivial Per-Vertex parameterization

    """
    script.add_command('per_ver
----------
Solution: 

    filter_xml = ''.join([
        '  <filter name="Parametrization: Trivial Per-Triangle ">\n',
        '    <Param name="sidedim"',
        'value="%d"' % sidedim,
        'description="Quads per line"',
        'type="RichInt"',
        'tooltip="Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation"',
        '/>\n',
        '    <Param name="textdim"',
        'value="%d"' % textdim,
        'description="Texture Dimension (px)"',
        'type="RichInt"',
        'tooltip="Gives an indication on how big the texture is"',
        '/>\n',
        '    <Param name="border"',
        'value="%d"' % border,
        'description="Inter-Triangle border (px)"',
        'type="RichInt"',
        'tooltip="Specifies how many pixels to be left between triangles in parametrization domain"',
        '/>\n',
        '    <Param name="method"',
        'value="%d"' % method,
        'description="Method"',
        'enum_val0="Basic"',
        'enum_val1="Space-optimizing"',
        'enum_cardinality="2"',
        'type="RichEnum"',
        'tooltip="Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain"'
        '/>\n',
        '  </filter>\n'])
    util.write_filter(script, filter_xml)
    return None
--------------------
Prompt: 
def add_virtual_columns_cartesian_velocities_to_spherical(self, x="x", y="y", z="z", vx="vx", vy="vy", vz="vz", vr="vr", vlong="vlong", vlat="vlat", distance=None):
        """Concert velocities from a cartesian to a spherical coordinate system

        TODO: errors

        :param x: name of x column (input)
        :param y:         y
        :param z:         z
        :param vx:       vx
        :param vy:       vy
        :param vz:       vz
        :param vr: name of the column for the radial velocity in the r direction (output)
        :param vlong: name of the column for the velocity component in the longitude direction  (output)
        :param vlat: name of the column for the velocity component in the latitude direction, positive points to the north pole (output)
        :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance
        :return:
        """
----------
Output: 
vx:        vx
        :param vy:        vy
        :param vz:        vz
        :param vr:        vr
        :param vlong:     vlong
        :param vlat:      vlat
        :param distance:  distance from the sun
        :return:
        """
        if distance is None:
            distance = self.distance
        self.add_column(x, "x")
        self.add_column(y, "y")
        self.add_column(z, "z")
        self.
----------
Solution: 

        # see http://www.astrosurf.com/jephem/library/li110spherCart_en.htm
        if distance is None:
            distance = "sqrt({x}**2+{y}**2+{z}**2)".format(**locals())
        self.add_virtual_column(vr, "({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}".format(**locals()))
        self.add_virtual_column(vlong, "-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)".format(**locals()))
        self.add_virtual_column(vlat, "-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( {distance}*sqrt({x}**2+{y}**2) )".format(**locals()))
--------------------
Prompt: 
def check_token(token):
    """ Verify http header token authentification """
----------
Output: 

    if token == '':
        return False
    else:
        return True


def check_user(user):
    """ Verify user authentification """
    if user == '':
        return False
    else:
        return True


def check_password(password):
    """ Verify user authentification """
    if password == '':
        return False
    else:
        return True


def check_email(email):
    """ Verify user email """
    if email == '':
        return False
   
----------
Solution: 

    user = models.User.objects(api_key=token).first()
    return user or None
--------------------
Prompt: 
def validate_value(self, value):
        """
        Validate new property value before setting it.

        value -- New value
        """
----------
Output: 

        if value is None:
            return

        if not isinstance(value, self.value_type):
            raise TypeError("Value must be of type %s" % self.value_type)

        if self.value_type is int:
            if value < 0:
                raise ValueError("Value must be positive")

        if self.value_type is float:
            if value < 0.0:
                raise ValueError("Value must be positive")

        if self.value_type is str:
            if not isinstance
----------
Solution: 

        if 'readOnly' in self.metadata and self.metadata['readOnly']:
            raise PropertyError('Read-only property')

        try:
            validate(value, self.metadata)
        except ValidationError:
            raise PropertyError('Invalid property value')
--------------------
Prompt: 
def pack_metadata_statement(self, metadata, receiver='', iss='', lifetime=0,
                                sign_alg=''):
        """
        Given a MetadataStatement instance create a signed JWT.

        :param metadata: Original metadata statement as a MetadataStatement
            instance
        :param receiver: Receiver (audience) of the JWT
        :param iss: Issuer ID if different from default
        :param lifetime: jWT signature life time
        :param sign_alg: JWT signature algorithm
        :return: A JWT
        """
----------
Output: 

        if not metadata:
            raise ValueError('No metadata statement provided')

        if not isinstance(metadata, MetadataStatement):
            raise ValueError('Metadata statement must be a MetadataStatement '
                             'instance')

        if not receiver:
            raise ValueError('No receiver provided')

        if not iss:
            iss = self.iss

        if not lifetime:
            lifetime = self.lifetime

        if not sign_alg:
            sign_alg = self.sign_alg

        # Create the JWT header
----------
Solution: 


        return self.self_signer.sign(metadata, receiver=receiver, iss=iss,
                                     lifetime=lifetime, sign_alg=sign_alg)
--------------------
Prompt: 
def contact(self, id):
        """
        Retrieve all details for a specific contact, including fields such as birthday and mood.

        Args:
            id (str): user identifier to lookup

        Returns:
            SkypeContact: resulting contact object
        """
----------
Output: 

        return SkypeContact(self, id)

    def contacts(self):
        """
        Retrieve all contacts, including fields such as birthday and mood.

        Returns:
            list: list of SkypeContact objects
        """
        return [SkypeContact(self, contact) for contact in self.get_contacts()]

    def get_contacts(self):
        """
        Retrieve all contacts, including fields such as birthday and mood.

        Returns:
            list: list of contact identifiers

----------
Solution: 

        try:
            json = self.skype.conn("POST", "{0}/users/batch/profiles".format(SkypeConnection.API_USER),
                                   json={"usernames": [id]}, auth=SkypeConnection.Auth.SkypeToken).json()
            contact = SkypeContact.fromRaw(self.skype, json[0])
            if contact.id not in self.contactIds:
                self.contactIds.append(contact.id)
            return self.merge(contact)
        except SkypeApiException as e:
            if len(e.args) >= 2 and getattr(e.args[1], "status_code", None) == 403:
                # Not a contact, so no permission to retrieve information.
                return None
            raise
--------------------
Prompt: 
def str_repeat(x, repeats):
    """Duplicate each string in a column.

    :param int repeats: number of times each string sample is to be duplicated.
    :returns: an expression containing the duplicated strings

    Example:

    >>> import vaex
    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']
    >>> df = vaex.from_arrays(text=text)
    >>> df
      #  text
      0  Something
      1  very pretty
      2  is coming
      3  our
      4  way.

    >>> df.text.str.repeat(3)
    Expression = str_repeat(text, 3)
    Length: 5 dtype: str (expression)
    ---------------------------------
    0        SomethingSomethingSomething
    1  very prettyvery prettyvery pretty
    2        is comingis comingis coming
    3                          ourourour
    4                       way.way.way.
    """
----------
Output: 

      2  is coming
      3  our
      4  way.
    >>> df.text_repeat(2)
      #  text
      0  Something
      0  Something
      1  very pretty
      1  very pretty
      2  is coming
      2  is coming
      3  our
      3  our
      4  way.
      4  way.
    """
    return x.str_repeat(repeats)


def str_slice(x, start, stop, step=None):
    """Slice
----------
Solution: 

    sl = _to_string_sequence(x).repeat(repeats)
    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)
--------------------
Prompt: 
def deprecated(*args):
    """ Deprecation warning decorator. Takes optional deprecation message,
    otherwise will use a generic warning. """
----------
Output: 

    if len(args) == 1:
        msg = args[0]
    else:
        msg = "This function is deprecated"
    def deprecated_decorator(func):
        def deprecated_func(*args, **kwargs):
            warnings.warn(msg, DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return deprecated_func
    return deprecated_decorator


def deprecated_class(*args):
    """ Deprecation warning decorator for classes. Takes optional deprecation
    message, otherwise
----------
Solution: 

    def wrap(func):
        def wrapped_func(*args, **kwargs):
            warnings.warn(msg, category=DeprecationWarning)
            return func(*args, **kwargs)
        return wrapped_func

    if len(args) == 1 and callable(args[0]):
        msg = "Function '%s' will be deprecated in future versions of " \
            "Neurosynth." % args[0].__name__
        return wrap(args[0])
    else:
        msg = args[0]
        return wrap
--------------------
Prompt: 
def get_last_activities(self, n):
        """Get all activity data for the last activity

        Keyword arguments:
        """
----------
Output: 

        return self.get_activities(n, 0)

    def get_activities(self, n, offset):
        """Get all activity data for the last activity

        Keyword arguments:
        """
        url = self.url + "/activities?n=" + str(n) + "&offset=" + str(offset)
        return self.get_data(url)

    def get_activity(self, activity_id):
        """Get activity data for a specific activity

        Keyword arguments:
        """
        url
----------
Solution: 

        filenames = self.get_activity_list().iloc[-n:].filename.tolist()
        last_activities = [self.get_activity(f) for f in filenames]
        return last_activities
--------------------
Prompt: 
def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):
    """
    Update a user in the database
    """
----------
Output: 

    user = context.dbsession.query(User).get(user_id)
    if user is None:
        raise click.ClickException("User not found")
    if update_role:
        user.role = update_role
    if add_institute:
        institute = context.dbsession.query(Institute).get(add_institute)
        if institute is None:
            raise click.ClickException("Institute not found")
        user.institutes.append(institute)
    if remove_admin:
        user
----------
Solution: 

    adapter = context.obj['adapter']

    user_obj = adapter.user(user_id)

    if not user_obj:
        LOG.warning("User %s could not be found", user_id)
        context.abort()

    existing_roles = set(user_obj.get('roles',[]))
    if update_role:
        if not update_role in user_obj['roles']:
            existing_roles = set(user_obj['roles'])
            existing_roles.add(update_role)
            LOG.info("Adding role %s to user", update_role)
        else:
            LOG.warning("User already have role %s", update_role)

    if remove_admin:
        try:
            existing_roles.remove('admin')
            LOG.info("Removing admin rights from user %s", user_id)
        except KeyError as err:
            LOG.info("User %s does not have admin rights", user_id)

    user_obj['roles'] = list(existing_roles)

    existing_institutes = set(user_obj.get('institutes',[]))
    for institute_id in add_institute:
        institute_obj = adapter.institute(institute_id)
        if not institute_obj:
            LOG.warning("Institute %s could not be found", institute_id)
        else:
            existing_institutes.add(institute_id)
            LOG.info("Adding institute %s to user", institute_id)

    for institute_id in remove_institute:
        try:
            existing_institutes.remove(institute_id)
            LOG.info("Removing institute %s from user", institute_id)
        except KeyError as err:
            LOG.info("User does not have access to institute %s", institute_id)

    user_obj['institutes'] = list(existing_institutes)

    updated_user = adapter.update_user(user_obj)
--------------------
Prompt: 
def _update_base(self):
        """update the base, including the URL for GitLab and the API endpoint.
        """
----------
Output: 

        self.base = self.base_url + self.api_endpoint

    def _update_headers(self):
        """update the headers for the requests.
        """
        self.headers = {
            'PRIVATE-TOKEN': self.token,
            'Content-Type': 'application/json'
        }

    def _update_session(self):
        """update the session for the requests.
        """
        self.session = requests.Session()
        self.session.headers.update(self.headers)


----------
Solution: 

        self.base = self._get_and_update_setting('SREGISTRY_GITLAB_BASE',
                                                 "https://gitlab.com/")
        self.api_base = "%s/api/v4" % self.base.strip('/')
        self.artifacts = self._get_and_update_setting('SREGISTRY_GITLAB_FOLDER',
                                                      'build')

        self.job = self._get_and_update_setting('SREGISTRY_GITLAB_JOB', 'build')

        bot.debug('      Api: %s' % self.api_base)
        bot.debug('Artifacts: %s' % self.artifacts)
        bot.debug('      Job: %s' % self.job)
--------------------
Prompt: 
def send(self, message):
        """ Send a message object

            :type message: data.OutgoingMessage
            :param message: The message to send
            :rtype: data.OutgoingMessage
            :returns: The sent message with populated fields
            :raises AssertionError: wrong provider name encountered (returned by the router, or provided to OutgoingMessage)
            :raises MessageSendError: generic errors
            :raises AuthError: provider authentication failed
            :raises LimitsError: sending limits exceeded
            :raises CreditError: not enough money on the account
        """
----------
Output: 
Error: provider credit exhausted
            :raises ProviderError: provider-specific errors
        """
        # TODO: add support for sending to multiple recipients
        # TODO: add support for sending to multiple providers
        # TODO: add support for sending to multiple providers with different recipients
        # TODO: add support for sending to multiple providers with different messages
        # TODO: add support for sending to multiple providers with different messages and recipients
        # TODO: add support for sending to multiple providers with different messages and recipients
        # TODO: add support for sending
----------
Solution: 

        # Which provider to use?
        provider_name = self._default_provider  # default
        if message.provider is not None:
            assert message.provider in self._providers, \
                'Unknown provider specified in OutgoingMessage.provideer: {}'.format(provider_name)
            provider = self.get_provider(message.provider)
        else:
            # Apply routing
            if message.routing_values is not None: # Use the default provider when no routing values are given
                # Routing values are present
                provider_name = self.router(message, *message.routing_values) or self._default_provider
                assert provider_name in self._providers, \
                    'Routing function returned an unknown provider name: {}'.format(provider_name)
            provider = self.get_provider(provider_name)

        # Set message provider name
        message.provider = provider.name

        # Send the message using the provider
        message = provider.send(message)

        # Emit the send event
        self.onSend(message)

        # Finish
        return message
--------------------
Prompt: 
def string_arg(ctx, obj):
    """
    Handles LiteralObjects as well as computable arguments
    """
----------
Output: 

    if isinstance(obj, LiteralObject):
        return obj.value
    else:
        return obj.eval(ctx)


def string_arg_list(ctx, obj):
    """
    Handles LiteralObjects as well as computable arguments
    """
    if isinstance(obj, LiteralObject):
        return obj.value
    else:
        return [x.eval(ctx) for x in obj]


def string_arg_dict(ctx, obj):
    """
    Handles LiteralObjects as well as
----------
Solution: 

    if hasattr(obj, 'compute'):
        obj = next(obj.compute(ctx), False)
    return to_string(obj)
--------------------
Prompt: 
def on_kill(self):
        """
        Cancel the submitted athena query
        """
----------
Output: 

        if self.query_id:
            self.athena_client.stop_query(self.query_id)

    def on_kill_callback(self, event):
        """
        Cancel the submitted athena query
        """
        self.on_kill()

    def on_execute(self, event):
        """
        Execute the athena query
        """
        self.query_id = self.athena_client.start_query_execution(self.query)

    def on_execute_callback(self,
----------
Solution: 

        if self.query_execution_id:
            self.log.info('⚰️⚰️⚰️ Received a kill Signal. Time to Die')
            self.log.info(
                'Stopping Query with executionId - %s', self.query_execution_id
            )
            response = self.hook.stop_query(self.query_execution_id)
            http_status_code = None
            try:
                http_status_code = response['ResponseMetadata']['HTTPStatusCode']
            except Exception as ex:
                self.log.error('Exception while cancelling query', ex)
            finally:
                if http_status_code is None or http_status_code != 200:
                    self.log.error('Unable to request query cancel on athena. Exiting')
                else:
                    self.log.info(
                        'Polling Athena for query with id %s to reach final state', self.query_execution_id
                    )
                    self.hook.poll_query_status(self.query_execution_id)
--------------------
Prompt: 
def get_fernet():
    """
    Deferred load of Fernet key.

    This function could fail either because Cryptography is not installed
    or because the Fernet key is invalid.

    :return: Fernet object
    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet
    """
----------
Output: 

    from cryptography.fernet import InvalidToken, Fernet

    try:
        return Fernet(conf.get('core', 'FERNET_KEY'))
    except (ImportError, InvalidToken):
        raise AirflowException(
            "Fernet key is missing or invalid. Please provide a valid key "
            "using the 'fernet_key' CLI command or in your airflow.cfg file"
        )


def get_password_manager():
    """
    Deferred load of Password Manager.

    This function could fail either because
----------
Solution: 

    global _fernet
    log = LoggingMixin().log

    if _fernet:
        return _fernet
    try:
        from cryptography.fernet import Fernet, MultiFernet, InvalidToken
        global InvalidFernetToken
        InvalidFernetToken = InvalidToken

    except BuiltinImportError:
        log.warning(
            "cryptography not found - values will not be stored encrypted."
        )
        _fernet = NullFernet()
        return _fernet

    try:
        fernet_key = configuration.conf.get('core', 'FERNET_KEY')
        if not fernet_key:
            log.warning(
                "empty cryptography key - values will not be stored encrypted."
            )
            _fernet = NullFernet()
        else:
            _fernet = MultiFernet([
                Fernet(fernet_part.encode('utf-8'))
                for fernet_part in fernet_key.split(',')
            ])
            _fernet.is_encrypted = True
    except (ValueError, TypeError) as ve:
        raise AirflowException("Could not create Fernet object: {}".format(ve))

    return _fernet
--------------------
Prompt: 
def update_board(self, query_params=None):
        """
        Update this board's information. Returns a new board.
        """
----------
Output: 

        return self.client.update_board(self.id, query_params)

    def delete(self, query_params=None):
        """
        Delete this board. Returns a new board.
        """
        return self.client.delete_board(self.id, query_params)

    def get_lists(self, query_params=None):
        """
        Get all lists on this board.
        """
        return self.client.get_lists(self.id, query_params)

    def get_list(self
----------
Solution: 

        board_json = self.fetch_json(
            uri_path=self.base_uri,
            http_method='PUT',
            query_params=query_params or {}
        )

        return self.create_board(board_json)
--------------------
Prompt: 
def compose_u3(theta1, phi1, lambda1, theta2, phi2, lambda2):
        """Return a triple theta, phi, lambda for the product.

        u3(theta, phi, lambda)
           = u3(theta1, phi1, lambda1).u3(theta2, phi2, lambda2)
           = Rz(phi1).Ry(theta1).Rz(lambda1+phi2).Ry(theta2).Rz(lambda2)
           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda2)
           = u3(theta', phi1 + phi', lambda2 + lambda')

        Return theta, phi, lambda.
        """
----------
Output: 
(phi1+lambda2).Ry(theta1+theta2).Rz(phi2)

        The parameters are in radians.
        """
        theta = theta1 + theta2
        phi = phi1 + lambda2
        lambda_ = lambda1 + phi2
        return theta, phi, lambda_

    def _u3_to_matrix(theta, phi, lambda_):
        """Return the 2x2 unitary matrix corresponding to the U3 gate.

        The parameters are in radians
----------
Solution: 

        # Careful with the factor of two in yzy_to_zyz
        thetap, phip, lambdap = Optimize1qGates.yzy_to_zyz((lambda1 + phi2), theta1, theta2)
        (theta, phi, lamb) = (thetap, phi1 + phip, lambda2 + lambdap)

        return (theta, phi, lamb)
--------------------
Prompt: 
def filter_trim(self, start=1, end=1, filt=True):
        """
        Remove points from the start and end of filter regions.
        
        Parameters
        ----------
        start, end : int
            The number of points to remove from the start and end of
            the specified filter.
        filt : valid filter string or bool
            Which filter to trim. If True, applies to currently active
            filters.
        """
----------
Output: 

        if filt is True:
            filt = self.active_filter
        if filt not in self.filters:
            raise ValueError('filter not found')
        filt = self.filters[filt]
        filt = filt[start:-end]
        self.filters[filt.name] = filt
        self.active_filter = filt.name

    def filter_trim_start(self, start=1, filt=True):
        """
        Remove points from the start of filter regions.
        
       
----------
Solution: 

        params = locals()
        del(params['self'])
            
        f = self.filt.grab_filt(filt)
        nf = filters.trim(f, start, end)
        
        self.filt.add('trimmed_filter',
                    nf,
                    'Trimmed Filter ({:.0f} start, {:.0f} end)'.format(start, end),
                    params, setn=self.filt.maxset + 1)
--------------------
Prompt: 
def verify_reg_list(self, obj, object_type):
        """Verify a list of registers."""
----------
Output: 

        if not isinstance(obj, list):
            raise ValueError(f"{object_type} must be a list")
        for reg in obj:
            self.verify_reg(reg, object_type)

    def verify_reg(self, obj, object_type):
        """Verify a register."""
        if not isinstance(obj, dict):
            raise ValueError(f"{object_type} must be a dict")
        if "name" not in obj:
            raise ValueError(f"{object_type} must
----------
Solution: 

        # We expect the object to be a bitlist or an idlist, we don't care.
        # We will iterate it and ensure everything in it is declared as a bit,
        # and throw if not.
        for children in obj.children:
            self.verify_reg(children, object_type)
--------------------
Prompt: 
def regression(self, slope=None):
        """regress tip values against branch values

        Parameters
        ----------
        slope : None, optional
            if given, the slope isn't optimized

        Returns
        -------
        dict
            regression parameters
        """
----------
Output: 

        if slope is None:
            slope, intercept, r_value, p_value, std_err = stats.linregress(self.branch, self.tip)
        else:
            intercept = self.tip.mean() - slope * self.branch.mean()
        return {'slope': slope, 'intercept': intercept, 'r_value': r_value, 'p_value': p_value, 'std_err': std_err}

    def plot(self, ax=None, **kwargs):
        """plot the tree


----------
Solution: 

        self._calculate_averages()

        clock_model = base_regression(self.tree.root.Q, slope)
        clock_model['r_val'] = self.explained_variance()

        return clock_model
--------------------
Prompt: 
def to_keep(datetimes,
            years=0, months=0, weeks=0, days=0,
            hours=0, minutes=0, seconds=0,
            firstweekday=SATURDAY, now=None):
    """
    Return a set of datetimes that should be kept, out of ``datetimes``.

    Keeps up to ``years``, ``months``, ``weeks``, ``days``,
    ``hours``, ``minutes``, and ``seconds`` in the past.

    When keeping weeks, it prefers to keep ``firstweekday``, which
    defaults to Saturday.

    If ``now`` is None, it will base its calculations on
    ``datetime.datetime.now()``. Datetimes after this point will always be
    kept.
    """
----------
Output: 
.

    If ``now`` is not given, the current time is used.

    If ``firstweekday`` is given, it is used as the first day of the week
    (default is Saturday).

    >>> from datetime import datetime
    >>> from dateutil.relativedelta import relativedelta
    >>> from dateutil.rrule import rrule, MONTHLY
    >>> from dateutil.rrule import WEEKLY, DAILY, HOURLY, MINUTELY
    >>> from dateutil.rrule import SECOND
----------
Solution: 

    datetimes = set(datetimes)
    return (filters.Years.filter(datetimes, number=years, now=now) |
            filters.Months.filter(datetimes, number=months, now=now) |
            filters.Weeks.filter(datetimes, number=weeks,
                                 firstweekday=firstweekday, now=now) |
            filters.Days.filter(datetimes, number=days, now=now) |
            filters.Hours.filter(datetimes, number=hours, now=now) |
            filters.Minutes.filter(datetimes, number=minutes, now=now) |
            filters.Seconds.filter(datetimes, number=seconds, now=now))
--------------------
Prompt: 
def set(self, client_id, code, request, *args, **kwargs):
        """Creates Grant object with the given params

        :param client_id: ID of the client
        :param code:
        :param request: OAuthlib request object
        """
----------
Output: 

        self.client_id = client_id
        self.code = code
        self.redirect_uri = request.redirect_uri
        self.scope = request.scopes
        self.expires = datetime.utcnow() + timedelta(seconds=request.expires_in)
        self.user = request.user
        self.expires_in = request.expires_in
        self.state = request.state
        self.nonce = request.nonce
        self.code_challenge = request.code_challenge

----------
Solution: 

        expires = datetime.utcnow() + timedelta(seconds=100)
        grant = self.model(
            client_id=request.client.client_id,
            code=code['code'],
            redirect_uri=request.redirect_uri,
            scope=' '.join(request.scopes),
            user=self.current_user(),
            expires=expires
        )
        self.session.add(grant)

        self.session.commit()
--------------------
Prompt: 
def parse_hpo_diseases(hpo_lines):
    """Parse hpo disease phenotypes
    
        Args:
            hpo_lines(iterable(str))
        
        Returns:
            diseases(dict): A dictionary with mim numbers as keys
    """
----------
Output: 

    diseases = {}
    for line in hpo_lines:
        if line.startswith('HP:000'):
            mim_number = line.split(':')[1].split(' ')[0]
            diseases[mim_number] = line.split(':')[2].strip()
    return diseases


def parse_hpo_genes(hpo_lines):
    """Parse hpo gene phenotypes
    
        Args:
            hpo_lines(iterable(str))
        
        Returns
----------
Solution: 

    diseases = {}
    LOG.info("Parsing hpo diseases...")
    for index, line in enumerate(hpo_lines):
        # First line is a header
        if index == 0:
            continue
        # Skip empty lines
        if not len(line) > 3:
            continue
        # Parse the info
        disease_info = parse_hpo_disease(line)
        # Skip the line if there where no info
        if not disease_info:
            continue
        disease_nr = disease_info['disease_nr']
        hgnc_symbol = disease_info['hgnc_symbol']
        hpo_term = disease_info['hpo_term']
        source = disease_info['source']
        disease_id = "{0}:{1}".format(source, disease_nr)
        
        if disease_id not in diseases:
            diseases[disease_id] = {
                'disease_nr': disease_nr,
                'source': source,
                'hgnc_symbols': set(),
                'hpo_terms': set(),
            }

        if hgnc_symbol:
            diseases[disease_id]['hgnc_symbols'].add(hgnc_symbol)
        if hpo_term:
            diseases[disease_id]['hpo_terms'].add(hpo_term)

    LOG.info("Parsing done.")
    return diseases
--------------------
Prompt: 
def _check_currency_format(self, format=None):
        """
        Summary.

        Args:
            format (TYPE, optional): Description

        Returns:
            name (TYPE): Description
        """
----------
Output: 

        if format is None:
            format = self.currency_format
        if format not in self.currency_formats:
            raise ValueError(
                'Invalid currency format: %s. Valid formats: %s' % (
                    format, self.currency_formats))
        return format

    def _check_currency_symbol(self, symbol=None):
        """
        Summary.

        Args:
            symbol (TYPE, optional): Description

        Returns:
            name (TYPE): Description
        """
        if symbol
----------
Solution: 

        defaults = self.settings['currency']['format']
        if hasattr(format, '__call__'):
            format = format()
        if is_str(format) and re.match('%v', format):

            # Create and return positive, negative and zero formats:
            return {
                'pos': format,
                'neg': format.replace("-", "").replace("%v", "-%v"),
                'zero': format
            }
        elif not format or not format['por'] or not re.match('%v',
                                                             format['pos']):
            self.settings['currency']['format'] = {
                'pos': defaults,
                'neg': defaults.replace("%v", "-%v"),
                'zero': defaults
            }
            return self.settings

        return format
--------------------
Prompt: 
def setMeterPassword(self, new_pwd, pwd="00000000"):
        """ Serial Call to set meter password.  USE WITH CAUTION.

        Args:
            new_pwd (str): 8 digit numeric password to set
            pwd (str): Old 8 digit numeric password.

        Returns:
            bool: True on completion with ACK.
        """
----------
Output: 

        self.write(f"{self.cmd_set_meter_password} {pwd} {new_pwd}")
        return self.read_ack()

    def setMeterPassword(self, new_pwd, pwd="00000000"):
        """ Serial Call to set meter password.  USE WITH CAUTION.

        Args:
            new_pwd (str): 8 digit numeric password to set
            pwd (str): Old 8 digit numeric password.

        Returns:
           
----------
Solution: 

        result = False
        self.setContext("setMeterPassword")
        try:
            if len(new_pwd) != 8 or len(pwd) != 8:
                self.writeCmdMsg("Passwords must be exactly eight characters.")
                self.setContext("")
                return result

            if not self.request(False):
                self.writeCmdMsg("Pre command read failed: check serial line.")
            else:
                if not self.serialCmdPwdAuth(pwd):
                    self.writeCmdMsg("Password failure")
                else:
                    req_pwd = binascii.hexlify(new_pwd.zfill(8))
                    req_str = "015731023030323028" + req_pwd + "2903"
                    req_str += self.calc_crc16(req_str[2:].decode("hex"))
                    self.m_serial_port.write(req_str.decode("hex"))
                    if self.m_serial_port.getResponse(self.getContext()).encode("hex") == "06":
                        self.writeCmdMsg("Success(setMeterPassword): 06 returned.")
                        result = True
            self.serialPostEnd()
        except:
            ekm_log(traceback.format_exc(sys.exc_info()))

        self.setContext("")
        return result
--------------------
Prompt: 
def getMessage(self):
        """
        Return the message for this LogRecord.

        Return the message for this LogRecord after merging any user-supplied
        arguments with the message.
        """
----------
Output: 

        msg = self.msg
        if self.args:
            msg = msg % self.args
        return msg

    def getMessageWithTime(self):
        """
        Return the message for this LogRecord, with the time added.

        Return the message for this LogRecord after merging any user-supplied
        arguments with the message.  If the time (or time.tzname) is not
        available in this LogRecord, use the current time or local time zone.
        """
        msg = self.getMessage()
        if self.ex
----------
Solution: 

        msg = self.msg
        if self.args:
            msg = msg.format(*self.args)
        return maybe_encode(msg)
--------------------
Prompt: 
def add_observer(self, callback, ntype, sender):
        """Add an observer callback to this notification center.

        The given callback will be called upon posting of notifications of
        the given type/sender and will receive any additional arguments passed
        to post_notification.

        Parameters
        ----------
        callback : callable
            The callable that will be called by :meth:`post_notification`
            as ``callback(ntype, sender, *args, **kwargs)
        ntype : hashable
            The notification type. If None, all notifications from sender
            will be posted.
        sender : hashable
            The notification sender. If None, all notifications of ntype
            will be posted.
        """
----------
Output: 
the notification center is notified of
            a notification of this type, the callback will be called.
        sender : hashable
            The notification sender. If the notification center is notified of
            a notification from this sender, the callback will be called.

        """
        if ntype not in self._observers:
            self._observers[ntype] = {}
        if sender not in self._observers[ntype]:
            self._observers[ntype][sender] = []
        self._observers[ntype][sender].
----------
Solution: 

        assert(callback != None)
        self.registered_types.add(ntype)
        self.registered_senders.add(sender)
        self.observers.setdefault((ntype,sender), set()).add(callback)
--------------------
Prompt: 
def cli_run():
    """docstring for argparse"""
----------
Output: 

    parser = argparse.ArgumentParser(description='Runs the CLI')
    parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')
    parser.add_argument('-l', '--list', action='store_true', help='List all available commands')
    parser.add_argument('-c', '--command', help='Run a command')
    parser.add_argument('-p', '--param', help='Command parameter')
    parser.add_argument('-f', '--file
----------
Solution: 

    parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow')
    parser.add_argument('query', help="What's the problem ?", type=str, nargs='+')
    parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda')
    args = parser.parse_args()
    main(args)
--------------------
Prompt: 
def send_email(self, to, kind, **kwargs):
        """ Sends an e-mail to the given address.

        to: The address
        kind: the ID for an e-mail kind; it should point to a subdirectory of
            self.template_prefix containing subject.txt and message.html, which
            are django templates for the subject and HTML message respectively.

        context: a context for rendering the e-mail.

        """
----------
Output: 

        subject = render_to_string(
            '%s/%s/subject.txt' % (self.template_prefix, kind),
            kwargs)
        message = render_to_string(
            '%s/%s/message.html' % (self.template_prefix, kind),
            kwargs)
        send_mail(subject, message, self.from_address, [to])

    def send_password_reset_email(self, user, **kwargs):
        """ Sends a password reset e-mail to the given user.


----------
Solution: 


        return __send_email__(self.template_prefix, to, kind, **kwargs)
--------------------
Prompt: 
def list_to_str(lst: list, content: str, indent: int=1):
    """recurs into list for string computing """
----------
Output: 

    if lst:
        return content + "\n" + " " * indent + str(lst[0]) + "\n" + list_to_str(lst[1:], content, indent)
    else:
        return content


def list_to_str_no_indent(lst: list, content: str):
    """recurs into list for string computing """
    if lst:
        return content + "\n" + str(lst[0]) + "\n" + list_to_str_no_indent(lst[1:], content)
----------
Solution: 

    for i in lst:
        if isinstance(i, indentable):
            content = i.to_str(content, indent)
        elif isinstance(i, list):
            content = list_to_str(i, content, indent)
        elif isinstance(i, str):
            content = catend(content, i, indent)
    return content
--------------------
Prompt: 
def add_btn_cancel(self,label_cancel):
        """
        Adds a cancel button to let the user cancel whatever choice they were given.
        
        This widget can be triggered by setting the label ``label_cancel`` to a string.
        
        This widget will be positioned slightly below the main label and to the right
        of the confirm button.
        """
----------
Output: 

        self.label_cancel = label_cancel
        self.btn_cancel = tk.Button(self,text=label_cancel,command=self.cancel)
        self.btn_cancel.pack(side=tk.RIGHT,anchor=tk.NW)
        self.btn_cancel.bind("<Return>",self.cancel)
        self.btn_cancel.bind("<Escape>",self.cancel)
        self.btn_cancel.bind("<FocusOut>",self.cancel)
        self.btn_cancel.bind("<Button-1>",self.cancel
----------
Solution: 

        # Cancel Button
        self.wbtn_cancel = button.Button("btn_cancel",self,self.window,self.peng,
                        pos=lambda sw,sh, bw,bh: (sw/2+4,sh/2-bh/2-bh*2),
                        size=[0,0],
                        label=label_cancel,
                        borderstyle=self.borderstyle
                        )
        self.wbtn_cancel.size = lambda sw,sh: (self.wbtn_cancel._label.font_size*8,self.wbtn_cancel._label.font_size*2)
        self.addWidget(self.wbtn_cancel)
        
        def f():
            self.doAction("cancel")
            self.exitDialog()
        self.wbtn_cancel.addAction("click",f)
--------------------
Prompt: 
def _link_field_to_dict(field):
        """ Utility for ripping apart github's Link header field.
        It's kind of ugly.
        """
----------
Output: 

        parts = field.split(';')
        url = parts[0].strip()
        params = {}
        for part in parts[1:]:
            param = part.split('=')
            params[param[0].strip()] = param[1].strip().strip('"')
        return url, params

    def _parse_link_header(header):
        """ Parse a link header into a dictionary of lists.
        """
        links = {}
        for field in header.split(','):
            url, params = _link_field
----------
Solution: 


        if not field:
            return dict()

        return dict([
            (
                part.split('; ')[1][5:-1],
                part.split('; ')[0][1:-1],
            ) for part in field.split(', ')
        ])
--------------------
Prompt: 
def _get_protobuf_kind(kind):
    """Converts py_zipkin's Kind to Protobuf's Kind.

    :param kind: py_zipkin's Kind.
    :type kind: py_zipkin.Kind
    :return: correcponding protobuf's kind value.
    :rtype: zipkin_pb2.Span.Kind
    """
----------
Output: 

    if kind == Kind.CLIENT:
        return zipkin_pb2.Span.CLIENT
    elif kind == Kind.SERVER:
        return zipkin_pb2.Span.SERVER
    elif kind == Kind.PRODUCER:
        return zipkin_pb2.Span.PRODUCER
    elif kind == Kind.CONSUMER:
        return zipkin_pb2.Span.CONSUMER
    else:
        raise ValueError('Unknown kind: {}'.format(kind))


def _get_protobuf_annotation_value
----------
Solution: 

    if kind == Kind.CLIENT:
        return zipkin_pb2.Span.CLIENT
    elif kind == Kind.SERVER:
        return zipkin_pb2.Span.SERVER
    elif kind == Kind.PRODUCER:
        return zipkin_pb2.Span.PRODUCER
    elif kind == Kind.CONSUMER:
        return zipkin_pb2.Span.CONSUMER
    return None
--------------------
Prompt: 
def _assert_recur_is_tail(node: Node) -> None:  # pylint: disable=too-many-branches
    """Assert that `recur` forms only appear in the tail position of this
    or child AST nodes.

    `recur` forms may only appear in `do` nodes (both literal and synthetic
    `do` nodes) and in either the :then or :else expression of an `if` node."""
----------
Output: 

    if isinstance(node, DoNode):
        assert node.is_synthetic or node.is_tail, (
            "recur form must be in tail position of synthetic do node"
        )
    elif isinstance(node, IfNode):
        assert (
            node.then_node is None or _assert_recur_is_tail(node.then_node)
        ), "recur form must be in tail position of synthetic do node"
        assert (
            node.else_node is None or _assert_recur_
----------
Solution: 

    if node.op == NodeOp.DO:
        assert isinstance(node, Do)
        for child in node.statements:
            _assert_no_recur(child)
        _assert_recur_is_tail(node.ret)
    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:
        assert isinstance(node, (Fn, FnMethod, Method))
        node.visit(_assert_recur_is_tail)
    elif node.op == NodeOp.IF:
        assert isinstance(node, If)
        _assert_no_recur(node.test)
        _assert_recur_is_tail(node.then)
        _assert_recur_is_tail(node.else_)
    elif node.op in {NodeOp.LET, NodeOp.LETFN}:
        assert isinstance(node, (Let, LetFn))
        for binding in node.bindings:
            assert binding.init is not None
            _assert_no_recur(binding.init)
        _assert_recur_is_tail(node.body)
    elif node.op == NodeOp.LOOP:
        assert isinstance(node, Loop)
        for binding in node.bindings:
            assert binding.init is not None
            _assert_no_recur(binding.init)
    elif node.op == NodeOp.RECUR:
        pass
    elif node.op == NodeOp.TRY:
        assert isinstance(node, Try)
        _assert_recur_is_tail(node.body)
        for catch in node.catches:
            _assert_recur_is_tail(catch)
        if node.finally_:
            _assert_no_recur(node.finally_)
    else:
        node.visit(_assert_no_recur)
--------------------
Prompt: 
def remove_namespaces(root):
    """Call this on an lxml.etree document to remove all namespaces"""
----------
Output: 

    for e in root.getiterator():
        e.tag = etree.QName(e).localname
        if e.text is None:
            e.text = ""
        if e.tail is None:
            e.tail = ""
        if e.attrib:
            for a in list(e.attrib):
                if a.startswith("{"):
                    del e.attrib[a]


def get_text(elem):
    """Get the text of an element, including tail text"""
    text =
----------
Solution: 

    for elem in root.getiterator():
        if not hasattr(elem.tag, 'find'):
            continue

        i = elem.tag.find('}')
        if i >= 0:
            elem.tag = elem.tag[i + 1:]

    objectify.deannotate(root, cleanup_namespaces=True)
--------------------
Prompt: 
def _block_tuple(iterator, dtypes, bsize=-1):
    """Pack rdd of tuples as tuples of arrays or scipy.sparse matrices."""
----------
Output: 

    if bsize == -1:
        bsize = len(iterator)
    if len(dtypes) == 1:
        return iterator.map(lambda x: tuple(x))
    else:
        return iterator.map(lambda x: tuple(
            _block_array(x, dtypes[i], bsize) for i in range(len(dtypes))))


def _block_array(iterator, dtype, bsize=-1):
    """Pack rdd of arrays as arrays or scipy.sparse matrices."""
    if b
----------
Solution: 

    i = 0
    blocked_tuple = None
    for tuple_i in iterator:
        if blocked_tuple is None:
            blocked_tuple = tuple([] for _ in range(len(tuple_i)))

        if (bsize > 0) and (i >= bsize):
            yield tuple(_pack_accumulated(x, dtype)
                        for x, dtype in zip(blocked_tuple, dtypes))
            blocked_tuple = tuple([] for _ in range(len(tuple_i)))
            i = 0

        for x_j, x in zip(tuple_i, blocked_tuple):
            x.append(x_j)
        i += 1
    if i > 0:
        yield tuple(_pack_accumulated(x, dtype)
                    for x, dtype in zip(blocked_tuple, dtypes))
--------------------
Prompt: 
def ddos_custom_policies(self):
        """Instance depends on the API version:

           * 2018-11-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_11_01.operations.DdosCustomPoliciesOperations>`
           * 2018-12-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_12_01.operations.DdosCustomPoliciesOperations>`
           * 2019-02-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`
        """
----------
Output: 
2_01.operations.DdosCustomPoliciesOperations>`
           * 2019-02-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`
           * 2019-04-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_04_01.oper
----------
Solution: 

        api_version = self._get_api_version('ddos_custom_policies')
        if api_version == '2018-11-01':
            from .v2018_11_01.operations import DdosCustomPoliciesOperations as OperationClass
        elif api_version == '2018-12-01':
            from .v2018_12_01.operations import DdosCustomPoliciesOperations as OperationClass
        elif api_version == '2019-02-01':
            from .v2019_02_01.operations import DdosCustomPoliciesOperations as OperationClass
        else:
            raise NotImplementedError("APIVersion {} is not available".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))
--------------------
Prompt: 
def update(self):
        """Update repository from its remote.

        Calling this method, the repository will be synchronized with
        the remote repository using 'fetch' command for 'heads' refs.
        Any commit stored in the local copy will be removed; refs
        will be overwritten.

        :raises RepositoryError: when an error occurs updating the
            repository
        """
----------
Output: 

        try:
            self.git.fetch(self.remote, self.remote_branch,
                           prune=True, update_heads=True)
        except GitCommandError as e:
            raise RepositoryError(e)

    def checkout(self, branch):
        """Checkout a branch.

        :param branch: branch name
        :type branch: str
        :raises RepositoryError: when an error occurs checking out the
            branch
        """
        try:
            self.git.checkout(branch)
       
----------
Solution: 

        cmd_update = ['git', 'fetch', 'origin', '+refs/heads/*:refs/heads/*', '--prune']
        self._exec(cmd_update, cwd=self.dirpath, env=self.gitenv)

        logger.debug("Git %s repository updated into %s",
                     self.uri, self.dirpath)
--------------------
Prompt: 
def setZeroResettableKWH(self, password="00000000"):
        """ Serial call to zero resettable kWh registers.

        Args:
            password (str): Optional password.

        Returns:
            bool: True on completion and ACK.
        """
----------
Output: 

        return self.setZeroResettableKWH(password)

    def setZeroResettableKWH(self, password="00000000"):
        """ Serial call to zero resettable kWh registers.

        Args:
            password (str): Optional password.

        Returns:
            bool: True on completion and ACK.
        """
        return self.setZeroResettableKWH(password)

    def setZeroResettableKWH(self, password="00000000
----------
Solution: 

        result = False
        self.setContext("setZeroResettableKWH")
        try:
            if not self.requestA():
                self.writeCmdMsg("Bad read CRC on setting")
            else:
                if not self.serialCmdPwdAuth(password):
                    self.writeCmdMsg("Password failure")
                else:
                    req_str = "0157310230304433282903"
                    req_str += self.calc_crc16(req_str[2:].decode("hex"))
                    self.m_serial_port.write(req_str.decode("hex"))
                    if self.m_serial_port.getResponse(self.getContext()).encode("hex") == "06":
                        self.writeCmdMsg("Success: 06 returned.")
                        result = True
            self.serialPostEnd()
        except:
            ekm_log(traceback.format_exc(sys.exc_info()))

        self.setContext("")
        return result
--------------------
Prompt: 
def add_heart_failure_handler(self, handler):
        """add a new handler for heart failure"""
----------
Output: 

        self._heart_failure_handlers.append(handler)

    def remove_heart_failure_handler(self, handler):
        """remove a handler for heart failure"""
        self._heart_failure_handlers.remove(handler)

    def add_heart_beat_handler(self, handler):
        """add a new handler for heart beat"""
        self._heart_beat_handlers.append(handler)

    def remove_heart_beat_handler(self, handler):
        """remove a handler
----------
Solution: 

        self.log.debug("heartbeat::new heart failure handler: %s", handler)
        self._failure_handlers.add(handler)
--------------------
Prompt: 
def like_hashtag(self, hashtag, amount=None):
    """ Likes last medias from hashtag """
----------
Output: 

    if amount is None:
        amount = self.max_likes_per_day
    if self.api.login_status != 1:
        self.api.login()
    hashtag = hashtag.strip("#")
    self.api.get_hashtag_info(hashtag)
    medias = self.api.get_hashtag_medias(hashtag)
    if len(medias) == 0:
        return False
    for media in medias:
        if amount == 0:
----------
Solution: 

    self.logger.info("Going to like media with hashtag #%s." % hashtag)
    medias = self.get_total_hashtag_medias(hashtag, amount)
    return self.like_medias(medias)
--------------------
Prompt: 
def __coord_fft_hz(n, sr=22050, **_kwargs):
    """Get the frequencies for FFT bins"""
----------
Output: 

    return np.linspace(0, sr / 2, n)


def __coord_fft_bin(n, sr=22050, **_kwargs):
    """Get the bin numbers for FFT frequencies"""
    return np.arange(n)


def __coord_fft_db(n, sr=22050, **_kwargs):
    """Get the dB values for FFT frequencies"""
    return 20 * np.log10(n)


def __
----------
Solution: 

    n_fft = 2 * (n - 1)
    # The following code centers the FFT bins at their frequencies
    # and clips to the non-negative frequency range [0, nyquist]
    basis = core.fft_frequencies(sr=sr, n_fft=n_fft)
    fmax = basis[-1]
    basis -= 0.5 * (basis[1] - basis[0])
    basis = np.append(np.maximum(0, basis), [fmax])
    return basis
--------------------
Prompt: 
def main(ctx, log_level, keeper_hostname, username, password):
    """ltd is a command-line client for LSST the Docs.

    Use ltd to upload new site builds, and to work with the LTD Keeper API.
    """
----------
Output: 

    import logging
    import sys

    from ltd.client import KeeperClient
    from ltd.client import KeeperClientError
    from ltd.client import KeeperClientTimeoutError
    from ltd.client import KeeperClientUnauthorizedError
    from ltd.client import KeeperClientUnknownError

    logging.basicConfig(
        level=log_level,
        format='%(asctime)s %(levelname)s %(message)s',
        datefmt='%Y-%m
----------
Solution: 

    ch = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s %(levelname)8s %(name)s | %(message)s')
    ch.setFormatter(formatter)

    logger = logging.getLogger('ltdconveyor')
    logger.addHandler(ch)
    logger.setLevel(log_level.upper())

    # Subcommands should use the click.pass_obj decorator to get this
    # ctx.obj object as the first argument.
    ctx.obj = {
        'keeper_hostname': keeper_hostname,
        'username': username,
        'password': password,
        'token': None
    }
--------------------
Prompt: 
def _fetch_from_archive(self, method, args):
        """Fetch data from the archive

        :param method: the name of the command to execute
        :param args: the arguments required by the command
        """
----------
Output: 

        # TODO: check if the archive is available
        # TODO: check if the archive is up to date
        # TODO: check if the archive is accessible
        # TODO: check if the archive is valid
        # TODO: check if the archive is not corrupted
        # TODO: check if the archive is not empty
        # TODO: check if the archive is not broken
        # TODO: check if the archive is not malicious
        # TODO: check if the archive is not malformed
        # TODO: check if the archive is not malicious
        # TODO: check
----------
Solution: 

        if not self.archive:
            raise ArchiveError(cause="Archive not provided")

        data = self.archive.retrieve(method, args, None)

        if isinstance(data, nntplib.NNTPTemporaryError):
            raise data

        return data
--------------------
Prompt: 
def attach_dynamic_tree_items(tree_alias, src_tree_items):
        """Attaches dynamic sitetrees items registered with `register_dynamic_trees()`
        to an initial (source) items list.

        :param str|unicode tree_alias:
        :param list src_tree_items:
        :rtype: list
        """
----------
Output: 

        if tree_alias not in _dynamic_trees:
            return src_tree_items

        for item in _dynamic_trees[tree_alias]:
            src_tree_items.append(item)

        return src_tree_items

    def attach_dynamic_tree_items_to_request(request, tree_alias, src_tree_items):
        """Attaches dynamic sitetrees items registered with `register_dynamic_trees()`
        to an initial (source) items list.

        :param HttpRequest request:
        :param
----------
Solution: 

        if not _DYNAMIC_TREES:
            return src_tree_items

        # This guarantees that a dynamic source stays intact,
        # no matter how dynamic sitetrees are attached.
        trees = deepcopy(_DYNAMIC_TREES)

        items = []
        if not src_tree_items:
            if _IDX_ORPHAN_TREES in trees and tree_alias in trees[_IDX_ORPHAN_TREES]:
                for tree in trees[_IDX_ORPHAN_TREES][tree_alias]:
                    items.extend(tree.dynamic_items)
        else:

            # TODO Seems to be underoptimized %)

            # Tree item attachment by alias.
            for static_item in list(src_tree_items):
                items.append(static_item)
                if not static_item.alias:
                    continue

                idx = _IDX_TPL % (tree_alias, static_item.alias)
                if idx not in trees:
                    continue

                for tree in trees[idx]:
                    tree.alias = tree_alias
                    for dyn_item in tree.dynamic_items:
                        if dyn_item.parent is None:
                            dyn_item.parent = static_item
                        # Unique IDs are required for the same trees attached
                        # to different parents.
                        dyn_item.id = generate_id_for(dyn_item)
                        items.append(dyn_item)

            # Tree root attachment.
            idx = _IDX_TPL % (tree_alias, None)
            if idx in _DYNAMIC_TREES:
                trees = deepcopy(_DYNAMIC_TREES)
                for tree in trees[idx]:
                    tree.alias = tree_alias
                    items.extend(tree.dynamic_items)

        return items
--------------------
Prompt: 
def snip_print(str,width = 75,print_full = 0,header = ''):
    """Print a string snipping the midsection to fit in width.

    print_full: mode control:
      - 0: only snip long strings
      - 1: send to page() directly.
      - 2: snip long strings and ask for full length viewing with page()
    Return 1 if snipping was necessary, 0 otherwise."""
----------
Output: 

    if len(str) <= width:
        if print_full:
            page(str)
        else:
            print(str)
        return 0
    else:
        if print_full:
            page(str)
        else:
            print(str[:width/2]+'...'+str[-width/2:])
        return 1

def snip_print_list(lst,width = 75,print_full = 0,header = ''):
    """Print a list of strings snipping the midsection to fit
----------
Solution: 


    if print_full == 1:
        page(header+str)
        return 0

    print header,
    if len(str) < width:
        print str
        snip = 0
    else:
        whalf = int((width -5)/2)
        print str[:whalf] + ' <...> ' + str[-whalf:]
        snip = 1
    if snip and print_full == 2:
        if raw_input(header+' Snipped. View (y/n)? [N]').lower() == 'y':
            page(str)
    return snip
--------------------
Prompt: 
def _choi_to_chi(data, input_dim, output_dim):
    """Transform Choi representation to the Chi representation."""
----------
Output: 

    if input_dim != output_dim:
        raise ValueError("Input and output dimensions must be equal.")
    if input_dim != 2:
        raise ValueError("Input and output dimensions must be 2.")

    data = np.array(data)
    if data.shape != (input_dim, input_dim, output_dim, output_dim):
        raise ValueError("Invalid shape of the Choi representation.")

    data = np.reshape(data, (input_dim, input_dim, output_dim, output_dim))
----------
Solution: 

    num_qubits = int(np.log2(input_dim))
    return _transform_to_pauli(data, num_qubits)
--------------------
Prompt: 
def rst_msg(self, text, opts={}):
        """Convert ReStructuredText and run through msg()"""
----------
Output: 

        return self.msg(text, opts, 'rst')

    def rst_msg_block(self, text, opts={}):
        """Convert ReStructuredText and run through msg_block()"""
        return self.msg_block(text, opts, 'rst')

    def rst_msg_block_list(self, text, opts={}):
        """Convert ReStructuredText and run through msg_block_list()"""
        return self.msg_block_list(text, opts, 'rst')

----------
Solution: 

        text = Mformat.rst_text(text,
                                'plain' == self.debugger.settings['highlight'],
                                self.debugger.settings['width'])
        return self.msg(text)
--------------------
Prompt: 
def measure_topology(fbasename=None, log=None, ml_version=ml_version):
    """Measures mesh topology

    Args:
        fbasename (str): input filename.
        log (str): filename to log output

    Returns:
        dict: dictionary with the following keys:
            vert_num (int): number of vertices
            edge_num (int): number of edges
            face_num (int): number of faces
            unref_vert_num (int): number or unreferenced vertices
            boundry_edge_num (int): number of boundary edges
            part_num (int): number of parts (components) in the mesh.
            manifold (bool): True if mesh is two-manifold, otherwise false.
            non_manifold_edge (int): number of non_manifold edges.
            non_manifold_vert (int): number of non-manifold verices
            genus (int or str): genus of the mesh, either a number or
                'undefined' if the mesh is non-manifold.
            holes (int or str): number of holes in the mesh, either a number
                or 'undefined' if the mesh is non-manifold.

    """
----------
Output: 
_vert_num (int): number of boundary vertices
            unref_edge_num (int): number of unreferenced edges
            unref_face_num (int): number of unreferenced faces
            unref_edge_num (int): number of unreferenced edges
            unref_face_num (int): number of unreferenced faces
            unref_vert_num (int): number of unreferenced vertices
            unref_edge_num (int): number of unreferenced edges
            unref_face_num (int): number of unrefer
----------
Solution: 

    ml_script1_file = 'TEMP3D_measure_topology.mlx'
    ml_script1 = mlx.FilterScript(file_in=fbasename, ml_version=ml_version)
    compute.measure_topology(ml_script1)
    ml_script1.save_to_file(ml_script1_file)
    ml_script1.run_script(log=log, script_file=ml_script1_file)
    topology = ml_script1.topology
    return topology
--------------------
Prompt: 
def import_submodule(mod, subname, fullname):
    """m = import_submodule(mod, subname, fullname)"""
----------
Output: 

    import importlib
    return importlib.import_module('.' + subname, mod.__name__)


def import_submodules(mod, subnames, fullname):
    """m = import_submodules(mod, subnames, fullname)"""
    import importlib
    for subname in subnames:
        importlib.import_module('.' + subname, mod.__name__)


def import_submodules_from_file(mod, subnames, fullname):
    """m = import_submodules_from_file
----------
Solution: 

    # Require:
    # if mod == None: subname == fullname
    # else: mod.__name__ + "." + subname == fullname

    global found_now
    if fullname in found_now and fullname in sys.modules:
        m = sys.modules[fullname]
    else:
        print 'Reloading', fullname
        found_now[fullname] = 1
        oldm = sys.modules.get(fullname, None)

        if mod is None:
            path = None
        elif hasattr(mod, '__path__'):
            path = mod.__path__
        else:
            return None

        try:
            # This appears to be necessary on Python 3, because imp.find_module()
            # tries to import standard libraries (like io) itself, and we don't
            # want them to be processed by our deep_import_hook.
            with replace_import_hook(original_import):
                fp, filename, stuff = imp.find_module(subname, path)
        except ImportError:
            return None

        try:
            m = imp.load_module(fullname, fp, filename, stuff)
        except:
            # load_module probably removed name from modules because of
            # the error.  Put back the original module object.
            if oldm:
                sys.modules[fullname] = oldm
            raise
        finally:
            if fp: fp.close()

        add_submodule(mod, m, fullname, subname)

    return m
--------------------
Prompt: 
def velocity_from_bundle(self, bundle):
        """[DEPRECATED] Return velocity, given the `coefficient_bundle()` return value."""
----------
Output: 

        warnings.warn(
            "velocity_from_bundle() is deprecated. Use velocity_from_bundle_coefficients() instead.",
            DeprecationWarning,
        )
        return self.velocity_from_bundle_coefficients(bundle)

    def velocity_from_bundle_coefficients(self, bundle):
        """Return velocity, given the `coefficient_bundle()` return value."""
        return np.dot(self.basis, bundle)

    def velocity_from_bundle_coefficients_derivative(
----------
Solution: 


        coefficients, days_per_set, T, twot1 = bundle
        coefficient_count = coefficients.shape[2]

        # Chebyshev derivative:

        dT = np.empty_like(T)
        dT[0] = 0.0
        dT[1] = 1.0
        dT[2] = twot1 + twot1
        for i in range(3, coefficient_count):
            dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]
        dT *= 2.0
        dT /= days_per_set

        return (dT.T * coefficients).sum(axis=2)
--------------------
Prompt: 
def update(self):
        """Update cameras and motion settings with latest from API."""
----------
Output: 

        self._update_cameras()
        self._update_motion_settings()

    def _update_cameras(self):
        """Update cameras with latest from API."""
        self.cameras = []
        for camera in self._api.cameras:
            if camera.id in self._api.camera_settings:
                self.cameras.append(
                    Camera(
                        camera,
                        self._api.camera_settings[camera.id],
                        self._api.camera_pres
----------
Solution: 

        cameras = self._api.camera_list()
        self._cameras_by_id = {v.camera_id: v for i, v in enumerate(cameras)}

        motion_settings = []
        for camera_id in self._cameras_by_id.keys():
            motion_setting = self._api.camera_event_motion_enum(camera_id)
            motion_settings.append(motion_setting)

        self._motion_settings_by_id = {
            v.camera_id: v for i, v in enumerate(motion_settings)}
--------------------
Prompt: 
def get_refs(self, location):
        """Return map of named refs (branches or tags) to commit hashes."""
----------
Output: 

        refs = {}
        for line in self.git("for-each-ref", "--format=%(refname:short) %(objectname)", "refs/heads", "refs/tags"):
            ref, commit = line.split()
            refs[ref] = commit
        return refs

    def get_tags(self):
        """Return map of tag names to commit hashes."""
        return self.get_refs("refs/tags")

    def get_branches(self):
        """Return map of branch names
----------
Solution: 

        output = call_subprocess([self.cmd, 'show-ref'],
                                 show_stdout=False, cwd=location)
        rv = {}
        for line in output.strip().splitlines():
            commit, ref = line.split(' ', 1)
            ref = ref.strip()
            ref_name = None
            if ref.startswith('refs/remotes/'):
                ref_name = ref[len('refs/remotes/'):]
            elif ref.startswith('refs/heads/'):
                ref_name = ref[len('refs/heads/'):]
            elif ref.startswith('refs/tags/'):
                ref_name = ref[len('refs/tags/'):]
            if ref_name is not None:
                rv[ref_name] = commit.strip()
        return rv
--------------------
Prompt: 
def main():
    """The main function. Instantiates a GameState object and then
    enters a REPL-like main loop, waiting for input, updating the state
    based on the input, then outputting the new state."""
----------
Output: 

    state = GameState()
    while True:
        state.print_state()
        input_ = input("> ")
        if input_ == "quit":
            break
        elif input_ == "help":
            print("Commands:")
            print("  help - print this message")
            print("  quit - quit the game")
            print("  move <direction> - move in the given direction")
            print("  look - look at the current location")
            print("  inventory - list the items you are carrying")
            print("
----------
Solution: 


    state = GameState()
    print(state)
    while state.running:
        input = get_single_char()

        state, should_advance = state.handle_input(input)
        if should_advance:
            state = state.advance_robots()
            state = state.check_game_end()

        print(state)

    print(state.message)
--------------------
Prompt: 
def delete_policy_name(self, name):
        """**Description**
            Delete the policy with the given name.

        **Arguments**
            - name: the name of the policy to delete

        **Success Return Value**
            The JSON object representing the now-deleted policy.

        **Example**
            `examples/delete_policy.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_policy.py>`_

        """
----------
Output: 

        return self.delete_policy(name=name)

    def delete_policy(self, name=None, id=None):
        """**Description**
            Delete the policy with the given name or id.

        **Arguments**
            - name: the name of the policy to delete
            - id: the id of the policy to delete

        **Success Return Value**
            The JSON object representing the now-deleted policy.

        **Example**
            `examples/delete_policy.py <https://github.com/draios/python-
----------
Solution: 

        res = requests.get(self.url + '/api/policies', headers=self.hdrs, verify=self.ssl_verify)
        if not self._checkResponse(res):
            return [False, self.lasterr]

        # Find the policy with the given name and delete it
        for policy in res.json()["policies"]:
            if policy["name"] == name:
                return self.delete_policy_id(policy["id"])

        return [False, "No policy with name {}".format(name)]
--------------------
Prompt: 
def share_vm_image(self, vm_image_name, permission):
        """
        Share an already replicated OS image. This operation is only for
        publishers. You have to be registered as image publisher with Windows
        Azure to be able to call this.

        vm_image_name:
            The name of the virtual machine image to share
        permission:
            The sharing permission: public, msdn, or private
        """
----------
Output: 

        self.vm_image_service.share_vm_image(vm_image_name, permission)

    def unshare_vm_image(self, vm_image_name):
        """
        Unshare an already replicated OS image. This operation is only for
        publishers. You have to be registered as image publisher with Windows
        Azure to be able to call this.

        vm_image_name:
            The name of the virtual machine image to unshare
        """
        self.vm_image_service.unshare_vm_image
----------
Solution: 

        _validate_not_none('vm_image_name', vm_image_name)
        _validate_not_none('permission', permission)

        path = self._get_sharing_path_using_vm_image_name(vm_image_name)
        query = '&permission=' + permission
        path = path + '?' + query.lstrip('&')

        return self._perform_put(
            path, None, as_async=True, x_ms_version='2015-04-01'
        )
--------------------
Prompt: 
def get_stats(self, username='', password='', organization='llnl', force=True):
        """
        Retrieves the traffic for the users of the given organization.
        Requires organization admin credentials token to access the data.
        """
----------
Output: 

        if not force and self.stats:
            return self.stats
        self.stats = {}
        if not username or not password:
            return self.stats
        try:
            response = requests.get(
                'https://api.github.com/orgs/{}/people'.format(organization),
                auth=(username, password),
                headers={'Accept': 'application/vnd.github.v3+json'})
            response.raise_for_status()
            for user in response.json():
                self.stats[user['
----------
Solution: 

        date = str(datetime.date.today())
        referrers_file_path =  ('../github_stats_output/referrers.csv')
        views_file_path =  ('../github_stats_output/views.csv')
        clones_file_path =  ('../github_stats_output/clones.csv')
        if force or not os.path.isfile(file_path):
            my_github.login(username, password)
            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1
            print 'Rate Limit: ' + str(calls_beginning)
            my_github.get_org(organization)
            my_github.get_traffic()
            views_row_count = my_github.check_data_redundancy(file_path=views_file_path,
                dict_to_check=self.views)
            clones_row_count = my_github.check_data_redundancy(file_path=clones_file_path,
                dict_to_check=self.clones)
            my_github.write_to_file(referrers_file_path=referrers_file_path,
                views_file_path=views_file_path,
                clones_file_path=clones_file_path,
                views_row_count=views_row_count,
                clones_row_count=clones_row_count)
            my_github.write_json(dict_to_write=self.referrers_json,
                path_ending_type='traffic_popular_referrers')
            my_github.write_json(dict_to_write=self.views_json,
                path_ending_type='traffic_views')
            my_github.write_json(dict_to_write=self.clones_json,
                path_ending_type='traffic_clones')
            my_github.write_json(dict_to_write=self.releases_json,
                path_ending_type='releases')
            calls_remaining = self.logged_in_gh.ratelimit_remaining
            calls_used = calls_beginning - calls_remaining
            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\nUsed '
                + str(calls_used) + ' API calls.')
--------------------
Prompt: 
def save_setting(self, setting_name, value):
        """Saves the setting value into the database."""
----------
Output: 

        self.settings[setting_name] = value
        self.save_settings()

    def save_settings(self):
        """Saves all the settings into the database."""
        self.settings.save()

    def get_setting(self, setting_name):
        """Returns the setting value from the database."""
        return self.settings[setting_name]

    def get_settings(self):
        """Returns all the settings from the database."""
        return self.settings

    def get_setting_names(
----------
Solution: 

        setting = self.get_setting(setting_name)
        if setting is None:
            setting = models.DashboardWidgetSettings.objects.create(
                widget_name=self.get_name(),
                setting_name=setting_name,
                value=value)
        setting.value = value
        setting.save()
        return setting
--------------------
Prompt: 
def get_extended_key_usage_from_certificate(certificate):
    """
    Given an X.509 certificate, extract and return the extendedKeyUsage
    extension.
    """
----------
Output: 

    ext = certificate.extensions.get_extension_for_oid(
        x509.oid.ExtensionOID.EXTENDED_KEY_USAGE
    )
    if ext is None:
        return None
    return ext.value


def get_key_usage_from_certificate(certificate):
    """
    Given an X.509 certificate, extract and return the keyUsage extension.
    """
    ext = certificate.extensions.get_extension_for_oid(
        x509.oid.ExtensionO
----------
Solution: 

    try:
        return certificate.extensions.get_extension_for_oid(
            x509.oid.ExtensionOID.EXTENDED_KEY_USAGE
        ).value
    except x509.ExtensionNotFound:
        return None
--------------------
Prompt: 
def join_html_attrs(attrs):
    """Joins the map structure into HTML attributes.

    The return value is a 2-tuple ``(template, ordered_values)``. It should be
    passed into :class:`markupsafe.Markup` to prevent XSS attacked.

    e.g.::

        >>> join_html_attrs({'href': '/', 'data-active': 'true'})
        ('data-active="{0}" href="{1}"', ['true', '/'])
    """
----------
Output: 

    template = ''
    values = []
    for key, value in attrs.items():
        if value is None:
            continue
        template += ' {0}="{1}"'.format(key, value)
        values.append(value)
    return template, values


def join_html_classes(classes):
    """Joins the list structure into HTML classes.

    The return value is a 2-tuple ``(template, ordered_values)``. It should be
    passed into :class:`markupsafe.Markup` to
----------
Solution: 

    attrs = collections.OrderedDict(freeze_dict(attrs or {}))
    template = ' '.join('%s="{%d}"' % (k, i) for i, k in enumerate(attrs))
    return template, list(attrs.values())
--------------------
Prompt: 
def restore_sys_module_state(self):
        """Restore the state of the sys module."""
----------
Output: 

        sys.modules['sys'] = self.sys_module

    def restore_builtins_module_state(self):
        """Restore the state of the builtins module."""
        sys.modules['builtins'] = self.builtins_module

    def restore_os_module_state(self):
        """Restore the state of the os module."""
        sys.modules['os'] = self.os_module

    def restore_os_path_module_state(self):
        """Restore the state of the os.path
----------
Solution: 

        try:
            for k, v in self._orig_sys_module_state.iteritems():
                setattr(sys, k, v)
        except AttributeError:
            pass
        # Reset what what done in self.init_sys_modules
        if self._orig_sys_modules_main_mod is not None:
            sys.modules[self._orig_sys_modules_main_name] = self._orig_sys_modules_main_mod
--------------------
Prompt: 
def dequantize_flow(dx, dy, max_val=0.02, denorm=True):
    """Recover from quantized flow.

    Args:
        dx (ndarray): Quantized dx.
        dy (ndarray): Quantized dy.
        max_val (float): Maximum value used when quantizing.
        denorm (bool): Whether to multiply flow values with width/height.

    Returns:
        ndarray: Dequantized flow.
    """
----------
Output: 

    dx = dx.astype(np.float32)
    dy = dy.astype(np.float32)
    dx = dx * max_val
    dy = dy * max_val
    if denorm:
        dx = dx * dx.shape[1]
        dy = dy * dy.shape[0]
    return dx, dy


def quantize_flow(flow, max_val=0.02, denorm=True):
    """Quantize flow.

    Args:
        flow (ndarray
----------
Solution: 

    assert dx.shape == dy.shape
    assert dx.ndim == 2 or (dx.ndim == 3 and dx.shape[-1] == 1)

    dx, dy = [dequantize(d, -max_val, max_val, 255) for d in [dx, dy]]

    if denorm:
        dx *= dx.shape[1]
        dy *= dx.shape[0]
    flow = np.dstack((dx, dy))
    return flow
--------------------
Prompt: 
def addCases(self, tupesValStmnts):
        """
        Add multiple case statements from iterable of tuleles
        (caseVal, statements)
        """
----------
Output: 

        for caseVal, statements in tupesValStmnts:
            self.addCase(caseVal, statements)

    def addCase(self, caseVal, statements):
        """
        Add a case statement
        """
        if not isinstance(statements, list):
            statements = [statements]
        self.cases.append((caseVal, statements))

    def getCase(self, caseVal):
        """
        Get a case statement
        """
        for caseVal_, statements in self.cases:
            if case
----------
Solution: 

        s = self
        for val, statements in tupesValStmnts:
            s = s.Case(val, statements)
        return s
--------------------
Prompt: 
def create_endpoint(port=None, service_name=None, host=None, use_defaults=True):
    """Creates a new Endpoint object.

    :param port: TCP/UDP port. Defaults to 0.
    :type port: int
    :param service_name: service name as a str. Defaults to 'unknown'.
    :type service_name: str
    :param host: ipv4 or ipv6 address of the host. Defaults to the
    current host ip.
    :type host: str
    :param use_defaults: whether to use defaults.
    :type use_defaults: bool
    :returns: zipkin Endpoint object
    """
----------
Output: 

    :param use_defaults: if True, uses the default values for the
    parameters. Defaults to True.
    :type use_defaults: bool
    :return: Endpoint object
    :rtype: Endpoint
    """
    if use_defaults:
        if port is None:
            port = 0
        if service_name is None:
            service_name = 'unknown'
        if host is None:
            host = get_host_ip()

    return Endpoint(port, service_name, host)

----------
Solution: 

    if use_defaults:
        if port is None:
            port = 0
        if service_name is None:
            service_name = 'unknown'
        if host is None:
            try:
                host = socket.gethostbyname(socket.gethostname())
            except socket.gaierror:
                host = '127.0.0.1'

    ipv4 = None
    ipv6 = None

    if host:
        # Check ipv4 or ipv6.
        try:
            socket.inet_pton(socket.AF_INET, host)
            ipv4 = host
        except socket.error:
            # If it's not an ipv4 address, maybe it's ipv6.
            try:
                socket.inet_pton(socket.AF_INET6, host)
                ipv6 = host
            except socket.error:
                # If it's neither ipv4 or ipv6, leave both ip addresses unset.
                pass

    return Endpoint(
        ipv4=ipv4,
        ipv6=ipv6,
        port=port,
        service_name=service_name,
    )
--------------------
Prompt: 
def main():
    """
    Parse argv for options and arguments, and start schema generation.
    """
----------
Output: 

    import argparse
    import sys

    parser = argparse.ArgumentParser(description='Generate a schema from a set of files.')
    parser.add_argument('-o', '--output', help='Output file.', default='schema.json')
    parser.add_argument('-d', '--directory', help='Directory to search for files.', default='.')
    parser.add_argument('-e', '--extension', help='File extension to search for.', default='.py')
    parser.add_argument('-v',
----------
Solution: 

    parser = optparse.OptionParser(usage="%prog [options] <model_path> [another_model_path...]",
                                   version=xtuml.version.complete_string,
                                   formatter=optparse.TitledHelpFormatter())
                                   
    parser.set_description(__doc__.strip())
    
    parser.add_option("-c", "--component", dest="component", metavar="NAME",
                      help="export sql schema for the component named NAME",
                      action="store", default=None)
    
    parser.add_option("-d", "--derived-attributes", dest="derived",
                      help="include derived attributes in the schema",
                      action="store_true", default=False)
    
    parser.add_option("-o", "--output", dest='output', metavar="PATH",
                      help="save sql schema to PATH (required)",
                      action="store", default=None)
    
    parser.add_option("-v", "--verbosity", dest='verbosity', action="count", 
                      help="increase debug logging level", default=2)

    
    (opts, args) = parser.parse_args()
    if len(args) == 0 or opts.output is None:
        parser.print_help()
        sys.exit(1)

    levels = {
              0: logging.ERROR,
              1: logging.WARNING,
              2: logging.INFO,
              3: logging.DEBUG,
    }
    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))

    loader = ooaofooa.Loader()
    for filename in args:
        loader.filename_input(filename)

    c = loader.build_component(opts.component, opts.derived)
    xtuml.persist_database(c, opts.output)
--------------------
Prompt: 
def disconnect(self, mol):
        """Break covalent bonds between metals and organic atoms under certain conditions.

        The algorithm works as follows:

        - Disconnect N, O, F from any metal.
        - Disconnect other non-metals from transition metals + Al (but not Hg, Ga, Ge, In, Sn, As, Tl, Pb, Bi, Po).
        - For every bond broken, adjust the charges of the begin and end atoms accordingly.

        :param mol: The input molecule.
        :type mol: rdkit.Chem.rdchem.Mol
        :return: The molecule with metals disconnected.
        :rtype: rdkit.Chem.rdchem.Mol
        """
----------
Output: 

        :type mol: :class:`~chempy.molecule.Molecule`
        :return: The disconnected molecule.
        :rtype: :class:`~chempy.molecule.Molecule`
        """

        # Disconnect N, O, F from any metal
        for atom in mol.atoms:
            if atom.element.number == 7 or atom.element.number == 8 or atom.element.number == 9:
                if atom.bonds:
                    for bond
----------
Solution: 

        log.debug('Running MetalDisconnector')
        # Remove bonds that match SMARTS
        for smarts in [self._metal_nof, self._metal_non]:
            pairs = mol.GetSubstructMatches(smarts)
            rwmol = Chem.RWMol(mol)
            orders = []
            for i, j in pairs:
                # TODO: Could get the valence contributions of the bond instead of GetBondTypeAsDouble?
                orders.append(int(mol.GetBondBetweenAtoms(i, j).GetBondTypeAsDouble()))
                rwmol.RemoveBond(i, j)
            # Adjust neighbouring charges accordingly
            mol = rwmol.GetMol()
            for n, (i, j) in enumerate(pairs):
                chg = orders[n]
                atom1 = mol.GetAtomWithIdx(i)
                atom1.SetFormalCharge(atom1.GetFormalCharge() + chg)
                atom2 = mol.GetAtomWithIdx(j)
                atom2.SetFormalCharge(atom2.GetFormalCharge() - chg)
                log.info('Removed covalent bond between %s and %s', atom1.GetSymbol(), atom2.GetSymbol())
        Chem.SanitizeMol(mol)
        return mol
--------------------
Prompt: 
def accept_alert(self, text=None, wait=None):
        """
        Execute the wrapped code, accepting an alert.

        Args:
            text (str | RegexObject, optional): Text to match against the text in the modal.
            wait (int | float, optional): Maximum time to wait for the modal to appear after
                executing the wrapped code.

        Raises:
            ModalNotFound: If a modal dialog hasn't been found.
        """
----------
Output: 

        self.execute_code(wait=wait)
        self.accept_modal(text=text)

    def dismiss_alert(self, text=None, wait=None):
        """
        Execute the wrapped code, dismissing an alert.

        Args:
            text (str | RegexObject, optional): Text to match against the text in the modal.
            wait (int | float, optional): Maximum time to wait for the modal to appear after
                executing the wrapped code.

        Raises:
            ModalNotFound: If a
----------
Solution: 


        wait = wait or capybara.default_max_wait_time
        with self.driver.accept_modal("alert", text=text, wait=wait):
            yield
--------------------
Prompt: 
def build_type(s_dt):
    """
    Build a partial xsd tree out of a S_DT and its sub types S_CDT, S_EDT, S_SDT and S_UDT.
    """
----------
Output: 

    if s_dt.s_cdt:
        s_cdt = s_dt.s_cdt
        s_cdt.s_dt = s_dt
        s_cdt.s_dt_id = s_dt.s_dt_id
        s_cdt.s_dt_name = s_dt.s_dt_name
        s_cdt.s_dt_type = s_dt.s_dt_type
        s_cdt.s_dt_version = s_dt.s_dt_
----------
Solution: 

    s_cdt = nav_one(s_dt).S_CDT[17]()
    if s_cdt:
        return build_core_type(s_cdt)
    
    s_edt = nav_one(s_dt).S_EDT[17]()
    if s_edt:
        return build_enum_type(s_edt)
    
    s_udt = nav_one(s_dt).S_UDT[17]()
    if s_udt:
        return build_user_type(s_udt)
--------------------
Prompt: 
def _get_obj_count_difference(objs1, objs2):
    """Returns count difference in two collections of Python objects."""
----------
Output: 

    return len(objs1) - len(objs2)


def _get_obj_count_difference_str(objs1, objs2):
    """Returns count difference in two collections of Python objects."""
    return "{} objects in the first collection, {} objects in the second collection".format(
        len(objs1), len(objs2)
    )


def _get_obj_count_difference_str_with_diff(objs1, objs2):
    """Returns count difference in
----------
Solution: 

    clean_obj_list1 = _process_in_memory_objects(objs1)
    clean_obj_list2 = _process_in_memory_objects(objs2)
    obj_count_1 = _get_object_count_by_type(clean_obj_list1)
    obj_count_2 = _get_object_count_by_type(clean_obj_list2)
    return obj_count_1 - obj_count_2
--------------------
Prompt: 
def api_options(self):
        """
        current spelling settings
        :return: api options as number
        """
----------
Output: 

        return self._api_options

    @api_options.setter
    def api_options(self, value):
        """
        current spelling settings
        :param value: api options as number
        """
        self._api_options = value

    @property
    def api_options_string(self):
        """
        current spelling settings
        :return: api options as string
        """
        return self._api_options_string

    @api_options_string.setter
    def api_options_string(
----------
Solution: 

        options = 0
        if self._ignore_uppercase:
            options |= 1
        if self._ignore_digits:
            options |= 2
        if self._ignore_urls:
            options |= 4
        if self._find_repeat_words:
            options |= 8
        if self._ignore_latin:
            options |= 16
        if self._flag_latin:
            options |= 128
        if self._by_words:
            options |= 256
        if self._ignore_capitalization:
            options |= 512
        if self._ignore_roman_numerals:
            options |= 2048
        return options
--------------------
Prompt: 
def get_all_intervals(self):
        """Returns the true list of intervals including the empty intervals."""
----------
Output: 

        intervals = []
        for i in range(self.size):
            intervals.append(self.get_interval(i))
        return intervals

    def get_interval(self, index):
        """Returns the interval at the given index."""
        if index < 0 or index >= self.size:
            raise IndexError("Index out of range.")
        return self.intervals[index]

    def get_interval_at_position(self, position):
        """Returns the interval at the given position."""
        if position <
----------
Solution: 

        ints = sorted(self.get_intervals(True))
        if self.tier_type == 'IntervalTier':
            if not ints:
                ints.append((self.xmin, self.xmax, ''))
            else:
                if ints[0][0] > self.xmin:
                    ints.insert(0, (self.xmin, ints[0][0], ''))
                if ints[-1][1] < self.xmax:
                    ints.append((ints[-1][1], self.xmax, ''))
                p = ints[-1]
                for index, i in reversed(list(enumerate(ints[:-1], 1))):
                    if p[0] - i[1] != 0:
                        ints.insert(index, (i[1], p[0], ''))
                    p = i
        return ints
--------------------
Prompt: 
def info_update(self, obj_id, data):
		"""Update metadata with of a specified object.
			See http://msdn.microsoft.com/en-us/library/live/hh243648.aspx
				for the list of RW keys for each object type."""
----------
Output: 

		return self.request('PUT', '/%s/%s' % (self.object_type, obj_id), data)

	def info_delete(self, obj_id):
		"""Delete metadata of a specified object.
			See http://msdn.microsoft.com/en-us/library/live/hh243648.aspx
				for the list of RW keys for each object type."""
		return self.request('DELETE', '/%s/%s' % (self.object_type
----------
Solution: 

		return self(obj_id, method='put', data=data, auth_header=True)
--------------------
Prompt: 
def hide_error_message(self):
        """
        This function hides the error message when all values are correct.
        """
----------
Output: 

        self.error_message.setVisible(False)

    def show_error_message(self):
        """
        This function shows the error message when all values are incorrect.
        """
        self.error_message.setVisible(True)

    def clear_fields(self):
        """
        This function clears all the fields.
        """
        self.name_field.clear()
        self.email_field.clear()
        self.password_field.clear()
        self.confirm_password_field.clear()


----------
Solution: 

        self.ui.error_label.setScaledContents(False)  # Warning image hiden.
        self.ui.error_text_label.hide()
--------------------
Prompt: 
def conditional_entropy(X, Y, base=2):
    """Calculates the conditional entropy, H(X|Y), in the given base

    Parameters
    ----------
    X: array-like (# samples)
        An array of values for which to compute the conditional entropy
    Y: array-like (# samples)
        An array of values for which to compute the conditional entropy
    base: integer (default: 2)
        The base in which to calculate conditional entropy

    Returns
    ----------
    conditional_entropy: float
        The conditional entropy calculated according to the equation H(X|Y) = H(X,Y) - H(Y)

    """
----------
Output: 
: float
        The conditional entropy of X given Y
    """
    # Calculate the joint entropy
    joint_entropy = entropy(X, Y, base=base)

    # Calculate the entropy of Y
    entropy_y = entropy(Y, base=base)

    # Calculate the conditional entropy
    conditional_entropy = joint_entropy - entropy_y

    return conditional_entropy


def mutual_information(X, Y, base=2):
    """Calculates the mutual information, I(X;Y),
----------
Solution: 

    return joint_entropy(X, Y, base=base) - entropy(Y, base=base)
--------------------
Prompt: 
def update_policy(self, id, policy):
        """ Create policy.

            https://www.nomadproject.io/api/acl-policies.html

            arguments:
                - name
                - policy
            returns: request.Response

            raises:
              - nomad.api.exceptions.BaseNomadException
              - nomad.api.exceptions.URLNotFoundNomadException
        """
----------
Output: 

        params = {
            'policy': policy,
        }
        return self._put(f"{self.endpoint}/{id}", params=params)

    def delete_policy(self, id):
        """ Delete policy.

            https://www.nomadproject.io/api/acl-policies.html

            arguments:
                - id
            returns: request.Response

            raises:
              - nomad.api.exceptions.BaseNomadException
              - nomad.api.exceptions.URLNotFound
----------
Solution: 

        return self.request("policy", id, json=policy, method="post")
--------------------
Prompt: 
def mark(self, value=1):
        """Record an event with the derive.

        :param value: counter value to record
        """
----------
Output: 

        self._check_state()
        self._client.record_gauge(self._name, value, tags=self._tags)

    def set_function(self, function):
        """Set a function to be called when the gauge is marked.

        :param function: function to be called when the gauge is marked
        """
        self._check_state()
        self._function = function

    def _check_state(self):
        """Check if the gauge is in a valid state.

        :raises: :class:`Runtime
----------
Solution: 

        last = self.last.get_and_set(value)
        if last <= value:
            value = value - last
        super(Derive, self).mark(value)
--------------------
Prompt: 
def __fetch_pull_requests(self, from_date, to_date):
        """Fetch the pull requests"""
----------
Output: 

        pull_requests = []
        for pull_request in self.client.pull_requests.list(
                state='all',
                sort='created',
                direction='asc',
                created_after=from_date,
                created_before=to_date):
            pull_requests.append(pull_request)
        return pull_requests

    def __fetch_commits(self, from_date, to_date):
        """Fetch the commits"""
        commits = []
        for commit in self.client.commits.list(

----------
Solution: 


        raw_pulls = self.client.pulls(from_date=from_date)
        for raw_pull in raw_pulls:
            pull = json.loads(raw_pull)

            if str_to_datetime(pull['updated_at']) > to_date:
                return

            self.__init_extra_pull_fields(pull)
            for field in TARGET_PULL_FIELDS:

                if not pull[field]:
                    continue

                if field == 'user':
                    pull[field + '_data'] = self.__get_user(pull[field]['login'])
                elif field == 'merged_by':
                    pull[field + '_data'] = self.__get_user(pull[field]['login'])
                elif field == 'review_comments':
                    pull[field + '_data'] = self.__get_pull_review_comments(pull['number'])
                elif field == 'requested_reviewers':
                    pull[field + '_data'] = self.__get_pull_requested_reviewers(pull['number'])
                elif field == 'commits':
                    pull[field + '_data'] = self.__get_pull_commits(pull['number'])

            yield pull
--------------------
Prompt: 
def connect(src, *destinations, exclude: set=None, fit=False):
    """
    Connect src (signals/interfaces/values) to all destinations

    :param exclude: interfaces on any level on src or destinations
        which should be excluded from connection process
    :param fit: auto fit source width to destination width
    """
----------
Output: 

    if exclude is None:
        exclude = set()

    if isinstance(src, Signal):
        src = [src]

    for destination in destinations:
        if isinstance(destination, Signal):
            destination = [destination]

        for dest in destination:
            if dest in exclude:
                continue

            if isinstance(dest, Signal):
                if fit:
                    src[0].fit_to(dest)
                src[0].connect(dest)
            elif isinstance(dest, Interface):

----------
Solution: 

    assignemnts = []

    if isinstance(src, HObjList):
        for dst in destinations:
            assert len(src) == len(dst), (src, dst)
        _destinations = [iter(d) for d in destinations]
        for _src in src:
            dsts = [next(d) for d in _destinations]
            assignemnts.append(connect(_src, *dsts, exclude=exclude, fit=fit))
    else:
        for dst in destinations:
            assignemnts.append(_connect(src, dst, exclude, fit))

    return assignemnts
--------------------
Prompt: 
def addLogicalInterfaceToDeviceType(self, typeId, logicalInterfaceId):
        """
        Adds a logical interface to a device type.
        Parameters:
            - typeId (string) - the device type
            - logicalInterfaceId (string) - the id returned by the platform on creation of the logical interface
            - description (string) - optional (not used)
        Throws APIException on failure.
        """
----------
Output: 

        url = self.url + "/device/type/" + typeId + "/logicalInterface/" + logicalInterfaceId
        self.logger.debug("url: %s", url)
        self.logger.debug("payload: %s", logicalInterfaceId)
        r = requests.put(url, data=logicalInterfaceId, headers=self.header)
        self.logger.debug("r.text: %s", r.text)
        if r.status_code == 200:
            return True
        else:
            raise APIException("
----------
Solution: 

        req = ApiClient.allDeviceTypeLogicalInterfacesUrl % (self.host, "/draft", typeId)
        body = {"id" : logicalInterfaceId}
#       body = {"name" : "required but not used!!!", "id" : logicalInterfaceId, "schemaId" : schemaId}
#       if description:
#           body["description"] = description
        resp = requests.post(req, auth=self.credentials, headers={"Content-Type":"application/json"}, data=json.dumps(body),
                        verify=self.verify)
        if resp.status_code == 201:
            self.logger.debug("Logical interface added to a device type")
        else:
            raise ibmiotf.APIException(resp.status_code, "HTTP error adding logical interface to a device type", resp)
        return resp.json()
--------------------
Prompt: 
def repository(self, owner, repository):
        """Fetch information about a repository."""
----------
Output: 

        return self.get(
            '/repos/{owner}/{repository}'.format(
                owner=owner, repository=repository
            )
        )

    def repository_branches(self, owner, repository):
        """Fetch branches for a repository."""
        return self.get(
            '/repos/{owner}/{repository}/branches'.format(
                owner=owner, repository=repository
            )
        )

    def repository_branch(self, owner, repository, branch):
        """Fetch a branch for a repository."""
       
----------
Solution: 


        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)

        logger.debug("DockerHub client requests: %s", url)

        response = self.fetch(url)

        return response.text
--------------------
Prompt: 
def target_address(self):
        """Return the authorative target of the link."""
----------
Output: 

        return self.target

    @property
    def target_type(self):
        """Return the type of the target."""
        return self.target.type

    @property
    def target_name(self):
        """Return the name of the target."""
        return self.target.name

    @property
    def target_address(self):
        """Return the authorative target of the link."""
        return self.target.address

    @property
    def target_address_type(self):
        """Return the
----------
Solution: 

        # If link is a receiver, target is determined by the local
        # value, else use the remote.
        if self._pn_link.is_receiver:
            return self._pn_link.target.address
        else:
            return self._pn_link.remote_target.address
--------------------
Prompt: 
def recv_unsub(self, id_=None):
        """DDP unsub handler."""
----------
Output: 

        if id_ is None:
            id_ = self.id
        self.send_unsub(id_)
        self.subscriptions.pop(id_)

    def send_unsub(self, id_):
        """DDP unsub handler."""
        self.send_message(
            msg_type="unsub",
            msg_id=id_,
        )

    def send_ping(self):
        """DDP ping handler."""
        self.send_message(
            msg_type="ping",
       
----------
Solution: 

        if id_:
            self.api.unsub(id_)
        else:
            self.reply('nosub')
--------------------
Prompt: 
def score(infile, outfile, classifier, xgb_autotune, apply_weights, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test):
    """
    Conduct semi-supervised learning and error-rate estimation for MS1, MS2 and transition-level data. 
    """
----------
Output: 
lfdr_method, lfdr_smooth_df, lfdr_smooth_log_pi0, lfdr_pi0_lambda, lfdr_pi0_method, lfdr_pi0_smooth_df, lfdr_pi0_smooth_log_pi0, lfdr_ss_initial_fdr, lfdr_ss_iteration_fdr, lfdr_ss_num_iter, lfdr_ss_main_score, lfdr_group_id, lfdr_param
----------
Solution: 


    if outfile is None:
        outfile = infile
    else:
        outfile = outfile

    # Prepare XGBoost-specific parameters
    xgb_hyperparams = {'autotune': xgb_autotune, 'autotune_num_rounds': 10, 'num_boost_round': 100, 'early_stopping_rounds': 10, 'test_size': 0.33}

    xgb_params = {'eta': 0.3, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': 1, 'alpha': 0, 'scale_pos_weight': 1, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}

    xgb_params_space = {'eta': hp.uniform('eta', 0.0, 0.3), 'gamma': hp.uniform('gamma', 0.0, 0.5), 'max_depth': hp.quniform('max_depth', 2, 8, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': hp.uniform('lambda', 0.0, 1.0), 'alpha': hp.uniform('alpha', 0.0, 1.0), 'scale_pos_weight': 1.0, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}

    if not apply_weights:
        PyProphetLearner(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test).run()
    else:
        PyProphetWeightApplier(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test, apply_weights).run()
--------------------
Prompt: 
def cleanup(controller, engines):
    """Cleanup routine to shut down all subprocesses we opened."""
----------
Output: 

    for engine in engines:
        engine.cleanup()
    controller.cleanup()


def main():
    """Main routine."""
    # Create the controller.
    controller = Controller()

    # Create the engines.
    engines = []
    engines.append(Engine(controller, "engine1", "engine1.py"))
    engines.append(Engine(controller, "engine2", "engine2.py"))

    # Start the engines.
    for engine in engines:
        engine.start()

    # Wait for the
----------
Solution: 

    import signal, time
    
    print('Starting cleanup')
    print('Stopping engines...')
    for e in engines:
        e.send_signal(signal.SIGINT)
    print('Stopping controller...')
    # so it can shut down its queues
    controller.send_signal(signal.SIGINT)
    time.sleep(0.1)
    print('Killing controller...')
    controller.kill()
    print('Cleanup done')
--------------------
Prompt: 
def reset(self):
        """Reset the input buffer and associated state."""
----------
Output: 

        self.buffer = []
        self.buffer_index = 0
        self.buffer_size = 0
        self.buffer_offset = 0
        self.buffer_length = 0
        self.buffer_length_limit = 0
        self.buffer_length_limit_set = False
        self.buffer_length_limit_exceeded = False
        self.buffer_length_limit_exceeded_count = 0
        self.buffer_length_limit_exceeded_count_limit = 0
        self
----------
Solution: 

        super(IPythonInputSplitter, self).reset()
        self._buffer_raw[:] = []
        self.source_raw = ''
        self.cell_magic_parts = []
        self.processing_cell_magic = False
--------------------
Prompt: 
def for_type_by_name(type_module, type_name, func):
    """
    Add a pretty printer for a type specified by the module and name of a type
    rather than the type object itself.
    """
----------
Output: 

    type_module = importlib.import_module(type_module)
    type_obj = getattr(type_module, type_name)
    return for_type(type_obj, func)


def for_type(type_obj, func):
    """
    Add a pretty printer for a type.
    """
    if not isinstance(type_obj, type):
        raise TypeError("type_obj must be a type")
    if not callable(func):
        raise TypeError("func must be callable")
    if not has
----------
Solution: 

    key = (type_module, type_name)
    oldfunc = _deferred_type_pprinters.get(key, None)
    if func is not None:
        # To support easy restoration of old pprinters, we need to ignore Nones.
        _deferred_type_pprinters[key] = func
    return oldfunc
--------------------
Prompt: 
def domain(self, domain=None, last_domain=None):
        """
        Manage the case that we want to test only a domain.

        :param domain: The domain or IP to test.
        :type domain: str

        :param last_domain:
            The last domain to test if we are testing a file.
        :type last_domain: str

        :param return_status: Tell us if we need to return the status.
        :type return_status: bool
        """
----------
Output: 

        if domain is None:
            domain = self.domain

        if last_domain is None:
            last_domain = self.last_domain

        if domain is None:
            return

        if last_domain is None:
            last_domain = domain

        if self.domain_type == "file":
            self.domain = domain
            self.last_domain = last_domain
            self.domain_type = "domain"
            self.domain_file = None
            self.domain_file_path = None
            self
----------
Solution: 


        # We print the header.
        self._print_header()

        if domain:
            # A domain is given.

            # We format and set the domain we are testing and treating.
            PyFunceble.INTERN["to_test"] = self._format_domain(domain)
        else:
            # A domain is not given.

            # We set the domain we are testing and treating to None.
            PyFunceble.INTERN["to_test"] = None

        if PyFunceble.INTERN["to_test"]:
            # The domain is given (Not None).

            if PyFunceble.CONFIGURATION["syntax"]:
                # The syntax mode is activated.

                # We get the status from Syntax.
                status = self.syntax_status.get()
            else:
                # We test and get the status of the domain.
                status, _ = self.status.get()

            # We run the file decision logic.
            self._file_decision(PyFunceble.INTERN["to_test"], last_domain, status)

            if PyFunceble.CONFIGURATION["simple"]:
                # The simple mode is activated.

                # We print the domain and the status.
                print(PyFunceble.INTERN["to_test"], status)

            # We return the tested domain and its status.
            return PyFunceble.INTERN["to_test"], status

        # We return None, there is nothing to test.
        return None
--------------------
Prompt: 
def varimp(self, use_pandas=False):
        """
        Pretty print the variable importances, or return them in a list/pandas DataFrame.

        :param bool use_pandas: If True, then the variable importances will be returned as a pandas data frame.

        :returns: A dictionary of lists or Pandas DataFrame instances.
        """
----------
Output: 

        if use_pandas:
            return pd.DataFrame(self.varimp, index=self.feature_names)
        else:
            return self.varimp

    def get_booster(self):
        """
        Get the underlying XGBoost Booster object.

        :returns: The underlying XGBoost Booster object.
        """
        return self.model

    def set_params(self, **params):
        """
        Set the parameters of the XGBoost model.

        :param params: The parameters
----------
Solution: 

        return {model.model_id: model.varimp(use_pandas) for model in self.models}
--------------------
Prompt: 
def statementsAreSame(statements: List[HdlStatement]) -> bool:
    """
    :return: True if all statements are same
    """
----------
Output: 

    if not statements:
        return True
    return all(s == statements[0] for s in statements[1:])


def statementsAreSameWithoutTypes(statements: List[HdlStatement]) -> bool:
    """
    :return: True if all statements are same without types
    """
    if not statements:
        return True
    return all(s.with_types(None) == statements[0].with_types(None)
               for s in statements[1:])


def statementsAreSameWithoutTypesAnd
----------
Solution: 

    iterator = iter(statements)
    try:
        first = next(iterator)
    except StopIteration:
        return True

    return all(first.isSame(rest) for rest in iterator)
--------------------
Prompt: 
def get_meta_netnode():
    """
    Get the netnode used to store settings metadata in the current IDB.
    Note that this implicitly uses the open IDB via the idc iterface.
    """
----------
Output: 

    return idc.get_meta_netnode()


def get_meta_netnode_by_id(id):
    """
    Get the netnode used to store settings metadata in the current IDB.
    Note that this implicitly uses the open IDB via the idc iterface.
    """
    return idc.get_meta_netnode_by_id(id)


def get_meta_netnode_by_name(name):
    """
    Get the netnode used to store settings metadata in the current IDB.

----------
Solution: 

    node_name = "$ {org:s}.{application:s}".format(
        org=IDA_SETTINGS_ORGANIZATION,
        application=IDA_SETTINGS_APPLICATION)
    return netnode.Netnode(node_name)
--------------------
Prompt: 
def setPin(self, old_pin, new_pin):
        """
        C_SetPIN

        :param old_pin: old PIN
        :param new_pin: new PIN
        """
----------
Output: 

        self.send_command(C_SetPIN, old_pin, new_pin)

    def setPin2(self, old_pin, new_pin):
        """
        C_SetPIN2

        :param old_pin: old PIN
        :param new_pin: new PIN
        """
        self.send_command(C_SetPIN2, old_pin, new_pin)

    def getVersion(self):
        """
        C_GetVersion
        """
        return self.send
----------
Solution: 

        old_pin1 = ckbytelist(old_pin)
        new_pin1 = ckbytelist(new_pin)
        rv = self.lib.C_SetPIN(self.session, old_pin1, new_pin1)
        if rv != CKR_OK:
            raise PyKCS11Error(rv)
--------------------
Prompt: 
def divide(df, new_column, column_1, column_2):
    """
    DEPRECATED -  use `formula` instead
    """
----------
Output: 

    df[new_column] = df[column_1] / df[column_2]
    return df


def divide_by_constant(df, new_column, column, constant):
    """
    DEPRECATED -  use `formula` instead
    """
    df[new_column] = df[column] / constant
    return df


def multiply(df, new_column, column_1, column_2):
    """
    DEPRECATED -  use `formula` instead
    """

----------
Solution: 

    return _basic_math_operation(df, new_column, column_1, column_2, op='truediv')
--------------------
Prompt: 
def _setup_template_file(self, template_file_path):
        """ Setup self.template

        Parameters
        ----------
        template_file_path: str
            Document template file path.
        """
----------
Output: 

        self.template = DocumentTemplate(template_file_path)

    def _setup_template_string(self, template_string):
        """ Setup self.template

        Parameters
        ----------
        template_string: str
            Document template string.
        """
        self.template = DocumentTemplate(template_string)

    def _setup_template_dict(self, template_dict):
        """ Setup self.template

        Parameters
        ----------
        template_dict: dict
            Document template dict.

----------
Solution: 

        try:
            template_file = template_file_path
            template_env = get_environment_for(template_file_path)
            template = template_env.get_template(os.path.basename(template_file))
        except:
            raise
        else:
            self._template_file = template_file
            self._template_env = template_env
            self.template = template
--------------------
Prompt: 
def pxconfig(self, line):
        """configure default targets/blocking for %px magics"""
----------
Output: 

        if line.strip() == '':
            self.px_config = {}
            return
        try:
            key, value = line.split('=')
        except ValueError:
            self.px_config = {}
            return
        self.px_config[key.strip()] = value.strip()

    def px(self, line):
        """%px <target> [<blocking>]

        Execute a pexpect command.

        If <target> is not specified, the default target is used.
       
----------
Solution: 

        args = magic_arguments.parse_argstring(self.pxconfig, line)
        if args.targets:
            self.view.targets = self._eval_target_str(args.targets)
        if args.block is not None:
            self.view.block = args.block
        if args.set_verbose is not None:
            self.verbose = args.set_verbose
--------------------
Prompt: 
def l(*members, meta=None) -> List:
    """Creates a new list from members."""
----------
Output: 

    return List(members, meta)


def l_(*members, meta=None) -> List:
    """Creates a new list from members."""
    return List(members, meta)


def l_from_dict(d: dict, meta=None) -> List:
    """Creates a new list from a dictionary."""
    return List(d.items(), meta)


def l_from_list(l: list, meta=None) -> List:
    """Creates a new list from a list."""
    return
----------
Solution: 

    return List(  # pylint: disable=abstract-class-instantiated
        plist(iterable=members), meta=meta
    )
--------------------
Prompt: 
def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):
    """wrapper"""
----------
Output: 

    if re.match(r'https?://www.fc2.com/', url):
        return fc2_download(url, output_dir, merge, info_only, **kwargs)
    elif re.match(r'https?://www.pixiv.net/', url):
        return pixiv_download(url, output_dir, merge, info_only, **kwargs)
    elif re.match(r'https?://www.nicovideo.jp/', url):
        return nico_download(url, output_dir
----------
Solution: 

    #'http://video.fc2.com/en/content/20151021bTVKnbEw'
    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'
    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'
    #'http://video.fc2.com/tw/content/20151021bTVKnbEw'
    hostname = urlparse(url).hostname
    if not ('fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname):
        return False
    upid = match1(url, r'.+/content/(\w+)')

    fc2video_download_by_upid(upid, output_dir, merge, info_only)
--------------------
Prompt: 
def tobin(deci_num, len=32):
    """
    Given a decimal number, returns a string bitfield of length = len
    Example: given deci_num = 1 and len = 10, it return 0000000001
    """
----------
Output: 

    bin_num = bin(deci_num)[2:]
    return bin_num.zfill(len)


def tobin_list(deci_nums, len=32):
    """
    Given a list of decimal numbers, returns a list of bitfields of length = len
    Example: given deci_nums = [1, 2, 3] and len = 10, it return [0000000001, 0000000010, 00000
----------
Solution: 

    bitstr = "".join(map(lambda y: str((deci_num >> y) & 1), range(len - 1, -1, -1)))
    return bitstr
--------------------
Prompt: 
def export_mt_variants(variants, sample_id):
    """Export mitochondrial variants for a case to create a MT excel report

    Args:
        variants(list): all MT variants for a case, sorted by position
        sample_id(str) : the id of a sample within the case

    Returns:
        document_lines(list): list of lines to include in the document
    """
----------
Output: 

    document_lines = []
    document_lines.append(
        "### Mitochondrial variants for sample {0}".format(sample_id))
    document_lines.append("| Variant | Allele |")
    document_lines.append("| ------- | ------ |")
    for variant in variants:
        document_lines.append(
            "| {0} | {1} |".format(variant.get_variant_id(),
                                   variant.get_variant_id().split("_")[1]))

----------
Solution: 

    document_lines = []
    for variant in variants:
        line = []
        position = variant.get('position')
        change = '>'.join([variant.get('reference'),variant.get('alternative')])
        line.append(position)
        line.append(change)
        line.append(str(position)+change)
        genes = []
        prot_effect = []
        for gene in variant.get('genes'):
            genes.append(gene.get('hgnc_symbol',''))
            for transcript in gene.get('transcripts'):
                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):
                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))
        line.append(','.join(prot_effect))
        line.append(','.join(genes))
        ref_ad = ''
        alt_ad = ''
        for sample in variant['samples']:
            if sample.get('sample_id') == sample_id:
                ref_ad = sample['allele_depths'][0]
                alt_ad = sample['allele_depths'][1]
        line.append(ref_ad)
        line.append(alt_ad)
        document_lines.append(line)
    return document_lines
--------------------
Prompt: 
def write_sky_params_to_file(self):
        """Writes the params to file that skytool_Free needs to generate the sky radiance distribution."""
----------
Output: 

        # Write the params to file
        with open(self.sky_params_file, 'w') as f:
            f.write(str(self.sky_params))

    def write_sky_params_to_file(self):
        """Writes the params to file that skytool_Free needs to generate the sky radiance distribution."""
        # Write the params to file
        with open(self.sky_params_file, 'w') as f:
            f.write(str(self.sky_params))

    def write
----------
Solution: 


        inp_file = self.sky_file + '_params.txt'
        lg.info('Writing Inputs to file : ' + inp_file)

        f = open(inp_file, 'w')

        f.write('verbose= ' + str(self.verbose) + '\n')
        f.write('band_count= ' + str(self.num_bands) + '\n')
        f.write('band_centres_data= ')
        f.write(",".join([str(wave) for wave in self.wavelengths]) + '\n')
        f.write('partition= ' + self.partition + '\n')
        f.write('vn= ' + str(self.vn) + '\n')
        f.write('hn= ' + str(self.hn) + '\n')
        f.write('rdif= ' + str(self.sky_r_dif) + '\n')
        f.write('theta_points= ')
        f.write(",".join([str(theta) for theta in self.theta_points]) + '\n')
        f.write('type= ' + self.sky_type + '\n')
        f.write('azimuth= ' + str(self.sky_azimuth) + '\n')
        f.write('zenith= ' + str(self.sky_zenith) + '\n')
        f.write('sky_save_fp= ' + inp_file.strip('_params.txt') + '\n')
        f.write('sky_image_save_fp= ' + self.sky_file + '.ppm' + '\n')
        f.write('sky_image_size= 256' + '\n')
        if self.sky_type == 'hlideal':
            f.write('C= ' + str(self.sky_c) + '\n')
            f.write('rdif= ' + str(self.sky_r_dif) + '\n')
        f.flush()
        f.close()
--------------------
Prompt: 
def defrag(filt, threshold=3, mode='include'):
    """
    'Defragment' a filter.

    Parameters
    ----------
    filt : boolean array
        A filter
    threshold : int
        Consecutive values equal to or below this threshold
        length are considered fragments, and will be removed.
    mode : str
        Wheter to change False fragments to True ('include')
        or True fragments to False ('exclude')

    Returns
    -------
    defragmented filter : boolean array
    """
----------
Output: 

    if mode not in ['include', 'exclude']:
        raise ValueError("mode must be 'include' or 'exclude'")

    # Find the fragments
    fragments = np.where(np.diff(filt) != 1)[0]
    fragments = np.insert(fragments, 0, 0)
    fragments = np.append(fragments, len(filt))

    # Remove fragments shorter than threshold
    fragments = fragments[fragments[1:] - fragments[:-1] >= threshold]

   
----------
Solution: 

    if bool_2_indices(filt) is None:
        return filt

    if mode == 'include':
        inds = bool_2_indices(~filt) + 1
        rep = True
    if mode == 'exclude':
        inds = bool_2_indices(filt) + 1
        rep = False

    rem = (np.diff(inds) <= threshold)[:, 0]

    cfilt = filt.copy()
    if any(rem):
        for lo, hi in inds[rem]:
            cfilt[lo:hi] = rep

    return cfilt
--------------------
Prompt: 
def _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku):
    """Build and return Sailthru purchase item object"""
----------
Output: 

    item = {
        'course_id': course_id,
        'course_url': course_url,
        'cost_in_cents': cost_in_cents,
        'mode': mode,
        'course_data': course_data,
        'sku': sku,
    }
    return item


def _build_purchase_items(course_id, course_url, cost_in_cents, mode, course_data, sku):
    """Build and return list of Sailthru purchase item objects
----------
Solution: 


    # build item description
    item = {
        'id': "{}-{}".format(course_id, mode),
        'url': course_url,
        'price': cost_in_cents,
        'qty': 1,
    }

    # get title from course info if we don't already have it from Sailthru
    if 'title' in course_data:
        item['title'] = course_data['title']
    else:
        # can't find, just invent title
        item['title'] = 'Course {} mode: {}'.format(course_id, mode)

    if 'tags' in course_data:
        item['tags'] = course_data['tags']

    # add vars to item
    item['vars'] = dict(course_data.get('vars', {}), mode=mode, course_run_id=course_id)

    item['vars']['purchase_sku'] = sku

    return item
--------------------
Prompt: 
def _kl_normal_normal(n_a, n_b, name=None):
  """Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.

  Args:
    n_a: instance of a Normal distribution object.
    n_b: instance of a Normal distribution object.
    name: (optional) Name to use for created operations.
      default is "kl_normal_normal".

  Returns:
    Batchwise KL(n_a || n_b)
  """
----------
Output: 
n_b)
  """
  with tf.name_scope(name, "kl_normal_normal", [n_a.loc, n_a.scale, n_b.loc,
                                                n_b.scale]):
    # TODO(b/68008390): Replace with tf.distribution.kl_divergence when it's
    # fixed.
    # return tf.distributions.kl_divergence(n_a, n_b)
    # TODO(b/6800839
----------
Solution: 

  with tf.name_scope(name or "kl_normal_normal"):
    one = tf.constant(1, dtype=n_a.dtype)
    two = tf.constant(2, dtype=n_a.dtype)
    half = tf.constant(0.5, dtype=n_a.dtype)
    s_a_squared = tf.square(n_a.scale)
    s_b_squared = tf.square(n_b.scale)
    ratio = s_a_squared / s_b_squared
    return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *
            (ratio - one - tf.math.log(ratio)))
--------------------
Prompt: 
def scan(self, *key_ranges, **kwargs):
        """Scan for FCs in the given id ranges.

        :param key_ranges:
          ``key_ranges`` should be a list of pairs of ranges. The first
          value is the lower bound id and the second value is the
          upper bound id. Use ``()`` in either position to leave it
          unbounded. If no ``key_ranges`` are given, then all FCs in
          the store are returned.
        :param [str] feature_names:
          A list of feature names to retrieve. When ``None``, all
          features are retrieved. Wildcards are allowed.
        :rtype: Iterable of ``(content_id, FC)``
        """
----------
Output: 
A list of feature names to return. If not given, all
          features are returned.
        :param [str] feature_types:
          A list of feature types to return. If not given, all
          features are returned.
        :param [str] feature_ids:
          A list of feature ids to return. If not given, all
          features are returned.
        :param [str] feature_tags:
          A list of feature tags to return. If not given, all
          features are returned.
        :param [str] feature_tags_not:
----------
Solution: 

        for hit in self._scan(*key_ranges, **kwargs):
            yield did(hit['_id']), self.fc_from_dict(hit['_source']['fc'])
--------------------
Prompt: 
def encode_span(self, span):
        """Encodes a single span to protobuf."""
----------
Output: 

        span_pb = self.span_pb_cls()
        span_pb.span_id = span.span_id
        span_pb.parent_span_id = span.parent_span_id
        span_pb.name = span.name
        span_pb.kind = span.kind
        span_pb.start_time.FromDatetime(span.start_time)
        span_pb.end_time.FromDatetime(span.end_time)
        span_pb.attributes.attributes.extend(
            [self._encode
----------
Solution: 

        if not protobuf.installed():
            raise ZipkinError(
                'protobuf encoding requires installing the protobuf\'s extra '
                'requirements. Use py-zipkin[protobuf] in your requirements.txt.'
            )

        pb_span = protobuf.create_protobuf_span(span)
        return protobuf.encode_pb_list([pb_span])
--------------------
Prompt: 
def verilogTypeOfSig(signalItem):
    """
    Check if is register or wire
    """
----------
Output: 

    if signalItem.is_register:
        return "reg"
    else:
        return "wire"


def verilogTypeOfSig(signalItem):
    """
    Check if is register or wire
    """
    if signalItem.is_register:
        return "reg"
    else:
        return "wire"


def verilogTypeOfSig(signalItem):
    """
    Check if is register or wire
    """
    if signalItem.is_register:
        return "reg"
    else:
----------
Solution: 

    driver_cnt = len(signalItem.drivers)
    if signalItem._const or driver_cnt > 1 or\
       arr_any(signalItem.drivers, _isEventDependentDriver):
        return SIGNAL_TYPE.REG
    else:
        if driver_cnt == 1:
            d = signalItem.drivers[0]
            if not isinstance(d, (Assignment, PortItem)):
                return SIGNAL_TYPE.REG

        return SIGNAL_TYPE.WIRE
--------------------
Prompt: 
def check_for_bucket(self, bucket_name):
        """
        Check if bucket_name exists.

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """
----------
Output: 

        try:
            self.s3_client.head_bucket(Bucket=bucket_name)
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                return False
            else:
                raise
        return True

    def create_bucket(self, bucket_name):
        """
        Create a bucket.

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """
        self.s3_client.create_bucket(B
----------
Solution: 

        try:
            self.get_conn().head_bucket(Bucket=bucket_name)
            return True
        except ClientError as e:
            self.log.info(e.response["Error"]["Message"])
            return False
--------------------
Prompt: 
def get_event_hub(self, hub_name):
        """
        Retrieves an existing event hub.

        hub_name:
            Name of the event hub.
        """
----------
Output: 

        return self.event_hubs.get(hub_name)

    def get_event_hubs(self):
        """
        Retrieves all event hubs.
        """
        return self.event_hubs

    def get_event_hub_names(self):
        """
        Retrieves all event hub names.
        """
        return self.event_hubs.keys()

    def get_event_hub_names_and_types(self):
        """
        Retrieves all event hub names and types
----------
Solution: 

        _validate_not_none('hub_name', hub_name)
        request = HTTPRequest()
        request.method = 'GET'
        request.host = self._get_host()
        request.path = '/' + _str(hub_name) + ''
        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access
        request.headers = self._update_service_bus_header(request)
        response = self._perform_request(request)

        return _convert_response_to_event_hub(response)
--------------------
Prompt: 
def get_plot_data(self):
        """ Generates the JSON report to plot the gene boxes

        Following the convention of the reports platform, this method returns
        a list of JSON/dict objects with the information about each entry in
        the abricate file. The information contained in this JSON is::

            {contig_id: <str>,
             seqRange: [<int>, <int>],
             gene: <str>,
             accession: <str>,
             coverage: <float>,
             identity: <float>
             }

        Note that the `seqRange` entry contains the position in the
        corresponding contig, not the absolute position in the whole assembly.

        Returns
        -------
        json_dic : list
            List of JSON/dict objects with the report data.
        """
----------
Output: 
       Returns
        -------
        list
            A list of JSON objects with the information about each entry in
            the abricate file.
        """
        data = []
        for contig in self.contigs:
            for gene in contig.genes:
                data.append({'contig_id': contig.id,
                             'seqRange': [gene.start, gene.end],
                             'gene': gene.name,
                             'accession': gene.accession,
                             'coverage': gene.cover
----------
Solution: 


        json_dic = {"plotData": []}
        sample_dic = {}
        sample_assembly_map = {}

        for entry in self.storage.values():

            sample_id = re.match("(.*)_abr", entry["log_file"]).groups()[0]
            if sample_id not in sample_dic:
                sample_dic[sample_id] = {}

            # Get contig ID using the same regex as in `assembly_report.py`
            # template
            contig_id = self._get_contig_id(entry["reference"])
            # Get database
            database = entry["database"]
            if database not in sample_dic[sample_id]:
                sample_dic[sample_id][database] = []

            # Update the sample-assembly correspondence dict
            if sample_id not in sample_assembly_map:
                sample_assembly_map[sample_id] = entry["infile"]

            sample_dic[sample_id][database].append(
                {"contig": contig_id,
                 "seqRange": entry["seq_range"],
                 "gene": entry["gene"].replace("'", ""),
                 "accession": entry["accession"],
                 "coverage": entry["coverage"],
                 "identity": entry["identity"],
                 },
            )

        for sample, data in sample_dic.items():
            json_dic["plotData"].append(
                {
                    "sample": sample,
                    "data": {"abricateXrange": data},
                    "assemblyFile": sample_assembly_map[sample]
                }
            )

        return json_dic
--------------------
Prompt: 
def usi(self):
        """
        Gets an USI string for the move.
        For example a move from 7A to 8A would be `7a8a` or `7a8a+` if it is
        a promotion.
        """
----------
Output: 

        if self.promotion:
            return "%s%s%s" % (self.from_square, self.to_square, self.promotion)
        else:
            return "%s%s" % (self.from_square, self.to_square)

    def __str__(self):
        """
        Gets a string representation of the move.
        """
        if self.promotion:
            return "%s%s%s" % (self.from_square, self.to_square, self.promotion)
       
----------
Solution: 

        if self:
            if self.drop_piece_type:
                return '{0}*{1}'.format(PIECE_SYMBOLS[self.drop_piece_type].upper(), SQUARE_NAMES[self.to_square])
            else:
                return SQUARE_NAMES[self.from_square] + SQUARE_NAMES[self.to_square] + \
                       ('+' if self.promotion else '')
        else:
            return '0000'
--------------------
Prompt: 
def load_institute(adapter, internal_id, display_name, sanger_recipients=None):
    """Load a institute into the database

        Args:
            adapter(MongoAdapter)
            internal_id(str)
            display_name(str)
            sanger_recipients(list(email))
    """
----------
Output: 

    institute = adapter.institute(internal_id=internal_id)
    if not institute:
        institute = adapter.institute(display_name=display_name)
    if not institute:
        institute = adapter.institute(internal_id=internal_id, display_name=display_name)
    if not institute:
        institute = adapter.institute(internal_id=internal_id, display_name=display_name,
                                      sanger_recipients=sanger_recipients)

----------
Solution: 


    institute_obj = build_institute(
        internal_id=internal_id,
        display_name=display_name,
        sanger_recipients=sanger_recipients
    )
    log.info("Loading institute {0} with display name {1}" \
             " into database".format(internal_id, display_name))

    adapter.add_institute(institute_obj)
--------------------
Prompt: 
def compute_nc(X, G):
    """Computes the novelty curve from the self-similarity matrix X and
        the gaussian kernel G."""
----------
Output: 

    n = X.shape[0]
    nc = np.zeros(n)
    for i in range(n):
        nc[i] = np.sum(G[:, i])
    return nc


def compute_nc_from_ssm(X, sigma=1):
    """Computes the novelty curve from the self-similarity matrix X and
        the gaussian kernel G."""
    n = X.shape[0]
    G = np.exp(-X / sigma)
    return compute_
----------
Solution: 

    N = X.shape[0]
    M = G.shape[0]
    nc = np.zeros(N)

    for i in range(M // 2, N - M // 2 + 1):
        nc[i] = np.sum(X[i - M // 2:i + M // 2, i - M // 2:i + M // 2] * G)

    # Normalize
    nc += nc.min()
    nc /= nc.max()
    return nc
--------------------
Prompt: 
def _get_boot_time():
    """Return system boot time (epoch in seconds)"""
----------
Output: 

    return int(open('/proc/uptime').read().split()[0])


def _get_cpu_times():
    """Return system cpu times (user, system, idle, iowait, irq, softirq, steal, guest, guest_nice)"""
    with open('/proc/stat') as f:
        data = f.readline()
    data = data.split()
    return (int(data[1]), int(data[2]), int(data[3]), int(data[4]), int(data[5]),
----------
Solution: 

    f = open('/proc/stat', 'r')
    try:
        for line in f:
            if line.startswith('btime'):
                return float(line.strip().split()[1])
        raise RuntimeError("line not found")
    finally:
        f.close()
--------------------
Prompt: 
def get_plugin_of_type(self, plugin_class):
        """
        Retrieve a plugin of desired class, KeyError raised otherwise
        """
----------
Output: 

        for plugin in self.plugins:
            if isinstance(plugin, plugin_class):
                return plugin
        raise KeyError("No plugin of type {} found".format(plugin_class))

    def get_plugin_by_name(self, plugin_name):
        """
        Retrieve a plugin by name, KeyError raised otherwise
        """
        for plugin in self.plugins:
            if plugin.name == plugin_name:
                return plugin
        raise KeyError("No plugin with name {} found".format(plugin_name))

   
----------
Solution: 

        logger.debug("Searching for plugin: %s", plugin_class)
        matches = [plugin for plugin in self.plugins.values() if isinstance(plugin, plugin_class)]
        if matches:
            if len(matches) > 1:
                logger.debug(
                    "More then one plugin of type %s found. Using first one.",
                    plugin_class)
            return matches[-1]
        else:
            raise KeyError("Requested plugin type not found: %s" % plugin_class)
--------------------
Prompt: 
def calculate_linear_predictor(model_matrix, model_coefficients, offset=None,
                               name=None):
  """Computes `model_matrix @ model_coefficients + offset`."""
----------
Output: 

  if offset is None:
    return model_matrix @ model_coefficients
  else:
    return model_matrix @ model_coefficients + offset


def calculate_linear_predictor_and_residuals(model_matrix, model_coefficients,
                                             offset=None, name=None):
  """Computes `model_matrix @ model_coefficients + offset` and residuals."""
  if offset is None:
    linear_predictor = model_matrix @ model_coefficients
  else:
   
----------
Solution: 

  with tf.compat.v1.name_scope(name, 'calculate_linear_predictor',
                               [model_matrix, model_coefficients, offset]):
    predicted_linear_response = tf.linalg.matvec(model_matrix,
                                                 model_coefficients)
    if offset is not None:
      predicted_linear_response += offset
    return predicted_linear_response
--------------------
Prompt: 
def render_hero_slider(context):
    """
    Renders the hero slider.

    """
----------
Output: 

    return context.render_template(
        "includes/partials/hero-slider.html",
        hero_slides=context.site.hero_slides,
    )


def render_footer(context):
    """
    Renders the footer.

    """
    return context.render_template(
        "includes/partials/footer.html",
        site=context.site,
    )


def render_footer_copyright(context):
    """
    Renders the footer copyright.

   
----------
Solution: 

    req = context.get('request')
    qs = SliderItem.objects.published(req).order_by('position')
    return {
        'slider_items': qs,
    }
--------------------
Prompt: 
def get_color(self, color, intensity=0):
        """ Returns a QColor for a given color code, or None if one cannot be
            constructed.
        """
----------
Output: 

        if color is None:
            return None
        if isinstance(color, str):
            color = color.lower()
            if color in self.colors:
                color = self.colors[color]
            else:
                return None
        if isinstance(color, tuple):
            color = QColor(*color)
        elif isinstance(color, QColor):
            pass
        else:
            return None
        if intensity:
            color.setAlpha(intensity)
        return color

    def get_color_name(self
----------
Solution: 

        if color is None:
            return None

        # Adjust for intensity, if possible.
        if color < 8 and intensity > 0:
            color += 8

        constructor = self.color_map.get(color, None)
        if isinstance(constructor, basestring):
            # If this is an X11 color name, we just hope there is a close SVG
            # color name. We could use QColor's static method
            # 'setAllowX11ColorNames()', but this is global and only available
            # on X11. It seems cleaner to aim for uniformity of behavior.
            return QtGui.QColor(constructor)

        elif isinstance(constructor, (tuple, list)):
            return QtGui.QColor(*constructor)

        return None
--------------------
Prompt: 
def update_z(self, z, indices=None):
        """
        Update partial or entire z.

        Args:
            z (numpy.ndarray or list): to-be-updated z
            indices (numpy.ndarray or list or optional): to-be-updated qubit indices

        Returns:
            Pauli: self

        Raises:
            QiskitError: when updating whole z, the number of qubits must be the same.
        """
----------
Output: 

        if indices is None:
            if isinstance(z, list):
                z = np.array(z)
            if isinstance(z, np.ndarray):
                if z.size == 1:
                    z = np.array([z])
                if z.size != self.num_qubits:
                    raise QiskitError(
                        "When updating whole z, the number of qubits must be the same."
                    )
                self._z = z
            else:
                raise QiskitError("z must
----------
Solution: 

        z = _make_np_bool(z)
        if indices is None:
            if len(self._z) != len(z):
                raise QiskitError("During updating whole z, you can not "
                                  "change the number of qubits.")
            self._z = z
        else:
            if not isinstance(indices, list) and not isinstance(indices, np.ndarray):
                indices = [indices]
            for p, idx in enumerate(indices):
                self._z[idx] = z[p]

        return self
--------------------
Prompt: 
def is_cptp(self, atol=None, rtol=None):
        """Return True if completely-positive trace-preserving."""
----------
Output: 

        return self.is_cptp(atol, rtol)

    def is_cptp(self, atol=None, rtol=None):
        """Return True if completely-positive trace-preserving."""
        if atol is None:
            atol = self._atol
        if rtol is None:
            rtol = self._rtol
        if self.dim < 2:
            return True
        if self.is_zero_trace():
            return True
        if not self.is_square:
           
----------
Solution: 

        if self._data[1] is not None:
            return False
        if atol is None:
            atol = self._atol
        if rtol is None:
            rtol = self._rtol
        accum = 0j
        for op in self._data[0]:
            accum += np.dot(np.transpose(np.conj(op)), op)
        return is_identity_matrix(accum, rtol=rtol, atol=atol)
--------------------
Prompt: 
def delete_clinvar_object(self, object_id, object_type, submission_id):
        """Remove a variant object from clinvar database and update the relative submission object

            Args:
                object_id(str) : the id of an object to remove from clinvar_collection database collection (a variant of a case)
                object_type(str) : either 'variant_data' or 'case_data'. It's a key in the clinvar_submission object.
                submission_id(str): the _id key of a clinvar submission

            Returns:
                updated_submission(obj): an updated clinvar submission
        """
----------
Output: 
_submission object

            Returns:
                None
        """
        # get the clinvar_submission object
        clinvar_submission = self.clinvar_collection.find_one({'_id': ObjectId(submission_id)})

        # remove the object from the clinvar_collection database collection
        self.clinvar_collection.delete_one({'_id': ObjectId(object_id)})

        # remove the object from the clinvar_submission object
        clinvar_submission[object_type].remove(object_id
----------
Solution: 


        LOG.info("Deleting clinvar object %s (%s)", object_id, object_type)

        # If it's a variant object to be removed:
        #   remove reference to it in the submission object 'variant_data' list field
        #   remove the variant object from clinvar collection
        #   remove casedata object from clinvar collection
        #   remove reference to it in the submission object 'caset_data' list field

        # if it's a casedata object to be removed:
        #   remove reference to it in the submission object 'caset_data' list field
        #   remove casedata object from clinvar collection

        result = ''

        if object_type == 'variant_data':
            # pull out a variant from submission object
            self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'variant_data': object_id} })

            variant_object = self.clinvar_collection.find_one( {'_id': object_id} )
            linking_id = variant_object.get("linking_id") #it's the original ID of the variant in scout, it's linking clinvar variants and casedata objects together

            # remove any object with that linking_id from clinvar_collection. This removes variant and casedata
            result = self.clinvar_collection.delete_many( {'linking_id': linking_id } )

        else: # remove case_data but keep variant in submission
            # delete the object itself from clinvar_collection
             result = self.clinvar_collection.delete_one( {'_id': object_id } )

        # in any case remove reference to it in the submission object 'caset_data' list field
        self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'case_data': object_id} })

        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id':submission_id}, { '$set' : {'updated_at': datetime.now()} }, return_document=pymongo.ReturnDocument.AFTER )

        return updated_submission
--------------------
Prompt: 
def _loadFromArray(self, dtype: HdlType, bitAddr: int) -> int:
        """
        Parse HArray type to this transaction template instance

        :return: address of it's end
        """
----------
Output: 

        if not isinstance(dtype, HArray):
            raise TypeError(
                f"{self.__class__.__name__} can't load from {dtype.__class__.__name__} type"
            )
        if not isinstance(bitAddr, int):
            raise TypeError(
                f"{self.__class__.__name__} can't load from {type(bitAddr).__name__} type"
            )
        if not isinstance(dtype.baseType, HdlType):
            raise TypeError(
                f
----------
Solution: 

        self.itemCnt = evalParam(dtype.size).val
        self.children = TransTmpl(
            dtype.elmType, 0, parent=self, origin=self.origin)
        return bitAddr + self.itemCnt * self.children.bitAddrEnd
--------------------
Prompt: 
def select(selector, obj):
    """Appy selector to obj and return matching nodes.

    If only one node is found, return it, otherwise return a list of matches.
    Returns False on syntax error. None if no results found.
    """
----------
Output: 

    if not isinstance(obj, (list, tuple)):
        obj = [obj]
    if not isinstance(selector, (list, tuple)):
        selector = [selector]
    matches = []
    for o in obj:
        for s in selector:
            if isinstance(s, str):
                try:
                    matches.append(o.select(s))
                except:
                    return False
            elif isinstance(s, dict):
                try:
                    matches.append(o.select(**s))
                except
----------
Solution: 


    parser = Parser(obj)
    try:
        return parser.parse(selector)
    except SelectorSyntaxError as e:
        log.exception(e)
        return False
--------------------
Prompt: 
def parse_host(host):
    """Parses host name and port number from a string.
    """
----------
Output: 

    if ':' in host:
        host, port = host.split(':')
        port = int(port)
    else:
        host, port = host, 80
    return host, port


def parse_url(url):
    """Parses a URL into a tuple of host, port, path.
    """
    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)
    host, port = parse_host(netloc)
    return host, port, path


def parse_
----------
Solution: 

    if re.match(r'^(\d+)$', host) is not None:
        return ("0.0.0.0", int(host))
    if re.match(r'^(\w+)://', host) is None:
        host = "//" + host
    o = parse.urlparse(host)
    hostname = o.hostname or "0.0.0.0"
    port = o.port or 0
    return (hostname, port)
--------------------
Prompt: 
def receive_nak_rebinding(self, pkt):
        """Receive NAK in REBINDING state."""
----------
Output: 

        # TODO: Implement
        pass

    def receive_nak_renewing(self, pkt):
        """Receive NAK in RENEWING state."""
        # TODO: Implement
        pass

    def receive_nak_releasing(self, pkt):
        """Receive NAK in RELEASING state."""
        # TODO: Implement
        pass

    def receive_nak_requesting(self, pkt):
        """Receive NAK in REQUESTING state."""
        # TODO: Im
----------
Solution: 

        logger.debug("C3.1. Received NAK?, in RENEWING state.")
        if self.process_received_nak(pkt):
            logger.debug("C3.1: T. Received NAK, in RENEWING state, "
                         "raise INIT.")
            raise self.INIT()
--------------------
Prompt: 
def diseases(context):
    """Show all diseases in the database"""
----------
Output: 

    diseases = context.db.diseases.find()
    return {"diseases": diseases}


def disease(context, disease_id):
    """Show a specific disease in the database"""
    disease = context.db.diseases.find_one({"_id": ObjectId(disease_id)})
    return {"disease": disease}


def disease_by_name(context, disease_name):
    """Show a specific disease in the database"""
    disease = context.db.diseases
----------
Solution: 

    LOG.info("Running scout view diseases")
    adapter = context.obj['adapter']

    disease_objs = adapter.disease_terms()

    nr_diseases = disease_objs.count()
    if nr_diseases == 0:
        click.echo("No diseases found")
    else:
        click.echo("Disease")
        for disease_obj in adapter.disease_terms():
            click.echo("{0}".format(disease_obj['_id']))
        LOG.info("{0} diseases found".format(nr_diseases))
--------------------
Prompt: 
def infix_to_postfix(nodes, *, recurse_types=None):
  """Convert a list of nodes in infix order to a list of nodes in postfix order.

  E.G. with normal algebraic precedence, 3 + 4 * 5 -> 3 4 5 * +
  """
----------
Output: 

  if recurse_types is None:
    recurse_types = (ast.BinOp, ast.UnaryOp)
  stack = []
  for node in nodes:
    if isinstance(node, recurse_types):
      stack.append(node)
    elif isinstance(node, ast.Num):
      while stack and isinstance(stack[-1], recurse_types):
        stack.pop()
      yield node
    elif isinstance(node, ast.Name):
      while stack and isinstance(stack[-
----------
Solution: 

  output = []
  operators = []

  for node in nodes:
    if isinstance(node, OperatorNode):
      # Drain out all operators whose precedence is gte the node's...
      cmp_operator = node.operator
      while operators:
        current_operator = operators[-1].operator
        if current_operator.precedence > cmp_operator.precedence or \
           current_operator.precedence == cmp_operator.precedence and current_operator.association == Association.left:
          output.append(operators.pop())
        else:
          break
      operators.append(node)
    else:
      if recurse_types is not None and node.node_type in recurse_types:
        output.extend(infix_to_postfix(node.children, recurse_types=recurse_types))
      else:
        output.append(node)

  return output + list(reversed(operators))
--------------------
Prompt: 
def get(cls, key, default=None):
        """
        Resembles the :meth:`dict.get` method.

        :returns:
            A configuration dictionary for specified provider.

        """
----------
Output: 

        if key not in cls._providers:
            return default
        return cls._providers[key]

    @classmethod
    def get_all(cls):
        """
        Returns a copy of the configuration dictionary.

        :returns:
            A copy of the configuration dictionary.

        """
        return cls._providers.copy()

    @classmethod
    def get_providers(cls):
        """
        Returns a list of the providers.

        :returns:
            A list of the providers
----------
Solution: 


        # Query datastore.
        result = cls.query(cls.provider_name == key).get()

        if result:
            result_dict = result.to_dict()

            # Use NDBOpenIDStore by default
            result_dict['store'] = NDBOpenIDStore

            # Convert coma-separated values to list. Currently only scope is
            # csv.
            for i in ('scope', ):
                prop = result_dict.get(i)
                if prop:
                    result_dict[i] = [s.strip() for s in prop.split(',')]
                else:
                    result_dict[i] = None

            return result_dict
        else:
            return default
--------------------
Prompt: 
def dump(obj, file, reducers=None, protocol=None):
    """Replacement for pickle.dump() using _LokyPickler."""
----------
Output: 

    if reducers is None:
        reducers = _default_reducers
    if protocol is None:
        protocol = pickle.HIGHEST_PROTOCOL
    pickler = _LokyPickler(file, protocol=protocol, reducers=reducers)
    pickler.dump(obj)


def dumps(obj, reducers=None, protocol=None):
    """Replacement for pickle.dumps() using _LokyPickler."""
    if reducers is None:
        reducers = _
----------
Solution: 

    global _LokyPickler
    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)
--------------------
Prompt: 
def rule(cls, rulename=None, erase=False):
    """Attach a method to a parsing class and register it as a parser rule.

       The method is registered with its name unless rulename is provided.
    """
----------
Output: 

    def decorator(func):
        if rulename is None:
            rulename = func.__name__
        func.rulename = rulename
        func.erase = erase
        cls.rules[rulename] = func
        return func
    return decorator

def rule_erase(cls, rulename=None):
    """Attach a method to a parsing class and register it as a parser rule.

       The method is registered with its name unless rulename is provided.
    """
    def decor
----------
Solution: 

    if not hasattr(cls, '_rules'):
        raise TypeError(
            "%s didn't seems to be a BasicParser subsclasse" % cls.__name__)
    class_hook_list = cls._hooks
    class_rule_list = cls._rules

    def wrapper(f):
        nonlocal rulename
        add_method(cls)(f)
        if rulename is None:
            rulename = f.__name__
        if not erase and (rulename in class_hook_list or rulename in class_rule_list):
            raise TypeError("%s is already define has rule or hook" % rulename)
        if '.' not in rulename:
            rulename = cls.__module__ + '.' + cls.__name__ + '.' + rulename
        set_one(class_rule_list, rulename, f)
        return f
    return wrapper
--------------------
Prompt: 
def download_url(url, root, filename=None, md5=None):
    """Download a file from a url and place it in root.

    Args:
        url (str): URL to download file from
        root (str): Directory to place downloaded file in
        filename (str, optional): Name to save the file under. If None, use the basename of the URL
        md5 (str, optional): MD5 checksum of the download. If None, do not check
    """
----------
Output: 

    from six.moves import urllib

    root = os.path.expanduser(root)
    if not filename:
        filename = os.path.basename(url)
    fpath = os.path.join(root, filename)

    makedir_exist_ok(root)

    # downloads file
    if os.path.isfile(fpath) and check_integrity(fpath, md5):
        print('Using downloaded and verified file: ' + fpath)
    else:
        try:
           
----------
Solution: 

    from six.moves import urllib

    root = os.path.expanduser(root)
    if not filename:
        filename = os.path.basename(url)
    fpath = os.path.join(root, filename)

    makedir_exist_ok(root)

    # downloads file
    if os.path.isfile(fpath) and check_integrity(fpath, md5):
        print('Using downloaded and verified file: ' + fpath)
    else:
        try:
            print('Downloading ' + url + ' to ' + fpath)
            urllib.request.urlretrieve(
                url, fpath,
                reporthook=gen_bar_updater()
            )
        except OSError:
            if url[:5] == 'https':
                url = url.replace('https:', 'http:')
                print('Failed download. Trying https -> http instead.'
                      ' Downloading ' + url + ' to ' + fpath)
                urllib.request.urlretrieve(
                    url, fpath,
                    reporthook=gen_bar_updater()
                )
--------------------
Prompt: 
def _in_buffer(self, position=None):
        """ Returns whether the current cursor (or, if specified, a position) is
            inside the editing region.
        """
----------
Output: 

        if position is None:
            position = self.get_cursor_position()
        return self.edit_start <= position <= self.edit_end

    def _get_edit_region(self):
        """ Returns the current editing region.
        """
        return self.edit_start, self.edit_end

    def _set_edit_region(self, start, end):
        """ Sets the current editing region.
        """
        self.edit_start = start
        self.edit_end = end

    edit_region
----------
Solution: 

        cursor = self._control.textCursor()
        if position is None:
            position = cursor.position()
        else:
            cursor.setPosition(position)
        line = cursor.blockNumber()
        prompt_line = self._get_prompt_cursor().blockNumber()
        if line == prompt_line:
            return position >= self._prompt_pos
        elif line > prompt_line:
            cursor.movePosition(QtGui.QTextCursor.StartOfBlock)
            prompt_pos = cursor.position() + len(self._continuation_prompt)
            return position >= prompt_pos
        return False
--------------------
Prompt: 
def split_type(self, typename):
        """
        Given a potentially complex type, split it into its base type and specializers
        """
----------
Output: 

        if typename.startswith('['):
            return self.split_type(typename[1:-1])
        elif typename.startswith('*'):
            return self.split_type(typename[1:])
        elif typename.startswith('const '):
            return self.split_type(typename[6:])
        elif typename.startswith('enum '):
            return typename[5:], []
        elif typename.startswith('struct '):
            return typename[7:], []
        elif typename.starts
----------
Solution: 


        name = self._canonicalize_type(typename)
        if '(' not in name:
            return name, False, []

        base, sub = name.split('(')
        if len(sub) == 0 or sub[-1] != ')':
            raise ArgumentError("syntax error in complex type, no matching ) found", passed_type=typename, basetype=base, subtype_string=sub)

        sub = sub[:-1]

        subs = sub.split(',')
        return base, True, subs
--------------------
Prompt: 
def get_response_code(url, timeout=10):
    """
    Visit the URL and return the HTTP response code in 'int'
    """
----------
Output: 

    try:
        response = requests.get(url, timeout=timeout)
        return response.status_code
    except requests.exceptions.RequestException:
        return None


def get_response_content(url, timeout=10):
    """
    Visit the URL and return the HTTP response content in 'str'
    """
    try:
        response = requests.get(url, timeout=timeout)
        return response.content
    except requests.exceptions.RequestException:
        return None


def get_response_json
----------
Solution: 

    try:    
        req = urllib2.urlopen(url, timeout=timeout)
    except HTTPError, e:
        return e.getcode()
    except Exception, _:
        fail("Couldn't reach the URL '%s'" % url)
    else:
        return req.getcode()
--------------------
Prompt: 
def asAMP(cls):
        """
        Returns the exception's name in an AMP Command friendly format.

        For example, given a class named ``ExampleExceptionClass``, returns
        ``"EXAMPLE_EXCEPTION_CLASS"``.
        """
----------
Output: 

        return cls.__name__.upper().replace('_', ' ')

    @classmethod
    def asString(cls):
        """
        Returns the exception's name in a string friendly format.

        For example, given a class named ``ExampleExceptionClass``, returns
        ``"Example Exception Class"``.
        """
        return cls.__name__.replace('_', ' ')

    @classmethod
    def asTitle(cls):
        """
        Returns the exception's name in a title friendly format.

        For example, given
----------
Solution: 

        parts = groupByUpperCase(cls.__name__)
        return cls, "_".join(part.upper() for part in parts)
--------------------
Prompt: 
def to_grayscale(img):
    """Convert PIL image to numpy grayscale array and numpy alpha array.

    Args:
      img (PIL.Image): PIL Image object.

    Returns:
      (gray, alpha): both numpy arrays.
    """
----------
Output: 

    gray = np.array(img.convert('L'))
    alpha = np.array(img.convert('RGBA'))[:, :, 3]
    return gray, alpha


def to_rgb(img):
    """Convert PIL image to numpy rgb array and numpy alpha array.

    Args:
      img (PIL.Image): PIL Image object.

    Returns:
      (rgb, alpha): both numpy arrays.
    """
    rgb = np.array(img.convert('RGB'))
    alpha =
----------
Solution: 

    gray = numpy.asarray(ImageOps.grayscale(img)).astype(numpy.float)

    imbands = img.getbands()
    alpha = None
    if 'A' in imbands:
        alpha = numpy.asarray(img.split()[-1]).astype(numpy.float)

    return gray, alpha
--------------------
Prompt: 
def _handle_options(self, options):
        """
        Handle the data from the options.

        :param options: The list of options from the rule.
        :type options: list

        :return: The list of domains to return globally.
        :rtype: list
        """
----------
Output: 

        domains = []
        for option in options:
            if option.startswith("domain:"):
                domain = option.replace("domain:", "")
                domains.append(domain)
        return domains

    def _get_domains(self, options):
        """
        Get the domains from the options.

        :param options: The list of options from the rule.
        :type options: list

        :return: The list of domains to return globally.
        :rtype: list
        """
        domains = []
        for
----------
Solution: 


        # We initiate a variable which will save our result
        result = []

        # We initiate the regex which will be used to extract the domain listed
        # under the option domain=
        regex_domain_option = r"domain=(.*)"

        for option in options:
            # We loop through the list of option.
            try:
                # We try to extract the list of domains from the currently read
                # option.
                domains = Regex(
                    option, regex_domain_option, return_data=True, rematch=True, group=0
                ).match()[-1]

                if domains:
                    # We could extract something.

                    if self.aggressive:  # pragma: no cover
                        result.extend(
                            [
                                x
                                for x in domains.split("|")
                                if x and not x.startswith("~")
                            ]
                        )
                    else:
                        # We return True.
                        return True
            except TypeError:
                pass

        # We return the result.
        return result
--------------------
Prompt: 
def to_fmt(self) -> fmt.indentable:
    """
    Return an Fmt representation for pretty-printing
    """
----------
Output: 

    return fmt.indentable(
        fmt.line("{} {}".format(self.name, self.type)),
        fmt.line("{} {}".format("default", self.default)),
        fmt.line("{} {}".format("description", self.description)),
        fmt.line("{} {}".format("required", self.required)),
        fmt.line("{} {}".format("enum", self.enum)),
        fmt.line("{} {}".format("minimum", self.minimum)),
        fmt.line("{} {}".format("maximum",
----------
Solution: 

    qual = "scope"
    txt = fmt.sep(" ", [qual])
    name = self.show_name()
    if name != "":
        txt.lsdata.append(name)
    if len(self._hsig) > 0 or len(self.mapTypeTranslate) > 0:
        lsb = []
        if len(self.mapTypeTranslate) > 0:
            lsb.append("translate:\n")
            lsb.append(fmt.end("\n", self.mapTypeTranslate.to_fmt()))
        for k in sorted(self._hsig.keys()):
            s = self._hsig[k]
            lsb.append(fmt.end("\n", [s.to_fmt()]))
        block = fmt.block(":\n", "", fmt.tab(lsb))
        txt.lsdata.append(block)
    return txt
--------------------
Prompt: 
def when_children_replaced ( self, object, listener, remove ):
        """ Sets up or removes a listener for children being replaced on a
            specified object.
        """
----------
Output: 

        if remove:
            self._remove_listener ( object, listener, 'children_replaced' )
        else:
            self._add_listener ( object, listener, 'children_replaced' )

    def when_children_added ( self, object, listener ):
        """ Sets up a listener for children being added to a specified object.
        """
        self._add_listener ( object, listener, 'children_added' )

    def when_children_added ( self, object, listener, remove ):
        """ Sets
----------
Solution: 

        object.on_trait_change( listener, "subgraphs", remove = remove,
                                dispatch = "fast_ui" )
        object.on_trait_change( listener, "clusters", remove = remove,
                                dispatch = "fast_ui" )
        object.on_trait_change( listener, "nodes", remove = remove,
                                dispatch = "fast_ui" )
        object.on_trait_change( listener, "edges", remove = remove,
                                dispatch = "fast_ui" )
--------------------
Prompt: 
def tokenize_line(line):
    """
    Tokenize a line:
    * split tokens on whitespace
    * treat quoted strings as a single token
    * drop comments
    * handle escaped spaces and comment delimiters
    """
----------
Output: 

    tokens = []
    token = ''
    in_quotes = False
    for c in line:
        if c == ' ' and not in_quotes:
            if token:
                tokens.append(token)
                token = ''
        elif c == '"':
            in_quotes = not in_quotes
        elif c == '#':
            break
        elif c == '\\':
            token += c
            token += next(line)
        else:
            token += c
    if token:
        tokens.append(token
----------
Solution: 

    ret = []
    escape = False
    quote = False
    tokbuf = ""
    ll = list(line)
    while len(ll) > 0:
        c = ll.pop(0)
        if c.isspace():
            if not quote and not escape:
                # end of token
                if len(tokbuf) > 0:
                    ret.append(tokbuf)

                tokbuf = ""
            elif quote:
                # in quotes
                tokbuf += c
            elif escape:
                # escaped space
                tokbuf += c
                escape = False
            else:
                tokbuf = ""

            continue

        if c == '\\':
            escape = True
            continue
        elif c == '"':
            if not escape:
                if quote:
                    # end of quote
                    ret.append(tokbuf)
                    tokbuf = ""
                    quote = False
                    continue
                else:
                    # beginning of quote
                    quote = True
                    continue
        elif c == ';':
            if not escape:
                # comment 
                ret.append(tokbuf)
                tokbuf = ""
                break
            
        # normal character
        tokbuf += c
        escape = False

    if len(tokbuf.strip(" ").strip("\n")) > 0:
        ret.append(tokbuf)

    return ret
--------------------
Prompt: 
def tasks(self):
        """Instance depends on the API version:

           * 2018-09-01: :class:`TasksOperations<azure.mgmt.containerregistry.v2018_09_01.operations.TasksOperations>`
        """
----------
Output: 

        api_version = self._get_api_version('tasks')
        if api_version == '2018-09-01':
            from .v2018_09_01.operations import TasksOperations as OperationClass
        else:
            raise ValueError("API version {} does not have operation group 'tasks'".format(api_version))
        return OperationClass(self._client, self._config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api
----------
Solution: 

        api_version = self._get_api_version('tasks')
        if api_version == '2018-09-01':
            from .v2018_09_01.operations import TasksOperations as OperationClass
        else:
            raise NotImplementedError("APIVersion {} is not available".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))
--------------------
Prompt: 
def f_supports(self, data):
        """Checks if input data is supported by the parameter."""
----------
Output: 

        return True

    def f_validate(self, data):
        """Validates input data."""
        return True

    def f_convert(self, data):
        """Converts input data to the parameter type."""
        return data

    def f_get_default(self):
        """Returns default value for the parameter."""
        return self.default

    def f_get_value(self):
        """Returns value of the parameter."""
        return self.value

    def f_set_value(
----------
Solution: 

        dtype = type(data)
        if dtype is tuple or dtype is list and len(data) == 0:
            return True  #  ArrayParameter does support empty tuples
        elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:
                return True  #  ArrayParameter supports empty numpy arrays
        else:
            return super(ArrayParameter, self).f_supports(data)
--------------------
Prompt: 
def moments_of_masked_time_series(time_series_tensor, broadcast_mask):
  """Compute mean and variance, accounting for a mask.

  Args:
    time_series_tensor: float `Tensor` time series of shape
      `concat([batch_shape, [num_timesteps]])`.
    broadcast_mask: bool `Tensor` of the same shape as `time_series`.
  Returns:
    mean: float `Tensor` of shape `batch_shape`.
    variance: float `Tensor` of shape `batch_shape`.
  """
----------
Output: 
of shape `batch_shape`.
  """
  # TODO(allenl): Add support for broadcasting over the batch dimension.
  # TODO(allenl): Add support for broadcasting over the time dimension.
  with tf.name_scope('moments_of_masked_time_series'):
    # TODO(allenl): Use tf.nn.moments when it supports masking.
    # TODO(allenl): Use tf.nn.compute_covariance when it supports masking.
    # TODO(allenl): Use tf.nn.normal
----------
Solution: 

  num_unmasked_entries = tf.cast(
      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),
      time_series_tensor.dtype)

  # Manually compute mean and variance, excluding masked entries.
  mean = (tf.reduce_sum(input_tensor=tf.where(
      broadcast_mask,
      tf.zeros_like(time_series_tensor),
      time_series_tensor), axis=-1) / num_unmasked_entries)
  variance = (tf.reduce_sum(input_tensor=tf.where(
      broadcast_mask,
      tf.zeros_like(time_series_tensor),
      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)
              / num_unmasked_entries)
  return mean, variance
--------------------
Prompt: 
def flush(self, full=False, all=False, force=False):
        """
        Call ``.save()`` on all modified entities in the session. Use when you
        want to flush changes to Redis, but don't want to lose your local
        session cache.

        See the ``.commit()`` method for arguments and their meanings.
        """
----------
Output: 

        if full:
            self._flush_full(force=force)
        elif all:
            self._flush_all(force=force)
        else:
            self._flush_modified(force=force)

    def _flush_full(self, force=False):
        """
        Flush all entities in the session.
        """
        self._flush_modified(force=force)
        self._flush_deleted(force=force)

    def _flush_all(self, force=False):
        """
       
----------
Solution: 

        self._init()

        return self.save(*self.known.values(), full=full, all=all, force=force)
--------------------
Prompt: 
def format_price_commas(price):
    """
    Formats prices, rounding (i.e. to the nearest whole number of pounds) with commas
    """
----------
Output: 

    return f"£{round(price, 2):,.2f}"


def format_price_no_commas(price):
    """
    Formats prices, rounding (i.e. to the nearest whole number of pounds) without commas
    """
    return f"£{round(price, 2):.2f}"


def format_price_no_pounds(price):
    """
    Formats prices, rounding (i.e. to the nearest whole number of pounds) without pounds sign
    """
   
----------
Solution: 

    if price is None:
        return None
    if price >= 0:
        return jinja2.Markup('&pound;{:,.2f}'.format(price))
    else:
        return jinja2.Markup('-&pound;{:,.2f}'.format(-price))
--------------------
Prompt: 
def _distribute_jobs(self):
        """
        Distributes jobs from the active job set to any waiting get_job
        callbacks.
        """
----------
Output: 

        if self.active_jobs:
            for callback in self.waiting_get_jobs:
                callback(self.active_jobs.pop())
            self.waiting_get_jobs = []

    def _get_job(self):
        """
        Returns the next job in the active job set.
        """
        if self.active_jobs:
            job = self.active_jobs.pop()
            self._distribute_jobs()
            return job
        else:
            self.waiting_
----------
Solution: 


        while (self._active_js.job_available()
                and len(self._ready_callbacks) > 0):
            job = self._active_js.get_job()
            self._job_sources[job] = self._active_js
            callback = self._ready_callbacks.popleft()
            callback(job)
--------------------
Prompt: 
def finalize(self, success_or_fail, message=''):
        """
        :param success_or_fail: string that is 'success' or 'fail'
        :param message:
        """
----------
Output: 

        if success_or_fail == 'success':
            self.success = True
            self.message = message
        elif success_or_fail == 'fail':
            self.success = False
            self.message = message
        else:
            raise Exception('success_or_fail must be "success" or "fail"')

    def __str__(self):
        return 'success: %s, message: %s' % (self.success, self.message)


class Command(object):
    """
    Base class for all commands.
----------
Solution: 

        self.logit.debug('String OutputPorts: %s' % self.__string_output_ports)
        if self.__string_output_ports:
            with open(os.path.join(self.output_path, 'ports.json'), 'w') as opf:
                json.dump(self.__string_output_ports, opf, indent=4)
            self.logit.debug('Ports.json written to %s' % os.path.join(self.output_path, 'ports.json'))

        with open(os.path.join(self.base_path, 'status.json'), 'w') as sf:
            json.dump({'status': success_or_fail, 'reason': message}, sf, indent=4)
--------------------
Prompt: 
def get_users_for_sis_course_id(self, sis_course_id, params={}):
        """
        Returns a list of users for the given sis course id.
        """
----------
Output: 

        return self._get_users_for_sis_id(sis_course_id, params)

    def get_users_for_sis_section_id(self, sis_section_id, params={}):
        """
        Returns a list of users for the given sis section id.
        """
        return self._get_users_for_sis_id(sis_section_id, params)

    def get_users_for_sis_account_id(self, sis_account_id, params
----------
Solution: 

        return self.get_users_for_course(
            self._sis_id(sis_course_id, sis_field="course"), params)
--------------------
Prompt: 
def sync_folder(self, path, bucket):
        """Syncs a local directory with an S3 bucket.
     
        Currently does not delete files from S3 that are not in the local directory.

        path: The path to the directory to sync to S3
        bucket: The name of the bucket on S3
        """
----------
Output: 

        # Get the list of files in the directory
        files = os.listdir(path)

        # Get the list of files in the bucket
        bucket_files = self.list_bucket(bucket)

        # Get the list of files in the bucket that are not in the directory
        bucket_files_not_in_dir = [file for file in bucket_files if file not in files]

        # Upload the files in the directory that are not in the bucket
        for file in files:
            if file not in bucket_files:
               
----------
Solution: 

        bucket = self.conn.get_bucket(bucket)
        local_files = self._get_local_files(path)
        s3_files = self._get_s3_files(bucket)
        for filename, hash in local_files.iteritems():
            s3_key = s3_files[filename]
            if s3_key is None:
                s3_key = Key(bucket)
                s3_key.key = filename
                s3_key.etag = '"!"'
            
            if s3_key.etag[1:-1] != hash[0]:
                s3_key.set_contents_from_filename(join(path, filename), md5=hash)
--------------------
Prompt: 
def new(self, *args, **kwargs):
        """
        Create and return a new instance.
        """
----------
Output: 

        return self.get_class()(*args, **kwargs)

    def get_class(self):
        """
        Return the class to be instantiated.
        """
        return self.cls

    def __call__(self, *args, **kwargs):
        """
        Create and return a new instance.
        """
        return self.new(*args, **kwargs)

    def __getattr__(self, name):
        """
        Delegate access to attributes to the instance.
        """
        return getattr(self.new(),
----------
Solution: 

        inst = self.clazz()
        self.storage.append(inst)
        
        # set all attributes with an initial default value
        referential_attributes = dict()
        for name, ty in self.attributes:
            if name not in self.referential_attributes:
                value = self.default_value(ty)
                setattr(inst, name, value)
            
        # set all positional arguments
        for attr, value in zip(self.attributes, args):
            name, ty = attr
            if name not in self.referential_attributes:
                setattr(inst, name, value)
            else:
                referential_attributes[name] = value
            
        # set all named arguments
        for name, value in kwargs.items():
            if name not in self.referential_attributes:
                setattr(inst, name, value)
            else:
                referential_attributes[name] = value
        
        if not referential_attributes:
            return inst
        
        # batch relate referential attributes 
        for link in self.links.values():
            if set(link.key_map.values()) - set(referential_attributes.keys()):
                continue
             
            kwargs = dict()
            for key, value in link.key_map.items():
                kwargs[key] = referential_attributes[value]
            
            if not kwargs:
                continue
            
            for other_inst in link.to_metaclass.query(kwargs):
                relate(other_inst, inst, link.rel_id, link.phrase)
        
        for name, value in referential_attributes.items():
            if getattr(inst, name) != value:
                logger.warning('unable to assign %s to %s', name, inst)
                
        return inst
--------------------
Prompt: 
def plotBrightLimitInV(gBright, pdf=False, png=False):
  """
  Plot the bright limit of Gaia in V as a function of (V-I).

  Parameters
  ----------

  gBright - The bright limit of Gaia in G
  """
----------
Output: 


  # Plot the bright limit of Gaia in V as a function of (V-I)
  fig = plt.figure(figsize=(10, 10))
  ax = fig.add_subplot(111)
  ax.plot(gBright, 'k-')
  ax.set_xlabel('V-I')
  ax.set_ylabel('Gaia bright limit in V')
  ax.set_xlim(0.5, 2.0)
  ax.set_ylim(0.
----------
Solution: 

  vmini=np.linspace(0.0,6.0,1001)
  gminv=gminvFromVmini(vmini)
  vBright=gBright-gminv

  fig=plt.figure(figsize=(10,6.5))
  plt.plot(vmini,vBright,'b-')
  plt.xlabel('$(V-I)$')
  plt.ylabel('Bright limit of Gaia in $V$')
  plt.xlim(0,6)
  plt.ylim(5,11)
  plt.grid(which='both')
  plt.title("Bright limit in $G$: {0}".format(gBright))

  if (pdf):
    plt.savefig('VBandBrightLimit.pdf')
  elif (png):
    plt.savefig('VBandBrightLimit.png')
  else:
    plt.show()
--------------------
Prompt: 
def _all_get_or_create_table(self, where, tablename, description, expectedrows=None):
        """Creates a new table, or if the table already exists, returns it."""
----------
Output: 

        if self.table_exists(tablename):
            return self.get_table(tablename)
        else:
            return self.create_table(tablename, description, where, expectedrows)

    def _all_get_or_create_view(self, where, viewname, description, expectedrows=None):
        """Creates a new view, or if the view already exists, returns it."""
        if self.view_exists(viewname):
            return self.get_view(viewname)
        else:

----------
Solution: 

        where_node = self._hdf5file.get_node(where)

        if not tablename in where_node:
            if not expectedrows is None:
                table = self._hdf5file.create_table(where=where_node, name=tablename,
                                              description=description, title=tablename,
                                              expectedrows=expectedrows,
                                              filters=self._all_get_filters())
            else:
                table = self._hdf5file.create_table(where=where_node, name=tablename,
                                              description=description, title=tablename,
                                              filters=self._all_get_filters())
        else:
            table = where_node._f_get_child(tablename)

        return table
--------------------
Prompt: 
def _align(self, axes, key_shape=None):
        """
        Align local bolt array so that axes for iteration are in the keys.

        This operation is applied before most functional operators.
        It ensures that the specified axes are valid, and might transpose/reshape
        the underlying array so that the functional operators can be applied
        over the correct records.

        Parameters
        ----------
        axes: tuple[int]
            One or more axes that will be iterated over by a functional operator

        Returns
        -------
        BoltArrayLocal
        """
----------
Output: 
------
        BoltArray
            The same array, but with the axes aligned
        """
        if key_shape is None:
            key_shape = self.key_shape
        if len(axes) == 0:
            return self
        if len(axes) == 1:
            if axes[0] == 0:
                return self
            if axes[0] == 1:
                return self.T
            if axes[0] == 2:
                return self.T.T
            if axes[0] == 
----------
Solution: 


        # ensure that the key axes are valid for an ndarray of this shape
        inshape(self.shape, axes)

        # compute the set of dimensions/axes that will be used to reshape
        remaining = [dim for dim in range(len(self.shape)) if dim not in axes]
        key_shape = key_shape if key_shape else [self.shape[axis] for axis in axes]
        remaining_shape = [self.shape[axis] for axis in remaining]
        linearized_shape = [prod(key_shape)] + remaining_shape

        # compute the transpose permutation
        transpose_order = axes + remaining

        # transpose the array so that the keys being mapped over come first, then linearize keys
        reshaped = self.transpose(*transpose_order).reshape(*linearized_shape)

        return reshaped
--------------------
Prompt: 
def info(self):
        """ Returns a description of the trait."""
----------
Output: 

        return "A trait that is a list of strings."

    def validate(self, obj, value):
        """ Validates that the value is a list of strings."""
        if not isinstance(value, list):
            raise TraitError("A list of strings is required.")
        for item in value:
            if not isinstance(item, basestring):
                raise TraitError("A list of strings is required.")


class ListOfInts(TraitType):
    """ A trait whose value is a list of integers."""


----------
Solution: 

        result = 'any of ' + repr(self.values)
        if self._allow_none:
            return result + ' or None'
        return result
--------------------
Prompt: 
def update_affinity_group(self, affinity_group_name, label,
                              description=None):
        """
        Updates the label and/or the description for an affinity group for the
        specified subscription.

        affinity_group_name:
            The name of the affinity group.
        label:
            A name for the affinity group. The name can be up to 100 characters
            in length.
        description:
            A description for the affinity group. The description can be up to
            1024 characters in length.
        """
----------
Output: 
4 characters in length.
        """
        return self.mgmt_client.update_affinity_group(
            affinity_group_name=affinity_group_name,
            label=label,
            description=description)

    def list_affinity_groups(self):
        """
        Lists the affinity groups for the specified subscription.
        """
        return self.mgmt_client.list_affinity_groups()

    def delete_affinity_group(self, affinity_group_name):
        """

----------
Solution: 

        _validate_not_none('affinity_group_name', affinity_group_name)
        _validate_not_none('label', label)
        return self._perform_put(
            '/' + self.subscription_id + '/affinitygroups/' +
            _str(affinity_group_name),
            _XmlSerializer.update_affinity_group_to_xml(label, description))
--------------------
Prompt: 
def HWProcess(cls, proc, ctx):
        """
        Serialize HWProcess instance

        :param scope: name scope to prevent name collisions
        """
----------
Output: 

        if proc.is_a(HWProcess):
            return cls.HWProcess(proc, ctx)
        elif proc.is_a(HWThread):
            return cls.HWThread(proc, ctx)
        elif proc.is_a(HWBlock):
            return cls.HWBlock(proc, ctx)
        elif proc.is_a(HWModule):
            return cls.HWModule(proc, ctx)
        elif proc.is_a(HWInterface):
            return cls.H
----------
Solution: 

        body = proc.statements
        childCtx = ctx.withIndent()
        statemets = [cls.asHdl(s, childCtx) for s in body]
        proc.name = ctx.scope.checkedName(proc.name, proc)

        return cls.methodTmpl.render(
            indent=getIndent(ctx.indent),
            name=proc.name,
            statements=statemets
        )
--------------------
Prompt: 
def _get_tol(tol, dtype, validate_args):
  """Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional."""
----------
Output: 

  if tol is None:
    return tf.constant(0, dtype=dtype)
  if validate_args:
    return tf.convert_to_tensor(tol, dtype=dtype)
  return tf.convert_to_tensor(tol, dtype=dtype, name='tol')


def _get_atol(atol, dtype, validate_args):
  """Gets a Tensor of type `dtype`, 0 if `atol` is None, validation optional."""
  if atol is None:
    return tf.constant(
----------
Solution: 

  if tol is None:
    return tf.convert_to_tensor(value=0, dtype=dtype)

  tol = tf.convert_to_tensor(value=tol, dtype=dtype)
  if validate_args:
    tol = distribution_util.with_dependencies([
        assert_util.assert_non_negative(
            tol, message="Argument 'tol' must be non-negative")
    ], tol)
  return tol
--------------------
Prompt: 
def get_item_objects(self, query_params=None):
        """
        Get the items for this checklist. Returns a list of ChecklistItem objects.
        """
----------
Output: 

        if query_params is None:
            query_params = {}
        return self._get_objects(
            self.get_item_url(),
            query_params,
            ChecklistItem,
            self.get_item_objects_from_json,
        )

    def get_item_objects_from_json(self, json_items):
        """
        Get the items for this checklist from a JSON response. Returns a list of ChecklistItem objects.
        """
        return [ChecklistItem(self, item) for item in
----------
Solution: 

        card = self.get_card()
        checklistitems_list = []
        for checklistitem_json in self.get_items(query_params):
            checklistitems_list.append(self.create_checklist_item(card.id, self.id, checklistitem_json))

        return checklistitems_list
--------------------
Prompt: 
def onTWriteCallback__init(self, sim):
        """
        Process for injecting of this callback loop into simulator
        """
----------
Output: 

        self.sim = sim
        self.sim.register_callback(self.onTWriteCallback)

    def onTWriteCallback(self, t, data):
        """
        Callback function for TWrite
        """
        if self.sim.get_state() == 'RUNNING':
            self.sim.set_state('STOPPED')
            self.sim.set_state('RUNNING')

    def onTWriteCallback__exit(self, sim):
        """
        Process for removing of this callback loop
----------
Solution: 

        yield from self.onTWriteCallback(sim)
        self.intf.t._sigInside.registerWriteCallback(
            self.onTWriteCallback,
            self.getEnable)
        self.intf.o._sigInside.registerWriteCallback(
            self.onTWriteCallback,
            self.getEnable)
--------------------
Prompt: 
def GetParam(tag, param, default=__SENTINEL):
    """ Convenience function for accessing tag parameters"""
----------
Output: 

    if default is __SENTINEL:
        default = None
    return tag.get(param, default)


def GetParamAsBool(tag, param, default=__SENTINEL):
    """ Convenience function for accessing tag parameters""" 
    if default is __SENTINEL:
        default = False
    return tag.get(param, default)


def GetParamAsInt(tag, param, default=__SENTINEL):
    """ Convenience function
----------
Solution: 

    if tag.HasParam(param):
        return tag.GetParam(param)
    else:
        if default == __SENTINEL:
            raise KeyError
        else:
            return default
--------------------
Prompt: 
def _connection_failed(self, error="Error not specified!"):
        """Clean up after connection failure detected."""
----------
Output: 

        self.connection_failed = True
        self.connection_failed_error = error
        self.connection_failed_time = time.time()
        self.connection_failed_count += 1
        self.connection_failed_count_time = time.time()
        self.connection_failed_count_total += 1
        self.connection_failed_count_total_time = time.time()
        self.connection_failed_count_total_time_start = self.connection_failed_count_total_time
        self.connection_failed
----------
Solution: 

        if not self._error:
            LOG.error("Connection failed: %s", str(error))
            self._error = error
--------------------
Prompt: 
def sequence_detection_negotiated(self):
        """
        After :meth:`step` has been called, this property will be set to
        True if the security context can use out-of-sequence message detection for messages
        protected by :meth:`get_mic` and :meth:`wrap`. False if OOS detection cannot be used.
        """
----------
Output: 

        return self._sequence_detection_negotiated

    @property
    def sequence_detection_supported(self):
        """
        After :meth:`step` has been called, this property will be set to
        True if the security context can use out-of-sequence message detection for messages
        protected by :meth:`get_mic` and :meth:`wrap`. False if OOS detection cannot be used.
        """
        return self._sequence_detection_supported

    @property
    def sequence_detection
----------
Solution: 

        return (
            self.flags & C.GSS_C_SEQUENCE_FLAG
        ) and (
            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)
        )
--------------------
Prompt: 
def domain_to_fqdn(domain, proto=None):
    """ returns a fully qualified app domain name """
----------
Output: 

    if not domain:
        return None
    if not proto:
        proto = 'http'
    return '%s://%s' % (proto, domain)


def fqdn_to_domain(fqdn):
    """ returns a domain name from a fully qualified app domain name """
    if not fqdn:
        return None
    return fqdn.split('://')[1]


def get_domain_from_request(request):
    """ returns the domain from a request """
    return fqdn_to_domain
----------
Solution: 

    from .generic import get_site_proto
    proto = proto or get_site_proto()
    fdqn = '{proto}://{domain}'.format(proto=proto, domain=domain)
    return fdqn
--------------------
Prompt: 
def default(self, o):
        """Implement this method in a subclass such that it returns a
        serializable object for ``o``, or calls the base implementation (to
        raise a ``TypeError``).

        For example, to support arbitrary iterators, you could implement
        default like this::

            def default(self, o):
                try:
                    iterable = iter(o)
                except TypeError:
                    pass
                else:
                    return list(iterable)
                return JSONEncoder.default(self, o)
        """
----------
Output: 
)
        """
        raise TypeError(repr(o) + " is not JSON serializable")

    def encode(self, o):
        """Return a JSON representation of a Python data structure.

        >>> from json import JSONEncoder
        >>> JSONEncoder().encode({"foo": ["bar", "baz"]})
        '{"foo": ["bar", "baz"]}'

        """
        # This is for extreme backward compatibility.
        if isinstance(o, basestring):
            return o
        # This should be a safe fall-
----------
Solution: 

        if isinstance(o, datetime):
            return http_date(o)
        if isinstance(o, uuid.UUID):
            return str(o)
        if hasattr(o, '__html__'):
            return text_type(o.__html__())
        return _json.JSONEncoder.default(self, o)
--------------------
Prompt: 
def merge(
            self, reservation_order_id, sources=None, custom_headers=None, raw=False, polling=True, **operation_config):
        """Merges two `Reservation`s.

        Merge the specified `Reservation`s into a new `Reservation`. The two
        `Reservation`s being merged must have same properties.

        :param reservation_order_id: Order Id of the reservation
        :type reservation_order_id: str
        :param sources: Format of the resource id should be
         /providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}
        :type sources: list[str]
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: The poller return type is ClientRawResponse, the
         direct response alongside the deserialized response
        :param polling: True for ARMPolling, False for no polling, or a
         polling object for personal polling strategy
        :return: An instance of LROPoller that returns list or
         ClientRawResponse<list> if raw==True
        :rtype:
         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]
         or
         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]
        :raises:
         :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`
        """
----------
Output: 
be
         /Providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}
        :type sources: list[str]
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: The poller return type is ClientRawResponse, the
         direct response alongside the deserialized response
        :param polling: True for ARMPolling, False for no polling, or a
         polling object for personal polling strategy
        :return: An instance of LROPol
----------
Solution: 

        raw_result = self._merge_initial(
            reservation_order_id=reservation_order_id,
            sources=sources,
            custom_headers=custom_headers,
            raw=True,
            **operation_config
        )

        def get_long_running_output(response):
            deserialized = self._deserialize('[ReservationResponse]', response)

            if raw:
                client_raw_response = ClientRawResponse(deserialized, response)
                return client_raw_response

            return deserialized

        lro_delay = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)
        elif polling is False: polling_method = NoPolling()
        else: polling_method = polling
        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)
--------------------
Prompt: 
def _get_concatenation(extractors, text, *, ignore_whitespace=True):
  """Returns a concatenation ParseNode whose children are the nodes returned by each of the
  methods in the extractors enumerable.

  If ignore_whitespace is True, whitespace will be ignored and then attached to the child it
  preceeded.
  """
----------
Output: 

  children = []
  for extractor in extractors:
    node = extractor(text)
    if node is None:
      continue
    if ignore_whitespace:
      children.extend(node.children)
    else:
      children.append(node)
  return ParseNode(children)


def _get_concatenation_with_whitespace(extractors, text):
  return _get_concatenation(extractors, text, ignore_whitespace=False)


def _get_conc
----------
Solution: 

  ignored_ws, use_text = _split_ignored(text, ignore_whitespace)

  extractor, *remaining = extractors

  child = _call_extractor(extractor, use_text)
  child.add_ignored(ignored_ws)

  # TODO: Should I set node.position = -len(text) for the case that ignored whitespace will cause
  #       the first child's position to not be the whitespace, and therefore the concatenation's
  #       position will be the first non-whitespace? I think not, but I'm adding this note in
  #       case that causes an issue I'm not seeing at the moment.
  node = ParseNode(ParseNodeType.concatenation, children=[child])

  if remaining:
    # child.consumed will include ignored whitespace, so we base the text we pass on on text rather
    # than use_text.
    return node.merged(_get_concatenation(remaining,
                                          text[child.consumed:],
                                          ignore_whitespace=ignore_whitespace))
  else:
    return node
--------------------
Prompt: 
def add_virtual_columns_rotation(self, x, y, xnew, ynew, angle_degrees, propagate_uncertainties=False):
        """Rotation in 2d.

        :param str x: Name/expression of x column
        :param str y: idem for y
        :param str xnew: name of transformed x column
        :param str ynew:
        :param float angle_degrees: rotation in degrees, anti clockwise
        :return:
        """
----------
Output: 

        angle_radians = angle_degrees * np.pi / 180.
        xnew_expr = 'cos({})*{} - sin({})*{}'.format(angle_radians, x, angle_radians, y)
        ynew_expr = 'sin({})*{} + cos({})*{}'.format(angle_radians, x, angle_radians, y)
        self.add_column(xnew, xnew_expr)
        self.add_column(ynew, ynew_expr)
       
----------
Solution: 

        x = _ensure_string_from_expression(x)
        y = _ensure_string_from_expression(y)
        theta = np.radians(angle_degrees)
        matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])
        m = matrix_name = x + "_" + y + "_rot"
        for i in range(2):
            for j in range(2):
                self.set_variable(matrix_name + "_%d%d" % (i, j), matrix[i, j].item())
        self[xnew] = self._expr("{m}_00 * {x} + {m}_01 * {y}".format(**locals()))
        self[ynew] = self._expr("{m}_10 * {x} + {m}_11 * {y}".format(**locals()))
        if propagate_uncertainties:
            self.propagate_uncertainties([self[xnew], self[ynew]])
--------------------
Prompt: 
def _call(self, resource, params):
        """Call to get a resource.

        :param method: resource to get
        :param params: dict with the HTTP parameters needed to get
            the given resource
        """
----------
Output: 

        return self._call_method(resource, params)

    def _call_method(self, resource, params):
        """Call to get a resource.

        :param method: resource to get
        :param params: dict with the HTTP parameters needed to get
            the given resource
        """
        url = self.url + resource
        if params:
            url += '?' + urlencode(params)
        response = self.session.get(url)
        return response

    def _call_post(self, resource, params):
       
----------
Solution: 

        url = self.URL % {'base': self.base_url, 'resource': resource}

        if self.api_token:
            params[self.PKEY] = self.api_token

        logger.debug("Redmine client requests: %s params: %s",
                     resource, str(params))

        r = self.fetch(url, payload=params, verify=False)

        return r.text
--------------------
Prompt: 
def maintained_selection():
    """Maintain selection during context

    Example:
        >>> with maintained_selection():
        ...     # Modify selection
        ...     cmds.select('node', replace=True)
        >>> # Selection restored

    """
----------
Output: 

    selection = cmds.ls(sl=True)
    try:
        yield
    finally:
        cmds.select(selection, replace=True)


def get_selection():
    """Get current selection

    Returns:
        list: List of selected nodes

    """
    return cmds.ls(sl=True)


def get_selection_transforms():
    """Get current selection

    Returns:
        list: List of selected nodes

    """
    return cmds.ls(sl=True, type='
----------
Solution: 


    previous_selection = cmds.ls(selection=True)
    try:
        yield
    finally:
        if previous_selection:
            cmds.select(previous_selection,
                        replace=True,
                        noExpand=True)
        else:
            cmds.select(deselect=True,
                        noExpand=True)
--------------------
Prompt: 
def get_marginal_distribution(self, index_points=None):
    """Compute the marginal of this GP over function values at `index_points`.

    Args:
      index_points: `float` `Tensor` representing finite (batch of) vector(s) of
        points in the index set over which the GP is defined. Shape has the form
        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature
        dimensions and must equal `kernel.feature_ndims` and `e` is the number
        (size) of index points in each batch. Ultimately this distribution
        corresponds to a `e`-dimensional multivariate normal. The batch shape
        must be broadcastable with `kernel.batch_shape` and any batch dims
        yielded by `mean_fn`.

    Returns:
      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,
        according to whether `index_points` consists of one or many index
        points, respectively.
    """
----------
Output: 
ndims` and `e` is the number
        (size) of points. The batch shape must be broadcastable with `kernel.batch_shape`
        and any batch dims yielded by `index_points`.

    Returns:
      marginal_distribution: a `tfp.distributions.Distribution` instance.
    """
    if index_points is None:
      index_points = self.index_points
    return self.marginal_distribution(index_points)

  def _make_marginal_distribution(self, index_
----------
Solution: 

    with self._name_scope('get_marginal_distribution'):
      # TODO(cgs): consider caching the result here, keyed on `index_points`.
      index_points = self._get_index_points(index_points)
      covariance = self._compute_covariance(index_points)
      loc = self._mean_fn(index_points)
      # If we're sure the number of index points is 1, we can just construct a
      # scalar Normal. This has computational benefits and supports things like
      # CDF that aren't otherwise straightforward to provide.
      if self._is_univariate_marginal(index_points):
        scale = tf.sqrt(covariance)
        # `loc` has a trailing 1 in the shape; squeeze it.
        loc = tf.squeeze(loc, axis=-1)
        return normal.Normal(
            loc=loc,
            scale=scale,
            validate_args=self._validate_args,
            allow_nan_stats=self._allow_nan_stats,
            name='marginal_distribution')
      else:
        scale = tf.linalg.LinearOperatorLowerTriangular(
            tf.linalg.cholesky(_add_diagonal_shift(covariance, self.jitter)),
            is_non_singular=True,
            name='GaussianProcessScaleLinearOperator')
        return mvn_linear_operator.MultivariateNormalLinearOperator(
            loc=loc,
            scale=scale,
            validate_args=self._validate_args,
            allow_nan_stats=self._allow_nan_stats,
            name='marginal_distribution')
--------------------
Prompt: 
def _reset(self, index, total, percentage_step, length):
        """Resets to the progressbar to start a new one"""
----------
Output: 

        self.index = index
        self.total = total
        self.percentage_step = percentage_step
        self.length = length
        self.percentage = 0
        self.percentage_done = 0
        self.percentage_left = 0
        self.percentage_left_done = 0
        self.percentage_left_left = 0
        self.percentage_left_left_done = 0
        self.percentage_left_left_left = 0
        self.percentage_left
----------
Solution: 

        self._start_time = datetime.datetime.now()
        self._start_index = index
        self._current_index = index
        self._percentage_step = percentage_step
        self._total = float(total)
        self._total_minus_one = total - 1
        self._length = length
        self._norm_factor = total * percentage_step / 100.0
        self._current_interval = int((index + 1.0) / self._norm_factor)
--------------------
Prompt: 
def _swap_ops_from_edge(edge, layout):
    """Generate list of ops to implement a SWAP gate along a coupling edge."""
----------
Output: 

    if edge[0] == edge[1]:
        return []

    if layout[edge[0]] == layout[edge[1]]:
        return [
            {
                'name': 'cx',
                'targets': [edge[0], edge[1]],
            },
            {
                'name': 'cx',
                'targets': [edge[1], edge[0]],
            },
        ]

    if layout[edge[0]] == layout[edge[1] - 1]:
        return [
            {

----------
Solution: 


    device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q')
    qreg_edge = [(device_qreg, i) for i in edge]

    # TODO shouldn't be making other nodes not by the DAG!!
    return [
        DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'})
    ]
--------------------
Prompt: 
def frames(self, key=None, timeoutSecs=60, **kwargs):
    if not (key is None or isinstance(key, (basestring, Key))):
        raise Exception("frames: key should be string or Key type %s %s" % (type(key), key))

    params_dict = {
        'find_compatible_models': 0,
        'row_offset': 0, # is offset working yet?
        'row_count': 5,
    }
    """
    Return a single Frame or all of the Frames in the h2o cluster.  The
    frames are contained in a list called "frames" at the top level of the
    result.  Currently the list is unordered.
    TODO:
    When find_compatible_models is implemented then the top level 
    dict will also contain a "models" list.
    """
----------
Output: 
list of Frames.

    :param key: (optional) The key of the Frame to be returned.
    :param timeoutSecs: (optional) How long to wait for the Frame to be ready.
    :param kwargs: (optional) Additional arguments to pass to the Frame.
    :return: A single Frame or list of Frames.
    """
    if key is None:
        key = Key.make_frames(kwargs.get('frame_id'))

    if isinstance(key, basestring):
        key = Key.make(
----------
Solution: 

    h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'frames', False)
    
    # key can be type Key? (from h2o_xl) str(key) should return
    if key:
        if isinstance(key, Key):
            keyStr = key.frame
        else:
            keyStr = key
        result = self.do_json_request('3/Frames.json/' + keyStr, timeout=timeoutSecs, params=params_dict)
    else:
        result = self.do_json_request('3/Frames.json', timeout=timeoutSecs, params=params_dict)
    return result
--------------------
Prompt: 
def queryGitHubFromFile(self, filePath, gitvars={}, verbosity=0, **kwargs):
        """Submit a GitHub GraphQL query from a file.

        Can only be used with GraphQL queries.
        For REST queries, see the 'queryGitHub' method.

        Args:
            filePath (str): A relative or absolute path to a file containing
                a GraphQL query.
                File may use comments and multi-line formatting.
                .. _GitHub GraphQL Explorer:
                   https://developer.github.com/v4/explorer/
            gitvars (Optional[Dict]): All query variables.
                Defaults to empty.
                GraphQL Only.
            verbosity (Optional[int]): Changes output verbosity levels.
                If < 0, all extra printouts are suppressed.
                If == 0, normal print statements are displayed.
                If > 0, additional status print statements are displayed.
                Defaults to 0.
            **kwargs: Keyword arguments for the 'queryGitHub' method.

        Returns:
            Dict: A JSON style dictionary.

        """
----------
Output: 
v4/explorer/
            gitvars (dict): A dictionary of GitHub variables.
                See the GitHub GraphQL Explorer for more information.
            verbosity (int): The verbosity level.

        Returns:
            dict: A dictionary of the query response.
        """
        query = self.readFile(filePath)
        return self.queryGitHub(query, gitvars, verbosity, **kwargs)

    def queryGitHub(self, query, gitvars={}, verbosity=0, **kwargs):
        """Submit a GitHub
----------
Solution: 

        gitquery = self._readGQL(filePath, verbose=(verbosity >= 0))
        return self.queryGitHub(gitquery, gitvars=gitvars, verbosity=verbosity, **kwargs)
--------------------
Prompt: 
def get_revision(self, location):
        """
        Return the maximum revision for all files under a given location
        """
----------
Output: 

        if not self.is_repository_root(location):
            raise NotImplementedError

        # Get the list of files in the repository
        files = self.get_list(location)

        # Get the maximum revision for each file
        max_revision = 0
        for file in files:
            revision = self.get_revision(file)
            if revision > max_revision:
                max_revision = revision

        return max_revision

    def get_revision(self, location):
        """
----------
Solution: 

        # Note: taken from setuptools.command.egg_info
        revision = 0

        for base, dirs, files in os.walk(location):
            if self.dirname not in dirs:
                dirs[:] = []
                continue    # no sense walking uncontrolled subdirs
            dirs.remove(self.dirname)
            entries_fn = os.path.join(base, self.dirname, 'entries')
            if not os.path.exists(entries_fn):
                # FIXME: should we warn?
                continue

            dirurl, localrev = self._get_svn_url_rev(base)

            if base == location:
                base_url = dirurl + '/'   # save the root url
            elif not dirurl or not dirurl.startswith(base_url):
                dirs[:] = []
                continue    # not part of the same svn tree, skip it
            revision = max(revision, localrev)
        return revision
--------------------
Prompt: 
def scientific_notation_elements(self, value, locale):
        """ Returns normalized scientific notation components of a value."""
----------
Output: 

        if value is None:
            return None, None, None
        if locale is None:
            locale = self.locale
        if locale is None:
            locale = 'en'
        try:
            value = locale.format_string(value, type='n')
        except:
            pass
        match = re.match(r'^([+-]?\d+(?:\.\d*)?)([eE])([+-]?\d+)$', value)
        if match is None:
            return None, None, None

----------
Solution: 

        # Normalize value to only have one lead digit.
        exp = value.adjusted()
        value = value * get_decimal_quantum(exp)
        assert value.adjusted() == 0

        # Shift exponent and value by the minimum number of leading digits
        # imposed by the rendering pattern. And always make that number
        # greater or equal to 1.
        lead_shift = max([1, min(self.int_prec)]) - 1
        exp = exp - lead_shift
        value = value * get_decimal_quantum(-lead_shift)

        # Get exponent sign symbol.
        exp_sign = ''
        if exp < 0:
            exp_sign = babel.numbers.get_minus_sign_symbol(locale)
        elif self.exp_plus:
            exp_sign = babel.numbers.get_plus_sign_symbol(locale)

        # Normalize exponent value now that we have the sign.
        exp = abs(exp)

        return value, exp, exp_sign
--------------------
Prompt: 
def difflag1(self):
        """
        Conduct a diff-1 transform on a numeric frame column.

        :returns: an H2OFrame where each element is equal to the corresponding element in the source
            frame minus the previous-row element in the same frame.
        """
----------
Output: 

        return H2OFrame._expr(ExprNode("diff1", self))

    def difflag2(self):
        """
        Conduct a diff-2 transform on a numeric frame column.

        :returns: an H2OFrame where each element is equal to the corresponding element in the source
            frame minus the previous-row element in the same frame.
        """
        return H2OFrame._expr(ExprNode("diff2", self))

    def difflag3(self):
        """
        Conduct a diff-3 transform on a
----------
Solution: 

        if self.ncols > 1:
            raise H2OValueError("Only single-column frames supported")
        if self.types[self.columns[0]] not in {"real", "int", "bool"}:
            raise H2OValueError("Numeric column expected")
        fr = H2OFrame._expr(expr=ExprNode("difflag1", self), cache=self._ex._cache)
        return fr
--------------------
Prompt: 
async def _run_spider_hook(self, hook_func):
        """
        Run hook before/after spider start crawling
        :param hook_func: aws function
        :return:
        """
----------
Output: 

        if hook_func:
            hook_func(self.spider)

    def _run_spider_start_requests(self):
        """
        Run spider start requests
        :return:
        """
        for request in self.spider.start_requests():
            self.crawler.engine.crawl(request, spider=self.spider)

    def _run_spider_close(self):
        """
        Run spider close
        :return:
        """
        self.spider.
----------
Solution: 

        if callable(hook_func):
            try:
                aws_hook_func = hook_func(weakref.proxy(self))
                if isawaitable(aws_hook_func):
                    await aws_hook_func
            except Exception as e:
                self.logger.error(f'<Hook {hook_func.__name__}: {e}')
--------------------
Prompt: 
def save(self, name=None, output='png', dirc=None):
        """Saves Bloch sphere to file of type ``format`` in directory ``dirc``.
        Args:
            name (str):
                Name of saved image. Must include path and format as well.
                i.e. '/Users/Paul/Desktop/bloch.png'
                This overrides the 'format' and 'dirc' arguments.
            output (str):
                Format of output image.
            dirc (str):
                Directory for output images. Defaults to current working directory.
        """
----------
Output: 

        """
        if name is None:
            if dirc is None:
                dirc = os.getcwd()
            name = os.path.join(dirc, 'bloch.' + output)
        plt.savefig(name, format=output, dpi=300)

    def _get_axes(self):
        """Returns the axes of the Bloch sphere.
        """
        return self._axes

    def _get_points(self):
        """Returns the points of the Bloch sphere.
----------
Solution: 


        self.render()
        if dirc:
            if not os.path.isdir(os.getcwd() + "/" + str(dirc)):
                os.makedirs(os.getcwd() + "/" + str(dirc))
        if name is None:
            if dirc:
                self.fig.savefig(os.getcwd() + "/" + str(dirc) + '/bloch_' +
                                 str(self.savenum) + '.' + output)
            else:
                self.fig.savefig(os.getcwd() + '/bloch_' + str(self.savenum) +
                                 '.' + output)
        else:
            self.fig.savefig(name)
        self.savenum += 1
        if self.fig:
            plt.close(self.fig)
--------------------
Prompt: 
def rejester_run(work_unit):
     """get a rejester.WorkUnit with KBA s3 path, fetch it, and save
     some counts about it.
     """
----------
Output: 

     # get the work unit
     work_unit = work_unit.get()
     # get the s3 path
     s3_path = work_unit.get_s3_path()
     # get the s3 bucket
     s3_bucket = work_unit.get_s3_bucket()
     # get the s3 key
     s3_key = work_unit.get_s3_key()
     # get the s3 object
     s3_object = s3_bucket.get_key(s3_key)
    
----------
Solution: 

     #fname = 'verify-chunks-%d-%d' % (os.getpid(), time.time())
     fname = work_unit.key.strip().split('/')[-1]
     
     output_dir_path = work_unit.data.get('output_dir_path', '/mnt')
     u = uuid.uuid3(uuid.UUID(int=0), work_unit.key.strip())
     path1 = u.hex[0]
     path2 = u.hex[1]
     fpath = os.path.join(output_dir_path, path1, path2, fname)
     if not os.path.exists(os.path.dirname(fpath)):
          os.makedirs(os.path.dirname(fpath))

     output = gzip.open(fpath + '-out.gz', 'wb')

     expected_si_count = int(fname.split('-')[1])

     max_tries = 20
     tries = 0
     while tries < max_tries:
          try:
               exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, stream_ids = \
                   attempt_fetch(work_unit, fpath)
               if si_count != expected_si_count:
                    print 'retrying because si_count = %d != %d expected_si_count' % (si_count, expected_si_count)
                    sys.stdout.flush()
                    tries += 1
                    continue
               else:
                    print 'succeeded in reading si_count = %d' % (si_count,)
                    sys.stdout.flush()
               output.write( '%s\t%d\t%d\t%d\t%d\t%s\t%s\n' % (
                         exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, 
                         work_unit.key.strip(), ','.join(['%s|%s' % tup for tup in stream_ids])) )
               break
          except Exception, exc:
               print 'broken?'
               print traceback.format_exc(exc)
               sys.stdout.flush()
               tries += 1
               output.write(traceback.format_exc(exc))

     output.close()
--------------------
Prompt: 
def get_arguments(self):
        """
        Extracts the specific arguments of this CLI
        """
----------
Output: 

        parser = argparse.ArgumentParser(
            description="A CLI for the Cisco DNA Center API",
            prog="dna_center",
            add_help=True,
            formatter_class=argparse.RawTextHelpFormatter,
        )
        parser.add_argument(
            "--version",
            action="version",
            version="%(prog)s (version {version})".format(version=__version__),
        )
        parser.add_argument(
            "--host",
            help="The hostname
----------
Solution: 


        AlarmModify.get_arguments(self)
        self._alarm_id = self.args.alarm_id if self.args.alarm_id is not None else None
        self.get_api_parameters()
--------------------
Prompt: 
def html_tags_for(self, asset_type, *args, **kwargs):
        """Return html tags for urls of asset_type
        """
----------
Output: 

        if asset_type not in self.asset_types:
            raise ValueError('asset_type must be one of %s' % self.asset_types)
        return self.html_tags_for_asset_type(asset_type, *args, **kwargs)

    def html_tags_for_asset_type(self, asset_type, *args, **kwargs):
        """Return html tags for urls of asset_type
        """
        raise NotImplementedError

    def html_tags_for_asset_type_with_id
----------
Solution: 

        html = []
        for ref in self.depends:
            html.append(self._ref(ref).html_tags_for(asset_type, *args, **kwargs))
        if asset_type in self.typed_bundles:
            html.append(render_asset_html_tags(asset_type, self.urls_for_self(asset_type, *args, **kwargs)))
        return "\n".join(html)
--------------------
Prompt: 
def read_key(self, key, bucket_name=None):
        """
        Reads a key from S3

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        """
----------
Output: 

        if bucket_name is None:
            bucket_name = self.bucket_name

        try:
            response = self.client.get_object(Bucket=bucket_name, Key=key)
            return response['Body'].read()
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                raise KeyError(f'Key {key} does not exist')
            else:
                raise e

    def write_key(self, key, data, bucket_name=None):
----------
Solution: 


        obj = self.get_key(key, bucket_name)
        return obj.get()['Body'].read().decode('utf-8')
--------------------
Prompt: 
def _map_type_to_dict(self, type_name):
        """ Maps a an instance type representation string (e.g. 'RESULT')
        to the corresponding dictionary in root.

        """
----------
Output: 

        return self._root[type_name]

    def _map_type_to_list(self, type_name):
        """ Maps a an instance type representation string (e.g. 'RESULT')
        to the corresponding list in root.

        """
        return self._root[type_name]

    def _map_type_to_list_of_dicts(self, type_name):
        """ Maps a an instance type representation string (e.g. 'RESULT')
        to the corresponding list of dictionaries in root.
----------
Solution: 

        root = self._root_instance

        if type_name == RESULT:
            return root._results
        elif type_name == PARAMETER:
            return root._parameters
        elif type_name == DERIVED_PARAMETER:
            return root._derived_parameters
        elif type_name == CONFIG:
            return root._config
        elif type_name == LEAF:
            return root._other_leaves
        else:
            raise RuntimeError('You shall not pass!')
--------------------
Prompt: 
def create(self, public=False, **kwargs):
        """Creates the device. Attempts to create private devices by default,
        but if public is set to true, creates public devices.

        You can also set other default properties by passing in the relevant information.
        For example, setting a device with the given nickname and description::

            dev.create(nickname="mydevice", description="This is an example")

        Furthermore, ConnectorDB supports creation of a device's streams immediately,
        which can considerably speed up device setup::

            dev.create(streams={
                "stream1": {"schema": '{\"type\":\"number\"}'}
            })

        Note that the schema must be encoded as a string when creating in this format.
        """
----------
Output: 


            dev.create(streams=[
                {
                    "name": "temperature",
                    "type": "float",
                    "description": "Temperature in degrees celsius"
                },
                {
                    "name": "humidity",
                    "type": "float",
                    "description": "Humidity in percent"
                }
            ])

        :param public: Whether to create a public device or not.
        :type public: bool
        :param kwargs: Any other properties to
----------
Solution: 

        kwargs["public"] = public
        self.metadata = self.db.create(self.path, kwargs).json()
--------------------
Prompt: 
def _check_init(self, node):
        """check that the __init__ method call super or ancestors'__init__
        method
        """
----------
Output: 

        if node.name == '__init__':
            if not self._check_super_init(node):
                self.add_message('super-init-not-called', node=node)

    def _check_super_init(self, node):
        """check that the __init__ method call super or ancestors'__init__
        method
        """
        if node.args.args:
            for arg in node.args.args:
                if arg.name == 'self':
                    break
            else:
                return False
----------
Solution: 

        if not self.linter.is_message_enabled(
            "super-init-not-called"
        ) and not self.linter.is_message_enabled("non-parent-init-called"):
            return
        klass_node = node.parent.frame()
        to_call = _ancestors_to_call(klass_node)
        not_called_yet = dict(to_call)
        for stmt in node.nodes_of_class(astroid.Call):
            expr = stmt.func
            if not isinstance(expr, astroid.Attribute) or expr.attrname != "__init__":
                continue
            # skip the test if using super
            if (
                isinstance(expr.expr, astroid.Call)
                and isinstance(expr.expr.func, astroid.Name)
                and expr.expr.func.name == "super"
            ):
                return
            try:
                for klass in expr.expr.infer():
                    if klass is astroid.Uninferable:
                        continue
                    # The infered klass can be super(), which was
                    # assigned to a variable and the `__init__`
                    # was called later.
                    #
                    # base = super()
                    # base.__init__(...)

                    if (
                        isinstance(klass, astroid.Instance)
                        and isinstance(klass._proxied, astroid.ClassDef)
                        and is_builtin_object(klass._proxied)
                        and klass._proxied.name == "super"
                    ):
                        return
                    if isinstance(klass, objects.Super):
                        return
                    try:
                        del not_called_yet[klass]
                    except KeyError:
                        if klass not in to_call:
                            self.add_message(
                                "non-parent-init-called", node=expr, args=klass.name
                            )
            except astroid.InferenceError:
                continue
        for klass, method in not_called_yet.items():
            cls = node_frame_class(method)
            if klass.name == "object" or (cls and cls.name == "object"):
                continue
            self.add_message("super-init-not-called", args=klass.name, node=node)
--------------------
Prompt: 
def analyse(self, traj, network, current_subrun, subrun_list, network_dict):
        """Calculates average Fano Factor of a network.

        :param traj:

            Trajectory container

            Expects:

            `results.monitors.spikes_e`: Data from SpikeMonitor for excitatory neurons

            Adds:

            `results.statistics.mean_fano_factor`: Average Fano Factor

        :param network:

            The BRIAN network

        :param current_subrun:

            BrianParameter

        :param subrun_list:

            Upcoming subruns, analysis is only performed if subruns is empty,
            aka the final subrun has finished.

        :param network_dict:

            Dictionary of items shared among componetns

        """
----------
Output: 
network

        :param current_subrun:

            The current subrun

        :param subrun_list:

            List of subruns

        :param network_dict:

            Dictionary of network parameters

        :return:

            None

        """

        # Get spike times
        spikes_e = traj.get("results.monitors.spikes_e")

        # Get spike times for each subrun
        spikes_e_subrun = []
        for subrun in subrun_
----------
Solution: 

        #Check if we finished all subruns
        if len(subrun_list)==0:
            spikes_e = traj.results.monitors.spikes_e

            time_window = traj.parameters.analysis.statistics.time_window
            start_time = traj.parameters.simulation.durations.initial_run
            end_time = start_time+traj.parameters.simulation.durations.measurement_run
            neuron_ids = traj.parameters.analysis.statistics.neuron_ids

            mean_ff = self._compute_mean_fano_factor(
                neuron_ids, spikes_e, time_window, start_time, end_time)

            traj.f_add_result('statistics.mean_fano_factor', mean_ff, comment='Average Fano '
                                                                      'Factor over all '
                                                                      'exc neurons')

            print('R_ee: %f, Mean FF: %f' % (traj.R_ee, mean_ff))
--------------------
Prompt: 
def _get_properties(config):
    """Returns a tuple of property classes and names.

    Property classes are fully qualified, such as 'abc.abstractproperty' and
    property names are the actual names, such as 'abstract_property'.
    """
----------
Output: 

    properties = []
    for name, value in config.items():
        if name.startswith('property_'):
            properties.append((value, name[9:]))
    return properties


def _get_property_classes(config):
    """Returns a tuple of property classes.

    Property classes are fully qualified, such as 'abc.abstractproperty' and
    property names are the actual names, such as 'abstract_property'.
    """
    properties = []
    for name, value in config.items():
        if name.startswith
----------
Solution: 

    property_classes = {BUILTIN_PROPERTY}
    property_names = set()  # Not returning 'property', it has its own check.
    if config is not None:
        property_classes.update(config.property_classes)
        property_names.update(
            (prop.rsplit(".", 1)[-1] for prop in config.property_classes)
        )
    return property_classes, property_names
--------------------
Prompt: 
def _call(self, utterances_batch: list, utterances_ids: Optional[list]=None) -> list:
        """
        Processes batch of utterances and returns corresponding responses batch.

        Each call of Agent passes incoming utterances batch through skills filter,
        agent skills, skills processor. Batch of dialog IDs can be provided, in
        other case utterances indexes in incoming batch are used as dialog IDs.

        Args:
            utterances_batch: Batch of incoming utterances.
            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.

        Returns:
            responses: A batch of responses corresponding to the
                utterance batch received by agent.
        """
----------
Output: 
to utterances.

        Returns:
            Batch of responses.
        """
        if utterances_ids is None:
            utterances_ids = [i for i in range(len(utterances_batch))]

        # filter utterances by skills
        utterances_batch, utterances_ids = self.filter_utterances(utterances_batch, utterances_ids)

        # process utterances by skills
        utterances_batch, utterances_ids = self.process_utterances(utterances_batch, utterances_ids)


----------
Solution: 

        batch_size = len(utterances_batch)
        ids = utterances_ids or list(range(batch_size))
        batch_history = [self.history[utt_id] for utt_id in ids]
        responses = []

        filtered = self.skills_filter(utterances_batch, batch_history)

        for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)):
            skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter]

            if skill_i_utt_indexes:
                skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes]
                skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes]
                res = [(None, 0.)] * batch_size
                predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids)

                for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence):
                    res[i] = (predicted, confidence)

                responses.append(res)

        responses = self.skills_processor(utterances_batch, batch_history, *responses)

        return responses
--------------------
Prompt: 
def _add_label_from_dict(self, query_params=None):
        """
        Add a label to this card, from a dictionary.
        """
----------
Output: 

        if query_params is None:
            query_params = {}
        query_params['id'] = self.id
        query_params['label'] = self.label
        query_params['color'] = self.color
        query_params['name'] = self.name
        query_params['value'] = self.value
        query_params['description'] = self.description
        query_params['due'] = self.due
        query_params['dueComplete'] = self.dueComplete
        query_params['idBoard'] = self.idBoard
----------
Solution: 

        return self.fetch_json(
            uri_path=self.base_uri + '/labels',
            http_method='POST',
            query_params=query_params or {}
        )
--------------------
Prompt: 
def diff_lorenz(value_array, sigma, beta, rho):
    """The Lorenz attractor differential equation

    :param value_array: 3d array containing the x,y, and z component values.
    :param sigma: Constant attractor parameter
    :param beta: FConstant attractor parameter
    :param rho: Constant attractor parameter

    :return: 3d array of the Lorenz system evaluated at `value_array`

    """
----------
Output: 

    x, y, z = value_array
    dx = sigma * (y - x)
    dy = x * (rho - z) - y
    dz = x * y - beta * z
    return np.array([dx, dy, dz])


def lorenz_system(x0, sigma, beta, rho, t_max, dt, n_steps):
    """The Lorenz attractor differential equation

    :param x0: 3d array containing the x,y, and z component values.
    :param
----------
Solution: 

    diff_array = np.zeros(3)
    diff_array[0] = sigma * (value_array[1]-value_array[0])
    diff_array[1] = value_array[0] * (rho - value_array[2]) - value_array[1]
    diff_array[2] = value_array[0] * value_array[1] - beta * value_array[2]

    return diff_array
--------------------
Prompt: 
def _onDeviceCommand(self, client, userdata, pahoMessage):
        """
        Internal callback for gateway command messages, parses source device from topic string and
        passes the information on to the registered device command callback
        """
----------
Output: 

        topic = pahoMessage.topic
        payload = pahoMessage.payload
        device = topic.split("/")[1]
        if device in self.devices:
            self.devices[device].onCommand(client, userdata, pahoMessage)
        else:
            print("Received command for unknown device: " + device)

    def _onDeviceStatus(self, client, userdata, pahoMessage):
        """
        Internal callback for gateway status messages, parses source device from topic string and
        passes the information on to
----------
Solution: 

        try:
            command = Command(pahoMessage, self._messageCodecs)
        except InvalidEventException as e:
            self.logger.critical(str(e))
        else:
            self.logger.debug("Received gateway command '%s'" % (command.command))
            if self.deviceCommandCallback:
                self.deviceCommandCallback(command)
--------------------
Prompt: 
def wait_next_block_factory(app, timeout=None):
    """Creates a `wait_next_block` function, that
    will wait `timeout` seconds (`None` = indefinitely)
    for a new block to appear.

    :param app: the app-instance the function should work for
    :param timeout: timeout in seconds
    """
----------
Output: 

    def wait_next_block(timeout=timeout):
        """Waits for a new block to appear.

        :param timeout: timeout in seconds
        """
        if timeout is None:
            timeout = 10000000000000000000000000000000000000000000000000000000000000000000000000000
----------
Solution: 


    chain = app.services.chain

    # setup new block callbacks and events
    new_block_evt = gevent.event.Event()

    def _on_new_block(app):
        log.DEV('new block mined')
        new_block_evt.set()
    chain.on_new_head_cbs.append(_on_new_block)

    def wait_next_block():
        bn = chain.chain.head.number
        chain.consensus_manager.log('waiting for new block', block=bn)
        new_block_evt.wait(timeout)
        new_block_evt.clear()
        if chain.chain.head.number > bn:
            chain.consensus_manager.log('new block event', block=chain.chain.head.number)
        elif chain.chain.head.number == bn:
            chain.consensus_manager.log('wait_next_block timed out', block=bn)

    return wait_next_block
--------------------
Prompt: 
def _set_affiliation(self, v, load=False):
        """
    Setter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_affiliation is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_affiliation() directly.
    """
----------
Output: 

        if not load:
            self._set_affiliation(v)
        else:
            self._affiliation = v

    def _get_affiliation(self):
        """
    Getter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)
    If this variable is read-only (config: false) in the
    source YANG file, then _get_affiliation is considered as a private
    method. Backends looking to populate this variable should
    do so via calling
----------
Solution: 

        if hasattr(v, "_utype"):
            v = v._utype(v)
        try:
            t = YANGDynClass(
                v,
                base=RestrictedClassType(
                    base_type=unicode,
                    restriction_type="dict_key",
                    restriction_arg={
                        u"napalm-star-wars:EMPIRE": {
                            "@namespace": u"https://napalm-yang.readthedocs.io/napalm-star-wars",
                            "@module": u"napalm-star-wars",
                        },
                        u"EMPIRE": {
                            "@namespace": u"https://napalm-yang.readthedocs.io/napalm-star-wars",
                            "@module": u"napalm-star-wars",
                        },
                        u"napalm-star-wars:REBEL_ALLIANCE": {
                            "@namespace": u"https://napalm-yang.readthedocs.io/napalm-star-wars",
                            "@module": u"napalm-star-wars",
                        },
                        u"REBEL_ALLIANCE": {
                            "@namespace": u"https://napalm-yang.readthedocs.io/napalm-star-wars",
                            "@module": u"napalm-star-wars",
                        },
                    },
                ),
                is_leaf=True,
                yang_name="affiliation",
                parent=self,
                path_helper=self._path_helper,
                extmethods=self._extmethods,
                register_paths=True,
                namespace="https://napalm-yang.readthedocs.io/napalm-star-wars",
                defining_module="napalm-star-wars",
                yang_type="identityref",
                is_config=True,
            )
        except (TypeError, ValueError):
            raise ValueError(
                {
                    "error-string": 
--------------------
Prompt: 
def run(self, cmd, start_opts=None, globals_=None, locals_=None):
        """ Run debugger on string `cmd' using builtin function eval
        and if that builtin exec.  Arguments `globals_' and `locals_'
        are the dictionaries to use for local and global variables. By
        default, the value of globals is globals(), the current global
        variables. If `locals_' is not given, it becomes a copy of
        `globals_'.

        Debugger.core.start settings are passed via optional
        dictionary `start_opts'. Overall debugger settings are in
        Debugger.settings which changed after an instance is created
        . Also see `run_eval' if what you want to run is an
        run_eval'able expression have that result returned and
        `run_call' if you want to debug function run_call.
        """
----------
Output: 
.run() is called to run the command.
        """
        if globals_ is None:
            globals_ = globals()
        if locals_ is None:
            locals_ = globals_.copy()
        self.runcmd(cmd, start_opts, globals_, locals_)

    def runcmd(self, cmd, start_opts=None, globals_=None, locals_=None):
        """ Run debugger on string `cmd' using builtin function eval
        and if that builtin exec.  Arguments `glob
----------
Solution: 

        if globals_ is None:
            globals_ = globals()
        if locals_ is None:
            locals_ = globals_
        if not isinstance(cmd, types.CodeType):
            self.eval_string = cmd
            cmd = cmd+'\n'
            pass
        retval = None
        self.core.start(start_opts)
        try:
            retval = eval(cmd, globals_, locals_)
        except SyntaxError:
            try:
                exec(cmd, globals_, locals_)
            except DebuggerQuit:
                pass
            except DebuggerQuit:
                pass
            pass
        except DebuggerQuit:
            pass
        finally:
            self.core.stop()
        return retval
--------------------
Prompt: 
def make_links_absolute(self, base_url=None, resolve_base_href=True,
                            handle_failures=None):
        """
        Make all links in the document absolute, given the
        ``base_url`` for the document (the full URL where the document
        came from), or if no ``base_url`` is given, then the ``.base_url``
        of the document.

        If ``resolve_base_href`` is true, then any ``<base href>``
        tags in the document are used *and* removed from the document.
        If it is false then any such tag is ignored.

        If ``handle_failures`` is None (default), a failure to process
        a URL will abort the processing.  If set to 'ignore', errors
        are ignored.  If set to 'discard', failing URLs will be removed.
        """
----------
Output: 
* the ``base_url`` is
        updated to the new ``href`` value.

        If ``handle_failures`` is true, then any links that fail to
        resolve are logged to the ``self.log`` list.

        If ``handle_failures`` is false, then any links that fail to
        resolve are ignored.

        If ``handle_failures`` is None, then any links that fail to
        resolve are logged to the ``self.log`` list, but the
        ``self.log`` list is not cleared before the links are

----------
Solution: 

        if base_url is None:
            base_url = self.base_url
            if base_url is None:
                raise TypeError(
                    "No base_url given, and the document has no base_url")
        if resolve_base_href:
            self.resolve_base_href()

        if handle_failures == 'ignore':
            def link_repl(href):
                try:
                    return urljoin(base_url, href)
                except ValueError:
                    return href
        elif handle_failures == 'discard':
            def link_repl(href):
                try:
                    return urljoin(base_url, href)
                except ValueError:
                    return None
        elif handle_failures is None:
            def link_repl(href):
                return urljoin(base_url, href)
        else:
            raise ValueError(
                "unexpected value for handle_failures: %r" % handle_failures)

        self.rewrite_links(link_repl)
--------------------
Prompt: 
def signin_card(card: SigninCard) -> Attachment:
        """
        Returns an attachment for a signin card. For channels that don't natively support signin cards an alternative
        message will be rendered. Will raise a TypeError if 'card' argument is not a SigninCard.
        :param card:
        :return:
        """
----------
Output: 

        if not isinstance(card, SigninCard):
            raise TypeError("'card' must be of type 'SigninCard'")

        return Attachment(
            content_type=CardConstants.content_type,
            content=card.to_dict(),
        )

    @staticmethod
    def thumbnail_card(card: ThumbnailCard) -> Attachment:
        """
        Returns an attachment for a thumbnail card. For channels that don't natively support thumbnail cards an
        alternative message will be rendered. Will raise a TypeError if
----------
Solution: 

        if not isinstance(card, SigninCard):
            raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of an SigninCard, '
                            'unable to prepare attachment.')

        return Attachment(content_type=CardFactory.content_types.signin_card,
                          content=card)
--------------------
Prompt: 
def clear(self):
        """
        Clear GDoc Spreadsheet by sending empty csv file.
        """
----------
Output: 

        self.send_csv_file(self.empty_csv_file)

    def send_csv_file(self, csv_file):
        """
        Send csv file to GDoc Spreadsheet.
        """
        self.logger.info("Sending csv file to GDoc Spreadsheet")
        self.logger.info(f"CSV file: {csv_file}")
        self.logger.info(f"GDoc Spreadsheet: {self.spreadsheet_id}")
        self.logger.info(f"GDoc Spreadsheet sheet: {
----------
Solution: 

        empty_file_path = os.path.join(self.temp_path, 'empty.csv')
        try:
            empty_file = open(empty_file_path, 'w')
            empty_file.write(',')
            empty_file.close()
        except IOError as e:
            raise PODocsError(e)

        self._upload_file_to_gdoc(empty_file_path, content_type='text/csv')

        os.remove(empty_file_path)
--------------------
Prompt: 
def add_item_to_basket(self, item, variant=VARIANT.MEDIUM, quantity=1):
        """
        Add an item to the current basket.

        :param Item item: Item from menu.
        :param int variant: Item SKU id. Ignored if the item is a side.
        :param int quantity: The quantity of item to be added.
        :return: A response having added an item to the current basket.
        :rtype: requests.Response
        """
----------
Output: 

        return self.post(
            'basket/add_item',
            data={
                'item_id': item.id,
                'variant': variant,
                'quantity': quantity
            }
        )

    def remove_item_from_basket(self, item, variant=VARIANT.MEDIUM, quantity=1):
        """
        Remove an item from the current basket.

        :param Item item: Item from menu.
        :param int variant: Item SKU id. Ignored if the item is a side.
----------
Solution: 

        item_type = item.type

        if item_type == 'Pizza':
            return self.add_pizza_to_basket(item, variant, quantity)
        elif item_type == 'Side':
            return self.add_side_to_basket(item, quantity)
        return None
--------------------
Prompt: 
def validate_expression(self, expression):
        """Validate an expression (may throw Exceptions)"""
----------
Output: 

        if expression is None:
            raise ValueError("expression is None")
        if not isinstance(expression, str):
            raise ValueError("expression is not a string")
        if not expression:
            raise ValueError("expression is empty")
        if not self.is_valid_expression(expression):
            raise ValueError("expression is invalid")

    def is_valid_expression(self, expression):
        """Check if an expression is valid"""
        if expression is None:
            return False
        if not isinstance(expression, str):

----------
Solution: 

        # return self.evaluate(expression, 0, 2)
        vars = set(self.get_column_names()) | set(self.variables.keys())
        funcs = set(expression_namespace.keys())
        return vaex.expresso.validate_expression(expression, vars, funcs)
--------------------
Prompt: 
def plot(self, fig=None):
        """Plot visualization of model prediction."""
----------
Output: 

        if fig is None:
            fig = plt.figure()

        ax = fig.add_subplot(111)
        ax.set_title("Model Prediction")
        ax.set_xlabel("x")
        ax.set_ylabel("y")
        ax.set_xlim(-1, 1)
        ax.set_ylim(-1, 1)
        ax.grid(True)

        x = np.linspace(-1, 1, 100)
        y = self.predict
----------
Solution: 

        if not fig:
            fig = plt.gcf()

        fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)
        ax = fig.add_subplot(1, 1, 1)

        sorted_overlapping_ports = sorted(
            [(p, self.results['port cycles'][p]) for p in self.machine['overlapping ports']],
            key=lambda x: x[1])

        yticks_labels = []
        yticks = []
        xticks_labels = []
        xticks = []

        # Plot configuration
        height = 0.9

        i = 0
        # T_OL
        colors = ([(254. / 255, 177. / 255., 178. / 255.)] +
                  [(255. / 255., 255. / 255., 255. / 255.)] * (len(sorted_overlapping_ports) - 1))
        for p, c in sorted_overlapping_ports:
            ax.barh(i, c, height, align='center', color=colors.pop(),
                    edgecolor=(0.5, 0.5, 0.5), linestyle='dashed')
            if i == len(sorted_overlapping_ports) - 1:
                ax.text(c / 2.0, i, '$T_\mathrm{OL}$', ha='center', va='center')
            yticks_labels.append(p)
            yticks.append(i)
            i += 1
        xticks.append(sorted_overlapping_ports[-1][1])
        xticks_labels.append('{:.1f}'.format(sorted_overlapping_ports[-1][1]))

        # T_nOL + memory transfers
        y = 0
        colors = [(187. / 255., 255 / 255., 188. / 255.)] * (len(self.results['cycles'])) + \
                 [(119. / 255, 194. / 255., 255. / 255.)]
        for k, v in [('nOL', self.results['T_nOL'])] + self.results['cycles']:
            ax.barh(i, v, height, y, align='center', color=colors.pop())
            ax.text(y + v / 2.0, i, '$T_\mathrm{' + k + '}$', ha='center', va='center')
            xticks.append(y + v)
            xticks_labels.append('{:.1f}'.format(y + v))
            y += v
        yticks_labels.append('LD')
        yticks.append(i)

        ax.tick_params(axis='y', which='both', left='off', right='off')
        ax.tick_params(axis='x', which='both', top='off')
        ax.set_xlabel('t [cy]')
        ax.set_ylabel('execution port')
        ax.set_yticks(yticks)
        ax.set_yticklabels(yticks_labels)
        ax.set_xticks(xticks)
        ax.set_xticklabels(xticks_labels, rotation='vertical')
        ax.xaxis.grid(alpha=0.7, linestyle='--')
        fig.savefig(self._args.ecm_plot)
--------------------
Prompt: 
def reduceCnf(cnf):
    """
    I just found a remarkably large bug in my SAT solver and found an
    interesting solution.
    Remove all b | -b
    (-b | b) & (b | -a) & (-b | a) & (a | -a)
    becomes
    (b | -a) & (-b | a)

    Remove all (-e) & (-e)
    (-e | a) & (-e | a) & (-e | a) & (-e | a)
    becomes
    (-e | a)
    (-b | b | c) becomes nothing, not (c)
    """
----------
Output: 
becomes
    (-e | a)
    """
    newCnf = []
    for clause in cnf:
        newClause = []
        for literal in clause:
            if literal[0] == '-':
                literal = literal[1:]
            if literal not in newClause:
                newClause.append(literal)
        newCnf.append(newClause)
    return newCnf

def reduceCnf2(cnf):
    """
    Remove all b | -b
    (-b
----------
Solution: 

    output = Cnf()
    for x in cnf.dis:
        dont_add = False
        for y in x:
            for z in x:
                if z == -y:
                    dont_add = True
                    break
            if dont_add: break
        if dont_add: continue
        # TODO: Is this necessary anymore? Probably not. Do statistical analysis.
        if x not in output.dis:
            output.dis |= frozenset([x])
    return output
--------------------
Prompt: 
def edit(self,index=None):
        """Edit a block.

        If no number is given, use the last block executed.

        This edits the in-memory copy of the demo, it does NOT modify the
        original source file.  If you want to do that, simply open the file in
        an editor and use reload() when you make changes to the file.  This
        method is meant to let you change a block during a demonstration for
        explanatory purposes, without damaging your original script."""
----------
Output: 

        if index is None:
            index = self.last_block
        if index is None:
            raise ValueError("No block to edit")
        if index < 0 or index >= len(self.blocks):
            raise ValueError("Block index out of range")
        self.blocks[index].edit()

    def reload(self):
        """Reload the demo from the original source file.

        This is useful if you have made changes to the demo source file.
        It will reload the original source file, and replace the in-
----------
Solution: 


        index = self._get_index(index)
        if index is None:
            return
        # decrease the index by one (unless we're at the very beginning), so
        # that the default demo.edit() call opens up the sblock we've last run
        if index>0:
            index -= 1

        filename = self.shell.mktempfile(self.src_blocks[index])
        self.shell.hooks.editor(filename,1)
        new_block = file_read(filename)
        # update the source and colored block
        self.src_blocks[index] = new_block
        self.src_blocks_colored[index] = self.ip_colorize(new_block)
        self.block_index = index
        # call to run with the newly edited index
        self()
--------------------
Prompt: 
def enum(*sequential, **named):
    """
    Build an enum statement
    """
----------
Output: 

    enums = dict(zip(sequential, range(len(sequential))), **named)
    return type('Enum', (), enums)


class Enum(object):
    """
    Enum class
    """
    def __init__(self, *args, **kwargs):
        self.args = args
        self.kwargs = kwargs

    def __repr__(self):
        return 'Enum({}, {})'.format(self.args, self.kwargs)

    def __str__(self):
        return 'Enum
----------
Solution: 

    #: build enums from parameter
    enums = dict(zip(sequential, range(len(sequential))), **named)
    enums['map'] = copy.copy(enums)
    #: build reverse mapping
    enums['rmap'] = {}
    for key, value in enums.items():
        if type(value) is int:
            enums['rmap'][value] = key
    return type('Enum', (), enums)
--------------------
Prompt: 
def check_part(state, name, part_msg, missing_msg=None, expand_msg=None):
    """Return child state with name part as its ast tree"""
----------
Output: 

    if name not in state.parts:
        if missing_msg is None:
            missing_msg = "missing part '%s'" % name
        raise ValueError(missing_msg)
    part = state.parts[name]
    if part.ast is None:
        if expand_msg is None:
            expand_msg = "expand part '%s'" % name
        raise ValueError(expand_msg)
    if part.ast.msg != part_msg:
        raise ValueError("part '%s' has wrong message '%s'" % (name,
----------
Solution: 


    if missing_msg is None:
        missing_msg = "Are you sure you defined the {{part}}? "
    if expand_msg is None:
        expand_msg = "Did you correctly specify the {{part}}? "

    if not part_msg:
        part_msg = name
    append_message = {"msg": expand_msg, "kwargs": {"part": part_msg}}

    has_part(state, name, missing_msg, append_message["kwargs"])

    stu_part = state.student_parts[name]
    sol_part = state.solution_parts[name]

    assert_ast(state, sol_part, append_message["kwargs"])

    return part_to_child(stu_part, sol_part, append_message, state)
--------------------
Prompt: 
def list_datasets(self, get_global_public):
        """
        Lists datasets in resources. Setting 'get_global_public' to 'True'
        will retrieve all public datasets in cloud. 'False' will get user's
        public datasets.

        Arguments:
            get_global_public (bool): True if user wants all public datasets in
                                      cloud. False if user wants only their
                                      public datasets.

        Returns:
            dict: Returns datasets in JSON format

        """
----------
Output: 

        if get_global_public:
            url = self.url + '/datasets'
        else:
            url = self.url + '/datasets/public'

        response = requests.get(url, headers=self.headers)
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(response.text)

    def get_dataset(self, dataset_id):
        """
        Gets dataset by id.

        Arguments:
            dataset_id (str
----------
Solution: 

        appending = ""
        if get_global_public:
            appending = "public"
        url = self.url() + "/resource/{}dataset/".format(appending)
        req = self.remote_utils.get_url(url)

        if req.status_code is not 200:
            raise RemoteDataNotFoundError('Could not find {}'.format(req.text))
        else:
            return req.json()
--------------------
Prompt: 
def is_known_type(self, type_name):
        """Check if type is known to the type system.

        Returns:
            bool: True if the type is a known instantiated simple type, False otherwise
        """
----------
Output: 

        return type_name in self.simple_types

    def get_type_name(self, type_name):
        """Get the type name of a type.

        Args:
            type_name (str): The type name to get the type name of.

        Returns:
            str: The type name of the type.
        """
        if type_name in self.simple_types:
            return type_name
        else:
            return self.simple_types[type_name]

    def get_type_info(
----------
Solution: 


        type_name = str(type_name)
        if type_name in self.known_types:
            return True

        return False
--------------------
Prompt: 
async def fetch_nearby(lat: float, long: float, limit: int = 10) -> Optional[List[Dict]]:
    """
    Gets wikipedia articles near a given set of coordinates.
    :raise ApiError: When there was an error connecting to the API.

    todo cache
    """
----------
Output: 

    params = {
        'action': 'query',
        'list': 'geosearch',
        'gsradius': '1000',
        'gscoord': f'{lat}|{long}',
        'gslimit': limit,
        'format': 'json',
        'gsprop': 'title|extracts',
        'gsdir': 'NW',
        'gsformat': 'json',
        'gsnamespace': '0',
        'gscontinue': ''
    }

    response = requests.get(WI
----------
Solution: 

    request_url = f"https://en.wikipedia.org/w/api.php?action=query" \
                  f"&list=geosearch" \
                  f"&gscoord={lat}%7C{long}" \
                  f"&gsradius=10000" \
                  f"&gslimit={limit}" \
                  f"&format=json"

    async with ClientSession() as session:
        try:
            async with session.get(request_url) as request:
                if request.status == 404:
                    return None
                data = (await request.json())["query"]["geosearch"]

        except ClientConnectionError as con_err:
            logger.debug(f"Could not connect to {con_err.host}")
            raise ApiError(f"Could not connect to {con_err.host}")
        except JSONDecodeError as dec_err:
            logger.error(f"Could not decode data: {dec_err}")
            raise ApiError(f"Could not decode data: {dec_err}")
        except KeyError:
            return None
        else:
            for location in data:
                location.pop("ns")
                location.pop("primary")
            return data
--------------------
Prompt: 
def create_symmetric_key(self, algorithm, length):
        """
        Create a symmetric key.

        Args:
            algorithm(CryptographicAlgorithm): An enumeration specifying the
                algorithm for which the created key will be compliant.
            length(int): The length of the key to be created. This value must
                be compliant with the constraints of the provided algorithm.

        Returns:
            dict: A dictionary containing the key data, with the following
                key/value fields:
                * value - the bytes of the key
                * format - a KeyFormatType enumeration for the bytes format

        Raises:
            InvalidField: Raised when the algorithm is unsupported or the
                length is incompatible with the algorithm.
            CryptographicFailure: Raised when the key generation process
                fails.

        Example:
            >>> engine = CryptographyEngine()
            >>> key = engine.create_symmetric_key(
            ...     CryptographicAlgorithm.AES, 256)
        """
----------
Output: 

                * algorithm - the algorithm for which the key is compliant
                * length - the length of the key
        """
        raise NotImplementedError()

    def create_asymmetric_key_pair(self, algorithm, length):
        """
        Create an asymmetric key pair.

        Args:
            algorithm(CryptographicAlgorithm): An enumeration specifying the
                algorithm for which the created key will be compliant.
            length(int): The length of the key to be created. This value must
                be compliant
----------
Solution: 

        if algorithm not in self._symmetric_key_algorithms.keys():
            raise exceptions.InvalidField(
                "The cryptographic algorithm {0} is not a supported symmetric "
                "key algorithm.".format(algorithm)
            )

        cryptography_algorithm = self._symmetric_key_algorithms.get(algorithm)

        if length not in cryptography_algorithm.key_sizes:
            raise exceptions.InvalidField(
                "The cryptographic length ({0}) is not valid for "
                "the cryptographic algorithm ({1}).".format(
                    length, algorithm.name
                )
            )

        self.logger.info(
            "Generating a {0} symmetric key with length: {1}".format(
                algorithm.name, length
            )
        )

        key_bytes = os.urandom(length // 8)
        try:
            cryptography_algorithm(key_bytes)
        except Exception as e:
            self.logger.exception(e)
            raise exceptions.CryptographicFailure(
                "Invalid bytes for the provided cryptographic algorithm.")

        return {'value': key_bytes, 'format': enums.KeyFormatType.RAW}
--------------------
Prompt: 
def _get_organisations(self):
        """
        :returns: list of organisations, sorted alphabetically
        :rtype: list(list(str))
        """
----------
Output: 

        organisations = []
        for org in self.organisations:
            organisations.append(org.get_name())
        organisations.sort()
        return organisations

    def _get_organisations_with_counts(self):
        """
        :returns: list of organisations, sorted alphabetically
        :rtype: list(list(str))
        """
        organisations = []
        for org in self.organisations:
            organisations.append([org.get_name(), org.get_count()])
----------
Solution: 

        organisations = []
        for child in self.vcard.getChildren():
            if child.name == "ORG":
                organisations.append(child.value)
        return sorted(organisations)
--------------------
Prompt: 
def clear(self):
        """
        Completely resets the database. This means that all information in
        the local cache and on disc will be erased.
        """
----------
Output: 

        self.db.clear()

    def get(self, key, default=None):
        """
        Returns the value for the given key. If the key is not present,
        the default value is returned.

        :param key: The key to look up.
        :param default: The default value to return if the key is not present.
        :return: The value for the given key.
        """
        return self.db.get(key, default)

    def set(self, key, value):
        """
        Sets
----------
Solution: 

        if not os.path.isdir(self.fdir):
            os.makedirs(self.fdir, exist_ok=True)
            return

        for f in os.listdir(self.fdir):
            del self[f]
--------------------
Prompt: 
def reset_selective(self, regex=None):
        """Clear selective variables from internal namespaces based on a
        specified regular expression.

        Parameters
        ----------
        regex : string or compiled pattern, optional
            A regular expression pattern that will be used in searching
            variable names in the users namespaces.
        """
----------
Output: 

        if regex is None:
            regex = '.*'
        elif not isinstance(regex, (str, bytes)):
            regex = regex.pattern
        names = [name for name in self.user_ns if re.match(regex, name)]
        for name in names:
            del self.user_ns[name]

    def reset_all(self):
        """Clear all variables from internal namespaces."""
        self.reset_user()
        self.reset_user_global()
        self.reset_user_ns()
       
----------
Solution: 

        if regex is not None:
            try:
                m = re.compile(regex)
            except TypeError:
                raise TypeError('regex must be a string or compiled pattern')
            # Search for keys in each namespace that match the given regex
            # If a match is found, delete the key/value pair.
            for ns in self.all_ns_refs:
                for var in ns:
                    if m.search(var):
                        del ns[var]
--------------------
Prompt: 
def load_from_path(path):
    """
    Load a spec from a given path, discovering specs if a directory is given.

    """
----------
Output: 

    if os.path.isdir(path):
        specs = []
        for root, dirs, files in os.walk(path):
            for name in files:
                if name.endswith('.spec'):
                    specs.append(os.path.join(root, name))
        if not specs:
            raise ValueError('No specs found in %s' % path)
        return specs
    elif os.path.isfile(path):
        return [path]
    else:
        raise ValueError('%s
----------
Solution: 


    if os.path.isdir(path):
        paths = discover(path)
    else:
        paths = [path]

    for path in paths:
        name = os.path.basename(os.path.splitext(path)[0])
        imp.load_source(name, path)
--------------------
Prompt: 
def _has_role(self, role_name_or_list):
        """
        Whether the user has this role name
        """
----------
Output: 

        if isinstance(role_name_or_list, list):
            return any(self.has_role(role_name) for role_name in role_name_or_list)
        else:
            return self.has_role(role_name_or_list)

    @property
    def is_admin(self):
        """
        Whether the user is an administrator
        """
        return self._has_role('admin')

    @property
    def is_active(self):
        """
        Whether the user is active
----------
Solution: 

        if not isinstance(role_name_or_list, list):
            role_name_or_list = [role_name_or_list]
        return any(
            [r.name in role_name_or_list for r in self.get_user_roles()])
--------------------
Prompt: 
def submit(self, executor, task, tag=None):
        """Submits a task to a provided executor

        :type executor: s3transfer.futures.BoundedExecutor
        :param executor: The executor to submit the callable to

        :type task: s3transfer.tasks.Task
        :param task: The task to submit to the executor

        :type tag: s3transfer.futures.TaskTag
        :param tag: A tag to associate to the submitted task

        :rtype: concurrent.futures.Future
        :returns: A future representing the submitted task
        """
----------
Output: 
:rtype: s3transfer.futures.Future
        :return: A future representing the submitted task
        """
        future = executor.submit(task.callable, *task.args, **task.kwargs)
        future.add_done_callback(self._done_callback)
        if tag is not None:
            future.add_tag(tag)
        return future

    def _done_callback(self, future):
        """Callback for when a future is done

        :type future: s3transfer.futures
----------
Solution: 

        logger.debug(
            "Submitting task %s to executor %s for transfer request: %s." % (
                task, executor, self.transfer_id)
        )
        future = executor.submit(task, tag=tag)
        # Add this created future to the list of associated future just
        # in case it is needed during cleanups.
        self.add_associated_future(future)
        future.add_done_callback(
            FunctionContainer(self.remove_associated_future, future))
        return future
--------------------
Prompt: 
def _get_streams(self):
        """
        Find the streams for OlympicChannel
        :return:
        """
----------
Output: 

        streams = HLSStream.parse_variant_playlist(self.session,
                                                   self.url)
        return streams


class OlympicChannel(Plugin):
    @classmethod
    def can_handle_url(cls, url):
        return _url_re.match(url)

    def _get_streams(self):
        match = _url_re.match(self.url)
        channel = match.group("channel")
        if channel == "olympic":
            return _OlympicStream.parse
----------
Solution: 

        match = self._url_re.match(self.url)
        type_of_stream = match.group('type')
        lang = re.search(r"/../", self.url).group(0)

        if type_of_stream == 'tv':
            path = re.search(r"tv/.*-\d/$", self.url).group(0)

            return self._get_live_streams(lang, path)
        elif type_of_stream == 'playback':
            path = re.search(r"/playback/.*/$", self.url).group(0)
            return self._get_vod_streams()
--------------------
Prompt: 
def compute_ffmc2d(X):
    """Computes the 2D-Fourier Magnitude Coefficients."""
----------
Output: 

    X = np.asarray(X)
    if X.ndim != 2:
        raise ValueError("X must be a 2D array.")
    X = np.fft.fft2(X)
    X = np.abs(X)
    return X


def compute_fftmc2d(X):
    """Computes the 2D-Fourier Magnitude Coefficients."""
    X = np.asarray(X)
    if X.ndim != 2:
        raise ValueError("
----------
Solution: 

    # 2d-fft
    fft2 = scipy.fftpack.fft2(X)

    # Magnitude
    fft2m = magnitude(fft2)

    # FFTshift and flatten
    fftshift = scipy.fftpack.fftshift(fft2m).flatten()

    #cmap = plt.cm.get_cmap('hot')
    #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation="nearest",
    #    aspect="auto", cmap=cmap)
    #plt.show()

    # Take out redundant components
    return fftshift[:fftshift.shape[0] // 2 + 1]
--------------------
Prompt: 
def prefilter_lines(self, lines, continue_prompt=False):
        """Prefilter multiple input lines of text.

        This is the main entry point for prefiltering multiple lines of
        input.  This simply calls :meth:`prefilter_line` for each line of
        input.

        This covers cases where there are multiple lines in the user entry,
        which is the case when the user goes back to a multiline history
        entry and presses enter.
        """
----------
Output: 

        for line in lines:
            line = self.prefilter_line(line, continue_prompt)
            if line is not None:
                yield line

    def prefilter_line(self, line, continue_prompt=False):
        """Prefilter a single input line of text.

        This is the main entry point for prefiltering a single line of input.
        This is called by :meth:`prefilter_lines` for each line of input.

        This covers cases where there is a single line of
----------
Solution: 

        llines = lines.rstrip('\n').split('\n')
        # We can get multiple lines in one shot, where multiline input 'blends'
        # into one line, in cases like recalling from the readline history
        # buffer.  We need to make sure that in such cases, we correctly
        # communicate downstream which line is first and which are continuation
        # ones.
        if len(llines) > 1:
            out = '\n'.join([self.prefilter_line(line, lnum>0)
                             for lnum, line in enumerate(llines) ])
        else:
            out = self.prefilter_line(llines[0], continue_prompt)

        return out
--------------------
Prompt: 
def get_service_certificate(self, service_name, thumbalgorithm, thumbprint):
        """
        Returns the public data for the specified X.509 certificate associated
        with a hosted service.

        service_name:
            Name of the hosted service.
        thumbalgorithm:
            The algorithm for the certificate's thumbprint.
        thumbprint:
            The hexadecimal representation of the thumbprint.
        """
----------
Output: 

        return self.get_certificate(service_name, thumbalgorithm, thumbprint)

    def get_certificate(self, service_name, thumbalgorithm, thumbprint):
        """
        Returns the public data for the specified X.509 certificate.

        service_name:
            Name of the hosted service.
        thumbalgorithm:
            The algorithm for the certificate's thumbprint.
        thumbprint:
            The hexadecimal representation of the thumbprint.
        """
        return self.get_resource(
            'certificates
----------
Solution: 

        _validate_not_none('service_name', service_name)
        _validate_not_none('thumbalgorithm', thumbalgorithm)
        _validate_not_none('thumbprint', thumbprint)
        return self._perform_get(
            '/' + self.subscription_id + '/services/hostedservices/' +
            _str(service_name) + '/certificates/' +
            _str(thumbalgorithm) + '-' + _str(thumbprint) + '',
            Certificate)
--------------------
Prompt: 
def get_default_args(func):
    """
    returns a dictionary of arg_name:default_values for the input function
    """
----------
Output: 

    argspec = inspect.getfullargspec(func)
    defaults = argspec.defaults
    if defaults is None:
        return {}
    else:
        return dict(zip(argspec.args[-len(defaults):], defaults))


def get_default_args_from_func(func):
    """
    returns a dictionary of arg_name:default_values for the input function
    """
    argspec = inspect.getfullargspec(func)
    defaults = argspec.defaults
    if defaults is None:

----------
Solution: 

    args, _, _, defaults, *rest = inspect.getfullargspec(func)
    return dict(zip(reversed(args), reversed(defaults)))
--------------------
Prompt: 
def index():
    """Display a list of all user institutes."""
----------
Output: 

    institutes = Institute.query.all()
    return render_template('institutes/index.html', institutes=institutes)


@bp.route('/new', methods=['GET', 'POST'])
@login_required
def new():
    """Create a new institute."""
    form = InstituteForm()
    if form.validate_on_submit():
        institute = Institute(name=form.name.data,
                              address=form.address.data,
                              city=form.city.data,
                             
----------
Solution: 

    institute_objs = user_institutes(store, current_user)
    institutes_count = ((institute_obj, store.cases(collaborator=institute_obj['_id']).count())
                        for institute_obj in institute_objs if institute_obj)
    return dict(institutes=institutes_count)
--------------------
Prompt: 
def _plotting(self, rank_metric, results, graph_num, outdir, 
                  format, figsize, pheno_pos='', pheno_neg=''):
        """ Plotting API.
            :param rank_metric: sorted pd.Series with rankings values.
            :param results: self.results
            :param data: preprocessed expression table

        """
----------
Output: 

        # plotting
        if self.plotting:
            # plotting
            if self.plotting == 'default':
                # plotting
                fig, ax = plt.subplots(figsize=figsize)
                ax.plot(rank_metric, color='black', linewidth=2)
                ax.set_xlabel('Genes')
                ax.set_ylabel('Ranking')
                ax.set_title('Ranking')
                fig.tight_layout()
                fig.savefig(os.path.join
----------
Solution: 

        
        # no values need to be returned
        if self._outdir is None: return
        #Plotting
        top_term = self.res2d.index[:graph_num]
        # multi-threading
        pool = Pool(self._processes)
        for gs in top_term:
            hit = results.get(gs)['hits_indices']
            NES = 'nes' if self.module != 'ssgsea' else 'es'
            term = gs.replace('/','_').replace(":","_")
            outfile = '{0}/{1}.{2}.{3}'.format(self.outdir, term, self.module, self.format)
            # gseaplot(rank_metric=rank_metric, term=term, hits_indices=hit,
            #           nes=results.get(gs)[NES], pval=results.get(gs)['pval'], 
            #           fdr=results.get(gs)['fdr'], RES=results.get(gs)['RES'],
            #           pheno_pos=pheno_pos, pheno_neg=pheno_neg, figsize=figsize,
            #           ofname=outfile)
            pool.apply_async(gseaplot, args=(rank_metric, term, hit, results.get(gs)[NES],
                                              results.get(gs)['pval'],results.get(gs)['fdr'],
                                              results.get(gs)['RES'],
                                              pheno_pos, pheno_neg, 
                                              figsize, 'seismic', outfile))
            if self.module == 'gsea':
                outfile2 = "{0}/{1}.heatmap.{2}".format(self.outdir, term, self.format)
                # heatmap(df=self.heatmat.iloc[hit, :], title=term, ofname=outfile2, 
                #         z_score=0, figsize=(self._width, len(hit)/2))
                pool.apply_async(heatmap, args=(self.heatmat.iloc[hit, :], 0, term, 
                                               (self._width, len(hit)/2+2), 'RdBu_r',
                                                True, True, outfile2))
        pool.close()
        pool.join()
--------------------
Prompt: 
def get(self, requestId):
        """
        Gets details of a device management request.
        It accepts requestId (string) as parameters
        In case of failure it throws APIException
        """
----------
Output: 

        try:
            response = self.api_client.get(
                self.api_path + '/' + requestId,
                None,
                self.transaction,
                self.configuration_attributes
            )
            return self.return_obj(response, requestId)
        except APIException as e:
            raise e

    def list(self, **kwargs):
        """
        Lists all device management requests.
        It accepts limit (integer) and offset (integer) as input parameters
        In case of failure it throws APIException
        """
----------
Solution: 

        url = MgmtRequests.mgmtSingleRequest % (requestId)
        r = self._apiClient.get(url)

        if r.status_code == 200:
            return r.json()
        else:
            raise ApiException(r)
--------------------
Prompt: 
def load(self, reload=False):
        """
        Load the config and defaults from files.
        """
----------
Output: 

        if not reload and self.loaded:
            return

        self.loaded = True

        # Load the config file
        if self.config_file is not None:
            self.load_config_file(self.config_file)

        # Load the defaults file
        if self.defaults_file is not None:
            self.load_defaults_file(self.defaults_file)

        # Load the environment variables
        self.load_env_vars()

        # Load the command line options
        self.load
----------
Solution: 

        if reload or not self._loaded:
            # load defaults
            if self._defaults_file and type(self._defaults_file) == str:
                self._defaults_file = File(self._defaults_file, parent=self._parent)
            defaults = {}
            if self._defaults_file:
                defaults = yaml.safe_load(self._defaults_file.read().replace('\t', '    '))

            # load data
            data = {}
            if self.exists:
                data = yaml.safe_load(self.read().replace('\t', '    '))

            # initialise with the loaded data
            self._defaults = defaults
            self._data = copy.deepcopy(self._defaults)
            self.update(data=data)

            # if specified, apply environment variables
            if self._apply_env:
                self.update(ConfigEnv(self._env_prefix))

            self._loaded = True

        return self
--------------------
Prompt: 
def walk_files(mgr):
    """
    Iterate over all files visible to ``mgr``.
    """
----------
Output: 

    for root, dirs, files in mgr.walk():
        for f in files:
            yield os.path.join(root, f)


def walk_dirs(mgr):
    """
    Iterate over all directories visible to ``mgr``.
    """
    for root, dirs, files in mgr.walk():
        for d in dirs:
            yield os.path.join(root, d)


def walk_dirs_and_files(mgr):
    """
    Iterate over all
----------
Solution: 

    for dir_, subdirs, files in walk_files(mgr):
        for file_ in files:
            yield file_
--------------------
Prompt: 
def data_types(self):
        """Return a list of data types."""
----------
Output: 

        return self._data_types

    @property
    def data_type_names(self):
        """Return a list of data type names."""
        return [d.name for d in self.data_types]

    @property
    def data_type_ids(self):
        """Return a list of data type ids."""
        return [d.id for d in self.data_types]

    @property
    def data_type_names_and_ids(self):
        """Return a list of data type names and ids
----------
Solution: 

        data = self.gencloud.project_data(self.id)
        return sorted(set(d.type for d in data))
--------------------
Prompt: 
def ProductsForm(category, products):
    """ Produces an appropriate _ProductsForm subclass for the given render
    type. """
----------
Output: 

    if category.render_type == 'select':
        return SelectProductsForm(category, products)
    elif category.render_type == 'checkbox':
        return CheckboxProductsForm(category, products)
    elif category.render_type == 'radio':
        return RadioProductsForm(category, products)
    else:
        raise ValueError("Unknown render type: %s" % category.render_type)


class _ProductsForm(Form):
    """ Base class for all product forms. """
    def __init__(self, category, products):

----------
Solution: 


    # Each Category.RENDER_TYPE value has a subclass here.
    cat = inventory.Category
    RENDER_TYPES = {
        cat.RENDER_TYPE_QUANTITY: _QuantityBoxProductsForm,
        cat.RENDER_TYPE_RADIO: _RadioButtonProductsForm,
        cat.RENDER_TYPE_ITEM_QUANTITY: _ItemQuantityProductsForm,
        cat.RENDER_TYPE_CHECKBOX: _CheckboxProductsForm,
    }

    # Produce a subclass of _ProductsForm which we can alter the base_fields on
    class ProductsForm(RENDER_TYPES[category.render_type]):
        pass

    products = list(products)
    products.sort(key=lambda prod: prod.order)

    ProductsForm.set_fields(category, products)

    if category.render_type == inventory.Category.RENDER_TYPE_ITEM_QUANTITY:
        ProductsForm = forms.formset_factory(
            ProductsForm,
            formset=_ItemQuantityProductsFormSet,
        )

    return ProductsForm
--------------------
Prompt: 
def chop(array, epsilon=1e-10):
    """
    Truncate small values of a complex array.

    Args:
        array (array_like): array to truncte small values.
        epsilon (float): threshold.

    Returns:
        np.array: A new operator with small values set to zero.
    """
----------
Output: 

    return np.array([complex(x.real, 0) if abs(x.imag) < epsilon else x for x in array])


def chop_real(array, epsilon=1e-10):
    """
    Truncate small values of a real array.

    Args:
        array (array_like): array to truncte small values.
        epsilon (float): threshold.

    Returns:
        np.array: A new operator with small values set to zero.
    """
    return np
----------
Solution: 

    ret = np.array(array)

    if np.isrealobj(ret):
        ret[abs(ret) < epsilon] = 0.0
    else:
        ret.real[abs(ret.real) < epsilon] = 0.0
        ret.imag[abs(ret.imag) < epsilon] = 0.0
    return ret
--------------------
Prompt: 
def make_clean_html(raw, stream_item=None, encoding=None):
    """Get a clean text representation of presumed HTML.

    Treat `raw` as though it is HTML, even if we have no idea what it
    really is, and attempt to get a properly formatted HTML document
    with all HTML-escaped characters converted to their unicode.

    This is called below by the `clean_html` transform stage, which
    interprets MIME-type.  If `character_encoding` is not provided,
    and `stream_item` is provided, then this falles back to
    :attr:`streamcorpus.StreamItem.body.encoding`.

    :param str raw: raw text to clean up
    :param stream_item: optional stream item with encoding metadata
    :type stream_item: :class:`streamcorpus.StreamItem`
    :returns: UTF-8-encoded byte string of cleaned HTML text
    :returntype: :class:`str`

    """
----------
Output: 
item` is not None, then we attempt to guess the
    encoding from the `Content-Type` header.

    """
    if stream_item is not None:
        if encoding is None:
            encoding = stream_item.get_encoding()
    if encoding is None:
        encoding = 'utf-8'
    try:
        raw = raw.decode(encoding)
    except UnicodeDecodeError:
        pass
    try:
        raw = raw.encode('utf-8')
    except UnicodeDecodeError:
        pass
    return
----------
Solution: 

    # Fix emails by protecting the <,> from HTML
    raw = fix_emails(raw)
    raw_decoded = nice_decode(raw, stream_item=stream_item, encoding=encoding)
    if raw_decoded is None:
        # give up on decoding it... maybe this should use force_unicode
        raw_decoded = raw

    # default attempt uses vanilla lxml.html
    try:
        root = lxml.html.document_fromstring(raw_decoded)
    except ValueError, exc:
        if 'with encoding declaration' in str(exc):
            root = lxml.html.document_fromstring(raw)
        else:
            raise

    # While we have the document parsed as a DOM, let's strip attributes.
    # (The HTML cleaner seems to only support whitelisting attributes.
    # As of now, we just want to blacklist a few.)
    lxml.etree.strip_attributes(root, 'class', 'id')

    # if that worked, then we will be able to generate a
    # valid HTML string
    fixed_html = lxml.html.tostring(root, encoding=unicode)

    # remove any ^M characters
    fixed_html = string.replace(fixed_html, '\r', ' ')

    # We drop utf8 characters that are above 0xFFFF as
    # Lingpipe seems to be doing the wrong thing with them.
    fixed_html = drop_invalid_and_upper_utf8_chars(fixed_html)

    # construct a Cleaner that removes any ``<script>`` tags,
    # Javascript, like an ``onclick`` attribute, comments, style
    # tags or attributes, ``<link>`` tags
    cleaner = lxml.html.clean.Cleaner(
        scripts=True, javascript=True,
        comments=True,
        # do not remove <html> <head> <title> etc
        page_structure=False,
        remove_tags=['base'],
        style=True, links=True)

    # now get the really sanitized HTML
    _clean_html = cleaner.clean_html(fixed_html)

    # generate pretty HTML in utf-8
    _clean_html = lxml.html.tostring(
        lxml.html.document_fromstring(_clean_html),
        method='html', encoding='utf-8',
        pretty_print=True,
        # include_meta_content_type=True
        )

    return uniform_html(_clean_html)
--------------------
Prompt: 
def func(self):
        """Alternative naming, you can use `node.func.name` instead of `node.f_func`"""
----------
Output: 

        return self.f_func

    def __str__(self):
        return self.f_func

    def __repr__(self):
        return self.f_func

    def __eq__(self, other):
        return self.f_func == other.f_func

    def __hash__(self):
        return hash(self.f_func)

    def __lt__(self, other):
        return self.f_func < other.f_func

    def __le__(self, other):
        return self
----------
Solution: 

        if self._func is None:
            self._func = NNTreeNodeFunc(self)
        return self._func
--------------------
Prompt: 
def load_java_messages_to_ignore():
    """
    Load in pickle file that contains dict structure with bad java messages to ignore per unit test
    or for all cases.  The ignored bad java info is stored in g_ok_java_messages dict.

    :return:
    """
----------
Output: 

    global g_ok_java_messages
    try:
        with open(os.path.join(os.path.dirname(__file__), 'java_messages_to_ignore.pickle'), 'rb') as f:
            g_ok_java_messages = pickle.load(f)
    except Exception as e:
        print("Error loading java messages to ignore: " + str(e))
        g_ok_java_messages = {}


def load_java_messages_to_ignore_for_unit_test(unit_test_
----------
Solution: 

    global g_ok_java_messages
    global g_java_message_pickle_filename

    if os.path.isfile(g_java_message_pickle_filename):
        with open(g_java_message_pickle_filename,'rb') as tfile:
            g_ok_java_messages = pickle.load(tfile)
    else:
        g_ok_java_messages["general"] = []
--------------------
Prompt: 
def rowCount(self, parent):
        """Return number of children *parent* index has."""
----------
Output: 

        return len(self.data[parent])

    def columnCount(self, parent):
        """Return number of columns *parent* index has."""
        return len(self.data[0])

    def data(self, index, role):
        """Return data for *index* and *role*."""
        if not index.isValid():
            return None
        if role != Qt.DisplayRole:
            return None
        return self.data[index.row()][index.column()]

    def headerData(self, section
----------
Solution: 

        if parent.column() > 0:
            return 0

        if parent.isValid():
            item = parent.internalPointer()
        else:
            item = self.root

        return len(item.children)
--------------------
Prompt: 
def sh(cmd, escape=True):
    """ Executes the given command.
    returns a 2-tuple with returncode (integer) and OUTPUT (string)
    """
----------
Output: 

    if escape:
        cmd = " ".join(map(pipes.quote, cmd))
    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    out, _ = p.communicate()
    return p.returncode, out


def sh_stdout(cmd, escape=True):
    """ Executes the given command.
    returns the output (string)
    """
    if escape:
        cmd = " ".join(map(pipes.
----------
Solution: 


    if escape:
        cmd = quote(cmd)

    process = Popen(cmd, stdout=PIPE, stderr=STDOUT, shell=True)
    output, unused_err = process.communicate()
    retcode = process.poll()

    return (retcode, output)
--------------------
Prompt: 
def abort(self, jobs=None, targets=None, block=None):
        """Abort specific jobs from the execution queues of target(s).

        This is a mechanism to prevent jobs that have already been submitted
        from executing.

        Parameters
        ----------

        jobs : msg_id, list of msg_ids, or AsyncResult
            The jobs to be aborted
            
            If unspecified/None: abort all outstanding jobs.

        """
----------
Output: 

        if jobs is None:
            jobs = []
        elif isinstance(jobs, AsyncResult):
            jobs = [jobs]
        elif isinstance(jobs, list):
            if not all(isinstance(job, AsyncResult) for job in jobs):
                raise TypeError("jobs must be a list of AsyncResults")
        elif isinstance(jobs, str):
            jobs = [jobs]
        else:
            raise TypeError("jobs must be a msg_id, list of msg_ids, or AsyncResult")

        if
----------
Solution: 

        block = self.block if block is None else block
        jobs = jobs if jobs is not None else list(self.outstanding)
        targets = self._build_targets(targets)[0]
        
        msg_ids = []
        if isinstance(jobs, (basestring,AsyncResult)):
            jobs = [jobs]
        bad_ids = filter(lambda obj: not isinstance(obj, (basestring, AsyncResult)), jobs)
        if bad_ids:
            raise TypeError("Invalid msg_id type %r, expected str or AsyncResult"%bad_ids[0])
        for j in jobs:
            if isinstance(j, AsyncResult):
                msg_ids.extend(j.msg_ids)
            else:
                msg_ids.append(j)
        content = dict(msg_ids=msg_ids)
        for t in targets:
            self.session.send(self._control_socket, 'abort_request',
                    content=content, ident=t)
        error = False
        if block:
            self._flush_ignored_control()
            for i in range(len(targets)):
                idents,msg = self.session.recv(self._control_socket,0)
                if self.debug:
                    pprint(msg)
                if msg['content']['status'] != 'ok':
                    error = self._unwrap_exception(msg['content'])
        else:
            self._ignored_control_replies += len(targets)
        if error:
            raise error
--------------------
Prompt: 
def format_execution_time(self, start=None, end=None):
        """
        Format the calculated time into a human readable format.

        :param start: A starting time.
        :type start: int|str

        :param stop: A ending time.
        :type stop: int|str

        :return: A human readable date.
        :rtype: str
        """
----------
Output: 

        if start is None:
            start = self.start
        if end is None:
            end = self.end

        if isinstance(start, str):
            start = int(start)
        if isinstance(end, str):
            end = int(end)

        if start is None or end is None:
            return None

        if start > end:
            start, end = end, start

        seconds = end - start
        days = seconds // 86400
        seconds -= days * 864
----------
Solution: 


        # We return the formatted execution time.
        return ":".join(list(self._calculate(start, end).values()))
--------------------
Prompt: 
def _all_load_skeleton(self, traj_node, hdf5_group):
        """Reloads skeleton data of a tree node"""
----------
Output: 

        # Load the skeleton
        skeleton_group = hdf5_group['skeleton']
        self.skeleton = Skeleton(skeleton_group)
        # Load the skeleton tree
        self.skeleton_tree = SkeletonTree(skeleton_group)
        # Load the skeleton tree
        self.skeleton_tree = SkeletonTree(skeleton_group)
        # Load the skeleton tree
        self.skeleton_tree = SkeletonTree(skeleton_group)
        # Load the skeleton tree
----------
Solution: 

        if traj_node.v_annotations.f_is_empty():
            self._ann_load_annotations(traj_node, hdf5_group)
        if traj_node.v_comment == '':
            comment = self._all_get_from_attrs(hdf5_group, HDF5StorageService.COMMENT)
            if comment is None:
                comment = ''
            traj_node.v_comment = comment
--------------------
Prompt: 
def overlapping(self, variant_obj):
        """Return overlapping variants.

        Look at the genes that a variant overlaps to.
        Then return all variants that overlap these genes.

        If variant_obj is sv it will return the overlapping snvs and oposite
        There is a problem when SVs are huge since there are to many overlapping variants.

        Args:
            variant_obj(dict)

        Returns:
            variants(iterable(dict))
        """
----------
Output: 

        variants = []
        if variant_obj["type"] == "sv":
            # get all overlapping variants
            variants = self.get_overlapping_variants(variant_obj)
            # get all overlapping snvs
            variants += self.get_overlapping_snvs(variant_obj)
            # get all overlapping oposites
            variants += self.get_overlapping_oposites(variant_obj)
        else:
            # get all overlapping variants
            variants = self.get_over
----------
Solution: 

        #This is the category of the variants that we want to collect
        category = 'snv' if variant_obj['category'] == 'sv' else 'sv'

        query = {
            '$and': [
                {'case_id': variant_obj['case_id']},
                {'category': category},
                {'hgnc_ids' : { '$in' : variant_obj['hgnc_ids']}}
            ]
        }

        sort_key = [('rank_score', pymongo.DESCENDING)]
        # We collect the 30 most severe overlapping variants
        variants = self.variant_collection.find(query).sort(sort_key).limit(30)

        return variants
--------------------
Prompt: 
def validate(self):
        """
        Verify that the contents of the SecretData object are valid.

        Raises:
            TypeError: if the types of any SecretData attributes are invalid.
        """
----------
Output: 

        if not isinstance(self.name, str):
            raise TypeError("SecretData.name must be a string.")
        if not isinstance(self.description, str):
            raise TypeError("SecretData.description must be a string.")
        if not isinstance(self.secret_type, str):
            raise TypeError("SecretData.secret_type must be a string.")
        if not isinstance(self.secret_data, dict):
            raise TypeError("SecretData.secret_data must be a dictionary.")
        if not isinstance(self.secret_data
----------
Solution: 

        if not isinstance(self.value, bytes):
            raise TypeError("secret value must be bytes")
        elif not isinstance(self.data_type, enums.SecretDataType):
            raise TypeError("secret data type must be a SecretDataType "
                            "enumeration")

        mask_count = len(self.cryptographic_usage_masks)
        for i in range(mask_count):
            mask = self.cryptographic_usage_masks[i]
            if not isinstance(mask, enums.CryptographicUsageMask):
                position = "({0} in list)".format(i)
                raise TypeError(
                    "secret data mask {0} must be a CryptographicUsageMask "
                    "enumeration".format(position))

        name_count = len(self.names)
        for i in range(name_count):
            name = self.names[i]
            if not isinstance(name, six.string_types):
                position = "({0} in list)".format(i)
                raise TypeError("secret data name {0} must be a string".format(
                    position))
--------------------
Prompt: 
def to_matrix(self):
        """Return a Numpy.array for the U3 gate."""
----------
Output: 

        return np.array([[1, 0, 0, 0],
                         [0, 1, 0, 0],
                         [0, 0, 1, 0],
                         [0, 0, 0, 1]], dtype=complex)

    def to_matrix(self):
        """Return a Numpy.array for the U3 gate."""
        return np.array([[1, 0, 0, 0],
                         [0, 1, 0, 0],
----------
Solution: 

        theta, phi, lam = self.params
        return numpy.array(
            [[
                numpy.cos(theta / 2),
                -numpy.exp(1j * lam) * numpy.sin(theta / 2)
            ],
             [
                 numpy.exp(1j * phi) * numpy.sin(theta / 2),
                 numpy.exp(1j * (phi + lam)) * numpy.cos(theta / 2)
             ]],
            dtype=complex)
--------------------
Prompt: 
def _merge_with_other_stm(self, other: "IfContainer") -> None:
        """
        Merge other statement to this statement
        """
----------
Output: 

        self.statements.extend(other.statements)
        self.else_statements.extend(other.else_statements)

    def _merge_with_other_stm(self, other: "WhileContainer") -> None:
        """
        Merge other statement to this statement
        """
        self.statements.extend(other.statements)

    def _merge_with_other_stm(self, other: "ForContainer") -> None:
        """
        Merge other statement to this statement
        """

----------
Solution: 

        merge = self._merge_statement_lists
        newCases = []
        for (c, caseA), (_, caseB) in zip(self.cases, other.cases):
            newCases.append((c, merge(caseA, caseB)))

        self.cases = newCases

        if self.default is not None:
            self.default = merge(self.default, other.default)

        self._on_merge(other)
--------------------
Prompt: 
def  main( argv ):
    """main program loop"""
----------
Output: 

    #
    #   get the command line arguments
    #
    try:
        opts, args = getopt.getopt(argv, "hv", ["help", "verbose"])
    except getopt.GetoptError:
        print("usage: %s [-v] [-h]" % argv[0])
        sys.exit(2)
    #
    #   process the command line arguments
    #
    verbose = False
    for opt, arg in opts:
        if opt in ("-h", "--help"):
            print
----------
Solution: 


    global output_dir

    try:
        opts, args = getopt.getopt( sys.argv[1:], \
                                    "hb",         \
                                    ["help", "backup"] )
    except getopt.GetoptError:
        usage()
        sys.exit( 2 )

    if args == []:
        usage()
        sys.exit( 1 )

    # process options
    #
    output_dir = None
    do_backup  = None

    for opt in opts:
        if opt[0] in ( "-h", "--help" ):
            usage()
            sys.exit( 0 )

        if opt[0] in ( "-b", "--backup" ):
            do_backup = 1

    # create context and processor
    source_processor = SourceProcessor()

    # retrieve the list of files to process
    file_list = make_file_list( args )
    for filename in file_list:
        source_processor.parse_file( filename )

        for block in source_processor.blocks:
            beautify_block( block )

        new_name = filename + ".new"
        ok       = None

        try:
            file = open( new_name, "wt" )
            for block in source_processor.blocks:
                for line in block.lines:
                    file.write( line )
                    file.write( "\n" )
            file.close()
        except:
            ok = 0
--------------------
Prompt: 
def checkfuncname(b, frame):
    """Check whether we should break here because of `b.funcname`."""
----------
Output: 

    if b.funcname is None:
        return False
    if b.funcname == '__init__':
        return False
    if b.funcname == '__new__':
        return False
    if b.funcname == '__call__':
        return False
    if b.funcname == '__getattr__':
        return False
    if b.funcname == '__setattr__':
        return False
    if b.funcname == '__delattr__':
        return False
    if b.funcname == '
----------
Solution: 

    if not b.funcname:
        # Breakpoint was set via line number.
        if b.line != frame.f_lineno:
            # Breakpoint was set at a line with a def statement and the function
            # defined is called: don't break.
            return False
        return True

    # Breakpoint set via function name.

    if frame.f_code.co_name != b.funcname:
        # It's not a function call, but rather execution of def statement.
        return False

    # We are in the right frame.
    if not b.func_first_executable_line:
        # The function is entered for the 1st time.
        b.func_first_executable_line = frame.f_lineno

    if b.func_first_executable_line != frame.f_lineno:
        # But we are not at the first line number: don't break.
        return False
    return True
--------------------
Prompt: 
def gene_variants(self, query=None,
                   category='snv', variant_type=['clinical'],
                   nr_of_variants=50, skip=0):
        """Return all variants seen in a given gene.

        If skip not equal to 0 skip the first n variants.

        Arguments:
            query(dict): A dictionary with querys for the database, including
            variant_type: 'clinical', 'research'
            category(str): 'sv', 'str', 'snv' or 'cancer'
            nr_of_variants(int): if -1 return all variants
            skip(int): How many variants to skip
        """
----------
Output: 

            nr_of_variants(int): Number of variants to return
            skip(int): Skip the first n variants

        Returns:
            list: A list of dictionaries with variant information
        """
        if query is None:
            query = {}
        query['gene'] = self.gene
        query['variant_type'] = variant_type
        query['category'] = category
        query['skip'] = skip
        query['nr_of_variants'] = nr_of_variants
        return self.variants(query)
----------
Solution: 


        mongo_variant_query = self.build_variant_query(query=query,
                                   category=category, variant_type=variant_type)

        sorting = [('rank_score', pymongo.DESCENDING)]

        if nr_of_variants == -1:
            nr_of_variants = 0 # This will return all variants
        else:
            nr_of_variants = skip + nr_of_variants

        result = self.variant_collection.find(
            mongo_variant_query
            ).sort(sorting).skip(skip).limit(nr_of_variants)

        return result
--------------------
Prompt: 
def filter_exclude_downhole(self, threshold, filt=True, samples=None, subset=None):
        """
        Exclude all points down-hole (after) the first excluded data.

        Parameters
        ----------
        threhold : int
            The minimum number of contiguous excluded data points
            that must exist before downhole exclusion occurs.
        file : valid filter string or bool
            Which filter to consider. If True, applies to currently active
            filters.
        """
----------
Output: 

        if filt:
            filt = self.filter
        else:
            filt = None
        if samples is None:
            samples = self.samples
        if subset is None:
            subset = self.subset
        if filt is None:
            filt = self.filter
        if filt is None:
            filt = self.filter
        if filt is None:
            filt = self.filter
        if filt is None:
            filt = self.filter
        if filt is None:
            filt
----------
Solution: 

        if samples is not None:
            subset = self.make_subset(samples)

        samples = self._get_samples(subset)

        for s in samples:
            self.data[s].filter_exclude_downhole(threshold, filt)
--------------------
Prompt: 
def match(self, path):
        """Return route handler with arguments if path matches this route.

        Arguments:
          path (str): Request path

        Returns:
          tuple or None: A tuple of three items:

            1. Route handler (callable)
            2. Positional arguments (list)
            3. Keyword arguments (dict)

          ``None`` if the route does not match the path.
        """
----------
Output: 

        if self.matcher.match(path):
            return self.handler, self.args, self.kwargs

    def __repr__(self):
        return '<Route %s %s>' % (self.method, self.path)


class Router(object):
    """A router that dispatches requests to routes.

    Arguments:
      routes (list): A list of :class:`Route` instances.
    """

    def __init__(self, routes):
        self.routes = routes

    def route(self,
----------
Solution: 

        match = self._re.search(path)
        if match is None:
            return None
        kwargs_indexes = match.re.groupindex.values()
        args_indexes = [i for i in range(1, match.re.groups + 1)
                          if i not in kwargs_indexes]
        args = [match.group(i) for i in args_indexes]
        kwargs = {}
        for name, index in match.re.groupindex.items():
            kwargs[name] = match.group(index)
        return self._callback, args, kwargs
--------------------
Prompt: 
def get_int(errmsg, arg, default=1, cmdname=None):
    """If arg is an int, use that otherwise take default."""
----------
Output: 

    try:
        return int(arg)
    except ValueError:
        if cmdname:
            errmsg = "Error: %s: %s" % (cmdname, errmsg)
        else:
            errmsg = "Error: %s" % errmsg
        print(errmsg)
        return default


def get_float(errmsg, arg, default=1.0, cmdname=None):
    """If arg is a float, use that otherwise take default."""
    try:
        return float(arg)
    except
----------
Solution: 

    if arg:
        try:
            # eval() is used so we will allow arithmetic expressions,
            # variables etc.
            default = int(eval(arg))
        except (SyntaxError, NameError, ValueError):
            if cmdname:
                errmsg("Command '%s' expects an integer; got: %s." %
                       (cmdname, str(arg)))
            else:
                errmsg('Expecting an integer, got: %s.' % str(arg))
                pass
            raise ValueError
    return default
--------------------
Prompt: 
def get(*args, **kwargs):
    """Get users."""
----------
Output: 

    return (
        User.query.filter_by(**kwargs)
        .order_by(User.id.desc())
        .all()
    )


def get_by_id(user_id):
    """Get user by id."""
    return User.query.filter_by(id=user_id).first()


def get_by_email(email):
    """Get user by email."""
    return User.query.filter_by(email=email).first()


def create(**kwargs):

----------
Solution: 

    from invenio.modules.oauth2server.models import Client
    q = Client.query
    return q.count(), q.all()
--------------------
Prompt: 
def verify(
            cls,
            timestamp: int,
            message_hash: SHA512Hash,
            signature: bytes,
    ) -> bool:
        """
        Verify a given NIST message hash and signature for a beacon value.

        :param timestamp: The timestamp of the record being verified.
        :param message_hash:
            The hash that was carried out over the message.
            This is an object belonging to the `Crypto.Hash` module.
        :param signature: The signature that needs to be validated.
        :return: True if verification is correct. False otherwise.
        """
----------
Output: 

        :return:
            True if the signature is valid, False otherwise.
        """
        return cls.verify_signature(
            message_hash=message_hash,
            signature=signature,
            public_key=cls.get_public_key(timestamp=timestamp),
        )

    @classmethod
    def get_public_key(
            cls,
            timestamp: int,
    ) -> RSAKey:
        """
        Get the public key for a given timestamp.

        :param timestamp: The timestamp of the
----------
Solution: 


        # Determine verifier type to use based on timestamp.
        if timestamp < 1496176860:
            verifier = cls._VERIFIER_20130905
        elif timestamp < 1502202360:
            verifier = None
        else:
            verifier = cls._VERIFIER_20170808

        # If a verifier exists to handle this problem, use it directly.
        # Else, we cannot verify the record and must mark it invalid.
        if verifier:
            result = verifier.verify(
                message_hash,
                signature,
            )
        else:
            result = False

        # Convert 1 to 'True', 'False' otherwise
        if isinstance(result, int):
            result = True if result == 1 else False

        return result
--------------------
Prompt: 
def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.AST]:
        """Eliminate dead code from function bodies."""
----------
Output: 

        if node.name == "__init__":
            return None
        if node.name == "__new__":
            return None
        if node.name == "__del__":
            return None
        if node.name == "__repr__":
            return None
        if node.name == "__str__":
            return None
        if node.name == "__format__":
            return None
        if node.name == "__getattr__":
            return None
        if node.name == "__setattr__":

----------
Solution: 

        new_node = self.generic_visit(node)
        assert isinstance(new_node, ast.FunctionDef)
        return ast.copy_location(
            ast.FunctionDef(
                name=new_node.name,
                args=new_node.args,
                body=_filter_dead_code(new_node.body),
                decorator_list=new_node.decorator_list,
                returns=new_node.returns,
            ),
            new_node,
        )
--------------------
Prompt: 
def register_success(self, nick, message, channel, cmd_channel):
        """\
        Received registration acknowledgement from the BotnetBot, as well as the
        name of the command channel, so join up and indicate that registration
        succeeded
        """
----------
Output: 

        self.log.info("Registration succeeded")
        self.log.info("Command channel: %s", cmd_channel)
        self.join(cmd_channel)
        self.send_message(cmd_channel, "Registration succeeded")

    def register_fail(self, nick, message, channel, cmd_channel):
        """\
        Received registration acknowledgement from the BotnetBot, but the
        registration failed
        """
        self.log.info("Registration failed")
        self.log.info("Command channel:
----------
Solution: 

        # the boss will tell what channel to join
        self.channel = cmd_channel
        self.conn.join(self.channel)
        
        # indicate that registered so we'll stop trying
        self.registered.set()
--------------------
Prompt: 
def csv_to_po(trans_csv_path, meta_csv_path, locale_root,
              po_files_path, header=None):
    """
    Converts GDocs spreadsheet generated csv file into po file.
    :param trans_csv_path: path to temporary file with translations
    :param meta_csv_path: path to temporary file with meta information
    :param locale_root: path to locale root folder containing directories
                        with languages
    :param po_files_path: path from lang directory to po file
    """
----------
Output: 
   :param header: list of headers to be used in po file
    :return: None
    """
    # read csv files
    with open(trans_csv_path, 'r') as f:
        trans_csv = csv.reader(f)
        trans_csv_list = list(trans_csv)

    with open(meta_csv_path, 'r') as f:
        meta_csv = csv.reader(f)
        meta_csv_list = list(meta_csv)

    # get meta information
    lang = meta
----------
Solution: 

    pattern = "^\w+.*po$"
    for root, dirs, files in os.walk(locale_root):
        for f in filter(lambda x: re.match(pattern, x), files):
            os.remove(os.path.join(root, f))

    # read title row and prepare descriptors for po files in each lang
    trans_reader = UnicodeReader(trans_csv_path)
    meta_reader = UnicodeReader(meta_csv_path)
    try:
        title_row = trans_reader.next()
    except StopIteration:
        # empty file
        return

    trans_languages = _prepare_locale_dirs(title_row[3:], locale_root)

    po_files = {}

    meta_reader.next()
    # go through every row in downloaded csv file
    for trans_row, meta_row in izip_longest(trans_reader, meta_reader):
        filename = trans_row[0].rstrip()
        metadata = meta_row[0].rstrip() if meta_row else METADATA_EMPTY
        comment = trans_row[1]
        msgid = trans_row[2]

        if filename not in po_files:
            _prepare_polib_files(po_files, filename, trans_languages,
                                 locale_root, po_files_path, header)

        _write_entries(po_files[filename], trans_languages, msgid,
                       trans_row[3:], metadata, comment)
    for filename in po_files:
        for lang in po_files[filename]:
            po_files[filename][lang].save()

    trans_reader.close()
    meta_reader.close()
--------------------
Prompt: 
def _add_header(self):
        """Add email header info."""
----------
Output: 

        self.msg['From'] = self.sender
        self.msg['To'] = self.recipients
        self.msg['Subject'] = self.subject
        self.msg.preamble = self.preamble

    def _add_body(self):
        """Add email body."""
        self.msg.attach(MIMEText(self.body, 'plain'))

    def _add_attachments(self):
        """Add attachments to email."""
        for file in self.attachments:
            with open(
----------
Solution: 

        self.message["From"] = self.from_
        self.message["Subject"] = self.subject
        if self.to:
            self.message["To"] = self.list_to_string(self.to)
        if self.cc:
            self.message["Cc"] = self.list_to_string(self.cc)
        if self.bcc:
            self.message["Bcc"] = self.list_to_string(self.bcc)
--------------------
Prompt: 
def url_dequery(url):
    """Return a URL with the query component removed.

    :param url: URL to dequery.
    :type url: str
    :rtype: str
    """
----------
Output: 

    return urlparse.urlunparse(urlparse.urlparse(url)._replace(query=''))


def url_query_add(url, key, value):
    """Add a key/value pair to a URL's query component.

    :param url: URL to add to.
    :type url: str
    :param key: Key to add.
    :type key: str
    :param value: Value to add.
    :type value: str
    :rtype: str
    """
    query = urlparse.parse
----------
Solution: 

    url = urlparse.urlparse(url)
    return urlparse.urlunparse((url.scheme,
                                url.netloc,
                                url.path,
                                url.params,
                                '',
                                url.fragment))
--------------------
Prompt: 
def rendered_content(self):
        """Returns a 'minified' version of the javascript content"""
----------
Output: 

        return self.content

    def rendered_content_with_comments(self):
        """Returns the javascript content with comments"""
        return self.content

    def rendered_content_with_comments_and_whitespace(self):
        """Returns the javascript content with comments and whitespace"""
        return self.content

    def rendered_content_with_comments_and_whitespace_and_line_breaks(self):
        """Returns the javascript content with comments and whitespace and line breaks"""
        return self.content
----------
Solution: 

        template = self.resolve_template(self.template_name)
        if django.VERSION[1] < 8:
            if template.name.endswith('.min'):
                return super(MinifiedJsTemplateResponse, self).rendered_content
        else:
            if template.template.name.endswith('.min'):
                return super(MinifiedJsTemplateResponse, self).rendered_content
        # if no minified template exists, minify the response
        content = super(MinifiedJsTemplateResponse, self).rendered_content
        content = jsmin.jsmin(content)
        return content
--------------------
Prompt: 
def get_except_handler_name(self, node):
        """
        Helper to get the exception name from an ExceptHandler node in both py2 and py3.

        """
----------
Output: 

        if node.type == 'except_handler':
            if node.type_name:
                return node.type_name.name
            else:
                return 'Exception'
        else:
            return node.name

    def get_except_handler_type(self, node):
        """
        Helper to get the exception type from an ExceptHandler node in both py2 and py3.

        """
        if node.type == 'except_handler':
            if node.type_name:
                return node.type_name.name
----------
Solution: 

        name = node.name
        if not name:
            return None

        if version_info < (3,):
            return name.id
        return name
--------------------
Prompt: 
def _get_live_streams(self, lang, path):
        """
        Get the live stream in a particular language
        :param lang:
        :param path:
        :return:
        """
----------
Output: 

        if lang == "en":
            return self._get_live_streams_en(path)
        elif lang == "fr":
            return self._get_live_streams_fr(path)
        elif lang == "es":
            return self._get_live_streams_es(path)
        elif lang == "de":
            return self._get_live_streams_de(path)
        elif lang == "it":
            return self._get_live_streams_it(path)
        elif lang == "pt":
----------
Solution: 

        res = self.session.http.get(self._live_api_url.format(lang, path))
        live_res = self.session.http.json(res)['default']['uid']
        post_data = '{"channel_url":"/api/channels/%s/"}' % live_res
        try:
            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['stream_url']
        except BaseException:
            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['channel_url']
        return HLSStream.parse_variant_playlist(self.session, stream_data)
--------------------
Prompt: 
def rotatePoint(self, pointX, pointY):
        """
        Rotates a point relative to the mesh origin by the angle specified in the angle property.
        Uses the angle formed between the segment linking the point of interest to the origin and
        the parallel intersecting the origin. This angle is called beta in the code.
        """
----------
Output: 

        # Calculate the angle between the point of interest and the origin
        beta = math.atan2(pointY, pointX)

        # Calculate the distance between the point of interest and the origin
        distance = math.sqrt(pointX**2 + pointY**2)

        # Calculate the new x and y coordinates of the point
        newX = math.cos(beta + self.angle) * distance
        newY = math.sin(beta + self.angle) * distance

        return newX, newY

    def rotatePoint
----------
Solution: 
    
        if(self.angle == 0 or self.angle == None):
            return(pointX, pointY)
              
        # 1. Compute the segment length
        length = math.sqrt((pointX - self.xll) ** 2 + (pointY - self.yll) ** 2)
        
        # 2. Compute beta
        beta = math.acos((pointX - self.xll) / length) 
        if(pointY < self.yll):
            beta = math.pi * 2 - beta
           
        # 3. Compute offsets
        offsetX = math.cos(beta) * length - math.cos(self._angle_rd + beta) * length
        offsetY = math.sin(self._angle_rd + beta) * length - math.sin(beta) * length 
        return (pointX - offsetX, pointY + offsetY)
--------------------
Prompt: 
def get_term_by_sis_id(self, sis_term_id):
        """
        Return a term resource for the passed SIS ID.
        """
----------
Output: 

        return self.get_by_path(
            'terms/sis_id:{sis_term_id}'.format(sis_term_id=sis_term_id)
        )

    def get_term_by_sis_term_slug(self, sis_term_slug):
        """
        Return a term resource for the passed SIS term slug.
        """
        return self.get_by_path(
            'terms/sis_term_slug:{sis_term_slug}'.
----------
Solution: 

        for term in self.get_all_terms():
            if term.sis_term_id == sis_term_id:
                return term
--------------------
Prompt: 
def set_hvac_mode(self, index, hvac_mode):
        """ possible hvac modes are auto, auxHeatOnly, cool, heat, off """
----------
Output: 

        self.hvac_modes[index] = hvac_mode

    def set_hvac_mode_by_name(self, name, hvac_mode):
        """ possible hvac modes are auto, auxHeatOnly, cool, heat, off """
        for index, mode in enumerate(self.hvac_modes):
            if mode == name:
                self.set_hvac_mode(index, hvac_mode)
                return
        raise Exception("Invalid hvac mode name: "
----------
Solution: 

        body = {"selection": {"selectionType": "thermostats",
                              "selectionMatch": self.thermostats[index]['identifier']},
                              "thermostat": {
                                  "settings": {
                                      "hvacMode": hvac_mode
                                  }
                              }}
        log_msg_action = "set HVAC mode"
        return self.make_request(body, log_msg_action)
--------------------
Prompt: 
def process_non_raw_string_token(self, prefix, string_body, start_row):
        """check for bad escapes in a non-raw string.

        prefix: lowercase string of eg 'ur' string prefix markers.
        string_body: the un-parsed body of the string, not including the quote
        marks.
        start_row: integer line number in the source.
        """
----------
Output: 

        # check for bad escapes in a non-raw string.
        if prefix == 'ur':
            # check for bad escapes in a non-raw string.
            if string_body.find('\\') != -1:
                self.add_message('bad-escape-in-unicode-string',
                                 node=self.node,
                                 args=(prefix, string_body),
                                 confidence=HIGH,
                                 column=self.node.col_offset)

    def process_raw_string_token(self,
----------
Solution: 

        # Walk through the string; if we see a backslash then escape the next
        # character, and skip over it.  If we see a non-escaped character,
        # alert, and continue.
        #
        # Accept a backslash when it escapes a backslash, or a quote, or
        # end-of-line, or one of the letters that introduce a special escape
        # sequence <http://docs.python.org/reference/lexical_analysis.html>
        #
        # TODO(mbp): Maybe give a separate warning about the rarely-used
        # \a \b \v \f?
        #
        # TODO(mbp): We could give the column of the problem character, but
        # add_message doesn't seem to have a way to pass it through at present.
        i = 0
        while True:
            i = string_body.find("\\", i)
            if i == -1:
                break
            # There must be a next character; having a backslash at the end
            # of the string would be a SyntaxError.
            next_char = string_body[i + 1]
            match = string_body[i : i + 2]
            if next_char in self.UNICODE_ESCAPE_CHARACTERS:
                if "u" in prefix:
                    pass
                elif (_PY3K or self._unicode_literals) and "b" not in prefix:
                    pass  # unicode by default
                else:
                    self.add_message(
                        "anomalous-unicode-escape-in-string",
                        line=start_row,
                        args=(match,),
                    )
            elif next_char not in self.ESCAPE_CHARACTERS:
                self.add_message(
                    "anomalous-backslash-in-string", line=start_row, args=(match,)
                )
            # Whether it was a valid escape or not, backslash followed by
            # another character can always be consumed whole: the second
            # character can never be the start of a new backslash escape.
            i += 2
--------------------
Prompt: 
def processFlat(self):
        """Main process.for flat segmentation.
        Returns
        -------
        est_idxs : np.array(N)
            Estimated times for the segment boundaries in frame indeces.
        est_labels : np.array(N-1)
            Estimated labels for the segments.
        """
----------
Output: 

        # Get the number of frames
        N = self.X.shape[0]

        # Get the number of frames
        N = self.X.shape[0]

        # Get the number of frames
        N = self.X.shape[0]

        # Get the number of frames
        N = self.X.shape[0]

        # Get the number of frames
        N = self.X.shape[0]

        # Get the number of frames
        N = self.X.shape[0]

       
----------
Solution: 

        self.config["hier"] = False
        est_idxs, est_labels, F = self.process()
        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1
        return self._postprocess(est_idxs, est_labels)
--------------------
Prompt: 
def finish_registration(self, heart):
        """Second half of engine registration, called after our HeartMonitor
        has received a beat from the Engine's Heart."""
----------
Output: 

        self.heart = heart
        self.heart.add_listener(self)
        self.heart.add_listener(self.heart_monitor)
        self.heart.add_listener(self.heart_monitor.heart_monitor)
        self.heart.add_listener(self.heart_monitor.heart_monitor.heart_monitor)
        self.heart.add_listener(self.heart_monitor.heart_monitor.heart_monitor.heart_monitor)
        self.heart.add_listener(self
----------
Solution: 

        try:
            (eid,queue,reg,purge) = self.incoming_registrations.pop(heart)
        except KeyError:
            self.log.error("registration::tried to finish nonexistant registration", exc_info=True)
            return
        self.log.info("registration::finished registering engine %i:%r", eid, queue)
        if purge is not None:
            purge.stop()
        control = queue
        self.ids.add(eid)
        self.keytable[eid] = queue
        self.engines[eid] = EngineConnector(id=eid, queue=queue, registration=reg,
                                    control=control, heartbeat=heart)
        self.by_ident[queue] = eid
        self.queues[eid] = list()
        self.tasks[eid] = list()
        self.completed[eid] = list()
        self.hearts[heart] = eid
        content = dict(id=eid, queue=self.engines[eid].queue.decode('ascii'))
        if self.notifier:
            self.session.send(self.notifier, "registration_notification", content=content)
        self.log.info("engine::Engine Connected: %i", eid)
--------------------
Prompt: 
def get_ticker_price(self, ticker,
                         startDate=None, endDate=None,
                         fmt='json', frequency='daily'):
        """By default, return latest EOD Composite Price for a stock ticker.
           On average, each feed contains 3 data sources.

            Supported tickers + Available Day Ranges are here:
            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip

            Args:
                ticker (string): Unique identifier for stock ticker
                startDate (string): Start of ticker range in YYYY-MM-DD format
                endDate (string): End of ticker range in YYYY-MM-DD format
                fmt (string): 'csv' or 'json'
                frequency (string): Resample frequency
        """
----------
Output: 
ticker symbol for a security.
                startDate (string): Start date for the data.
                endDate (string): End date for the data.
                fmt (string): Format of the output.
                frequency (string): Frequency of the data.

            Returns:
                dict: Dictionary of data.
        """
        if startDate is None:
            startDate = self.startDate
        if endDate is None:
            endDate = self.endDate

        url = self.base_url + 'tiingo/daily/{}/prices'.
----------
Solution: 

        url = self._get_url(ticker, frequency)
        params = {
            'format': fmt if fmt != "object" else 'json',  # conversion local
            'resampleFreq': frequency
        }

        if startDate:
            params['startDate'] = startDate
        if endDate:
            params['endDate'] = endDate

        # TODO: evaluate whether to stream CSV to cache on disk, or
        # load as array in memory, or just pass plain text
        response = self._request('GET', url, params=params)
        if fmt == "json":
            return response.json()
        elif fmt == "object":
            data = response.json()
            return [dict_to_object(item, "TickerPrice") for item in data]
        else:
            return response.content.decode("utf-8")
--------------------
Prompt: 
def _plant_trie(strings: _List[str]) -> dict:
    """Create a Trie out of a list of words and return an atomic regex pattern.

    The corresponding Regex should match much faster than a simple Regex union.
    """
----------
Output: 

    trie = {}
    for string in strings:
        node = trie
        for char in string:
            node = node.setdefault(char, {})
        node["#"] = True
    return trie


def _plant_trie_to_regex(trie: dict) -> str:
    """Convert a Trie to a Regex pattern."""
    return "".join(
        [
            "(" + "|".join(
                [
                    _plant_trie_to_regex(node)
                   
----------
Solution: 

    # plant the trie
    trie = {}
    for string in strings:
        d = trie
        for char in string:
            d[char] = char in d and d[char] or {}
            d = d[char]
        d[''] = None  # EOS
    return trie
--------------------
Prompt: 
def normalize_profile(in_profile, log=False, return_offset = True):
    """return a normalized version of a profile matrix

    Parameters
    ----------
    in_profile : np.array
        shape Lxq, will be normalized to one across each row
    log : bool, optional
        treat the input as log probabilities
    return_offset : bool, optional
        return the log of the scale factor for each row

    Returns
    -------
    tuple
        normalized profile (fresh np object) and offset (if return_offset==True)
    """
----------
Output: 
offset (np object)
    """
    if log:
        in_profile = np.exp(in_profile)
    in_profile = in_profile / np.sum(in_profile, axis=1, keepdims=True)
    if log:
        in_profile = np.log(in_profile)
    if return_offset:
        return in_profile, np.log(np.sum(in_profile, axis=1))
    else:
        return in_profile


def normalize_profile_by_row(in
----------
Solution: 

    if log:
        tmp_prefactor = in_profile.max(axis=1)
        tmp_prof = np.exp(in_profile.T - tmp_prefactor).T
    else:
        tmp_prefactor = 0.0
        tmp_prof = in_profile

    norm_vector = tmp_prof.sum(axis=1)
    return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)),
            (np.log(norm_vector) + tmp_prefactor) if return_offset else None)
--------------------
Prompt: 
def delete_dashboard(self, dashboard):
        """**Description**
            Deletes a dashboard.

        **Arguments**
            - **dashboard**: the dashboard object as returned by :func:`~SdcClient.get_dashboards`.

        **Success Return Value**
            `None`.

        **Example**
            `examples/delete_dashboard.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_dashboard.py>`_
        """
----------
Output: 

        self._delete_resource(dashboard)

    def get_dashboards(self, query_parameters=None):
        """**Description**
            Returns a list of dashboards.

        **Arguments**
            - **query_parameters**: `dict` of query parameters to append to the request.

        **Success Return Value**
            A list of dashboard objects as returned by :func:`~SdcClient.get_dashboard`.

        **Example**
            `examples/get_dashboards.py <https://github.com/draios
----------
Solution: 

        if 'id' not in dashboard:
            return [False, "Invalid dashboard format"]

        res = requests.delete(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, verify=self.ssl_verify)
        if not self._checkResponse(res):
            return [False, self.lasterr]

        return [True, None]
--------------------
Prompt: 
def _make_masked_initializer(mask, initializer):
  """Returns a masked version of the given initializer."""
----------
Output: 

  def _initializer(shape, dtype=None, partition_info=None):
    if dtype is None:
      raise ValueError("If a partition_info is passed, a dtype must be "
                       "specified")
    if mask is None:
      return initializer(shape, dtype, partition_info)
    else:
      return tf.where(mask, initializer(shape, dtype, partition_info),
                      tf.zeros(shape, dtype))
  return _initializer


def _make_masked_variable(mask
----------
Solution: 

  initializer = tf.keras.initializers.get(initializer)
  def masked_initializer(shape, dtype=None, partition_info=None):
    # If no `partition_info` is given, then don't pass it to `initializer`, as
    # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which
    # don't accept a `partition_info` argument).
    if partition_info is None:
      x = initializer(shape, dtype)
    else:
      x = initializer(shape, dtype, partition_info)
    return tf.cast(mask, x.dtype) * x
  return masked_initializer
--------------------
Prompt: 
def wipe(ctx):
    """Drop the mongo database given."""
----------
Output: 

    if ctx.invoked_subcommand is None:
        print("No subcommand given.")


@wipe.command()
@click.argument('db')
def database(db):
    """Drop the mongo database given."""
    db = get_db(db)
    db.drop_database(db)


@wipe.command()
@click.argument('collection')
def collection(collection):
    """Drop the mongo collection given."""
    collection = get_collection(collection)
    collection.drop()


@
----------
Solution: 

    LOG.info("Running scout wipe")
    db_name = ctx.obj['mongodb']
    LOG.info("Dropping database %s", db_name)
    try:
        ctx.obj['client'].drop_database(db_name)
    except Exception as err:
        LOG.warning(err)
        ctx.abort()
    LOG.info("Dropped whole database")
--------------------
Prompt: 
def new_contact(cls, address_book, supported_private_objects, version,
            localize_dates):
        """Use this to create a new and empty contact."""
----------
Output: 

        return cls(address_book, supported_private_objects, version,
            localize_dates)

    def __init__(self, address_book, supported_private_objects, version,
            localize_dates):
        """Initialize a new contact."""
        self._address_book = address_book
        self._supported_private_objects = supported_private_objects
        self._version = version
        self._localize_dates = localize_dates
        self._properties = {}
        self._properties_by_id = {}

----------
Solution: 

        return cls(address_book, None, supported_private_objects, version,
                localize_dates)
--------------------
Prompt: 
def sample_variants(self, variants, sample_name, category = 'snv'):
        """Given a list of variants get variant objects found in a specific patient

        Args:
            variants(list): a list of variant ids
            sample_name(str): a sample display name
            category(str): 'snv', 'sv' ..

        Returns:
            result(iterable(Variant))
        """
----------
Output: 

        if category == 'snv':
            return self.get_variants_by_ids(variants, sample_name)
        elif category == 'sv':
            return self.get_sv_variants_by_ids(variants, sample_name)
        else:
            raise ValueError('Unsupported category: {}'.format(category))

    def get_variants_by_ids(self, variant_ids, sample_name):
        """Given a list of variant ids get variant objects found in a specific patient

        Ar
----------
Solution: 

        LOG.info('Retrieving variants for subject : {0}'.format(sample_name))
        has_allele = re.compile('1|2') # a non wild-type allele is called at least once in this sample

        query = {
            '$and': [
                {'_id' : { '$in' : variants}},
                {'category' : category},
                {'samples': {
                    '$elemMatch': { 'display_name' : sample_name, 'genotype_call': { '$regex' : has_allele } }
                }}
            ]
        }

        result = self.variant_collection.find(query)
        return result
--------------------
Prompt: 
def _register_by_id_managed_msg(self, msgid, line, is_disabled=True):
        """If the msgid is a numeric one, then register it to inform the user
        it could furnish instead a symbolic msgid."""
----------
Output: 

        if msgid.isdigit():
            self.register_by_id(msgid, line, is_disabled)

    def register_by_id(self, msgid, line, is_disabled=True):
        """Register a message to the catalog.

        :param msgid: the message identifier
        :param line: the line where the message was found
        :param is_disabled: whether the message is disabled or not
        """
        if msgid in self.catalog:
            self.catalog[msgid].add_line(line)
----------
Solution: 

        try:
            message_definitions = self.msgs_store.get_message_definitions(msgid)
            for message_definition in message_definitions:
                if msgid == message_definition.msgid:
                    MessagesHandlerMixIn.__by_id_managed_msgs.append(
                        (
                            self.current_name,
                            message_definition.msgid,
                            message_definition.symbol,
                            line,
                            is_disabled,
                        )
                    )
        except UnknownMessageError:
            pass
--------------------
Prompt: 
def pretty(obj, verbose=False, max_width=79, newline='\n'):
    """
    Pretty print the object's representation.
    """
----------
Output: 

    if verbose:
        return repr(obj)
    else:
        return _pretty(obj, max_width, newline)


def _pretty(obj, max_width, newline):
    """
    Pretty print the object's representation.
    """
    if isinstance(obj, str):
        return obj
    elif isinstance(obj, bytes):
        return repr(obj)
    elif isinstance(obj, (list, tuple)):
        return _pretty_list(obj, max_width, newline)
    elif
----------
Solution: 

    stream = StringIO()
    printer = RepresentationPrinter(stream, verbose, max_width, newline)
    printer.pretty(obj)
    printer.flush()
    return stream.getvalue()
--------------------
Prompt: 
def __match_intervals(intervals_from, intervals_to, strict=True):  # pragma: no cover
    """Numba-accelerated interval matching algorithm.

    """
----------
Output: 

    from numba import njit

    @njit
    def match_intervals(intervals_from, intervals_to, strict):
        """Numba-accelerated interval matching algorithm.

        """
        n_from = intervals_from.shape[0]
        n_to = intervals_to.shape[0]

        matches = np.zeros((n_from, n_to), dtype=np.bool_)

        for i in range(n_from):
            for j in range(n_to):

----------
Solution: 

    # sort index of the interval starts
    start_index = np.argsort(intervals_to[:, 0])

    # sort index of the interval ends
    end_index = np.argsort(intervals_to[:, 1])

    # and sorted values of starts
    start_sorted = intervals_to[start_index, 0]
    # and ends
    end_sorted = intervals_to[end_index, 1]

    search_ends = np.searchsorted(start_sorted, intervals_from[:, 1], side='right')
    search_starts = np.searchsorted(end_sorted, intervals_from[:, 0], side='left')

    output = np.empty(len(intervals_from), dtype=numba.uint32)
    for i in range(len(intervals_from)):
        query = intervals_from[i]

        # Find the intervals that start after our query ends
        after_query = search_ends[i]
        # And the intervals that end after our query begins
        before_query = search_starts[i]

        # Candidates for overlapping have to (end after we start) and (begin before we end)
        candidates = set(start_index[:after_query]) & set(end_index[before_query:])

        # Proceed as before
        if len(candidates) > 0:
            output[i] = __match_interval_overlaps(query, intervals_to, candidates)
        elif strict:
            # Numba only lets us use compile-time constants in exception messages
            raise ParameterError
        else:
            # Find the closest interval
            # (start_index[after_query] - query[1]) is the distance to the next interval
            # (query[0] - end_index[before_query])
            dist_before = np.inf
            dist_after = np.inf
            if search_starts[i] > 0:
                dist_before = query[0] - end_sorted[search_starts[i]-1]
            if search_ends[i] + 1 < len(intervals_to):
                dist_after = start_sorted[search_ends[i]+1] - query[1]
            if dist_before < dist_after:
                output[i] = end_index[search_starts[i]-1]
            else:
                output[i] = start_index[search_ends[i]+1]
    return output
--------------------
Prompt: 
def on_key_press(self,symbol,modifiers):
        """
        Keyboard event handler handling only the escape key.
        
        If an escape key press is detected, mouse exclusivity is toggled via :py:meth:`PengWindow.toggle_exclusivity()`\ .
        """
----------
Output: 

        if symbol == pyglet.window.key.ESCAPE:
            self.toggle_exclusivity()

    def on_mouse_press(self,x,y,button,modifiers):
        """
        Mouse event handler handling only the left mouse button.
        
        If the left mouse button is pressed, the mouse is locked via :py:meth:`PengWindow.set_exclusive_mouse()`\ .
        """
        if button == pyglet.window.mouse.LEFT:
            self.set_exclusive_mouse
----------
Solution: 

        if symbol == key.ESCAPE:
            self.world.peng.window.toggle_exclusivity()
            return pyglet.event.EVENT_HANDLED
--------------------
Prompt: 
def pprint_simple_array(val, displaywidth, msg_nocr, msg, lineprefix=''):
    """Try to pretty print a simple case where a list is not nested.
    Return True if we can do it and False if not. """
----------
Output: 

    if not isinstance(val, list):
        return False
    if len(val) == 0:
        return False
    if not isinstance(val[0], list):
        return False
    if len(val[0]) != len(val):
        return False
    if not all(isinstance(v, str) for v in val):
        return False
    if not all(len(v) == len(val[0]) for v in val):
        return False
    if not all(len(v) == len(val[
----------
Solution: 


    if type(val) != list:
        return False

    numeric = True
    for i in range(len(val)):
        if not (type(val[i]) in [bool, float, int]):
            numeric = False
            if not (type(val[i]) in [bool, float, int, bytes]):
                return False
            pass
        pass
    mess = columnize([repr(v) for v in val],
                     opts={"arrange_array": True,
                           "lineprefix": lineprefix,
                           "displaywidth": int(displaywidth)-3,
                           'ljust': not numeric})
    msg_nocr(mess)
    return True
--------------------
Prompt: 
def size(self, source):
    """Get the size component of the given s3url. If it is a
       directory, combine the sizes of all the files under
       that directory. Subdirectories will not be counted unless
       --recursive option is set.
    """
----------
Output: 

    if source.endswith('/'):
        source = source[:-1]
    if source.startswith('s3://'):
        source = source[5:]
    if source.startswith('s3n://'):
        source = source[6:]
    if source.startswith('s3a://'):
        source = source[6:]
    if source.startswith('s3n+ssl://'):
        source = source[11:]
    if source.startswith('s3a+ssl://'):
        source = source[11:]
----------
Solution: 

    result = []
    for src in self.source_expand(source):
      size = 0
      for f in self.s3walk(src):
        size += f['size']
      result.append((src, size))

    return result
--------------------
Prompt: 
def parse_compounds(compound_info, case_id, variant_type):
    """Get a list with compounds objects for this variant.

        Arguments:
            compound_info(str): A Variant dictionary
            case_id (str): unique family id
            variant_type(str): 'research' or 'clinical'

        Returns:
            compounds(list(dict)): A list of compounds
    """
----------
Output: 

    compounds = []
    for compound in compound_info:
        compound_id = compound.get('compound_id')
        if compound_id:
            compound_obj = {
                'case_id': case_id,
                'variant_id': compound.get('variant_id'),
                'variant_type': variant_type,
                'compound_id': compound_id,
                'compound_name': compound.get('compound_name'),
                'compound_type': comp
----------
Solution: 

    # We need the case to construct the correct id
    compounds = []
    if compound_info:
        for family_info in compound_info.split(','):
            splitted_entry = family_info.split(':')
            # This is the family id
            if splitted_entry[0] == case_id:
                for compound in splitted_entry[1].split('|'):
                    splitted_compound = compound.split('>')
                    compound_obj = {}
                    compound_name = splitted_compound[0]
                    compound_obj['variant'] = generate_md5_key(compound_name.split('_') +
                                                               [variant_type, case_id])

                    try:
                        compound_score = float(splitted_compound[1])
                    except (TypeError, IndexError):
                        compound_score = 0.0

                    compound_obj['score'] = compound_score
                    compound_obj['display_name'] = compound_name

                    compounds.append(compound_obj)

    return compounds
--------------------
Prompt: 
def any_slug_field(field, **kwargs):
    """
    Return random value for SlugField
    >>> result = any_field(models.SlugField())
    >>> type(result)
    <type 'str'>
    >>> from django.core.validators import slug_re
    >>> re.match(slug_re, result) is not None
    True
    """
----------
Output: 

    return any_field(field, **kwargs)


def any_url_field(field, **kwargs):
    """
    Return random value for URLField
    >>> result = any_field(models.URLField())
    >>> type(result)
    <type 'str'>
    >>> result.startswith('http://')
    True
    """
    return any_field(field, **kwargs)


def any_ip_address_field(field, **kwargs):
   
----------
Solution: 

    letters = ascii_letters + digits + '_-'
    return xunit.any_string(letters = letters, max_length = field.max_length)
--------------------
Prompt: 
def factorize(self):
        """ Factorize s.t. CUR = data

            Updated Values
            --------------
            .C : updated values for C.
            .U : updated values for U.
            .R : updated values for R.
        """
----------
Output: 

        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a valid matrix
        # TODO: Check if CUR is a
----------
Solution: 


        [prow, pcol] = self.sample_probability()

        self._rid = self.sample(self._rrank, prow)
        self._cid = self.sample(self._crank, pcol)

        self._cmdinit()

        self.computeUCR()
--------------------
Prompt: 
def _deduplicate_indexed_slices(values, indices):
    """Sums `values` associated with any non-unique `indices`.
    Args:
      values: A `Tensor` with rank >= 1.
      indices: A one-dimensional integer `Tensor`, indexing into the first
      dimension of `values` (as in an IndexedSlices object).
    Returns:
      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a
      de-duplicated version of `indices` and `summed_values` contains the sum of
      `values` slices associated with each unique index.
    """
----------
Output: 
indices` and `summed_values` contains the sum of
      `values` at each unique index. The returned `unique_indices` will be a
      de-duplicated version of `indices` with no guaranteed ordering.
    """
    unique_indices, idx = tf.unique(indices)
    summed_values = tf.unsorted_segment_sum(values, idx, tf.size(unique_indices))
    return summed_values, unique_indices


def _deduplicate_indexed_slices(index
----------
Solution: 

    unique_indices, new_index_positions = tf.unique(indices)
    summed_values = tf.unsorted_segment_sum(values,
                                            new_index_positions,
                                            tf.shape(unique_indices)[0])
    return (summed_values, unique_indices)
--------------------
Prompt: 
def token_validate_with_login(self, **kwargs):
        """
        Authenticate a user with a TMDb username and password.  The user
        must have a verified email address and be registered on TMDb.

        Args:
            request_token: The token you generated for the user to approve.
            username: The user's username on TMDb.
            password: The user's password on TMDb.

        Returns:
            A dict respresentation of the JSON returned from the API.
        """
----------
Output: 

        return self.request(
            'GET',
            'authentication/token/validate_with_login',
            **kwargs
        )

    def token_validate_with_token(self, **kwargs):
        """
        Authenticate a user with a TMDb username and password.  The user
        must have a verified email address and be registered on TMDb.

        Args:
            request_token: The token you generated for the user to approve.
            username: The user's username on TMDb.
            password: The user'
----------
Solution: 

        path = self._get_path('token_validate_with_login')

        response = self._GET(path, kwargs)
        self._set_attrs_to_values(response)
        return response
--------------------
Prompt: 
def setLang(self,lang):
        """
        Sets the default language for all domains.
        
        For recommendations regarding the format of the language code, see
        :py:class:`TranslationManager`\ .
        
        Note that the ``lang`` parameter of both :py:meth:`translate()` and
        :py:meth:`translate_lazy()` will override this setting.
        
        Also note that the code won't be checked for existence or plausibility.
        This may cause the fallback strings to be displayed instead if the language
        does not exist.
        
        Calling this method will cause the ``setlang`` action and the
        :peng3d:event`peng3d:i18n.set_lang` event to be triggered. Note that both
        action and event will be triggered even if the language did not actually change.
        
        This method also automatically updates the :confval:`i18n.lang` config value.
        """
----------
Output: 
the
        language is invalid.
        
        :param lang: The language code.
        :type lang: str
        """
        self._lang = lang
    
    def getLang(self):
        """
        Returns the default language for all domains.
        
        :return: The language code.
        :rtype: str
        """
        return self._lang
    
    def getDomain(self,domain):
        """
        Returns the translation domain for the given domain name.
        
        :param domain: The domain name.
       
----------
Solution: 

        self.lang = lang
        self.peng.cfg["i18n.lang"] = lang
        
        if lang not in self.cache:
            self.cache[lang]={}
        
        self.doAction("setlang")
        self.peng.sendEvent("peng3d:i18n.set_lang",{"lang":self.lang,"i18n":self})
--------------------
Prompt: 
def end_group(self, dedent=0, close=''):
        """End a group. See `begin_group` for more details."""
----------
Output: 

        self.write(self.indent * dedent, close)
        self.indent -= 1

    def begin_group(self, name, dedent=0, open=''):
        """Begin a group.

        A group is a block of code that is indented.

        Parameters
        ----------
        name : str
            The name of the group.
        dedent : int, optional
            The number of indents to dedent the group.
        open : str, optional
            The opening string of the group.

       
----------
Solution: 

        self.indentation -= dedent
        group = self.group_stack.pop()
        if not group.breakables:
            self.group_queue.remove(group)
        if close:
            self.text(close)
--------------------
Prompt: 
def revoke_token(self, token, token_type_hint, request, *args, **kwargs):
        """Revoke an access or refresh token.
        """
----------
Output: 

        token = self.get_token(token)
        if token is None:
            raise errors.InvalidRequestError(
                description='Invalid token: {}'.format(token))

        if token_type_hint == 'access_token':
            token.revoke()
        elif token_type_hint == 'refresh_token':
            token.refresh_token.revoke()
        else:
            raise errors.InvalidRequestError(
                description='Unsupported token type: {}'.format(
                    token_type_hint))
----------
Solution: 

        if token_type_hint:
            tok = self._tokengetter(**{token_type_hint: token})
        else:
            tok = self._tokengetter(access_token=token)
            if not tok:
                tok = self._tokengetter(refresh_token=token)

        if tok:
            request.client_id = tok.client_id
            request.user = tok.user
            tok.delete()
            return True

        msg = 'Invalid token supplied.'
        log.debug(msg)
        request.error_message = msg
        return False
--------------------
Prompt: 
def is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:
    """Check if the postponed evaluation of annotations is enabled"""
----------
Output: 

    return node.root().get_bool_value(POSTPONED_EVALUATION_ENABLED_KEY, default=False)


def is_postponed_evaluation_enabled_for_node(node: astroid.node_classes.NodeNG) -> bool:
    """Check if the postponed evaluation of annotations is enabled for a node"""
    return is_postponed_evaluation_enabled(node) and not is_postponed_evaluation_disabled_for_node(node)


def is_post
----------
Solution: 

    name = "annotations"
    module = node.root()
    stmt = module.locals.get(name)
    return (
        stmt
        and isinstance(stmt[0], astroid.ImportFrom)
        and stmt[0].modname == "__future__"
    )
--------------------
Prompt: 
def _log(cls, level, msg, **kwargs):
        """
        Logs a message with pre-formatted prefix.

        :param int level:
            Logging level as specified in the
            `login module <http://docs.python.org/2/library/logging.html>`_ of
            Python standard library.

        :param str msg:
            The actual message.

        """
----------
Output: 

        if not cls.logger:
            return

        if cls.logger.isEnabledFor(level):
            cls.logger.log(level, "[%s] %s", cls.__name__, msg, **kwargs)

    @classmethod
    def debug(cls, msg, **kwargs):
        """
        Logs a message with DEBUG level.

        :param str msg:
            The actual message.

        """
        cls._log(logging.DEBUG, msg, **kwargs)

    @classmethod

----------
Solution: 


        logger = getattr(cls, '_logger', None) or authomatic.core._logger
        logger.log(
            level, ': '.join(
                ('authomatic', cls.__name__, msg)), **kwargs)
--------------------
Prompt: 
def print_list_lines(self, filename, first, last):
        """The printing (as opposed to the parsing part of a 'list'
        command."""
----------
Output: 

        if first is None:
            first = 1
        if last is None:
            last = self.num_lines
        if last < first:
            first, last = last, first
        if first < 1:
            first = 1
        if last > self.num_lines:
            last = self.num_lines
        if first > last:
            first, last = last, first
        if first == last:
            self.write('%d\n' % first)
        else:
            self.write('%d
----------
Solution: 

        try:
            Colors = self.color_scheme_table.active_colors
            ColorsNormal = Colors.Normal
            tpl_line = '%%s%s%%s %s%%s' % (Colors.lineno, ColorsNormal)
            tpl_line_em = '%%s%s%%s %s%%s%s' % (Colors.linenoEm, Colors.line, ColorsNormal)
            src = []
            for lineno in range(first, last+1):
                line = linecache.getline(filename, lineno)
                if not line:
                    break

                if lineno == self.curframe.f_lineno:
                    line = self.__format_line(tpl_line_em, filename, lineno, line, arrow = True)
                else:
                    line = self.__format_line(tpl_line, filename, lineno, line, arrow = False)

                src.append(line)
                self.lineno = lineno

            print >>io.stdout, ''.join(src)

        except KeyboardInterrupt:
            pass
--------------------
Prompt: 
def _propagate_mean(mean, linop, dist):
  """Propagate a mean through linear Gaussian transformation."""
----------
Output: 

  if linop is None:
    return mean
  if dist is None:
    return linop.matvec(mean)
  else:
    return linop.matvec(mean) + dist.mean()


def _propagate_covariance(cov, linop, dist):
  """Propagate a covariance through linear Gaussian transformation."""
  if linop is None:
    return cov
  if dist is None:
    return linop.matvec(cov) @ linop.matvec(cov).T
 
----------
Solution: 

  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]
--------------------
Prompt: 
def get_cdata(self, *args):
        """
        all args-->_cffi_backend.buffer
        Returns-->cdata (if a SINGLE argument was provided)
                  LIST of cdata (if a args was a tuple or list)
        """
----------
Output: 

        if len(args) == 1:
            return self._cffi_backend.buffer(args[0])
        else:
            return [self._cffi_backend.buffer(arg) for arg in args]

    def get_cdata_pointer(self, *args):
        """
        all args-->_cffi_backend.buffer
        Returns-->cdata (if a SINGLE argument was provided)
                  LIST of cdata (if a args was a tuple or list)
        """
        if len(args
----------
Solution: 

        res = tuple([
            self.from_buffer(x) for x in args
        ])

        if len(res) == 0:
            return None
        elif len(res) == 1:
            return res[0]
        else:
            return res
--------------------
Prompt: 
def sourceWatchdog(self):
        """Watchdog timer function. 

        Recreates sources which have not generated events in 10*interval if
        they have watchdog set to true in their configuration
        """
----------
Output: 

        if self.watchdog_timer is not None:
            self.watchdog_timer.cancel()
        self.watchdog_timer = None
        for source in self.sources:
            if source.watchdog:
                if source.last_event_time is None:
                    self.logger.info("Watchdog: recreating source %s" % source.name)
                    self.createSource(source.name)

    def createSource(self, name):
        """Creates a source.

        Args:
            name
----------
Solution: 

        for i, source in enumerate(self.sources):
            if not source.config.get('watchdog', False):
                continue 
            sn = repr(source)
            last = self.lastEvents.get(source, None)
            if last:
                try:
                    if last < (time.time()-(source.inter*10)):
                        log.msg("Trying to restart stale source %s: %ss" % (
                            sn, int(time.time() - last)
                        ))

                        s = self.sources.pop(i)
                        try:
                            s.t.stop()
                        except Exception as e:
                            log.msg("Could not stop timer for %s: %s" % (
                                sn, e))

                        config = copy.deepcopy(s.config)

                        del self.lastEvents[source]
                        del s, source

                        source = self.createSource(config)

                        reactor.callLater(0, self._startSource, source)
                except Exception as e:
                    log.msg("Could not reset source %s: %s" % (
                        sn, e))
--------------------
Prompt: 
def get_key_for_purpose_and_type(self, purpose, key_type):
        """
        Gets a list of keys that match the purpose and key_type, and returns the first key in that list
        Note, if there are many keys that match the criteria, the one you get back will be random from that list
        :returns: A key object that matches the criteria
        """
----------
Output: 

        keys = self.get_keys_for_purpose_and_type(purpose, key_type)
        if len(keys) == 0:
            raise Exception("No keys found for purpose: " + str(purpose) + " and key_type: " + str(key_type))
        return keys[0]

    def get_keys_for_purpose_and_type(self, purpose, key_type):
        """
        Gets a list of keys that match the purpose and key_type
        :returns: A list
----------
Solution: 

        key = [key for key in self.keys.values() if key.purpose == purpose and key.key_type == key_type]
        try:
            return key[0]
        except IndexError:
            return None
--------------------
Prompt: 
def _trj_fill_run_table(self, traj, start, stop):
        """Fills the `run` overview table with information.

        Will also update new information.

        """
----------
Output: 

        # Get the run table
        run_table = self.run_table

        # Get the run table model
        run_model = run_table.model()

        # Get the run table selection
        run_selection = run_table.selectionModel()

        # Get the run table selection model
        run_selection_model = run_selection.model()

        # Get the run table selection model row count
        run_selection_model_row_count = run_selection_model.rowCount()

        # Get the run table selection model
----------
Solution: 


        def _make_row(info_dict):
            row = (info_dict['idx'],
                   info_dict['name'],
                   info_dict['time'],
                   info_dict['timestamp'],
                   info_dict['finish_timestamp'],
                   info_dict['runtime'],
                   info_dict['parameter_summary'],
                   info_dict['short_environment_hexsha'],
                   info_dict['completed'])
            return row

        runtable = getattr(self._overview_group, 'runs')

        rows = []
        updated_run_information = traj._updated_run_information
        for idx in range(start, stop):
            info_dict = traj._run_information[traj._single_run_ids[idx]]
            rows.append(_make_row(info_dict))
            updated_run_information.discard(idx)

        if rows:
            runtable.append(rows)
            runtable.flush()

        # Store all runs that are updated and that have not been stored yet
        rows = []
        indices = []
        for idx in updated_run_information:
            info_dict = traj.f_get_run_information(idx, copy=False)
            rows.append(_make_row(info_dict))
            indices.append(idx)

        if rows:
            runtable.modify_coordinates(indices, rows)

        traj._updated_run_information = set()
--------------------
Prompt: 
def to_service(self, service, version):
        """Sets the service name and version the request should target

        Args:
            service (str): The name of the service as displayed in the services.json file
            version (str): The version of the service as displayed in the services.json file

        Returns:
            The request builder instance in order to chain calls
        """
----------
Output: 

        self._service = service
        self._version = version
        return self

    def with_path(self, path):
        """Sets the path the request should target

        Args:
            path (str): The path the request should target

        Returns:
            The request builder instance in order to chain calls
        """
        self._path = path
        return self

    def with_method(self, method):
        """Sets the method the request should use

        Args:
            method (str): The method the request
----------
Solution: 

        service_url = self._service_locator.get_service_url(service, version)
        return self.__copy_and_set('service_url', self.__strip_trailing_slashes(service_url))
--------------------
Prompt: 
def _cmd(self, cmd, *args, **kw):
        """
        write a single command, with variable number of arguments. after the
        command, the device must return ACK
        """
----------
Output: 

        self.write(cmd)
        self.write(*args)
        self.write(b'\r')
        self.expect(b'ACK')

    def _cmd_ack(self, cmd, *args, **kw):
        """
        write a single command, with variable number of arguments. after the
        command, the device must return ACK
        """
        self.write(cmd)
        self.write(*args)
        self.write(b'\r')
        self.expect(b'ACK')

    def _cmd
----------
Solution: 

        ok = kw.setdefault('ok', False)

        self._wakeup()
        if args:
            cmd = "%s %s" % (cmd, ' '.join(str(a) for a in args))
        for i in xrange(3):
            log.info("send: " + cmd)
            self.port.write(cmd + '\n')
            if ok:
                ack = self.port.read(len(self.OK))  # read OK
                log_raw('read', ack)
                if ack == self.OK:
                    return
            else:
                ack = self.port.read(len(self.ACK))  # read ACK
                log_raw('read', ack)
                if ack == self.ACK:
                    return
        raise NoDeviceException('Can not access weather station')
--------------------
Prompt: 
def prefilter_lines(self, lines, continue_prompt=False):
        """Prefilter multiple input lines of text.

        This is the main entry point for prefiltering multiple lines of
        input.  This simply calls :meth:`prefilter_line` for each line of
        input.

        This covers cases where there are multiple lines in the user entry,
        which is the case when the user goes back to a multiline history
        entry and presses enter.
        """
----------
Output: 

        for line in lines:
            self.prefilter_line(line, continue_prompt)

    def prefilter_line(self, line, continue_prompt=False):
        """Prefilter a single input line of text.

        This is the main entry point for prefiltering a single line of input.

        This covers cases where there is a single line of input, which is the
        case when the user presses enter.
        """
        if self.prefilter_func is None:
            return line

----------
Solution: 

        llines = lines.rstrip('\n').split('\n')
        # We can get multiple lines in one shot, where multiline input 'blends'
        # into one line, in cases like recalling from the readline history
        # buffer.  We need to make sure that in such cases, we correctly
        # communicate downstream which line is first and which are continuation
        # ones.
        if len(llines) > 1:
            out = '\n'.join([self.prefilter_line(line, lnum>0)
                             for lnum, line in enumerate(llines) ])
        else:
            out = self.prefilter_line(llines[0], continue_prompt)

        return out
--------------------
Prompt: 
def app_template_filter(self, name=None):
        """Register a custom template filter, available application wide.  Like
        :meth:`Flask.template_filter` but for a blueprint.

        :param name: the optional name of the filter, otherwise the
                     function name will be used.
        """
----------
Output: 

        def decorator(f):
            self.add_app_template_filter(f, name=name)
            return f
        return decorator

    def add_app_template_filter(self, f, name=None):
        """Register a custom template filter, available application wide.  Like
        :meth:`Flask.add_template_filter` but for a blueprint.

        :param name: the optional name of the filter, otherwise the
                     function name will be used.
        """
        if name is None:
           
----------
Solution: 

        def decorator(f):
            self.add_app_template_filter(f, name=name)
            return f
        return decorator
--------------------
Prompt: 
def _replace_at_index(x, index, replacement):
  """Replaces an element at supplied index."""
----------
Output: 

  return x[:index] + [replacement] + x[index + 1:]


def _replace_at_index_in_list(x, index, replacement):
  """Replaces an element at supplied index."""
  return [_replace_at_index(x_i, index, replacement) for x_i in x]


def _replace_at_index_in_dict(x, index, replacement):
  """Replaces an element at supplied index."""
  return {k: _replace_at_index(x_i,
----------
Solution: 

  x_new = tf.concat([x[:index], tf.expand_dims(replacement, axis=0),
                     x[(index + 1):]], axis=0)
  return x_new
--------------------
Prompt: 
def converted(self):
        """
        :rtype: {str: object}
        """
----------
Output: 

        return self._converted

    @converted.setter
    def converted(self, value):
        """
        :param value: {str: object}
        :type value: dict
        """
        self._converted = value

    @property
    def converted_to_string(self):
        """
        :rtype: {str: str}
        """
        return self._converted_to_string

    @converted_to_string.setter
    def converted_to_string(self, value):

----------
Solution: 

        if self._converted is None:
            self._converted = self.converter(self.name, self.value)
        return self._converted
--------------------
Prompt: 
def categories(self):
        """
        Return the list of levels for an enum (categorical) column.

        This function can only be applied to single-column categorical frame.
        """
----------
Output: 

        if self._is_categorical:
            return self._categorical_column.categories
        else:
            raise ValueError("This function can only be applied to single-column categorical frame.")

    def is_categorical(self):
        """
        Return True if the frame is categorical.
        """
        return self._is_categorical

    def is_continuous(self):
        """
        Return True if the frame is continuous.
        """
        return self._is_continuous

    def is_
----------
Solution: 

        if self.ncols != 1:
            raise H2OValueError("This operation only applies to a single factor column")
        if self.types[self.names[0]] != "enum":
            raise H2OValueError("Input is not a factor. This operation only applies to a single factor column")
        return self.levels()[0]
--------------------
Prompt: 
def mixin(cls, mixin_cls):
        """Decorator for mixing in additional functionality into field type

        Example:

        >>> @Integer.mixin
        ... class IntegerPostgresExtensions:
        ...     postgres_type = 'INT'
        ...
        ...     def postgres_dump(self, obj):
        ...         self.dump(obj) + "::integer"

        Is roughly equivalent to:

        >>> Integer.postgres_type = 'INT'
        ...
        ... def postgres_dump(self, obj):
        ...     self.dump(obj) + "::integer"
        ...
        ... Integer.postgres_dump = postgres_dump

        """
----------
Output: 
, obj):
        ...     self.dump(obj) + "::integer"
        ...
        ... Integer.postgres_dump = postgres_dump

        """
        for name, value in mixin_cls.__dict__.items():
            if name.startswith('_'):
                continue
            setattr(cls, name, value)
        return cls

    def __init__(self, *args, **kwargs):
        super(Field, self).__init__(*args, **kwargs)
        self.name = self.name or self.__
----------
Solution: 

        for item_name in dir(mixin_cls):
            if item_name.startswith("__"):
                # don't copy magic properties
                continue
            item = getattr(mixin_cls, item_name)

            if isinstance(item, types.MethodType):
                # unbound method will cause problems
                # so get the underlying function instead
                item = item.im_func

            setattr(cls, item_name, item)
        return mixin_cls
--------------------
Prompt: 
def get_annotation_data_between_times(self, id_tier, start, end):
        """Gives the annotations within the times.
        When the tier contains reference annotations this will be returned,
        check :func:`get_ref_annotation_data_between_times` for the format.

        :param str id_tier: Name of the tier.
        :param int start: Start time of the annotation.
        :param int end: End time of the annotation.
        :returns: List of annotations within that time.
        :raises KeyError: If the tier is non existent.
        """
----------
Output: 
TierNotFoundError: When the tier is not found.
        :raises TierNotReferenceError: When the tier is not a reference tier.
        """
        tier = self.get_tier(id_tier)
        if not tier.is_reference:
            raise TierNotReferenceError(
                "Tier {} is not a reference tier".format(id_tier))
        return tier.get_ref_annotation_data_between_times(start, end)

    def get_ref_annotation_data_between_times(self, id_t
----------
Solution: 

        if self.tiers[id_tier][1]:
            return self.get_ref_annotation_data_between_times(
                id_tier, start, end)
        anns = ((self.timeslots[a[0]], self.timeslots[a[1]], a[2])
                for a in self.tiers[id_tier][0].values())
        return sorted(a for a in anns if a[1] >= start and a[0] <= end)
--------------------
Prompt: 
def __deftype_method_recur_to_py_ast(
    ctx: GeneratorContext, node: Recur
) -> GeneratedPyAST:
    """Return a Python AST node for `recur` occurring inside a `deftype*` method."""
----------
Output: 

    return ctx.py_ast_from_node(node.expr)


def __deftype_method_recur_to_ast(
    ctx: GeneratorContext, node: Recur
) -> GeneratedAST:
    """Return an AST node for `recur` occurring inside a `deftype*` method."""
    return ctx.ast_from_node(node.expr)


def __deftype_method_recur_to_c_ast(
    ctx: GeneratorContext, node: Recur
)
----------
Solution: 

    assert node.op == NodeOp.RECUR
    recur_nodes: List[ast.AST] = []
    recur_deps: List[ast.AST] = []
    for expr in node.exprs:
        expr_ast = gen_py_ast(ctx, expr)
        recur_nodes.append(expr_ast.node)
        recur_deps.extend(expr_ast.dependencies)

    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)
    assert this_entry is not None, "Field type local must have this"

    return GeneratedPyAST(
        node=ast.Call(
            func=_TRAMPOLINE_ARGS_FN_NAME,
            args=list(
                chain(
                    [
                        ast.NameConstant(ctx.recur_point.is_variadic),
                        ast.Name(id=this_entry.munged, ctx=ast.Load()),
                    ],
                    recur_nodes,
                )
            ),
            keywords=[],
        ),
        dependencies=recur_deps,
    )
--------------------
Prompt: 
def _random_adjspecies_pair():
    """Return an ordered 2-tuple containing a species and a describer."""
----------
Output: 

    return (
        random.choice(species),
        random.choice(describers),
    )


def _random_adjspecies_pair():
    """Return an ordered 2-tuple containing a species and a describer."""
    return (
        random.choice(species),
        random.choice(describers),
    )


def _random_adjspecies_pair_with_species_in_first_place():
    """Return an ordered 2-tuple containing a species and a describer."""
    return (
----------
Solution: 

    describer, desc_position = random_describer()
    if desc_position == 'prefix':
        return (describer, random_species())
    elif desc_position == 'suffix':
        return (random_species(), describer)
--------------------
Prompt: 
def _run_cmd_line_code(self):
        """Run code or file specified at the command-line"""
----------
Output: 

        if self.args.code:
            self.run_code(self.args.code)
        elif self.args.file:
            self.run_file(self.args.file)
        else:
            self.run_file(self.args.file)

    def _run_file(self, filename):
        """Run a file"""
        with open(filename, 'r') as f:
            code = f.read()
        self.run_code(code)

    def _run_code(self, code
----------
Solution: 

        if self.code_to_run:
            line = self.code_to_run
            try:
                self.log.info("Running code given at command line (c=): %s" %
                              line)
                self.shell.run_cell(line, store_history=False)
            except:
                self.log.warn("Error in executing line in user namespace: %s" %
                              line)
                self.shell.showtraceback()

        # Like Python itself, ignore the second if the first of these is present
        elif self.file_to_run:
            fname = self.file_to_run
            try:
                self._exec_file(fname)
            except:
                self.log.warn("Error in executing file in user namespace: %s" %
                              fname)
                self.shell.showtraceback()
--------------------
Prompt: 
def _first_glimpse_sensor(self, x_t):
        """
        Compute first glimpse position using down-sampled image.
        """
----------
Output: 

        x_t = x_t.view(x_t.size(0), -1)
        x_t = self.first_glimpse_sensor(x_t)
        x_t = x_t.view(x_t.size(0), -1, 1, 1)
        return x_t

    def _second_glimpse_sensor(self, x_t):
        """
        Compute second glimpse position using down-sampled image.
        """
        x_t = x
----------
Solution: 

        downsampled_img = theano.tensor.signal.downsample.max_pool_2d(x_t, (4,4))
        downsampled_img = downsampled_img.flatten()
        first_l = T.dot(downsampled_img, self.W_f)
        if self.disable_reinforce:
            wf_grad = self.W_f
            if self.random_glimpse:
                first_l = self.srng.uniform((2,), low=-1.7, high=1.7)
        else:
            sampled_l_t = self._sample_gaussian(first_l, self.cov)
            sampled_pdf = self._multi_gaussian_pdf(disconnected_grad(sampled_l_t), first_l)
            wf_grad = T.grad(T.log(sampled_pdf), self.W_f)
            first_l = sampled_l_t
        return first_l, wf_grad
--------------------
Prompt: 
def makeReturnFormat(self):
        """ Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. """
----------
Output: 

        return self.m_req.strip(self.m_reserved)

    def makeReturnFormat(self):
        """ Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. """
        return self.m_req.strip(self.m_reserved)

    def makeReturnFormat(self):
        """ Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. """
        return self.m_req.strip(self.m_reserved)


----------
Solution: 

        for fld in self.m_blk_a:
            compare_fld = fld.upper()
            if not "RESERVED" in compare_fld and not "CRC" in compare_fld:
                self.m_req[fld] = self.m_blk_a[fld]
        pass
--------------------
Prompt: 
def add_attachment(self, filename, open_file):
        """
        Adds an attachment to this card.
        """
----------
Output: 

        return self.trello.add_attachment(self.id, filename, open_file)

    def add_checklist(self, name):
        """
        Adds a checklist to this card.
        """
        return self.trello.add_checklist(self.id, name)

    def add_label(self, label):
        """
        Adds a label to this card.
        """
        return self.trello.add_label(self.id, label)

    def add_member
----------
Solution: 

        fields = {
            'api_key': self.client.api_key,
            'token': self.client.user_auth_token
        }

        content_type, body = self.encode_multipart_formdata(
            fields=fields,
            filename=filename,
            file_values=open_file
        )

        return self.fetch_json(
            uri_path=self.base_uri + '/attachments',
            http_method='POST',
            body=body,
            headers={'Content-Type': content_type},
        )
--------------------
Prompt: 
def configure(self, options, conf):
        """
        Configure plugin.
        """
----------
Output: 

        self.options = options
        self.conf = conf

    def get_name(self):
        """
        Return plugin name.
        """
        return self.name

    def get_description(self):
        """
        Return plugin description.
        """
        return self.description

    def get_version(self):
        """
        Return plugin version.
        """
        return self.version

    def get_author(self):
        """
        Return plugin author.
        """
        return self.author


----------
Solution: 

        try:
            self.status.pop('active')
        except KeyError:
            pass
        super(Coverage, self).configure(options, conf)
        if conf.worker:
            return
        if self.enabled:
            try:
                import coverage
            except ImportError:
                log.error("Coverage not available: "
                          "unable to import coverage module")
                self.enabled = False
                return
        self.conf = conf
        self.coverErase = options.cover_erase
        self.coverTests = options.cover_tests
        self.coverPackages = []
        if options.cover_packages:
            for pkgs in [tolist(x) for x in options.cover_packages]:
                self.coverPackages.extend(pkgs)
        self.coverInclusive = options.cover_inclusive
        if self.coverPackages:
            log.info("Coverage report will include only packages: %s",
                     self.coverPackages)
        self.coverHtmlDir = None
        if options.cover_html:
            self.coverHtmlDir = options.cover_html_dir
            log.debug('Will put HTML coverage report in %s', self.coverHtmlDir)
        self.coverBranches = options.cover_branches
        self.coverXmlFile = None
        if options.cover_min_percentage:
            self.coverMinPercentage = int(options.cover_min_percentage.rstrip('%'))
        if options.cover_xml:
            self.coverXmlFile = options.cover_xml_file
            log.debug('Will put XML coverage report in %s', self.coverXmlFile)
        if self.enabled:
            self.status['active'] = True
            self.coverInstance = coverage.coverage(auto_data=False,
                branch=self.coverBranches, data_suffix=None)
--------------------
Prompt: 
def _select_next_server(self):
        """
        Looks up in the server pool for an available server
        and attempts to connect.
        """
----------
Output: 

        # TODO: implement
        pass

    def _select_next_server_and_connect(self):
        """
        Looks up in the server pool for an available server
        and attempts to connect.
        """
        # TODO: implement
        pass

    def _select_next_server_and_connect_with_timeout(self, timeout):
        """
        Looks up in the server pool for an available server
        and attempts to connect.
        """
        # TODO: implement
        pass

    def _select_next_server_
----------
Solution: 


        while True:
            if len(self._server_pool) == 0:
                self._current_server = None
                raise ErrNoServers

            now = time.monotonic()
            s = self._server_pool.pop(0)
            if self.options["max_reconnect_attempts"] > 0:
                if s.reconnects > self.options["max_reconnect_attempts"]:
                    # Discard server since already tried to reconnect too many times
                    continue

            # Not yet exceeded max_reconnect_attempts so can still use
            # this server in the future.
            self._server_pool.append(s)
            if s.last_attempt is not None and now < s.last_attempt + self.options["reconnect_time_wait"]:
                # Backoff connecting to server if we attempted recently.
                yield from asyncio.sleep(self.options["reconnect_time_wait"], loop=self._loop)
            try:
                s.last_attempt = time.monotonic()
                r, w = yield from asyncio.open_connection(
                    s.uri.hostname,
                    s.uri.port,
                    loop=self._loop,
                    limit=DEFAULT_BUFFER_SIZE)
                self._current_server = s

                # We keep a reference to the initial transport we used when
                # establishing the connection in case we later upgrade to TLS
                # after getting the first INFO message. This is in order to
                # prevent the GC closing the socket after we send CONNECT
                # and replace the transport.
                #
                # See https://github.com/nats-io/asyncio-nats/issues/43
                self._bare_io_reader = self._io_reader = r
                self._bare_io_writer = self._io_writer = w
                break
            except Exception as e:
                s.last_attempt = time.monotonic()
                s.reconnects += 1

                self._err = e
                if self._error_cb is not None:
                    yield from self._error_cb(e)
                continue
--------------------
Prompt: 
def time_remaining_est(self, completed_iter):
        """Estimate the remaining time left.

        Parameters:
            completed_iter (int): Number of iterations completed.

        Returns:
            est_time: Estimated time remaining.
        """
----------
Output: 

        if self.time_est is None:
            return None
        if self.time_est.total_time is None:
            return None
        if self.time_est.total_time == 0:
            return None
        if self.time_est.total_time < 1:
            return None
        if self.time_est.total_time < 10:
            return None
        if self.time_est.total_time < 60:
            return None
        if self.time_est.total_time
----------
Solution: 

        if completed_iter:
            t_r_est = (time.time() - self.t_start) / \
                completed_iter*(self.iter-completed_iter)
        else:
            t_r_est = 0
        date_time = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=t_r_est)
        time_string = "%02d:%02d:%02d:%02d" % \
            (date_time.day - 1, date_time.hour, date_time.minute, date_time.second)

        return time_string
--------------------
Prompt: 
def bkg_subtract(self, analytes=None, errtype='stderr', focus_stage='despiked'):
        """
        Subtract calculated background from data.

        Must run bkg_calc first!

        Parameters
        ----------
        analytes : str or iterable
            Which analyte(s) to subtract.
        errtype : str
            Which type of error to propagate. default is 'stderr'.
        focus_stage : str
            Which stage of analysis to apply processing to. 
            Defaults to 'despiked' if present, or 'rawdata' if not. 
            Can be one of:
            * 'rawdata': raw data, loaded from csv file.
            * 'despiked': despiked data.
            * 'signal'/'background': isolated signal and background data.
              Created by self.separate, after signal and background
              regions have been identified by self.autorange.
            * 'bkgsub': background subtracted data, created by 
              self.bkg_correct
            * 'ratios': element ratio data, created by self.ratio.
            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.
        """
----------
Output: 
default is 'despiked'.

        Returns
        -------
        None
        """
        if analytes is None:
            analytes = self.analytes
        if not isinstance(analytes, (list, tuple, np.ndarray)):
            analytes = [analytes]
        if focus_stage not in self.stages:
            raise ValueError('focus_stage must be one of the following: {}'.format(self.stages))
        if errtype not in ['stderr', 'std']:
            raise ValueError('err
----------
Solution: 

        if analytes is None:
            analytes = self.analytes
        elif isinstance(analytes, str):
            analytes = [analytes]

        if focus_stage == 'despiked':
            if 'despiked' not in self.stages_complete:
                focus_stage = 'rawdata'

        # make uncertainty-aware background interpolators
        bkg_interps = {}
        for a in analytes:
            bkg_interps[a] = un_interp1d(x=self.bkg['calc']['uTime'],
                                         y=un.uarray(self.bkg['calc'][a]['mean'],
                                                     self.bkg['calc'][a][errtype]))
        self.bkg_interps = bkg_interps

        # apply background corrections
        with self.pbar.set(total=len(self.data), desc='Background Subtraction') as prog:
            for d in self.data.values():
                # [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), None, focus_stage=focus_stage) for a in analytes]
                [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), ~d.sig, focus_stage=focus_stage) for a in analytes]
                d.setfocus('bkgsub')

                prog.update()

        self.stages_complete.update(['bkgsub'])
        self.focus_stage = 'bkgsub'
        return
--------------------
Prompt: 
def find_bp(self, filename, lineno, frame):
        """Determine which breakpoint for this file:line is to be acted upon.

        Called only if we know there is a bpt at this
        location.  Returns breakpoint that was triggered and a flag
        that indicates if it is ok to delete a temporary breakpoint.

        """
----------
Output: 

        # First, check if we have a temporary breakpoint at this
        # location.  If so, we must delete it.
        bp = self.get_temporary_breakpoint(filename, lineno)
        if bp is not None:
            return bp, True

        # Next, check if we have a permanent breakpoint at this
        # location.  If so, we must return it.
        bp = self.get_breakpoint(filename, lineno)
        if bp is not None:
            return bp,
----------
Solution: 

        possibles = self.bplist[filename, lineno]
        for i in range(0, len(possibles)):
            b = possibles[i]
            if not b.enabled:
                continue
            if not checkfuncname(b, frame):
                continue
            # Count every hit when bp is enabled
            b.hits += 1
            if not b.condition:
                # If unconditional, and ignoring, go on to next, else
                # break
                if b.ignore > 0:
                    b.ignore = b.ignore -1
                    continue
                else:
                    # breakpoint and marker that's ok to delete if
                    # temporary
                    return (b, True)
            else:
                # Conditional bp.
                # Ignore count applies only to those bpt hits where the
                # condition evaluates to true.
                try:
                    val = eval(b.condition, frame.f_globals, frame.f_locals)
                    if val:
                        if b.ignore > 0:
                            b.ignore = b.ignore -1
                            # continue
                        else:
                            return (b, True)
                    # else:
                    #   continue
                except:
                    # if eval fails, most conservative thing is to
                    # stop on breakpoint regardless of ignore count.
                    # Don't delete temporary, as another hint to user.
                    return (b, False)
                pass
            pass
        return (None, None)
--------------------
Prompt: 
def _to_dataframe(self):
        """Read and return the dataset contents as a pandas DataFrame."""
----------
Output: 

        return self._to_pandas()

    def _to_pandas(self):
        """Read and return the dataset contents as a pandas DataFrame."""
        return self._to_table().to_pandas()

    def _to_table(self):
        """Read and return the dataset contents as a pyarrow.Table."""
        return self._to_pyarrow()

    def _to_pyarrow(self):
        """Read and return the dataset contents as a pyarrow.Table."""
        return self._to_arrow()

   
----------
Solution: 

        #TODO: figure out why passing in the opened stream directly gives invalid data
        data = self.read_as_binary()
        reader = BytesIO(data)
        return deserialize_dataframe(reader, self.data_type_id)
--------------------
Prompt: 
def create_tfs_git_client(url, token=None):
    """
    Creates a TFS Git Client to pull Git repo info
    """
----------
Output: 

    if token:
        tfs_git_client = TfsGitClient(url, token)
    else:
        tfs_git_client = TfsGitClient(url)
    return tfs_git_client


def get_git_repo_info(tfs_git_client, repo_name):
    """
    Gets the Git repo info for the given repo name
    """
    git_repo_info = tfs_git_client.get_git_repo_info(repo_name)
    return git_repo_
----------
Solution: 

    if token is None:
        token = os.environ.get('TFS_API_TOKEN', None)

    tfs_connection = create_tfs_connection(url, token)
    tfs_git_client = tfs_connection.get_client('vsts.git.v4_1.git_client.GitClient')

    if tfs_git_client is None:
        msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'
        raise RuntimeError(msg, url)

    return tfs_git_client
--------------------
Prompt: 
def canFetchMore(self, index):
        """Return if more data available for *index*."""
----------
Output: 

        return self.index < len(self.data)

    def fetchMore(self, index):
        """Fetch more data for *index*."""
        self.beginInsertRows(QtCore.QModelIndex(), index, index)
        self.endInsertRows()

    def data(self, index, role):
        """Return data for *index*."""
        if not index.isValid():
            return None
        if role == QtCore.Qt.DisplayRole:
            return self.data[index.row()]
        return None
----------
Solution: 

        if not index.isValid():
            item = self.root
        else:
            item = index.internalPointer()

        return item.canFetchMore()
--------------------
Prompt: 
def info_formatter(info):
    """Produce a sequence of formatted lines from info.

    `info` is a sequence of pairs (label, data).  The produced lines are
    nicely formatted, ready to print.

    """
----------
Output: 

    lines = []
    for label, data in info:
        if isinstance(data, str):
            lines.append(f'{label}: {data}')
        elif isinstance(data, (list, tuple)):
            lines.append(f'{label}:')
            for line in data:
                lines.append(f'  {line}')
        elif isinstance(data, dict):
            lines.append(f'{label}:')
            for key, value in data.items():
                lines.append(f'  {key
----------
Solution: 

    label_len = max([len(l) for l, _d in info])
    for label, data in info:
        if data == []:
            data = "-none-"
        if isinstance(data, (list, tuple)):
            prefix = "%*s:" % (label_len, label)
            for e in data:
                yield "%*s %s" % (label_len+1, prefix, e)
                prefix = ""
        else:
            yield "%*s: %s" % (label_len, label, data)
--------------------
Prompt: 
async def handle_request(self, request: Request
                             ) -> typing.Tuple[AsyncGeneratorType, Response]:
        """
        Wrap request with middleware.
        :param request:
        :return:
        """
----------
Output: 

        for middleware in self.middlewares:
            request = middleware(request)
        return self.app(request)

    def __call__(self, request: Request) -> typing.Tuple[AsyncGeneratorType, Response]:
        """
        Wrap request with middleware.
        :param request:
        :return:
        """
        return self.handle_request(request)

    def add_middleware(self, middleware: typing.Callable) -> None:
        """
        Add middleware to the list.
       
----------
Solution: 

        callback_result, response = None, None

        await self._run_request_middleware(request)
        try:
            callback_result, response = await request.fetch_callback(self.sem)
        except NotImplementedParseError as e:
            self.logger.error(e)
        except NothingMatchedError as e:
            self.logger.error(f'<Item: {str(e).lower()}>')
        except Exception as e:
            self.logger.error(f'<Callback[{request.callback.__name__}]: {e}')
        await self._run_response_middleware(request, response)
        await self._process_response(request=request, response=response)
        return callback_result, response
--------------------
Prompt: 
def get_connection(host='localhost', port=27017, username=None, password=None,
                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):
    """Get a client to the mongo database

        host(str): Host of database
        port(int): Port of database
        username(str)
        password(str)
        uri(str)
        authdb (str): database to use for authentication
        timeout(int): How long should the client try to connect

    """
----------
Output: 
   Returns:
        pymongo.MongoClient: Client to the database
    """
    if uri is not None:
        client = MongoClient(uri, *args, **kwargs)
    elif mongodb is not None:
        client = MongoClient(mongodb, *args, **kwargs)
    else:
        client = MongoClient(host, port, *args, **kwargs)

    if username is not None and password is not None:
        client.admin.authenticate(username, password, source=authdb)

    return client


def
----------
Solution: 

    authdb = authdb or mongodb
    if uri is None:
        if username and password:
            uri = ("mongodb://{}:{}@{}:{}/{}"
                   .format(quote_plus(username), quote_plus(password), host, port, authdb))
            log_uri = ("mongodb://{}:****@{}:{}/{}"
                   .format(quote_plus(username), host, port, authdb))
        else:
            log_uri = uri = "mongodb://%s:%s" % (host, port)
            

    LOG.info("Try to connect to %s" % log_uri)
    try:
        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)
    except ServerSelectionTimeoutError as err:
        LOG.warning("Connection Refused")
        raise ConnectionFailure

    LOG.info("Connection established")
    return client
--------------------
Prompt: 
def _get_job(self, project_id, job_id):
        """
        Gets a MLEngine job based on the job name.

        :return: MLEngine job object if succeed.
        :rtype: dict

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned from server
        """
----------
Output: 

        try:
            return self.get_job(project_id, job_id)
        except HttpError as e:
            if e.resp.status == 404:
                return None
            else:
                raise

    def _get_job_status(self, project_id, job_id):
        """
        Gets the status of a MLEngine job based on the job name.

        :return: MLEngine job status if succeed.
        :rtype: dict

        Raises:
           
----------
Solution: 

        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)
        request = self._mlengine.projects().jobs().get(name=job_name)
        while True:
            try:
                return request.execute()
            except HttpError as e:
                if e.resp.status == 429:
                    # polling after 30 seconds when quota failure occurs
                    time.sleep(30)
                else:
                    self.log.error('Failed to get MLEngine job: {}'.format(e))
                    raise
--------------------
Prompt: 
def _closest_date(target_dt, date_list, before_target=None):
    """
    This function finds the date in a list closest to the target date.
    An optional parameter can be given to get the closest before or after.

    :param target_dt: The target date
    :type target_dt: datetime.date
    :param date_list: The list of dates to search
    :type date_list: list[datetime.date]
    :param before_target: closest before or after the target
    :type before_target: bool or None
    :returns: The closest date
    :rtype: datetime.date or None
    """
----------
Output: 
target: bool
    :return: The closest date
    :rtype: datetime.date
    """
    if before_target is None:
        before_target = False

    if before_target:
        date_list = sorted(date_list, reverse=True)

    closest_date = None
    closest_date_diff = None

    for date in date_list:
        date_diff = (target_dt - date).days
        if closest_date is None or (before_target and date_diff > closest_date_diff)
----------
Solution: 

    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max
    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max
    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt
    if before_target is None:
        return min(date_list, key=fnone).date()
    if before_target:
        return min(date_list, key=fb).date()
    else:
        return min(date_list, key=fa).date()
--------------------
Prompt: 
def _has_parent_of_type(node, node_type, statement):
    """Check if the given node has a parent of the given type."""
----------
Output: 

    if node.parent is None:
        return False
    if node.parent.type == node_type:
        return True
    return _has_parent_of_type(node.parent, node_type, statement)


def _get_parent_of_type(node, node_type, statement):
    """Get the parent of the given type."""
    if node.parent is None:
        return None
    if node.parent.type == node_type:
        return node.parent
    return _get_parent_of_type
----------
Solution: 

    parent = node.parent
    while not isinstance(parent, node_type) and statement.parent_of(parent):
        parent = parent.parent
    return isinstance(parent, node_type)
--------------------
Prompt: 
def _wrap_command(cmds, cls, strict=True):
    """Wrap a setup command

    Parameters
    ----------
    cmds: list(str)
        The names of the other commands to run prior to the command.
    strict: boolean, optional
        Whether to raise errors when a pre-command fails.
    """
----------
Output: 

    def _wrap(func):
        @wraps(func)
        def _wrapper(self, *args, **kwargs):
            for cmd in cmds:
                if not hasattr(self, cmd):
                    raise AttributeError(
                        "Command {0} not found. "
                        "Did you forget to import it?".format(cmd))
                getattr(self, cmd)(strict=strict)
            return func(self, *args, **kwargs)
        return _wrapper
    return _wrap


def _
----------
Solution: 

    class WrappedCommand(cls):

        def run(self):
            if not getattr(self, 'uninstall', None):
                try:
                    [self.run_command(cmd) for cmd in cmds]
                except Exception:
                    if strict:
                        raise
                    else:
                        pass
            # update package data
            update_package_data(self.distribution)

            result = cls.run(self)
            return result
    return WrappedCommand
--------------------
Prompt: 
def as_tuple(self):
        """
        :rtype: (str, object)
        """
----------
Output: 

        return (self.name, self.value)

    def __repr__(self):
        return "Parameter({0}, {1})".format(self.name, self.value)

    def __str__(self):
        return "{0}={1}".format(self.name, self.value)

    def __eq__(self, other):
        if isinstance(other, Parameter):
            return self.name == other.name and self.value == other.value
        return False

    def __ne__(self, other):

----------
Solution: 

        if self._as_tuple is None:
            self._as_tuple = self.converted.items()[0]
        return self._as_tuple
--------------------
Prompt: 
def add_virtual_columns_cartesian_to_spherical(self, x="x", y="y", z="z", alpha="l", delta="b", distance="distance", radians=False, center=None, center_name="solar_position"):
        """Convert cartesian to spherical coordinates.



        :param x:
        :param y:
        :param z:
        :param alpha:
        :param delta: name for polar angle, ranges from -90 to 90 (or -pi to pi when radians is True).
        :param distance:
        :param radians:
        :param center:
        :param center_name:
        :return:
        """
----------
Output: 
ians=True)
        :param distance: name for distance from center
        :param radians: if True, use radians instead of degrees
        :param center: center of the coordinate system
        :param center_name: name for center of the coordinate system
        :return:
        """
        if center is None:
            center = self.get_center(center_name)
        if center is not None:
            x = x - center[0]
            y = y - center[1]
            z = z - center[2]
        if rad
----------
Solution: 

        transform = "" if radians else "*180./pi"

        if center is not None:
            self.add_variable(center_name, center)
        if center is not None and center[0] != 0:
            x = "({x} - {center_name}[0])".format(**locals())
        if center is not None and center[1] != 0:
            y = "({y} - {center_name}[1])".format(**locals())
        if center is not None and center[2] != 0:
            z = "({z} - {center_name}[2])".format(**locals())
        self.add_virtual_column(distance, "sqrt({x}**2 + {y}**2 + {z}**2)".format(**locals()))
        # self.add_virtual_column(alpha, "((arctan2({y}, {x}) + 2*pi) % (2*pi)){transform}".format(**locals()))
        self.add_virtual_column(alpha, "arctan2({y}, {x}){transform}".format(**locals()))
        self.add_virtual_column(delta, "(-arccos({z}/{distance})+pi/2){transform}".format(**locals()))
--------------------
Prompt: 
def unix_ts(dtval):
    """Convert datetime into a unix timestamp.
    This is the equivalent to Python 3's int(datetime.timestamp()).

    :param dt: datetime to convert
    """
----------
Output: 

    return int(dtval.timestamp())


def unix_ts_to_datetime(ts):
    """Convert a unix timestamp into a datetime.

    :param ts: unix timestamp to convert
    """
    return datetime.fromtimestamp(ts)


def unix_ts_to_datetime_str(ts):
    """Convert a unix timestamp into a datetime string.

    :param ts: unix timestamp to convert
    """
    return unix_ts_to_datetime(ts).strftime("%Y-%m-%d %H:%M:%
----------
Solution: 

    epoch = datetime(1970, 1, 1, 0, 0, tzinfo=tzutc())
    delta = (dtval - epoch)
    return delta.days * 24 * 3600 + delta.seconds
--------------------
Prompt: 
def path_dispatch_kwarg(mname, path_default, returns_model):
    """
    Parameterized decorator for methods that accept path as a second
    argument.
    """
----------
Output: 

    def decorator(f):
        @wraps(f)
        def wrapper(self, path=path_default, **kwargs):
            if path is None:
                path = path_default
            if path is not None:
                path = self._get_path(path)
            return f(self, path, **kwargs)
        return wrapper
    return decorator


def path_dispatch_arg(mname, path_default, returns_model):
    """
    Parameterized decorator for methods that accept path as a first
   
----------
Solution: 

    def _wrapper(self, path=path_default, **kwargs):
        prefix, mgr, mgr_path = _resolve_path(path, self.managers)
        result = getattr(mgr, mname)(path=mgr_path, **kwargs)
        if returns_model and prefix:
            return _apply_prefix(prefix, result)
        else:
            return result
    return _wrapper
--------------------
Prompt: 
def get_label ( self, object ):
        """ Gets the label to display for a specified object.
        """
----------
Output: 

        return object.name

    def get_icon ( self, object ):
        """ Gets the icon to display for a specified object.
        """
        return object.icon

    def get_tooltip ( self, object ):
        """ Gets the tooltip to display for a specified object.
        """
        return object.tooltip

    def get_display_name ( self, object ):
        """ Gets the display name to display for a specified object.
        """
        return object.display_name

    def get_
----------
Solution: 

        label = self.label
        if label[:1] == '=':
            return label[1:]

        label = xgetattr( object, label, '' )

        if self.formatter is None:
            return label

        return self.formatter( object, label )
--------------------
Prompt: 
def create(self, request):
        """
        Log in django staff user
        """
----------
Output: 

        data = request.data
        username = data.get('username')
        password = data.get('password')
        user = authenticate(username=username, password=password)
        if user is not None:
            if user.is_active:
                login(request, user)
                return Response(status=status.HTTP_200_OK)
            else:
                return Response(status=status.HTTP_400_BAD_REQUEST)
        else:
            return Response(status=status.HTTP_400_B
----------
Solution: 

        # TODO: Decorate api with sensitive post parameters as Django admin do?
        # from django.utils.decorators import method_decorator
        # from django.views.decorators.debug import sensitive_post_parameters
        # sensitive_post_parameters_m = method_decorator(sensitive_post_parameters())

        login_form = AuthenticationForm(request, data=request.data)

        if not login_form.is_valid():
            raise serializers.ValidationError(login_form.errors)

        auth_login(request, login_form.get_user())

        serializer = UserSerializer(request.user)
        return Response(serializer.data, status=status.HTTP_200_OK)
--------------------
Prompt: 
def remove(self, recursive=True, ignore_error=True):
        """
        Remove the directory.
        """
----------
Output: 

        if not self.exists():
            return
        if recursive:
            for f in self.listdir():
                if f.isdir():
                    f.remove(recursive=True)
                else:
                    f.remove()
        try:
            os.rmdir(self.path)
        except OSError:
            if not ignore_error:
                raise

    def listdir(self):
        """
        Return a list of the files and directories in the directory.
        """
        return [self.joinpath(f
----------
Solution: 

        try:
            if recursive or self._cleanup == 'recursive':
                shutil.rmtree(self.path)
            else:
                os.rmdir(self.path)
        except Exception as e:
            if not ignore_error:
                raise e
--------------------
Prompt: 
def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':
        """Merge the failure message from another status into this one.

        Whichever status represents parsing that has gone the farthest is
        retained. If both statuses have gone the same distance, then the
        expected values from both are retained.

        Args:
            status: The status to merge into this one.

        Returns:
            This ``Status`` which may have ``farthest`` and ``expected``
            updated accordingly.
        """
----------
Output: 

        """
        if status.farthest is None:
            return self
        if self.farthest is None:
            return status
        if self.farthest > status.farthest:
            return self
        if self.farthest < status.farthest:
            return status
        if self.expected is None:
            return status
        if status.expected is None:
            return self
        return self.copy(expected=self.expected + status.expected)

    def __repr__(self) -> str:

----------
Solution: 

        if status is None or status.farthest is None:
            # No new message; simply return unchanged
            pass
        elif self.farthest is None:
            # No current message to compare to; use the message from status
            self.farthest = status.farthest
            self.expected = status.expected
        elif status.farthest.position < self.farthest.position:
            # New message is not farther; keep current message
            pass
        elif status.farthest.position > self.farthest.position:
            # New message is farther than current message; replace with new message
            self.farthest = status.farthest
            self.expected = status.expected
        else:
            # New message and current message are equally far; merge messages
            self.expected = status.expected + self.expected

        return self
--------------------
Prompt: 
def activity(self, value):
        """
        Used to set TurnContext._activity when a context object is created. Only takes instances of Activities.
        :param value:
        :return:
        """
----------
Output: 

        if not isinstance(value, Activity):
            raise TypeError("TurnContext.activity must be of type Activity")
        self._activity = value

    @property
    def adapter(self):
        """
        Gets the Adapter for this context.
        :return:
        """
        return self._adapter

    @adapter.setter
    def adapter(self, value):
        """
        Used to set TurnContext._adapter when a context object is created. Only takes instances of BotAdapter.
        :param value:
        :return:
----------
Solution: 

        if not isinstance(value, Activity):
            raise TypeError('TurnContext: cannot set `activity` to a type other than Activity.')
        else:
            self._activity = value
--------------------
Prompt: 
def _post_cutout_no_chunking_blosc(self, token, channel,
                                       x_start, y_start, z_start,
                                       data, resolution):
        """
        Accepts data in zyx. !!!
        """
----------
Output: 

        # get the data
        data = data.astype(np.float32)
        # get the shape
        shape = data.shape
        # get the resolution
        resolution = resolution.astype(np.float32)
        # get the size
        size = np.prod(shape)
        # get the dtype
        dtype = data.dtype
        # get the chunking
        chunking = self.get_chunking(shape)
        # get the blosc compression
        compression = self.get_blosc_compression(
----------
Solution: 

        data = numpy.expand_dims(data, axis=0)
        blosc_data = blosc.pack_array(data)
        url = self.url("{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/".format(
            token, channel,
            resolution,
            x_start, x_start + data.shape[3],
            y_start, y_start + data.shape[2],
            z_start, z_start + data.shape[1]
        ))

        req = self.remote_utils.post_url(url, data=blosc_data, headers={
            'Content-Type': 'application/octet-stream'
        })

        if req.status_code is not 200:
            raise RemoteDataUploadError(req.text)
        else:
            return True
--------------------
Prompt: 
def current(self):
        """ bool: Whether this window is the window in which commands are being executed. """
----------
Output: 

        return self._current

    @property
    def focused(self):
        """ bool: Whether this window is the focused window. """
        return self._focused

    @property
    def visible(self):
        """ bool: Whether this window is visible. """
        return self._visible

    @property
    def active(self):
        """ bool: Whether this window is the active window. """
        return self._active

    @property
    def focused_view(self):
        """ sublime.View: The focused view.
----------
Solution: 

        try:
            return self.driver.current_window_handle == self.handle
        except self.driver.no_such_window_error:
            return False
--------------------
Prompt: 
def autodetect_files(self):
        """ Attempt to detect requirements files in the current working directory """
----------
Output: 

        if self.files:
            return
        for filename in ('requirements.txt', 'requirements.pip', 'requirements.in'):
            if os.path.exists(filename):
                self.files.append(filename)

    def parse_requirements(self, filename):
        """ Parse a requirements file """
        with open(filename) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    self.parse_line(line)

    def parse
----------
Solution: 

        if self._is_valid_requirements_file('requirements.txt'):
            self.filenames.append('requirements.txt')

        if self._is_valid_requirements_file('requirements.pip'):  # pragma: nocover
            self.filenames.append('requirements.pip')

        if os.path.isdir('requirements'):
            for filename in os.listdir('requirements'):
                file_path = os.path.join('requirements', filename)
                if self._is_valid_requirements_file(file_path):
                    self.filenames.append(file_path)
        self._check_inclusions_recursively()
--------------------
Prompt: 
def format_extension(self):
        """The format extension of asset.
        Example::

            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')
            >>> attrs.format_extension
            '.js'

            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')
            >>> attrs.format_extension
            '.js'
        """
----------
Output: 

        return self.extension.split('.')[0]

    @property
    def format_extension_with_dot(self):
        """The format extension of asset with a leading dot.
        Example::

            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')
            >>> attrs.format_extension_with_dot
            '.js'

            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')
            >>> attrs.format_extension_with
----------
Solution: 

        for extension in reversed(self.extensions):
            compiler = self.environment.compilers.get(extension)
            if not compiler and self.environment.mimetypes.get(extension):
                return extension
