import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score
import torch.nn as nn

tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModelForSequenceClassification.from_pretrained("microsoft/codebert-base")
N_MSD = 5
class CustomClassificationHead(nn.Module):
    def __init__(self,config):
        super(CustomClassificationHead, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(N_MSD)])
        self.regressor = nn.Linear(config.hidden_size, 2)
    
    def forward(self, features, **kwargs):
        # featuresの形状は [batch_size, sequence_length, hidden_size] を想定
        batch_size, sequence_length, hidden_size = features.size()

        # featuresを [batch_size, hidden_size] に変形
        features = features.view(batch_size, -1, hidden_size).mean(dim=1)

        # 線形変換とドロップアウトを適用
        features = self.dense(features)
        logits = sum([self.regressor(dropout(features)) for dropout in self.dropouts]) / N_MSD
        return logits

# Replace the classification head
model.classifier = CustomClassificationHead(model.config)


model_path = 'saved_model/model_20240206_065502.pth' 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()

# define the dataset
#DATASET_PATH = 'datasets/Python/temp_test'
DATASET_PATH = 'datasets/go/test'
class CodeDataset(Dataset):
    def __init__(self, directory):
        self.samples = []

        # humanのコードを収集
        human_code_dir = os.path.join(directory, 'human')
        for filename in os.listdir(human_code_dir):
            with open(os.path.join(human_code_dir, filename), 'r') as f:
                code = f.read()
                self.samples.append((code, 0))

        # chatGPTのコードを収集
        AI_code_dir = os.path.join(directory, 'AI')
        for filename in os.listdir(AI_code_dir):
            with open(os.path.join(AI_code_dir, filename), 'r') as f:
                code = f.read()
                self.samples.append((code, 1))
            
    
    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        code, label = self.samples[index]
        inputs = tokenizer.encode_plus(code, padding='max_length', max_length=512, truncation=True)
        input_ids = inputs['input_ids']
        attention_mask = inputs['attention_mask']
        return {'input_ids': torch.tensor(input_ids, dtype=torch.long), 'attention_mask': torch.tensor(attention_mask, dtype=torch.long), 'labels': torch.tensor(label, dtype=torch.long)}

datasets = CodeDataset(DATASET_PATH)
test_dataloader = DataLoader(datasets, batch_size=32, shuffle=False)
"""
file_path = 'temp_test_data/temp.java'

with open(file_path, 'r') as file:
    code_sample = file.read()

inputs = tokenizer.encode_plus(code_sample, return_tensors='pt', max_length=512, truncation=True)

with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=1)

if predictions[0] == 0:
    print("This code sample is predicted to be written by a human.")
else:
    print("This code sample is predicted to be generated by AI (ChatGPT).")
"""

y_true = []
y_pred = []
with torch.no_grad():
    for batch in test_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        y_true += labels.tolist()
        y_pred += predictions.tolist()
accuracy = accuracy_score(y_true, y_pred)
print(accuracy)


