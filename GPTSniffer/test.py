import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score

tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
model = AutoModelForSequenceClassification.from_pretrained("microsoft/codebert-base")
model_path = 'saved_model/model_20231219_101133.pth' 
model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
model.eval()

# define the dataset
DATASET_PATH = 'datasets/Python/temp_test'
class CodeDataset(Dataset):
    def __init__(self, directory):
        self.samples = []

        # humanのコードを収集
        human_code_dir = os.path.join(directory, 'human')
        for filename in os.listdir(human_code_dir):
            with open(os.path.join(human_code_dir, filename), 'r') as f:
                code = f.read()
                self.samples.append((code, 0))

        # chatGPTのコードを収集
        AI_code_dir = os.path.join(directory, 'AI')
        for filename in os.listdir(AI_code_dir):
            with open(os.path.join(AI_code_dir, filename), 'r') as f:
                code = f.read()
                self.samples.append((code, 1))
            
    
    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        code, label = self.samples[index]
        inputs = tokenizer.encode_plus(code, padding='max_length', max_length=512, truncation=True)
        input_ids = inputs['input_ids']
        attention_mask = inputs['attention_mask']
        return {'input_ids': torch.tensor(input_ids, dtype=torch.long), 'attention_mask': torch.tensor(attention_mask, dtype=torch.long), 'labels': torch.tensor(label, dtype=torch.long)}

datasets = CodeDataset(DATASET_PATH)
test_dataloader = DataLoader(datasets, batch_size=32, shuffle=False)
"""
file_path = 'temp_test_data/temp.java'

with open(file_path, 'r') as file:
    code_sample = file.read()

inputs = tokenizer.encode_plus(code_sample, return_tensors='pt', max_length=512, truncation=True)

with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=1)

if predictions[0] == 0:
    print("This code sample is predicted to be written by a human.")
else:
    print("This code sample is predicted to be generated by AI (ChatGPT).")
"""

y_true = []
y_pred = []
with torch.no_grad():
    for batch in test_dataloader:
        input_ids = batch['input_ids'].to('cpu')
        attention_mask = batch['attention_mask'].to('cpu')
        labels = batch['labels'].to('cpu')
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        y_true += labels.tolist()
        y_pred += predictions.tolist()
accuracy = accuracy_score(y_true, y_pred)
print(accuracy)


